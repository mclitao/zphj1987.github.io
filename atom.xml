<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[但行好事，莫问前程]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-11-09T08:05:26.886Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue4]]></title>
    <link href="http://www.zphj1987.com/2016/11/09/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue4/"/>
    <id>http://www.zphj1987.com/2016/11/09/Ceph用户邮件列表Vol45-Issue4/</id>
    <published>2016-11-09T03:37:34.000Z</published>
    <updated>2016-11-09T08:05:26.886Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayfour.jpg" alt=""><br></center></p>
<h2 id="ceph_Vol_45_Issue_4">ceph Vol 45 Issue 4</h2><h3 id="1-upgrade_from_v0-94-6_or_lower_and_‘failed_to_encode_map_X_with_expected_crc’">1.upgrade from v0.94.6 or lower and ‘failed to encode map X with expected crc’</h3><blockquote>
<p>f user upgrades the cluster from a prior release to v0.94.7 or up by<br>following the steps:</p>
<ol>
<li>upgrade the monitors first,</li>
<li>and then the OSDs.</li>
</ol>
<p>It is expected that the cluster log will be flooded with messages like:</p>
<p>2016-07-12 08:42:42.1234567 osd.1234 [WRN] failed to encode map e4321<br>with expected crc</p>
</blockquote>
<p>这个是开发者kefu chai发出来的邮件，是提醒用户注意一个升级的问题的，先介绍下这个问题<br><a id="more"></a></p>
<p>因为在ceph的hammer的0.94.7版本开始采用了一种新的osdmap的编码方式，在更新了以后，mon会用新的编码方式发送新的增量osdmap到其他osd，但是老的osd上还是老的编码方式，就会产生CRC错误，提示不匹配，然后OSD就会向MON请求全量的osdmap<br>对于一个很大的ceph集群就会有下面的问题</p>
<p>1、mon会因为这个clog产生大量消息flood<br>2、mon因为需要发送全量的osdmap增加负载<br>3、网络会被大量的osdmap的全量的消息占用<br>4、因为osdmap更新和网络的大量请求，客户端出现slow request</p>
<p>对于已经升级的了集群解决办法是：<br>先降低到之前的版本</p>
<ul>
<li>升级OSD的机器到新的版本</li>
<li>升级MON的机器到新的版本</li>
</ul>
<p>如果准备计划升级的集群</p>
<ul>
<li>先升级OSD的机器到新的版本</li>
<li>再升级MON的机器到新的版本</li>
</ul>
<p>目前社区准备解决这个问题（<a href="http://tracker.ceph.com/issues/17386" target="_blank" rel="external">Issue</a>）</p>
<p>目前可以用上面的方法避免</p>
<h3 id="问题重现方法">问题重现方法</h3><p>配置一个0.94.6或者以下的集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure></p>
<p>配置好了后升级<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">7</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure></p>
<p>升级以后如果不重启进程实际上是没更新的，根据官方的建议先重启mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart mon</span><br></pre></td></tr></table></figure></p>
<p>然后重启osd，然后查看ceph -w<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart osd</span><br></pre></td></tr></table></figure></p>
<p>可以看到failed to encode map e4321 with expected crc</p>
<h3 id="换一种升级方式">换一种升级方式</h3><p>先降级，这里用yum的方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum <span class="built_in">history</span> list</span><br></pre></td></tr></table></figure></p>
<p>找到刚刚upgrade的编号<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum <span class="built_in">history</span> undo <span class="number">289</span></span><br></pre></td></tr></table></figure></p>
<p>就可以降级到0.94.6了</p>
<h3 id="新的操作">新的操作</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">7</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure>
<p>先重启OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart osd</span><br></pre></td></tr></table></figure></p>
<p>再重启mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart mon</span><br></pre></td></tr></table></figure></p>
<p>再观察<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -w</span><br></pre></td></tr></table></figure></p>
<p>已经没有提示了</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-09</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayfour.jpg" alt=""><br></center></p>
<h2 id="ceph_Vol_45_Issue_4">ceph Vol 45 Issue 4</h2><h3 id="1-upgrade_from_v0-94-6_or_lower_and_‘failed_to_encode_map_X_with_expected_crc’">1.upgrade from v0.94.6 or lower and ‘failed to encode map X with expected crc’</h3><blockquote>
<p>f user upgrades the cluster from a prior release to v0.94.7 or up by<br>following the steps:</p>
<ol>
<li>upgrade the monitors first,</li>
<li>and then the OSDs.</li>
</ol>
<p>It is expected that the cluster log will be flooded with messages like:</p>
<p>2016-07-12 08:42:42.1234567 osd.1234 [WRN] failed to encode map e4321<br>with expected crc</p>
</blockquote>
<p>这个是开发者kefu chai发出来的邮件，是提醒用户注意一个升级的问题的，先介绍下这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue3]]></title>
    <link href="http://www.zphj1987.com/2016/11/08/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue3/"/>
    <id>http://www.zphj1987.com/2016/11/08/Ceph用户邮件列表Vol45-Issue3/</id>
    <published>2016-11-08T09:26:01.000Z</published>
    <updated>2016-11-09T08:03:13.726Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daythree.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_3">ceph Vol 45 Issue 3</h2><h3 id="1-Crash_in_ceph_readdir-">1.Crash in ceph_readdir.</h3><blockquote>
<p>Hello, </p>
<p>I’ve been investigating the following crash with cephfs:<br>···<br>According to the state of the ceph_inoide_info this means that<br>ceph_dir_is_complete_ordered would return true and the second condition<br>should also be true since ptr_pos is held in r12 and the dir size is 26496.<br>So the dentry being passed should be the 2953 % 512 = 393 in the<br>cache_ctl.dentries array.<br>Unfortunately my crashdump excldues the page cache pages and I cannot really see<br>what are the contents of the dentries array. </p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>Could you provide any info on how to further debug this </p>
</blockquote>
<p>作者在使用cephfs的时候遇上了崩溃的情况，readdir的操作</p>
<p>Yan, Zheng已经对这个bug进行了修复</p>
<p><a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=af5e5eb574776cdf1b756a27cc437bff257e22fe" target="_blank" rel="external">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=af5e5eb574776cdf1b756a27cc437bff257e22fe</a><br><a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a3d714c33632ef6bfdfaacc74ae6ba297b4c5820" target="_blank" rel="external">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a3d714c33632ef6bfdfaacc74ae6ba297b4c5820</a></p>
<p>但是这个是提交到Linux kernel的4.6的分支里面去了的，所以目前从官方版本来说是4.6或者更新的版本才会解决</p>
<p>这个问题只能是说遇到了再升级内核了</p>
<h3 id="2-Can’t_activate_OSD">2.Can’t activate OSD</h3><blockquote>
<p>Hello all,</p>
<p>Over the past few weeks I’ve been trying to go through the Quick Ceph Deploy<br>tutorial at:</p>
<p>ceph-deploy osd activate ceph02:/dev/sdc ceph03:/dev/sdc</p>
<p>part. It never actually seems to activate the OSD and eventually times out:</p>
<p>[ceph03][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate  —mark-init systemd —mount /dev/sdc<br>[ceph03][WARNIN] main_activate: path = /dev/sdc<br>[ceph03][WARNIN] No data was received after 300 seconds, disconnecting…</p>
</blockquote>
<p>作者在部署osd的时候出现无法激活osd的问题，最后在别人的帮助下发现了问题，在交换机上创建了 VLAN ，但没允许jumbo packets，所以出现了问题</p>
<p>另外一个人也出现了类似的问题，通过升级了parted解决问题（from 3.1 from the CentOS7 base）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -Uhv ftp://<span class="number">195.220</span>.<span class="number">108.108</span>/linux/fedora/linux/updates/<span class="number">22</span>/x86_64/p/parted-<span class="number">3.2</span>-<span class="number">16</span>.fc22.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>这个一般没什么问题，确实定位到这里再升级了，一般情况下很少出现不能activate osd的情况</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
<tr>
<td style="text-align:center">完成Issue 3</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daythree.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_3">ceph Vol 45 Issue 3</h2><h3 id="1-Crash_in_ceph_readdir-">1.Crash in ceph_readdir.</h3><blockquote>
<p>Hello, </p>
<p>I’ve been investigating the following crash with cephfs:<br>···<br>According to the state of the ceph_inoide_info this means that<br>ceph_dir_is_complete_ordered would return true and the second condition<br>should also be true since ptr_pos is held in r12 and the dir size is 26496.<br>So the dentry being passed should be the 2953 % 512 = 393 in the<br>cache_ctl.dentries array.<br>Unfortunately my crashdump excldues the page cache pages and I cannot really see<br>what are the contents of the dentries array. </p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue2]]></title>
    <link href="http://www.zphj1987.com/2016/11/07/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue2/"/>
    <id>http://www.zphj1987.com/2016/11/07/Ceph用户邮件列表Vol45-Issue2/</id>
    <published>2016-11-07T05:26:01.000Z</published>
    <updated>2016-11-09T07:59:58.147Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daytwo.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_2">ceph Vol 45 Issue 2</h2><h3 id="1-CephFS:_No_space_left_on_device">1.CephFS: No space left on device</h3><blockquote>
<p>After upgrading to 10.2.3 we frequently see messages like</p>
<p>‘rm: cannot remove ‘…’: No space left on device</p>
<p>The folders we are trying to delete contain approx. 50K files 193 KB each.</p>
<p>The cluster state and storage available are both OK:</p>
<p>   cluster 98d72518-6619-4b5c-b148-9a781ef13bcb<br>     health HEALTH_WARN<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>     monmap e1: 1 mons at {000-s-ragnarok=XXX.XXX.XXX.XXX:6789/0}<br>            election epoch 11, quorum 0 000-s-ragnarok<br>      fsmap e62643: 1/1/1 up {0=000-s-ragnarok=up:active}<br>     osdmap e20203: 16 osds: 16 up, 16 in<br>            flags sortbitwise<br>      pgmap v15284654: 1088 pgs, 2 pools, 11263 GB data, 40801 kobjects<br>            23048 GB used, 6745 GB / 29793 GB avail<br>                1085 active+clean<br>                   2 active+clean+scrubbing<br>                   1 active+clean+scrubbing+deep</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>Has anybody experienced this issue so far?</p>
</blockquote>
<p>这个问题是作者在升级了一个集群以后（jewel 10.2.3），做删除的时候，发现提示了 No space left on device，按正常的理解做删除不会出现提示空间不足</p>
<p>这个地方的原因是，有一个参数会对目录的entry做一个最大值的控制<code>mds_bal_fragment_size_max</code>,而这个参数实际上在做删除操作的时候，当文件被unlink的时候，被放入待删除区的时候，这个也是被限制住的，所以需要调整这个参数，如果有上百万的文件被等待删除的时候，可能就会出现这个情况,并且出现 <code>failing to respond to cache pressure</code> 我们根据自己的需要去设置这个值</p>
<p>默认的 mds_bal_fragment_size_max=100000，也就是单个文件10万文件，如果不调整，单目录写入10万文件就能出现上面的问题</p>
<p>这个地方可以用命令来监控mds的当前状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># ceph daemonperf mds.lab8106</span></span><br><span class="line">-----mds------ --mds_server-- ---objecter--- -----mds_cache----- ---mds_<span class="built_in">log</span>---- </span><br><span class="line">rlat inos caps|hsr  hcs  hcr |writ <span class="built_in">read</span> actv|recd recy stry purg|segs evts subm|</span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |  <span class="number">0</span>    <span class="number">0</span>   <span class="number">36</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k   <span class="number">0</span> | <span class="number">33</span>   <span class="number">29</span>k   <span class="number">0</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |  <span class="number">6</span>    <span class="number">0</span>   <span class="number">34</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k   <span class="number">6</span> | <span class="number">33</span>   <span class="number">29</span>k   <span class="number">6</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> | <span class="number">24</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k  <span class="number">24</span> | <span class="number">32</span>   <span class="number">29</span>k  <span class="number">24</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> | <span class="number">42</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k  <span class="number">42</span> | <span class="number">32</span>   <span class="number">29</span>k  <span class="number">42</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">972</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">144</span>k <span class="number">970</span> | <span class="number">33</span>   <span class="number">27</span>k <span class="number">971</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">905</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">143</span>k <span class="number">905</span> | <span class="number">31</span>   <span class="number">28</span>k <span class="number">906</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">969</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">142</span>k <span class="number">969</span> | <span class="number">32</span>   <span class="number">29</span>k <span class="number">970</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">601</span>    <span class="number">0</span>   <span class="number">31</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">141</span>k <span class="number">601</span> | <span class="number">33</span>   <span class="number">29</span>k <span class="number">602</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方还有一个硬链接删除以后没有释放stry的问题，最新版的master里面已经合进去了代码（<a href="https://github.com/ukernel/ceph/commit/edc84d905a1f0e3c504f427cc4693c7a98561e7c" target="_blank" rel="external">scan_link</a>）</p>
<p>修复过程如下<br>执行flush MDS journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph daemon mds.xxx flush journal</span><br></pre></td></tr></table></figure></p>
<p>停止掉所有mds<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">stop all mds</span><br></pre></td></tr></table></figure></p>
<p>执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephfs-data-scan scan_links<span class="string">'</span></span><br></pre></td></tr></table></figure></p>
<p>重启mds<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">restart mds</span><br></pre></td></tr></table></figure></p>
<p>执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">run <span class="string">'ceph daemon mds.x scrub_path / recursive repair'</span></span><br></pre></td></tr></table></figure></p>
<p>执行完了以后去对目录进行一次ll，可以看到mds_cache的stry的就会被清理干净了</p>
<p>这个问题就可以解决了,实际测试中在换了新版本以后，重启后然后进行目录的ll，也能清空stry</p>
<h3 id="2-_Blog_post_about_Ceph_cache_tiers_-_feedback_welcome">2. Blog post about Ceph cache tiers - feedback welcome</h3><blockquote>
<p>Hi all,</p>
<p>as it took quite a while until we got our Ceph cache working (and we’re still hit but some unexpected things, see the thread Ceph with cache pool - disk usage / cleanup), I thought it might be good to write a summary of what I (believe) to know up to this point.</p>
<p>Any feedback, especially corrections is highly welcome!</p>
<p><a href="http://maybebuggy.de/post/ceph-cache-tier/" target="_blank" rel="external">http://maybebuggy.de/post/ceph-cache-tier/</a></p>
<p>Greetings<br>-Sascha-</p>
</blockquote>
<p>这是一篇分享文，作者因为最近想深入研究下ceph的cache pool，作者写的文章非常的好，这里先直接翻译这篇文章，然后再加入我自己的相关数据</p>
<h3 id="blog原文"><a href="http://maybebuggy.de/post/ceph-cache-tier/" target="_blank" rel="external">blog原文</a></h3><p>作者想启动blog写下自己的Openstack和Ceph的相关经验，第一个话题就选择了<code>Ceph cache tiering</code>, 作者的使用场景为短时间的虚拟机，用来跑测试的，这种场景他们准备用Nvme做一个缓冲池来加速的虚拟机</p>
<p>cache 相关的一些参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">target_max_bytes</span><br><span class="line">target_max_objects</span><br><span class="line">cache_target_dirty_ratio</span><br><span class="line">cache_target_full_ratio</span><br><span class="line">cache_min_flush_age</span><br><span class="line">cache_min_evict_age</span><br></pre></td></tr></table></figure></p>
<p>Jewel版本还新加入了一个参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cache_target_dirty_high_ratio</span><br></pre></td></tr></table></figure></p>
<p>作者的想法是先把数据写入到缓冲池当中，等后面某个时刻再写入到真实的存储池的当中</p>
<p>Flushing vs. Evicting<br>Flushing是将缓冲池中的数据刷到真实的存储池当中去，但是并不去删除缓冲池里面缓存的数据，只有clean的数据才能被evic，如果是dirty的数据做evic，那么先要flush到真实存储池，然后再删除掉</p>
<p>Cache 调整</p>
<p>Ceph的是不能够自动确定缓存池的大小，所以这里需要配置一个缓冲池的绝对大小，flush/evic将无法工作。</p>
<p>设置了上限以后，相关的参数就是cache_target_full_ratio和cache_target_dirty_ratio。这些参数是控制什么时候进行flush和evic的</p>
<p>这个dirty ratio是比较难设置的值，需要根据场景进行相关的调整</p>
<p>新版本里面到了dirty_high_ratio才开始下刷</p>
<p>还有cache_min_flush_age和cache_min_evict_age这个控制，这个一般来说到了设定的阀值前，这些对象的留存时间应该是要够老的，能够被触发清理掉的</p>
<p>通过ceph df detail 可以观测你的存储池的数据的情况</p>
<p>里面会有一些0字节对象的，缓冲池的0字节对象是数据已经被删除了，防止刷新的时候又要操作对象。在真实存储池中的0字节对象是数据已经在缓冲池当中，但没有刷新到缓冲池</p>
<h3 id="案例测试">案例测试</h3><p>基于上面的控制，下面我们来具体看下这些参数的实际效果是怎样的，这样我们才能真正在实际场景当中做到精准的控制</p>
<p>首先我们要对参数分类</p>
<ul>
<li>缓冲池的总大小，这个大小分成两类一个对象个数控制，一个大小的控制</li>
<li>flush和evic的百分比，这个百分比既按照大小进行控制，也按照对象进行控制</li>
<li>flush和evic的时间控制</li>
</ul>
<p>分好类以后，我们就开始我们的测试，基于对象的数目的控制，比较容易观察，我们就用对象控制来举例子</p>
<h3 id="创建一个缓冲池的环境">创建一个缓冲池的环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create testpool <span class="number">24</span> <span class="number">24</span> </span><br><span class="line">ceph osd pool create cachepool <span class="number">24</span> <span class="number">24</span></span><br><span class="line">ceph osd tier add  testpool cachepool</span><br><span class="line">ceph osd tier cache-mode  cachepool writeback</span><br><span class="line">ceph osd tier <span class="built_in">set</span>-overlay  testpool cachepool</span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_<span class="built_in">type</span> bloom</span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_count <span class="number">1</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_period <span class="number">3600</span></span><br></pre></td></tr></table></figure>
<p>上面的操作是基本的一些操作、我们现在做参数相关的调整<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool  target_max_bytes <span class="number">1000000000000</span></span><br></pre></td></tr></table></figure></p>
<p>为了排除干扰，我们把 target_max_bytes设置成了1T，我们的测试数据很少，肯定不会触发这个大小</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool target_max_objects <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p>设置缓冲池的对象max为1000<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_target_dirty_ratio <span class="number">0.4</span></span><br></pre></td></tr></table></figure></p>
<p>设置dirty_ratio为0.4，也就是0.4为判断为dirty的阀值<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_target_full_ratio <span class="number">0.8</span></span><br></pre></td></tr></table></figure></p>
<p>设置cache_target_full_ratio为0.8，即超过80%的时候需要evic<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_flush_age <span class="number">600</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_evict_age <span class="number">1800</span></span><br></pre></td></tr></table></figure></p>
<p>设置两个flush和evic的时间，这个时间周期比我写入的数据的时间周期大很多，这个等下会调整这个</p>
<p>开启一个终端动态观察存储池的对象变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># watch ceph df</span></span><br><span class="line">Every <span class="number">2.0</span>s: ceph df                                                                    </span><br><span class="line"></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED</span><br><span class="line">    <span class="number">834</span>G      <span class="number">833</span>G         <span class="number">958</span>M          <span class="number">0.11</span></span><br><span class="line">POOLS:</span><br><span class="line">    NAME          ID     USED       %USED     MAX AVAIL     OBJECTS</span><br><span class="line">    rbd           <span class="number">0</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">277</span>G           <span class="number">0</span></span><br><span class="line">    metadata	  <span class="number">1</span>	  <span class="number">61953</span>k      <span class="number">0.01</span>          <span class="number">416</span>G          <span class="number">39</span></span><br><span class="line">    data          <span class="number">2</span>	  <span class="number">50500</span>k      <span class="number">0.01</span>          <span class="number">416</span>G       <span class="number">50501</span></span><br><span class="line">    testpool	  <span class="number">5</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">416</span>G           <span class="number">0</span></span><br><span class="line">    cachepool     <span class="number">6</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">416</span>G           <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>尝试写入数据并且观察，到了1000左右的时候停止<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p testpool bench <span class="number">100</span> write  -b <span class="number">4</span>K --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>可以观察到cachepool的对象数目大概在1100-1200之间，一直写也会是这个数字，在停止写以后，观察cachepool的对象数目在960左右，我们设置的 target_max_objects 为1000，在超过了这个值以后，并且写停止的情况下，系统会把这个cache pool的对象控制在比target_max少50左右，现在我们修改下<code>cache_min_evict_age</code>这个参数，看下会发生些什么</p>
<p>我们把这个参数调整为30<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_evict_age <span class="number">30</span></span><br></pre></td></tr></table></figure></p>
<p>设置完了以后，可以看到cache pool的对象数目在 744左右，现在再写入数据，然后等待，看下会是多少，还是756，如果按我们设置的<code>cache_target_full_ratio</code>0.8就正好是800，我们尝试再次调整大cache_min_evict_age看下情况，对象维持在960左右，根据这个测试，基本上可以看出来是如何控制缓存的数据了，下面用一张图来看下这个问题</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/cache.png" alt=""></p>
<p>来总结一下：</p>
<ul>
<li>如果cache pool对象到了 target_max_objects，那么会边flush，边evic，然后因为前面有客户端请求，这个时候实际是会阻塞的</li>
<li>如果停止了写请求，系统会自动将cache pool的对象控制在比 target_max_objects 少一点点</li>
<li>如果时间周期到了cache_min_evict_age，那么系统会自动将cache pool的对象控制在比 cache_target_full_ratio 少一点点</li>
<li>同理如果到了cache_min_flush_age，那么会将对象往真实的存储池flush到 cache_target_dirty_ratio 少一点点</li>
</ul>
<p>也就是ratio是给定了一个比例，然后时间到了就去将缓存控制到指定的ratio，这个地方就需要根据需要去控制缓冲池数据是留有多少的缓存余地的</p>
<p>使用命令清空缓冲池的数据，会将数据flush到真实存储池，然后将数据evic掉</p>
<p>关于缓冲池的就写这么多了，实际环境是要根据自己的使用场景去制定这些值的，从而能保证缓冲池能真正起到作用，上面的例子是基于对象的控制的，基于大小的控制是一样的，只是将对象数的设置换成了大小即可，然后尽量去放大对象的控制</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p cachepool cache-try-flush-evict-all</span><br></pre></td></tr></table></figure>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-07</td>
</tr>
<tr>
<td style="text-align:center">完成缓冲池相关</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daytwo.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_2">ceph Vol 45 Issue 2</h2><h3 id="1-CephFS:_No_space_left_on_device">1.CephFS: No space left on device</h3><blockquote>
<p>After upgrading to 10.2.3 we frequently see messages like</p>
<p>‘rm: cannot remove ‘…’: No space left on device</p>
<p>The folders we are trying to delete contain approx. 50K files 193 KB each.</p>
<p>The cluster state and storage available are both OK:</p>
<p>   cluster 98d72518-6619-4b5c-b148-9a781ef13bcb<br>     health HEALTH_WARN<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>     monmap e1: 1 mons at {000-s-ragnarok=XXX.XXX.XXX.XXX:6789/0}<br>            election epoch 11, quorum 0 000-s-ragnarok<br>      fsmap e62643: 1/1/1 up {0=000-s-ragnarok=up:active}<br>     osdmap e20203: 16 osds: 16 up, 16 in<br>            flags sortbitwise<br>      pgmap v15284654: 1088 pgs, 2 pools, 11263 GB data, 40801 kobjects<br>            23048 GB used, 6745 GB / 29793 GB avail<br>                1085 active+clean<br>                   2 active+clean+scrubbing<br>                   1 active+clean+scrubbing+deep</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue1]]></title>
    <link href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/"/>
    <id>http://www.zphj1987.com/2016/11/04/Ceph用户邮件列表Vol45-Issue1/</id>
    <published>2016-11-04T03:57:25.000Z</published>
    <updated>2016-11-09T07:52:50.245Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayone.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html" target="_blank" rel="external">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br><a id="more"></a><br>I tried to mark the PG as complete using ceph-objectstore-tool but it did not help as the PG is in fact complete but for some reason blocks.</p>
<p>I tried recreating an empty osd.87 and importing the PG exported from other replica but it did not help.</p>
<p>Can someone help me please? This is really important.</p>
</blockquote>
<p>这个问题是作者一个集群中(ceph 0.94.5)出现了一个磁盘损坏以后造成了一些对象的丢失，然后在做了一定的处理以后，集群状态已经正常了，但是还是新的请求会出现block的状态，这个情况下如何处理才能让集群正常，作者贴出了pg dump，ceph -s,ceph osd dump相关信息，当出现异常的时候，需要人协助的时候，应该提供这些信息方便其他人定位问题，最后这个问题作者自己给出了自己的解决办法，出现的时候影响是当时的流量只有正常情况下的10%了，影响还是很大的</p>
<h3 id="复现问题过程">复现问题过程</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd put testremove testremove</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd map rbd testremove</span></span><br><span class="line">osdmap e85 pool <span class="string">'rbd'</span> (<span class="number">0</span>) object <span class="string">'testremove'</span> -&gt; pg <span class="number">0</span>.eaf226a7 (<span class="number">0.27</span>) -&gt; up ([<span class="number">1</span>,<span class="number">0</span>], p1) acting</span><br></pre></td></tr></table></figure>
<p>写入文件,找到文件，然后去后台删除对象<br>然后停止掉其中一个OSD，这里选择停掉主OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">1</span></span><br><span class="line">ceph osd out <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看状态pg被锁住状态active+degrade，不会迁移完整,并且会检测到了有数据unfound了</p>
<p>然后向这个对象发起get请求<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd get testremove testfile</span></span><br></pre></td></tr></table></figure></p>
<p>前端rados请求会卡住，后端出现 requests are blocked</p>
<p>看下如何处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph pg <span class="number">0.27</span> mark_unfound_lost delete</span><br></pre></td></tr></table></figure></p>
<p>邮件列表作者的环境，这个命令也无法执行，直接卡死，后来发现有个执行窗口，就是这个对象所在的PG的OSD在启动过程中还是可以接受命令的，就在这个执行窗口执行这个命令就可以解决了</p>
<p>执行了以后可以执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados  -p rbd  get testremove  a</span></span><br><span class="line">error getting rbd/testremove: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>这个时候查询集群的状态可以看到，集群已经正常的恢复了，不会因为一个对象的丢失造成集群的PG状态卡在待迁移状态</p>
<p>可以看到请求是失败的但是不会像之前一样卡死的状态，卡死是比失败更严重的一种状态</p>
<p>如果不想看到老的 slow request ,那么就重启这个卡住的PG所在的osd，如果本来就正常了，那么这个异常状态就会消失</p>
<p>这个是一个需要人工干预的状态，实际上模拟的就是对象丢失的场景，什么情况下会对象丢失，一般来说，底层磁盘的故障，写下去的对象当时记录着有，正好写入完成又准备写副本的时候，磁盘坏了，这个就有比较高的概率出现，所以出现了坏盘要尽早更换</p>
<p>本系列是只会对列表的当天的非re进行一个汇总，这样保持了一个问题的追踪都在一篇里面，所以这一天只有这一个问题</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-04</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayone.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph日历格子之2016年10月(Vol45)]]></title>
    <link href="http://www.zphj1987.com/2016/11/03/Ceph%E6%97%A5%E5%8E%86%E6%A0%BC%E5%AD%90%E4%B9%8B2016%E5%B9%B410%E6%9C%88-Vol45/"/>
    <id>http://www.zphj1987.com/2016/11/03/Ceph日历格子之2016年10月-Vol45/</id>
    <published>2016-11-03T09:51:33.000Z</published>
    <updated>2016-11-08T10:01:03.670Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br><a id="more"></a></p>
<h2 id="二、内容">二、内容</h2><p>Ceph邮件列表2016年10月Issue格子   </p>
<h3 id="Vol_45">Vol 45</h3><table>
<thead>
<tr>
<th style="text-align:center">一</th>
<th style="text-align:center">二</th>
<th style="text-align:center">三</th>
<th style="text-align:center">四</th>
<th style="text-align:center">五</th>
<th style="text-align:center">六</th>
<th style="text-align:center">日</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/" target="_blank" rel="external">Issue 1</a></td>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/07/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue2/" target="_blank" rel="external">Issue2</a></td>
</tr>
<tr>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/08/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue3/" target="_blank" rel="external">Issue 3</a></td>
<td style="text-align:center">Issue 4</td>
<td style="text-align:center">Issue 5</td>
<td style="text-align:center">Issue 6</td>
<td style="text-align:center">Issue 7</td>
<td style="text-align:center">Issue 8</td>
<td style="text-align:center">Issue 9</td>
</tr>
<tr>
<td style="text-align:center">Issue 10</td>
<td style="text-align:center">Issue 11</td>
<td style="text-align:center">Issue12</td>
<td style="text-align:center">Issue 13</td>
<td style="text-align:center">Issue 14</td>
<td style="text-align:center">Issue 15</td>
<td style="text-align:center">Issue 16</td>
</tr>
<tr>
<td style="text-align:center">Issue 17</td>
<td style="text-align:center">Issue 18</td>
<td style="text-align:center">Issue 19</td>
<td style="text-align:center">Issue 20</td>
<td style="text-align:center">Issue 21</td>
<td style="text-align:center">Issue 22</td>
<td style="text-align:center">Issue 23</td>
</tr>
<tr>
<td style="text-align:center">Issue 24</td>
<td style="text-align:center">Issue 25</td>
<td style="text-align:center">Issue 26</td>
<td style="text-align:center">Issue 27</td>
<td style="text-align:center">Issue 28</td>
<td style="text-align:center">Issue 29</td>
<td style="text-align:center">Issue 30</td>
</tr>
<tr>
<td style="text-align:center">Issue 31</td>
</tr>
</tbody>
</table>
<h2 id="三、本月主要问题如下：">三、本月主要问题如下：</h2><p>如何处理对象丢失引起的PG状态不对的问题(Issue1)<br>处理Cephfs的No space left on device(Issue2)<br>处理mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure(Issue2)<br>处理stry not clean(Issue2)<br>关于缓冲池相关参数的控制（Issue2）<br>Cephfs readdir出现crash的问题（Issue3）<br>无法 activate OSD（Issue3）</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-03</td>
</tr>
<tr>
<td style="text-align:center">增加Issue2</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-07</td>
</tr>
<tr>
<td style="text-align:center">完成Issue2,完成Issue3</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph部署的时候修改默认权重]]></title>
    <link href="http://www.zphj1987.com/2016/11/02/Ceph%E9%83%A8%E7%BD%B2%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%9D%83%E9%87%8D/"/>
    <id>http://www.zphj1987.com/2016/11/02/Ceph部署的时候修改默认权重/</id>
    <published>2016-11-02T07:46:07.000Z</published>
    <updated>2016-11-02T09:49:40.801Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br><a id="more"></a></p>
<h2 id="二、修改">二、修改</h2><p>如果统一的初始值，那么直接添加参数即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_crush_initial_weight</span><br></pre></td></tr></table></figure></p>
<p>如果想自己添加算法，那么就根据下面的去做就可以了</p>
<h3 id="2-1_centos+jewel">2.1 centos+jewel</h3><p>修改：<br>/usr/lib/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k <span class="variable">$data</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/107374182 ; r = sprintf("%.4f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改这个地方的值就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">2</span>`</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2_centos+hammer">2.2 centos+hammer</h3><p>修改 /etc/init.d/ceph<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(df -P -k $osd_data/. | tail -1 | awk '&#123; print sprintf("%.2f",$2/1073741824)</span> &#125;')"</span></span><br></pre></td></tr></table></figure></p>
<p>修改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(echo 5)</span>"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-3_ubuntu+hammer">2.3 ubuntu+hammer</h3><p>由于ubuntu用initctl控制服务，不是用的/etc/init.d/ceph/,所以要修改另外的一个路径<br>修改/usr/libexec/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k /var/lib/ceph/osd/<span class="variable">$&#123;cluster:-ceph&#125;</span>-<span class="variable">$id</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/1073741824 ; r = sprintf("%.2f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">8</span>`</span><br></pre></td></tr></table></figure></p>
<h2 id="三、总结">三、总结</h2><p>这个比较简单，通过修改取值就可以改变默认配置了,上面的可以根据自己的需求加入算法即可</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday深圳20161029总结]]></title>
    <link href="http://www.zphj1987.com/2016/10/31/Cephday%E6%B7%B1%E5%9C%B320161029%E6%80%BB%E7%BB%93/"/>
    <id>http://www.zphj1987.com/2016/10/31/Cephday深圳20161029总结/</id>
    <published>2016-10-31T04:34:15.000Z</published>
    <updated>2016-10-31T07:31:46.461Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br><a id="more"></a></p>
<h2 id="二、PPT内容">二、PPT内容</h2><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/01_%E4%BC%81%E4%B8%9A%E7%BA%A7Ceph%E4%B9%8B%E8%B7%AF%20-%20iSCSI%E5%AE%9E%E8%B7%B5%E4%B8%8E%E4%BC%98%E5%8C%96%20by_%E9%82%B1%E5%B0%9A%E9%AB%98.pdf" width="850" height="530"></center>


<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/02_YY%E4%BA%91%E5%B9%B3%E5%8F%B0Ceph%E5%AE%9E%E8%B7%B5%20by_%E6%88%9A%E6%98%B1.pdf" width="850" height="690"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/03_%E4%B8%AD%E5%85%B4%E4%BA%91%E5%AD%98%E5%82%A8%E4%B9%8B%E8%B7%AFby_%E9%AA%86%E7%A7%91%E5%AD%A6&%E4%BB%BB%E7%84%95%E6%96%87.pdf" width="850" height="540"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/04_Swift%20Using%20Ceph%20Backend%20by_%E4%BA%8E%E8%88%AA.pdf" width="850" height="530"></center>

<h2 id="三、总结">三、总结</h2><p>第一篇来自杉岩数据的分享，主要集中在ceph在iscsi方面的实现，从开源解决方案来说，有一些弊端，或者高级的属性无法支持，在这个基础上，杉岩数据给出了自己的企业级解决方案，增加了企业级的支持，从功能来说还是增加了不少功能，如果能开源当然就更好了</p>
<p>第二篇来自YY云平台的分享，讲述了ceph在YY平台中的实践，这里面比较有用的是超多osd的线程过载问题，以及ubuntu下的系统检查触发的坑，这个都是很有价值的分享，ubuntu下面有几个类似的检查服务我曾经使用的时候也踩过类似的坑</p>
<p>第三篇来自中兴的分享，中兴基于ceph有一套商业存储，这里面比较有用的分享是mon数据的恢复问题，这个功能一开始的出来的时候我就进行了测试，触发了一个bug，然后有个人进行更深入的测试，从这次分享来看，深入测试的那个人应该来自中兴的，这里面提出了多种方案，其中的基于map的重建是之前没见过的，其他两种都是实践过，没有问题，再一个分享就是文件系统的配额的，这个给出的两种方案也是可行的，一种基于内核态的，一种基于用户态的，这个都是比较好的分享</p>
<p>第四篇来自奥思数据分享，主要集中在swift对象存储的分享，这个接触不多，研究对象存储的同学可以看看有什么可以借鉴的没</p>
<p>从上面的几篇分享来看，都是很好的分享，分享者自己都提出问题给出了解决方案，或者指明了一些方向，在技术相对封闭了环境下也是不可多得资料，希望类似的活动越来越多，多进行交流，提高行业整体水平，在ceph布道过程中，ceph中国社区在背后默默做了很多的工作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-31</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix实现自定义自动发现的流程]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix实现自定义自动发现的流程/</id>
    <published>2016-10-28T05:56:24.000Z</published>
    <updated>2016-11-01T03:56:27.977Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>这里我的宏就是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="comment">#POOLNAME&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>然后要创建一个监控项原型：<br>也是一个名称和一个键值：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- 名称：<span class="built_in">test</span> on <span class="variable">$1</span></span><br><span class="line">- 键值：ceph.pools.used[&#123;<span class="comment">#POOLNAME&#125;]</span></span><br></pre></td></tr></table></figure>
<p>这个地方名称可以用参数形式，包含的就是下面的那个键值中的参数对应的位置的值</p>
<p>然后需要去写一个这样的键值的收集<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">UserParameter=ceph.pools, python /sbin/ceph-status.py pools</span><br><span class="line">UserParameter=ceph.pools.used[*], python /sbin/ceph-status.py pool_used <span class="variable">$1</span></span><br></pre></td></tr></table></figure></p>
<p>测试下效果：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools.used["rbd"]</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方，上面的是去做的自动发现，下面的键值就是根据上面的自动发现返回的值去作为一个参数进行新的查询，后面的$1就是将参数传到收集的脚本里面去的，这样就能根据自动发现的不同的名称返回来不同的值，从而添加不同的监控项目，而不需要自己一个个添加了</p>
<h2 id="三、总结">三、总结</h2><p>自动发现实际上就是需要首先去获得需要监控的值，然后将这个值作为一个新的参数传递到另外一个收集数据的item里面去，这样就可以了，这里是做的最基本的单项数据的获取，后面应该会遇到多重变量的情况，就需要赋值多重变量，到时需要用到再记录下</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix自动发现的python方式数据生成]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84python%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix自动发现的python方式数据生成/</id>
    <published>2016-10-27T17:11:08.000Z</published>
    <updated>2016-10-27T17:29:38.340Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br><a id="more"></a></p>
<p>我们同样的来看看ceph原生的命令的json接口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s -f json</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"health"</span>:&#123;<span class="string">"health"</span>:&#123;<span class="string">"health_services"</span>:[&#123;<span class="string">"mons"</span>:[&#123;<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"kb_total"</span>:<span class="number">52403200</span>,<span class="string">"kb_used"</span>:<span class="number">32905432</span>,<span class="string">"kb_avail"</span>:<span class="number">19497768</span>,<span class="string">"avail_percent"</span>:<span class="number">37</span>,<span class="string">"last_updated"</span>:<span class="string">"2016-10-28 01:15:29.431854"</span>,<span class="string">"store_stats"</span>&#123;<span class="string">"bytes_total"</span>:<span class="number">20206814</span>,<span class="string">"bytes_sst"</span>:<span class="number">16929998</span>,<span class="string">"bytes_log"</span>:<span class="number">3080192</span>,<span class="string">"bytes_misc"</span>:<span class="number">196624</span>,<span class="string">"last_updated"</span>:<span class="string">"0.000000"</span>&#125;,<span class="string">"health"</span>:<span class="string">"HEALTH_OK"</span>&#125;]&#125;]&#125;,<span class="string">"timechecks"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">4</span>,<span class="string">"round"</span>:<span class="number">0</span>,<span class="string">"round_status"</span>:<span class="string">"finished"</span>&#125;,<span class="string">"summary"</span>:[],<span class="string">"overall_status"</span>:<span class="string">"HEALTH_OK"</span>,<span class="string">"detail"</span>:[]&#125;,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"election_epoch"</span>:<span class="number">4</span>,<span class="string">"quorum"</span>:[<span class="number">0</span>],<span class="string">"quorum_names"</span>:[<span class="string">"lab8106"</span>],<span class="string">"monmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">1</span>,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"modified"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"created"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"mons"</span>:[&#123;<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"addr"</span>:<span class="string">"192.168.8.106:6789\/0"</span>&#125;]&#125;,<span class="string">"osdmap"</span>:&#123;<span class="string">"osdmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">63</span>,<span class="string">"num_osds"</span>:<span class="number">2</span>,<span class="string">"num_up_osds"</span>:<span class="number">2</span>,<span class="string">"num_in_osds"</span>:<span class="number">2</span>,<span class="string">"full"</span>:<span class="literal">false</span>,<span class="string">"nearfull"</span>:<span class="literal">false</span>,<span class="string">"num_remapped_pgs"</span>:<span class="number">0</span>&#125;&#125;,<span class="string">"pgmap"</span>:&#123;<span class="string">"pgs_by_state"</span>:[&#123;<span class="string">"state_name"</span>:<span class="string">"active+clean"</span>,<span class="string">"count"</span>:<span class="number">80</span>&#125;],<span class="string">"version"</span>:<span class="number">19174</span>,<span class="string">"num_pgs"</span>:<span class="number">80</span>,<span class="string">"data_bytes"</span>:<span class="number">45848191333</span>,<span class="string">"bytes_used"</span>:<span class="number">45966077952</span>,<span class="string">"bytes_avail"</span>:<span class="number">551592390656</span>,<span class="string">"bytes_total"</span>:<span class="number">597558468608</span>&#125;,<span class="string">"fsmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">5</span>,<span class="string">"id"</span>:<span class="number">1</span>,<span class="string">"up"</span>:<span class="number">1</span>,<span class="string">"in"</span>:<span class="number">1</span>,<span class="string">"max"</span>:<span class="number">1</span>,<span class="string">"by_rank"</span>:[&#123;<span class="string">"filesystem_id"</span>:<span class="number">1</span>,<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"status"</span>:<span class="string">"up:active"</span>&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样也是这个类型的数据，好了，这里直接上代码：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def get_cluster_pools():</span><br><span class="line">    try:</span><br><span class="line">        pool_list=[]</span><br><span class="line">        data_dic = &#123;&#125;</span><br><span class="line">        cluster_pools = commands.getoutput(<span class="string">'timeout 10 ceph osd pool ls -f json 2&gt;/dev/null'</span>)</span><br><span class="line">        json_str = json.loads(cluster_pools)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_str:</span><br><span class="line">            pool_dic = &#123;&#125;</span><br><span class="line">            pool_dic[<span class="string">'&#123;#POOLNAME&#125;'</span>] = str(item)</span><br><span class="line">            pool_list.append(pool_dic)</span><br><span class="line">        data_dic[<span class="string">'data'</span>] = pool_list</span><br><span class="line">        <span class="built_in">return</span> json.dumps(data_dic,separators=(<span class="string">','</span>, <span class="string">':'</span>))</span><br><span class="line">    except:</span><br><span class="line">        <span class="built_in">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>输出如下<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;&#34;data&#34;:[&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;rbd&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;metedata&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;data&#34;&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>跟上面的格式一样了，关键在对字典进行赋值的处理，然后进行一个空格处理就完成了</p>
<h2 id="三、总结">三、总结</h2><p>还是接触的太少，造成简单的处理都需要花费比较久的时间</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br>]]>
    
    </summary>
    
      <category term="zabbix" scheme="http://www.zphj1987.com/tags/zabbix/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker与Ceph的分与合]]></title>
    <link href="http://www.zphj1987.com/2016/10/19/Docker%E4%B8%8ECeph%E7%9A%84%E5%88%86%E4%B8%8E%E5%90%88/"/>
    <id>http://www.zphj1987.com/2016/10/19/Docker与Ceph的分与合/</id>
    <published>2016-10-19T15:46:12.000Z</published>
    <updated>2016-10-19T15:48:24.553Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br><a id="more"></a></p>
<h2 id="二、实践">二、实践</h2><h3 id="2-1、配置docker的基础环境">2.1、配置docker的基础环境</h3><p>拉取基础镜像<br>这个是拉取的灵雀云的docker仓库的centos<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull index.alauda.cn/library/centos</span><br></pre></td></tr></table></figure></p>
<p>启动docker进程,并且设置自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure></p>
<p>查询当前机器上面的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">index.alauda.cn/library/centos   latest              <span class="number">904</span>d6c400333        <span class="number">4</span> months ago        <span class="number">196.7</span> MB</span><br></pre></td></tr></table></figure></p>
<p>我们先对我们的镜像做一些基本的设置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -itd --name=cephbase --net=host --pid=host index.alauda.cn/library/centos /bin/bash</span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker attach cephbase</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 /]<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                                                                     Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/docker-<span class="number">8</span>:<span class="number">2</span>-<span class="number">83216</span>-dd340d1f6a68b6849b9500c4e6f9b7fb1901c3c0cb1ce0d7336f5104a1ef4a10   <span class="number">10</span>G  <span class="number">240</span>M  <span class="number">9.8</span>G   <span class="number">3</span>% /</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda2                                                                                       <span class="number">50</span>G   <span class="number">31</span>G   <span class="number">20</span>G  <span class="number">62</span>% /etc/hosts</span><br><span class="line">shm</span><br></pre></td></tr></table></figure></p>
<p>可以看到我们已经进入了容器内部了，下面需要做的事情，就是将ceph运行需要的一些软件装上去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install wget --nogpgcheck</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># rm -rf /etc/yum.repos.d/*.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># vi /etc/yum.repos.d/ceph.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install ceph ceph-deploy</span></span><br></pre></td></tr></table></figure></p>
<p>检查软件版本装对了没<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">10.2</span>.<span class="number">3</span> (ecc23778eb545d8dd55e2e4735b53cc93f92e65b)</span><br><span class="line">[root@lab8106 /]<span class="comment"># ceph-deploy --version</span></span><br><span class="line"><span class="number">1.5</span>.<span class="number">36</span></span><br></pre></td></tr></table></figure></p>
<p>可以退出了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>查看之前的容器的ID<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE                            COMMAND             CREATED             STATUS                      PORTS               NAMES</span><br><span class="line"><span class="number">48420</span>c9955b5        index.alauda.cn/library/centos   <span class="string">"/bin/bash"</span>         About an hour ago   Exited (<span class="number">0</span>) <span class="number">14</span> seconds ago                       cephbase</span><br></pre></td></tr></table></figure></p>
<p>将容器保存为一个新的镜像，cephbase<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker commit 48420c9955b5 cephbase</span></span><br><span class="line">sha256:ffe236ee2bb61d2809bf1f4c03596f83b9c0e8a6<span class="built_in">fc</span>2eb9013a81abb25be833e9</span><br></pre></td></tr></table></figure></p>
<p>查看当前的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">cephbase                         latest              ffe236ee2bb6        About a minute ago   <span class="number">1.39</span> GB</span><br></pre></td></tr></table></figure></p>
<p>基础镜像就完成，包括了ceph运行需要的软件</p>
<p>我们来创建mon的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=monnode --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph  -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>进入到容器当中去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>在容器当中执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 deploy]<span class="comment"># ceph-deploy mon create lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>我们来创建osd的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=osd0 --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph -v /var/lib/ceph/osd/ceph-<span class="number">0</span>:/var/lib/ceph/osd/ceph-<span class="number">0</span> -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>我们将网络映射到主机上，也就是容器和主机公用网络和主机名，然后把本地的一个数据盘的目录映射进去用于osd的部署，这里都是使用-v进行映射</p>
<p>这个地方因为是centos7，所以systemctl内部是无法使用的，而ceph是需要这个来控制服务的，所以需要提权，并且把入口改为/sbin/init</p>
<h2 id="三、回顾流程">三、回顾流程</h2><ul>
<li>下载centos基础镜像 </li>
<li>修改镜像的内容并提交为新的镜像</li>
<li>基于新的镜像启动容器（采用host映射，目录映射，所有数据都是留在物理机）</li>
<li>进入容器进行ceph的部署 </li>
<li>进入容器启动相关进程</li>
</ul>
<p>这样ceph是运行到了docker中，即使把docker容器销毁掉，因为基于主机名和网络的配置跟宿主机是一致的，所以直接在宿主机上也是能马上启动起来的</p>
<h2 id="四、为何用容器">四、为何用容器</h2><p>基于容器的技术是最近几年开始火起来的，目前的云计算还处于火热期，openstack还是显得比较重型的，很多时候我们只需要的是一个能够运行我们web服务的环境，然后容器技术就应运而生了，直接启动一个容器，就能实现，这个对于宿主机来说方便的只是启动一个进程那么简单</p>
<p>对于庞大复杂的服务来说，如何做到环境一致也是一直很难做到的，一排物理机，因为各种各样的原因，升级，重装系统，很难保证整套系统基础环境的一致性，而基于docker的环境就能很方便的实现这个，相当于把整个运行环境打了一个包，所有的宿主机能够很方便的统一到相同的环境，即使重装了宿主机，也能方便的用一两条命令将环境部署到统一，比如上面所说的ceph，升级了基础镜像内的软件包，然后将所有的运行进程进行一次重启，就相当于运行了一个新的环境</p>
<p>容器还能够做的事情就是能够很便捷的把一个复杂环境运行起来，特别对于web类的服务，一台机器上可以跑一排的对外服务，即使出了问题，也能很快的再运行起来，这个对于传统的环境来说就是很难实现的，这里讲一下calamari，这个监控系统不是很复杂，但是因为依赖的软件的问题，造成很多人无法正常运行起来，这个后面我会出一个集成好calamari的docker环境，实现一键运行</p>
<p>在低版本的os上能够运行高版本的服务，比如在centos6上运行centos7的docker环境</p>
<h2 id="五、总结">五、总结</h2><p>本篇的文章的标题为docker与ceph的分与合，一套系统除了自身需要稳定性以外，系统自身最好不要受制于其他系统，需要在设计初期就能保证，各个模块都能轻松的剥离，否则很容易受制于另外一套系统，所以基于上面的方案来说，docker和ceph既是合在一起的，也是分开的,本篇只是讲了一个框架，实际部署ceph的过程当中还是有一些小问题需要具体处理的，不是很难，权限问题，目录问题</p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-19</td>
</tr>
</tbody>
</table>
<h2 id="附录：">附录：</h2><p>docker的常用操作<br>查询镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br></pre></td></tr></table></figure></p>
<p>查询容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br></pre></td></tr></table></figure></p>
<p>删除容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rm 64f617dfada5</span></span><br></pre></td></tr></table></figure></p>
<p>删除镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rmi node</span></span><br></pre></td></tr></table></figure></p>
<p>进入容器内部<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>让容器执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec monnode uptime</span></span><br></pre></td></tr></table></figure></p>
<p>退出容器,不停止容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ctrl+p然后ctrl+q</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中PG和PGP的区别]]></title>
    <link href="http://www.zphj1987.com/2016/10/19/Ceph%E4%B8%ADPG%E5%92%8CPGP%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://www.zphj1987.com/2016/10/19/Ceph中PG和PGP的区别/</id>
    <published>2016-10-19T07:26:28.000Z</published>
    <updated>2016-10-19T07:27:09.332Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/pgnew.png" alt="pg"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>首先来一段英文关于PG和PGP区别的解释：</p>
<blockquote>
<p>PG = Placement Group<br>PGP = Placement Group for Placement purpose </p>
<p>pg_num = number of placement groups mapped to an OSD</p>
<p>When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD. </p>
<p>Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role.<br>By Karan Singh </p>
</blockquote>
<p>以上是来自邮件列表的 <code>Karan Singh</code> 的PG和PGP的相关解释，他也是 <code>Learning Ceph</code> 和 <code>Ceph Cookbook</code> 的作者，以上的解释没有问题，我们来看下具体在集群里面具体作用<br><a id="more"></a></p>
<h2 id="二、实践">二、实践</h2><p>环境准备，因为是测试环境，我只准备了两台机器，每台机器4个OSD，所以做了一些参数的设置，让数据尽量散列<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">osd_crush_chooseleaf_type = 0</span><br></pre></td></tr></table></figure></p>
<p>以上为修改的参数，这个是让我的环境故障域为OSD分组的</p>
<p>创建测试需要的存储池<br>我们初始情况只创建一个名为testpool包含6个PG的存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool create testpool 6 6</span></span><br><span class="line">pool <span class="string">'testpool'</span> created</span><br></pre></td></tr></table></figure></p>
<p>我们看一下默认创建完了后的PG分布情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">0</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">0</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">0</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">0</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">0</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">0</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>我们写入一些对象，因为我们关心的不仅是pg的变动，同样关心PG内对象有没有移动,所以需要准备一些测试数据，这个调用原生rados接口写最方便<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p testpool bench <span class="number">20</span> write --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>我们再来查询一次<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">75</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">83</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">144</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">146</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到写入了一些数据，其中的第二列为这个PG当中的对象的数目，第三列为PG所在的OSD</p>
<h3 id="增加PG测试">增加PG测试</h3><p>我们来扩大PG再看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool set testpool pg_num 12</span></span><br><span class="line"><span class="built_in">set</span> pool <span class="number">1</span> pg_num to <span class="number">12</span></span><br></pre></td></tr></table></figure></p>
<p>再次查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">37</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.9</span> <span class="number">38</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">41</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.8</span> <span class="number">42</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1</span>.b <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.7</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">48</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.6</span> <span class="number">49</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1</span>.a <span class="number">49</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到上面新加上的PG的分布还是基于老的分布组合，并没有出现新的OSD组合，因为我们当前的设置是pgp为6,那么三个OSD的组合的个数就是6个，因为当前为12个pg，分布只能从6种组合里面挑选，所以会有重复的组合</p>
<p>根据上面的分布情况，可以确定的是，增加PG操作会引起PG内部对象分裂，分裂的份数是根据新增PG组合重复情况来的，比如上面的情况</p>
<ul>
<li>1.1的对象分成了两份[3,6,0]</li>
<li>1.3的对象分成了三份[4,1,2]</li>
<li>1.4的对象没有拆分[3,0,4]</li>
</ul>
<p>结论：增加PG会引起PG内的对象分裂，也就是在OSD上创建了新的PG目录，然后进行部分对象的move的操作</p>
<h3 id="增加PGP测试">增加PGP测试</h3><p>我们将原来的PGP从6调整到12<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool set testpool pgp_num 12</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1</span>.a <span class="number">49</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1</span>.b <span class="number">48</span> [<span class="number">1</span>,<span class="number">6</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.1</span> <span class="number">37</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">41</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">48</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br><span class="line"><span class="number">1.7</span> <span class="number">48</span> [<span class="number">1</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.6</span> <span class="number">49</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line"><span class="number">1.9</span> <span class="number">38</span> [<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.8</span> <span class="number">42</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到PG里面的对象并没有发生变化，而PG所在的对应关系发生了变化<br>我们看下与调整PGP前的对比<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">*1.1 37 [3,6,0]          1.1 37 [3,6,0]*&#10;1.9 38 [3,6,0]          1.9 38 [1,4,2]&#10;*1.0 41 [7,0,6]          1.0 41 [7,0,6]*&#10;1.8 42 [7,0,6]          1.8 42 [1,2,3]&#10;*1.3 48 [4,1,2]          1.3 48 [4,1,2]*&#10;1.b 48 [4,1,2]          1.b 48 [1,6,2]&#10;1.7 48 [4,1,2]          1.7 48 [1,6,0]&#10;*1.2 48 [7,4,1]          1.2 48 [7,4,1]*&#10;1.6 49 [7,4,1]          1.6 49 [3,6,7]&#10;1.a 49 [7,4,1]          1.a 49 [1,2,6]&#10;*1.5 86 [4,6,3]          1.5 86 [4,6,3]*&#10;*1.4 80 [3,0,4]          1.4 80 [3,0,4]*</span><br></pre></td></tr></table></figure></p>
<p>可以看到其中最原始的6个PG的分布并没有变化（标注了*号），变化的是后增加的PG，也就是将重复的PG分布进行新分布，这里并不是随机完全打散，而是根据需要去进行重分布</p>
<p>结论：调整PGP不会引起PG内的对象的分裂，但是会引起PG的分布的变动</p>
<h2 id="三、总结">三、总结</h2><ul>
<li>PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数</li>
<li>PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中</li>
<li>PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动</li>
</ul>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/pgnew.png" alt="pg"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>首先来一段英文关于PG和PGP区别的解释：</p>
<blockquote>
<p>PG = Placement Group<br>PGP = Placement Group for Placement purpose </p>
<p>pg_num = number of placement groups mapped to an OSD</p>
<p>When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD. </p>
<p>Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role.<br>By Karan Singh </p>
</blockquote>
<p>以上是来自邮件列表的 <code>Karan Singh</code> 的PG和PGP的相关解释，他也是 <code>Learning Ceph</code> 和 <code>Ceph Cookbook</code> 的作者，以上的解释没有问题，我们来看下具体在集群里面具体作用<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[配置Ceph的IPV6集群]]></title>
    <link href="http://www.zphj1987.com/2016/10/17/%E9%85%8D%E7%BD%AECeph%E7%9A%84IPV6%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.zphj1987.com/2016/10/17/配置Ceph的IPV6集群/</id>
    <published>2016-10-17T08:19:22.000Z</published>
    <updated>2016-10-31T07:35:25.618Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/ipv6.jpeg" alt="ipv6"><br></center><br>一、前言<br>对于IPV6实在是非常的陌生，所以本篇开始会讲一下最基本的网络配置，首先让网络能通起来，最开始就是因为不熟悉IPV6,而直接使用了link local地址，造成了mon部署的时候进程无法绑定到IP，从而端口没有启动，这个是在ceph社区群友 <code>ceph-长沙-柠檬</code> 同学的帮助下才发现问题的</p>
<p>IPV6是会有个link local地址的，在一个接口可以配置很多IPv6地址，所以学习路由就有可能出现很多下一跳。所以出现Link Local地址唯一标识一个节点。在本地链路看到下一跳都是对端的Link Local地址。这个地址一般是以fe80开头的，子网掩码为64，这个地方需要给机器配置一个唯一的全局单播地址</p>
<blockquote>
<p>However, with IPv6, all (IPv6) interfaces will have a link local address. This address is intended to allow communications over the attached links and so is defined to be usable only on that link.</p>
</blockquote>
<a id="more"></a>
<h2 id="二、网络配置">二、网络配置</h2><p>linux下用默认的网卡配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ifconfig </span></span><br><span class="line">eno16777736: flags=<span class="number">4163</span>&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu <span class="number">1500</span></span><br><span class="line">        inet <span class="number">192.168</span>.<span class="number">0.46</span>  netmask <span class="number">255.255</span>.<span class="number">0.0</span>  broadcast <span class="number">192.168</span>.<span class="number">255.255</span></span><br><span class="line">        inet6 fe80::<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x20&lt;link&gt;</span><br><span class="line">        ether <span class="number">00</span>:<span class="number">0</span>c:<span class="number">29</span>:c5:<span class="number">5</span>a:<span class="number">4</span>b  txqueuelen <span class="number">1000</span>  (Ethernet)</span><br><span class="line">        RX packets <span class="number">18422</span>  bytes <span class="number">1254119</span> (<span class="number">1.1</span> MiB)</span><br><span class="line">        RX errors <span class="number">0</span>  dropped <span class="number">6</span>  overruns <span class="number">0</span>  frame <span class="number">0</span></span><br><span class="line">        TX packets <span class="number">1938</span>  bytes <span class="number">890164</span> (<span class="number">869.3</span> KiB)</span><br><span class="line">        TX errors <span class="number">0</span>  dropped <span class="number">0</span> overruns <span class="number">0</span>  carrier <span class="number">0</span>  collisions <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h3 id="取消NetworkManager管理">取消NetworkManager管理</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl <span class="built_in">disable</span> NetworkManager</span><br><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure>
<p>以免NetworkManager的干扰</p>
<p>这个地方我没有做自定义的IPV6的设置，让其默认的生成的地方，可以看到上面的node1的link local地址地址为 fe80::20c:29ff:fec5:5a4b<br>我的另外一台的地址为 fe80::20c:29ff:feda:6849</p>
<blockquote>
<p>node1 fe80::20c:29ff:fec5:5a4b  prefixlen 64<br>node2 fe80::20c:29ff:feda:6849 prefixlen 64</p>
</blockquote>
<p>这个地方都是没有单播地址的，需要配置一个</p>
<p>配置的时候关闭掉ipv4的IP，防止影响，确认配置的就是ipv6环境，去掉IPv4的配置即可，我的网卡配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TYPE=<span class="string">"Ethernet"</span></span><br><span class="line">BOOTPROTO=<span class="string">"static"</span></span><br><span class="line">DEFROUTE=<span class="string">"yes"</span></span><br><span class="line">PEERDNS=<span class="string">"yes"</span></span><br><span class="line">PEERROUTES=<span class="string">"yes"</span></span><br><span class="line">NM_CONTROLLED=no</span><br><span class="line">IPV4_FAILURE_FATAL=<span class="string">"no"</span></span><br><span class="line">IPV6INIT=<span class="string">"yes"</span></span><br><span class="line">IPV6ADDR=<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b/<span class="number">64</span></span><br><span class="line">IPV6_AUTOCONF=no</span><br><span class="line">IPV6_DEFROUTE=<span class="string">"yes"</span></span><br><span class="line">IPV6_PEERDNS=<span class="string">"yes"</span></span><br><span class="line">IPV6_PEERROUTES=<span class="string">"yes"</span></span><br><span class="line">IPV6_FAILURE_FATAL=<span class="string">"no"</span></span><br><span class="line">NAME=<span class="string">"eno16777736"</span></span><br><span class="line">UUID=<span class="string">"0146f40c-6f4d-4c63-a9cd-7f89264613f3"</span></span><br><span class="line">DEVICE=<span class="string">"eno16777736"</span></span><br><span class="line">ONBOOT=<span class="string">"yes"</span></span><br></pre></td></tr></table></figure></p>
<p>检查配置情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ifconfig </span></span><br><span class="line">eno16777736: flags=<span class="number">4163</span>&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu <span class="number">1500</span></span><br><span class="line">        inet6 fe80::<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x20&lt;link&gt;</span><br><span class="line">        inet6 <span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x0&lt;global&gt;</span><br><span class="line">        ether <span class="number">00</span>:<span class="number">0</span>c:<span class="number">29</span>:c5:<span class="number">5</span>a:<span class="number">4</span>b  txqueuelen <span class="number">1000</span>  (Ethernet)</span><br><span class="line">        RX packets <span class="number">9133</span>  bytes <span class="number">597664</span> (<span class="number">583.6</span> KiB)</span><br><span class="line">        RX errors <span class="number">0</span>  dropped <span class="number">1</span>  overruns <span class="number">0</span>  frame <span class="number">0</span></span><br><span class="line">        TX packets <span class="number">466</span>  bytes <span class="number">137983</span> (<span class="number">134.7</span> KiB)</span><br><span class="line">        TX errors <span class="number">0</span>  dropped <span class="number">0</span> overruns <span class="number">0</span>  carrier <span class="number">0</span>  collisions <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到有两个inet6这样就是对的了</p>
<blockquote>
<p>windows远程ssh连接的方式:ssh 2008:20c:20c:20c:20c:29ff:fec5:5a4b</p>
</blockquote>
<h3 id="配置hosts">配置hosts</h3><p>在配置文件/etc/hosts中添加如下内容<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2008:20c:20c:20c:20c:29ff:fec5:5a4b node1&#10;2008:20c:20c:20c:20c:29ff:feda:6849 node2</span><br></pre></td></tr></table></figure></p>
<p>检测是否连通<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># ping6 -I eno16777736 2008:20c:20c:20c:20c:29ff:feda:6849</span></span><br></pre></td></tr></table></figure></p>
<p>ping主机名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># ping6 -I eno16777736 node2</span></span><br></pre></td></tr></table></figure></p>
<p>注意ping6需要加上网卡名称</p>
<p>同样的操作在node2上也配置好，网络到这里就配置好了</p>
<h2 id="三、集群配置">三、集群配置</h2><h3 id="创建初始配置文件">创建初始配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph-deploy new node1</span></span><br><span class="line">[root@node1 ceph]<span class="comment"># cat ceph.conf </span></span><br><span class="line">[global]</span><br><span class="line">fsid = f0bf4130<span class="operator">-f</span>4f0-<span class="number">4214</span>-<span class="number">8</span>b98-<span class="number">67103</span>ad55d65</span><br><span class="line">ms_<span class="built_in">bind</span>_ipv6 = <span class="literal">true</span></span><br><span class="line">mon_initial_members = node1</span><br><span class="line">mon_host = [<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b]</span><br><span class="line">auth_cluster_required =cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure>
<h3 id="创建mon">创建mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph-deploy mon create node1</span></span><br></pre></td></tr></table></figure>
<h3 id="检查状态">检查状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster d2882f75-<span class="number">1209</span>-<span class="number">4667</span>-bef8-<span class="number">3051</span>c84cb83c</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            no osds</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;node1=[<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b]:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> node1</span><br><span class="line">     osdmap e8: <span class="number">0</span> osds: <span class="number">0</span> up, <span class="number">0</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v2664: <span class="number">0</span> pgs, <span class="number">0</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">0</span> kB used, <span class="number">0</span> kB / <span class="number">0</span> kB avail</span><br></pre></td></tr></table></figure>
<h3 id="检查端口">检查端口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># netstat -tunlp|grep tcp6</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1155</span>/sshd           </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> ::<span class="number">1</span>:<span class="number">25</span>                  :::*                    LISTEN      <span class="number">1294</span>/master         </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> <span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">2</span>:<span class="number">6789</span> :::*                    LISTEN      <span class="number">8997</span>/ceph-mon</span><br></pre></td></tr></table></figure>
<p>可以看到集群已经正确的监听在了ipv6上了，后续的操作跟普通的IPV4集群一样的</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/ipv6.jpeg" alt="ipv6"><br></center><br>一、前言<br>对于IPV6实在是非常的陌生，所以本篇开始会讲一下最基本的网络配置，首先让网络能通起来，最开始就是因为不熟悉IPV6,而直接使用了link local地址，造成了mon部署的时候进程无法绑定到IP，从而端口没有启动，这个是在ceph社区群友 <code>ceph-长沙-柠檬</code> 同学的帮助下才发现问题的</p>
<p>IPV6是会有个link local地址的，在一个接口可以配置很多IPv6地址，所以学习路由就有可能出现很多下一跳。所以出现Link Local地址唯一标识一个节点。在本地链路看到下一跳都是对端的Link Local地址。这个地址一般是以fe80开头的，子网掩码为64，这个地方需要给机器配置一个唯一的全局单播地址</p>
<blockquote>
<p>However, with IPv6, all (IPv6) interfaces will have a link local address. This address is intended to allow communications over the attached links and so is defined to be usable only on that link.</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph技能树]]></title>
    <link href="http://www.zphj1987.com/2016/10/17/Ceph%E6%8A%80%E8%83%BD%E6%A0%91/"/>
    <id>http://www.zphj1987.com/2016/10/17/Ceph技能树/</id>
    <published>2016-10-17T06:32:50.000Z</published>
    <updated>2016-10-31T06:51:57.542Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>
<a id="more"></a>
<h2 id="二、技能图">二、技能图</h2><p><div class="video-container"><embed src="https://coggle.it/diagram/WAR1HK8sAvMFM_kd/a0b529bd1fc87c52d12a2b18a5b602929218d60384ca9664cc243e1cbcf72649" allowfullscreen="true"><br></div><br>本图支持下载为pdf,PNG，或者mm文件，Chrome支持滚轮缩放，下载PDF效果还不错</p>
<h2 id="三、变更记录">三、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中国社区QQ群数据分析]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph中国社区QQ群数据分析/</id>
    <published>2016-10-13T15:59:58.000Z</published>
    <updated>2016-10-13T16:09:33.153Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>
<a id="more"></a>
<h2 id="三、数据展示">三、数据展示</h2><p>第一张图包含信息如下</p>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephchina/ceph%E7%A4%BE%E5%8C%BA.png" alt=""><br></center>
</li>
<li><p>男性居多</p>
</li>
<li>80后居多</li>
</ul>
<p>图示的好处就是告诉你，多，大概多成一个什么程度</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph的参数mon_osd_down_out_subtree_limit细解]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E7%9A%84%E5%8F%82%E6%95%B0mon-osd-down-out-subtree-limit%E7%BB%86%E8%A7%A3/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph的参数mon-osd-down-out-subtree-limit细解/</id>
    <published>2016-10-13T03:34:29.000Z</published>
    <updated>2016-10-13T03:53:59.910Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br><a id="more"></a><br>参数，那么这个参数做什么用的，我们来看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph --show-config|grep mon_osd_down_out_subtree_limit</span></span><br><span class="line">mon_osd_down_out_subtree_<span class="built_in">limit</span> = rack</span><br></pre></td></tr></table></figure></p>
<p>首先解释下这个参数是做什么的，这个是控制标记为out的最小子树(bucket)，默认的这个为rack，这个可能我们平时感知不到这个有什么作用，大部分情况下，我们一般都为主机分组或者做了故障域，也很少做到测试去触发它，本篇文章将告诉你这个参数在什么情况下生效，对我们又有什么作用</p>
<p>准备两个物理节点，每个节点上3个osd，一共六个osd，上面的down out的时间已经修改为20s，那么会在20s后出现out的情况</p>
<h2 id="三、测试过程">三、测试过程</h2><h3 id="3-1_测试默认参数停止一台主机单个OSD">3.1 测试默认参数停止一台主机单个OSD</h3><p>首先用默认的<code>mon_osd_down_out_subtree_limit = rack</code>去做测试<br>开启几个监控终端方便观察<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -w</span><br><span class="line">watch ceph osd tree</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/mutiscreen1.png" alt="screen"></p>
<p>在其中的一台上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:15:39.673898 mon.0 [INF] osd.5 out (down for 20.253201)&#10;2016-10-13 10:15:39.757399 mon.0 [INF] osdmap e60: 6 osds: 5 up, 5 in</span><br></pre></td></tr></table></figure></p>
<p>停止一个后正常out</p>
<h3 id="3-2_测试默认参数停止掉一台主机所有osd">3.2 测试默认参数停止掉一台主机所有osd</h3><p>我们再来停止一台主机所有osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:17:09.699129 mon.0 [INF] osd.3 out (down for 23.966959)&#10;2016-10-13 10:17:09.699178 mon.0 [INF] osd.4 out (down for 23.966958)&#10;2016-10-13 10:17:09.699222 mon.0 [INF] osd.5 out (down for 23.966958)</span><br></pre></td></tr></table></figure></p>
<p>可以看到这台主机上的节点全部都正常out了</p>
<h3 id="3-3_测试修改参数后停止一台主机单个OSD">3.3 测试修改参数后停止一台主机单个OSD</h3><p>我们再调整下参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = rack</span><br></pre></td></tr></table></figure></p>
<p>将这个参数设置为host<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = host</span><br></pre></td></tr></table></figure></p>
<p>重启所有的进程，让配置生效，我们测试下只断一个osd的时候能不能out<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>停止掉osd.5<br>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:48:45.612206 mon.0 [INF] osd.5 out (down for 21.966238)</span><br></pre></td></tr></table></figure></p>
<p>可以看到可以osd.5可以正常的out</p>
<h3 id="3-4_测试修改参数后停止一台主机所有OSD">3.4 测试修改参数后停止一台主机所有OSD</h3><p>我们再来停止lab8107的所有的osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>停止掉 lab8107 所有的osd,可以看到没有out了,这个是因为把故障out设置为host级别了，这个地方出现host级别故障的时候，就不进行迁移了</p>
<h2 id="四、总结">四、总结</h2><p>关键的地方在于总结了，首先我们要想一想，ceph机器的迁移开不开（noout），关于这个问题，一定有两个答案</p>
<ul>
<li>开，不开的话，盘再坏怎么办，就会丢数据了</li>
<li>不开，人工触发，默认的情况下迁移数据会影响前端业务</li>
</ul>
<p>这里这个参数其实就是将我们的问题更加细腻的控制了，我们现在根据这个参数就能做到，迁移可以开，坏掉一个盘的时候我让它迁移，一个盘的数据恢复影响和时间是可以接受的，主机损坏我不让他迁移，为什么？主机损坏你去让他迁移，首先会生成一份数据，等主机好了，数据又要删除一份数据，这个对于磁盘都是消耗，主机级别的故障一定是可修复的，这个地方主机down机，主机电源损坏，这部分数据都是在的，那么这个地方就是需要人工去做这个修复的工作的，对于前端的服务是透明的，默认的控制是down rack才不去标记out，这个当然你也可以控制为这个，比如有个rack掉电，就不做恢复，如果down了两台主机，让他去做恢复，当然个人不建议这么做，这个控制就是自己去判断这个地方需要做不</p>
<p>ceph里面还是提供了一些细微粒度的控制，值得去与实际的应用场景结合，当然默认的参数已经能应付大部分的场景，控制的更细只是让其变得更好</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[创建一个自定义名称的Ceph集群]]></title>
    <link href="http://www.zphj1987.com/2016/10/12/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E7%9A%84Ceph%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.zphj1987.com/2016/10/12/创建一个自定义名称的Ceph集群/</id>
    <published>2016-10-12T02:44:17.000Z</published>
    <updated>2016-10-12T02:45:39.610Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>
<a id="more"></a>
<h2 id="三、简单的创建过程">三、简单的创建过程</h2><p>创建mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph mon create lab8107</span><br></pre></td></tr></table></figure></p>
<p>获取部署密钥<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph gatherkeys lab8107</span><br></pre></td></tr></table></figure></p>
<p>部署osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph osd prepare lab8107:/dev/sdb</span><br><span class="line">ceph-deploy  --cluster myceph osd activate lab8107:/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>查询集群状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph --cluster myceph <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<h2 id="四、总结">四、总结</h2><p>最简单的修改名称主要步骤就在这里了，关键部分就是修改那几个文件里面的集群的名称，这个里面是用一个变量写成了ceph，根据自己的需要进行修改即可</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[编译一个支持多线程的php安装包]]></title>
    <link href="http://www.zphj1987.com/2016/10/10/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84php%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    <id>http://www.zphj1987.com/2016/10/10/编译一个支持多线程的php安装包/</id>
    <published>2016-10-10T04:27:18.000Z</published>
    <updated>2016-10-10T04:44:29.648Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br><a id="more"></a></p>
<h2 id="二、获取源码">二、获取源码</h2><p>地址：<br><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="symbol">https:</span>/<span class="regexp">/uk.repo.webtatic.com/yum</span><span class="regexp">/el7/</span><span class="constant">SRPMS/RPMS/</span></span><br></pre></td></tr></table></figure></p>
<p>这个是webtatic发行的php版本，做了一些修改和优化</p>
<p>选择需要的版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># wget https://uk.repo.webtatic.com/yum/el7/SRPMS/RPMS/php56w-5.6.26-1.w7.src.rpm</span></span><br></pre></td></tr></table></figure></p>
<p>解压安装包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpm2cpio php56w-5.6.26-1.w7.src.rpm |cpio -div</span></span><br></pre></td></tr></table></figure></p>
<p>解压完成了后，当前目录下面会有很多文件<br>修改当前目录下面的php56.spec<br>在编译相关的configure后面增加</p>
<blockquote>
<p>—enable-maintainer-zts</p>
</blockquote>
<p>拷贝解压和修改的文件到源码编译目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># cp -ra * /root/rpmbuild/SOURCES/</span></span><br></pre></td></tr></table></figure></p>
<h2 id="三、编译rpm包">三、编译rpm包</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpmbuild -bb php56.spec</span></span><br></pre></td></tr></table></figure>
<p>如果提示缺依赖，就把相关的依赖包安装好就可以了，编译环境最好跟最终使用环境是一样的环境，执行完成了以后，会生成rpm安装包</p>
<h2 id="四、增加多线程支持">四、增加多线程支持</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pecl install pthreads-<span class="number">2.0</span>.<span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>这个会下载源码，然后自动编译成可用的内核模块，将这个内核模块的配置文件和模块文件拷贝到最终使用环境即可</p>
<p>检查是否安装成功<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># php -m|grep pth</span></span><br><span class="line">pthreads</span><br></pre></td></tr></table></figure></p>
<p>可用看到已经支持了</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[fio测试数据的可视化]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/fio%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://www.zphj1987.com/2016/09/28/fio测试数据的可视化/</id>
    <published>2016-09-28T10:13:04.000Z</published>
    <updated>2016-09-28T10:17:48.586Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br><a id="more"></a></p>
<h2 id="图例">图例</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fio/fiokeshuhua.png" alt=""><br></center>

<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python生成csv中文乱码解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/Python%E7%94%9F%E6%88%90csv%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/28/Python生成csv中文乱码的解决办法/</id>
    <published>2016-09-28T06:36:43.000Z</published>
    <updated>2016-09-28T06:39:34.698Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br><a id="more"></a></p>
<h2 id="二、处理过程">二、处理过程</h2><h3 id="原始代码">原始代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line"><span class="comment">#import codecs</span></span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line"><span class="comment">#    csvfile.write(codecs.BOM_UTF8)</span></span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>运行以后：<br>Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/2cve2nr8jyy4chs7kvur5wwt/image_1atnnp5i41b7lf7tumgj6175k9.png" alt="image_1atnnp5i41b7lf7tumgj6175k9.png-4.3kB"></p>
<h3 id="修改代码">修改代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line">import codecs</span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line">    csvfile.write(codecs.BOM_UTF8)</span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>跟上面的代码相比，引入了两行代码</p>
<blockquote>
<p>import codecs<br>csvfile.write(codecs.BOM_UTF8)</p>
</blockquote>
<p>我们再来看效果Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/k9m15wfa83wbrhuc6b0xyftg/image_1atnnsp1713931d1h1e641l4f13kim.png" alt="image_1atnnsp1713931d1h1e641l4f13kim.png-3.5kB"><br>问题解决</p>
<h2 id="三、总结">三、总结</h2><p>网上找了一些资料，这个方式比较快而简单，就先用这个方式解决，方法有很多</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[不小心清空了Ceph的OSD的分区表如何恢复]]></title>
    <link href="http://www.zphj1987.com/2016/09/24/%E4%B8%8D%E5%B0%8F%E5%BF%83%E6%B8%85%E7%A9%BA%E4%BA%86Ceph%E7%9A%84OSD%E7%9A%84%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/09/24/不小心清空了Ceph的OSD的分区表如何恢复/</id>
    <published>2016-09-23T16:56:27.000Z</published>
    <updated>2016-09-23T17:07:47.286Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br><a id="more"></a></p>
<h2 id="二、实践过程">二、实践过程</h2><p>我们现在有一个正常的集群，我们假设这些分区都是一致的，用的是默认的分区的方式，我们先来看看默认的分区方式是怎样的</p>
<h3 id="2-1_破坏环境">2.1 破坏环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list</span></span><br><span class="line">···</span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 ceph data, active, cluster ceph, osd.<span class="number">0</span>, journal /dev/sdb2</span><br><span class="line"> /dev/sdb2 ceph journal, <span class="keyword">for</span> /dev/sdb1</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>查看分区情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>来一个破坏，这里是破坏 <code>osd.0</code>，对应盘符 <code>/dev/sdb</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy disk zap lab8106:/dev/sdb</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">34</span>): /usr/bin/ceph-deploy disk zap lab8106:/dev/sdb</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] Warning: The kernel is still using the old partition table.</span><br><span class="line">[lab8106][DEBUG ] The new table will be used at the next reboot.</span><br><span class="line">[lab8106][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or</span><br><span class="line">[lab8106][DEBUG ] other utilities.</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>即使这个 osd 被使用在，还是被破坏了，这里假设上面的就是一个误操作，我们看下带来了哪些变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">58</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">02</span> /var/lib/ceph/osd/ceph-<span class="number">0</span>/journal -&gt; /dev/disk/by-partuuid/bd81471d-<span class="number">13</span>ff-<span class="number">44</span>ce-<span class="number">8</span>a33-<span class="number">92</span>a8df9e8eee</span><br></pre></td></tr></table></figure></p>
<p>如果你用命令行看，就可以看到上面的链接已经变红了，分区没有了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list </span></span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line"> /dev/sdb2 other</span><br></pre></td></tr></table></figure></p>
<p>已经跟上面有变化了，没有ceph的相关信息了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br></pre></td></tr></table></figure></p>
<p>分区表完全没有信息了，到这我们可以确定分区表完全没了，如果现在重启将会发生什么？重启以后这个磁盘就是一个裸盘，没有分区的裸盘</p>
<h4 id="2-2_处理办法">2.2 处理办法</h4><p>首先一个办法就是当这个OSD坏了，然后直接按照删除节点，添加节点就可以了，这个应该是最主流，最通用的处理办法，但是这个在生产环境环境当中造成的数据迁移还是非常大的，我们尝试做恢复，这就是本篇主要讲的东西</p>
<h5 id="关闭迁移">关闭迁移</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd set noout</span></span><br></pre></td></tr></table></figure>
<h5 id="停止OSD">停止OSD</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>现在的OSD还是有进程的，所以需要停止掉再做处理<br>通过其他节点查看分区的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdc  unit s print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdc: <span class="number">585937500</span>s</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start     End         Size        File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">2048</span>s     <span class="number">2097152</span>s    <span class="number">2095105</span>s                 ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">2099200</span>s  <span class="number">585937466</span>s  <span class="number">583838267</span>s  xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>我们现在进行分区表的恢复，记住上面的数值，我print的时候是加了unit s这个是要精确的值的,下面的创建会用到的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2099200s 585937466s</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2048s 2097152s</span></span><br></pre></td></tr></table></figure>
<p>我们再来检查下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name     Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               primary</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          primary</span><br></pre></td></tr></table></figure></p>
<p>分区表已经回来了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># umount /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># partprobe</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新挂载看看，没有问题，还要做下其他的处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-0/journal</span></span><br></pre></td></tr></table></figure></p>
<p>我们先删除掉journal的链接文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-osd -i 0 --osd-journal=/dev/sdb2 --mkjournal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">09</span>-<span class="number">24</span> <span class="number">00</span>:<span class="number">36</span>:<span class="number">06.595992</span> <span class="number">7</span>f9d0afbc880 -<span class="number">1</span> created new journal /dev/sdb2 <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ln -s /dev/sdb2 /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">9</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">37</span> journal -&gt; /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>上面操作就是创建journal相关的,注意下我上面的操作—osd-journal=/dev/sdb2这个地方，我是便于识别，这个地方要写上dev/sdb2的uuid的路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /dev/disk/by-partuuid/03fc6039-ad80-4b8d-86ec-aeee14fb3bb6 </span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">33</span> /dev/disk/by-partuuid/<span class="number">03</span><span class="built_in">fc</span>6039-ad80-<span class="number">4</span>b8d-<span class="number">86</span>ec-aeee14fb3bb6 -&gt; ../../sdb2</span><br></pre></td></tr></table></figure></p>
<p>也就是这个链接的这一串，这个防止盘符串了情况下journal无法找到的问题</p>
<h4 id="启动osd">启动osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># systemctl start ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>检查下，到这osd就正常的恢复了</p>
<h2 id="三、为什么有这篇">三、为什么有这篇</h2><p>一直都知道分区表是可以恢复的，也一直知道会有误操作，但是一直没有去把ceph中完整流程走下来，前两天一个哥们环境副本一，然后自己给搞错了，出现不得不恢复的情况，正好自己一直想把这个问题的处理办法给记录下来，所以就有了这篇，万一哪天有人碰到了，就把这篇发给他</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-24</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
