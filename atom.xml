<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-06-13T09:19:01.017Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[推荐一本性能相关的书]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%9C%AC%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B9%A6/"/>
    <id>http://www.zphj1987.com/2016/06/13/推荐一本性能相关的书/</id>
    <published>2016-06-13T08:57:38.000Z</published>
    <updated>2016-06-13T09:19:01.017Z</updated>
    <content type="html"><![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>

<a id="more"></a>
<p>京东纸质书购买链接：<a href="http://item.jd.com/11755695.html" target="_blank" rel="external">性能之巅</a></p>
<h3 id="英文版本下载地址：">英文版本下载地址：</h3><p>地址：<a href="http://pan.baidu.com/s/1i5LJTxN" target="_blank" rel="external">http://pan.baidu.com/s/1i5LJTxN</a><br>密码：nam5</p>
<h3 id="中文电子版">中文电子版</h3><p>联系我<br>QQ:199383004</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[加速OSD的启动]]></title>
    <link href="http://www.zphj1987.com/2016/06/07/%E5%8A%A0%E9%80%9FOSD%E7%9A%84%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/06/07/加速OSD的启动/</id>
    <published>2016-06-07T14:50:02.000Z</published>
    <updated>2016-06-07T15:03:50.697Z</updated>
    <content type="html"><![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br><a id="more"></a></p>
<h3 id="一、问题分析">一、问题分析</h3><p>OSD重启是需要重视的，这个地方是ceph的一个设计的弱点。ceph集群有很多的OSD进程，OSD管理对磁盘上的对象的访问，磁盘的对象被分布到PG组当中，对象有相同的分布，副本会在相同的PG当中存在，如果不理解可以看看（<a href="http://docs.ceph.com/docs/master/architecture/" target="_blank" rel="external">ceph概览</a>）</p>
<p>当集群OSD进程出现down的情况，会被mon认为 “OUT” 了，这个 “OUT” 不是触发迁移的那个 “OUT”，是不服务的 “OUT” ,这个OSD上受影响的PG的I/O请求会被其他拥有这个PG的OSD接管，当OSD重新启动的时候,OSD会被加入进来，将会检查PG，看是否有在down的期间错过东西，然后进行更新，这里问题就来了，启动之后会访问磁盘检查PG是否有缺失的东西进行更新，会进行一定量的数据恢复，同时会开始接受新的IO的请求，如果本来磁盘就只剩很少的余量，那么一旦请求发送到这个OSD上，那么性能将会开始下降</p>
<p>如果去看ceph的邮件列表，在极端情况下，这种效应会让整个集群停机，这发生在OSD太忙了，连心跳都无法回复，然后mon就会把它标记为down，这个时候OSD的进程都还在的，这个时候客户端的请求会导入到其他的OSD上，然后负载小了，OSD又会自己进来，然后又开始响应请求了，然后之前没有受影响的OSD节点，需要把新写入的数据同步过来，这个又增加了其他的OSD的负载了，一旦集群接近I/O的限制，也会让其他的OSD无法响应了，结果就是整个集群的OSD在反复的”in”和”out”状态之间变化了，集群在这种情况下，就无法接收客户端的请求了，如果不人工干预甚至无法恢复正常，这个在高负载下是很好复现出来的;另外一种较轻的情况，在OSD重启过程，I/O可能会hung住，影响性能.如果不能避免，至少能想办法去降低这个影响</p>
<p>我们能做些什么？在ceph开发者列表当中有开发者提出了这个<a href="http://thread.gmane.org/gmane.comp.file-systems.ceph.user/25881/focus=25890" target="_blank" rel="external">设计上需要修复</a>，这个估计需要等很久以后的事情了，我们能做什么来降低这个的影响？最明显的一点是要保证集群有足够的I/O的余量，另一种思路就是减少关键过程启动检查和接收I/O的竞争</p>
<h3 id="二、减少OSD启动过程当中的IO">二、减少OSD启动过程当中的IO</h3><p>OSD在启动的时候可以预测到磁盘的访问的模式。我们可以了解这个访问模式，然后提前将文件读取到内核的缓存当中。这样这些文件在启动的时候就不需要再次访问磁盘了，意味着更少的磁盘消耗和更好的性能</p>
<p>现在来定位下OSD启动过程中做了哪些事情，使用性能大师 Brendan Gregg 的 <a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a> 工具，一个OSD启动的过程如下：</p>
<h4 id="2-1_OSD启动过程">2.1 OSD启动过程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># opensnoop ceph-7</span></span><br><span class="line">Tracing open()s <span class="keyword">for</span> filenames containing <span class="string">"ceph-7"</span>. Ctrl-C to end.</span><br><span class="line">COMM             PID      FD FILE</span><br><span class="line">ceph             osd     <span class="number">0</span>x3 /var/lib/ceph/osd/ceph-<span class="number">7</span>/</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/<span class="built_in">type</span></span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/magic</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/whoami</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/ceph_fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/superblock</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/fiemap_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/xattr_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/current</span><br><span class="line">ceph             osd     <span class="number">0</span>xe /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/commit_op_seq</span><br><span class="line">ceph             osd     <span class="number">0</span>xf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/LOCK</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/CURRENT</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000135</span></span><br></pre></td></tr></table></figure>
<p>开始的时候，OSD读取了很多元数据文件，没有什么特别的<br>下面读取omap的数据库文件，读取了一部分的osdmap文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000137</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000144</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000142</span></span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000142</span>.dbtmp</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000138</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">298</span>__0_AC96EE75__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.3</span>b_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">297</span>__0_AC96EEA5__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.34</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.20</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.22</span>_head</span><br></pre></td></tr></table></figure></p>
<p>可以看到读取一个sst后，就会继续读取pg的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000139</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.ec_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>e_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>b_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000141</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>fb_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.cf_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>f_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000140</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.8</span>f_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>c_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>e_head</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会读取每个pg里面的<em>head</em>文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x17 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.23</span>a_head/__head_0000023A__0</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x18 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.1</span>a2_head/DIR_2/DIR_A/DIR_1/__head_000001A2__0</span><br><span class="line">&lt;...&gt;            <span class="number">23689</span>  <span class="number">0</span>x19 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>ea_head/__head_000002EA__0</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会进行osdmap文件的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x3a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">299</span>__0_C67CF872__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">299</span>__0_AC96EF05__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">300</span>__0_C67CF142__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4f /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">300</span>__0_AC96E415__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x6e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">301</span>__0_C67CF612__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x55 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">301</span>__0_AC96E5A5__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>xbf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">302</span>__0_C67CF7A2__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x60 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">302</span>__0_AC96E575__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x86 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">303</span>__0_C67CF772__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x6a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">303</span>__0_AC96FA05__none</span><br><span class="line">s</span><br></pre></td></tr></table></figure></p>
<p>我们无法确定哪些对象将需要读取，单我们知道，所有的OMAP和元数据文件将会打开，<em>head</em>文件将会打开</p>
<h4 id="2-2_使用vmtouch进行预读取">2.2 使用vmtouch进行预读取</h4><p>下面将进入 <a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> ,这个小工具能够读取文件并锁定到内存当中，这样后续的I/O请求能够从缓存当中读取它们，这样就减少了对磁盘的访问请求<br>在这里我们的访问模式是这样的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">7</span>]<span class="comment"># vmtouch -t /var/lib/ceph/osd/ceph-7/current/meta/ /var/lib/ceph/osd/ceph-7/current/omap/</span></span><br><span class="line">          Files: <span class="number">618</span></span><br><span class="line">     Directories: <span class="number">6</span></span><br><span class="line">   Touched Pages: <span class="number">3972</span> (<span class="number">15</span>M)</span><br><span class="line">         Elapsed: <span class="number">0.009621</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>关于这个vmtouch很好使用也很强大，可以使用  <code>vmtouch -L</code> 将数据锁定到内存当中去，这里用 <code>-t</code> 也可以，使用 <code>-v</code>  参数能打印更多详细的信息,这个效果有多大？这个原作者的效果很好，我的环境太小，看不出太多的效果，但是从原理上看，应该是会有用的，我的读取过程跟原作者的读取过程有一定的差别，作者的数据库文件是 <code>ldb</code> ，我的环境是 <code>sst</code>，并且作者的压力应该是很大的情况下的，我的环境较小</p>
<h3 id="三、判断是否有作用">三、判断是否有作用</h3><p>一个很好的衡量的方法就是看启动过程当中的 <code>peering</code> 的阶段的长度,<code>peering</code>状态是osd做相互的协调的，PG的请求在这个时候是无法响应的，理想状况下这个过程会很快，无法察觉，如果集群集群处于高负载或者过载状态，这个持续的时间就会很久，然后关闭一个OSD，然后等待一分钟，以便让一部分写入只写到了其他OSD，在down掉的OSD启动后，需要从其他OSD恢复一些数据，然后重新打开，从日志当中，绘制一段时间的<code>peering</code>状态PG的数目，score是统计的所有时间线上<code>peering</code>状态的计数的总和</p>
<p>为了验证这个vmtouch将会减少 <code>peering</code> 的状态,将负载压到略小于集群满载情况</p>
<h4 id="第一个实验是OSD重启（无vmtouch）">第一个实验是OSD重启（无vmtouch）</h4><p><img src="http://static.zybuluo.com/zphj1987/xisprl91q5ljn99rknb1ku8g/vmtouchno.png" alt="vmtouchno.png-22.3kB"><br>可以看到超过30s时，大量的pg是peering状态，导致集群出现缓慢</p>
<h4 id="第二个实验中使用vmtouch预读取OMAP的数据库文件">第二个实验中使用vmtouch预读取OMAP的数据库文件</h4><p><img src="http://static.zybuluo.com/zphj1987/d64ak6lbnzywokpl6j8zkune/vmtouch.png" alt="vmtouch.png-17.9kB"><br>这些 <code>peering</code> 状态并没有消失，但是可以看到有很大的改善 <code>peering</code> 会更早的开始（OMAP已经加载），总体的score也要小很多，这个是一个很不错的结果</p>
<h3 id="四、结论">四、结论</h3><p>根据之前监测到的读取数据的情况，预读取文件，能够有不错的改善，虽然不是完整的解决方案，但是能够帮助改善一个痛点，从长远来看，希望ceph能改进设计，是这个情况消失</p>
<h3 id="五、总结">五、总结</h3><p>本章节里面介绍了两个工具<br><strong>opensnoop</strong><br>这个工具已经存在了很久很久了，也是到现在才看到的，一个用于监控文件的操作，是Gregg 大师的作品，仅仅是一个shell脚本就能实现监控，关键还在于其对操作系统的了解<br><strong>vmtouch</strong><br>这个是将数据加载到内存的，以前关注的是清理内存，其实在某些场景下，能够预加载到内存将会解决很多问题，关键看怎么去用了</p>
<h3 id="六、参考文章">六、参考文章</h3><p><a href="https://blog.flyingcircus.io/2016/03/11/improving-ceph-osd-start-up-behaviour-with-vmtouch/" target="_blank" rel="external">Improving Ceph OSD start-up behaviour with vmtouch</a><br><a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a><br><a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a></p>
<h3 id="七、变更记录">七、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-07</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd无法map(rbd feature disable)]]></title>
    <link href="http://www.zphj1987.com/2016/06/07/rbd%E6%97%A0%E6%B3%95map(rbd-feature-disable)/"/>
    <id>http://www.zphj1987.com/2016/06/07/rbd无法map(rbd-feature-disable)/</id>
    <published>2016-06-07T05:02:44.000Z</published>
    <updated>2016-06-07T05:03:53.613Z</updated>
    <content type="html"><![CDATA[<p>在jewel版本下默认开启了rbd的一些属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph --show-config|grep rbd|grep features </span></span><br><span class="line">rbd_default_features = <span class="number">61</span></span><br></pre></td></tr></table></figure></p>
<p>RBD属性表：<br><img src="http://static.zybuluo.com/zphj1987/1amw9dopkoohq90dfwme7z6p/%E5%B1%9E%E6%80%A7.png" alt="此处输入图片的描述"><br>61的意思是上面图中的bit码相加得到的值<br><a id="more"></a><br>对rbd进行内核的map操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map mytest</span></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature <span class="built_in">set</span> mismatch. You can <span class="built_in">disable</span> features unsupported by the kernel with <span class="string">"rbd feature disable"</span>.</span><br><span class="line">In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">"dmesg | tail"</span> or so.</span><br><span class="line">rbd: map failed: (<span class="number">6</span>) No such device or address</span><br></pre></td></tr></table></figure></p>
<p>根据提示查询打印的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dmesg | tail</span></span><br><span class="line">[<span class="number">10440.462708</span>] rbd: image mytest: image uses unsupported features: <span class="number">0</span>x3c</span><br></pre></td></tr></table></figure></p>
<p>这个地方提示的很清楚了，不支持的属性0x3c，0x3c是16进制的数值，换算成10进制是3*16+12=60<br>60的意思是不支持：</p>
<blockquote>
<p>32+16+8+4 = exclusive-lock, object-map, fast-diff, deep-flatten<br>也就是不支持这些属性，现在动态关闭这些属性</p>
</blockquote>
<p>查看当前使用的image属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info mytest</span></span><br><span class="line">rbd image <span class="string">'mytest'</span>:</span><br><span class="line">	size <span class="number">2000</span> MB <span class="keyword">in</span> <span class="number">500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>开启的属性有4个是不支持的，关闭这些属性<br>语法是：</p>
<blockquote>
<p>rbd feature disable {poolname}/{imagename} {feature}<br>具体到这个测试的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest deep-flatten</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest fast-diff</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest object-map</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest exclusive-lock</span></span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>再次查询image的info信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info mytest</span></span><br><span class="line">rbd image <span class="string">'mytest'</span>:</span><br><span class="line">	size <span class="number">2000</span> MB <span class="keyword">in</span> <span class="number">500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到已经关闭了不支持的属性<br>进行kernel rbd 的map的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map mytest</span></span><br><span class="line">/dev/rbd1</span><br></pre></td></tr></table></figure></p>
<p>如果不想动态的关闭，那么在创建rbd之前，在配置文件中设置这个参数即可</p>
<blockquote>
<p>rbd_default_features = 3</p>
</blockquote>
<p>关于属性支持的，目前到内核4.6仍然只支持</p>
<blockquote>
<p>layering,striping = 1 + 2</p>
</blockquote>
<p>这两个属性</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>在jewel版本下默认开启了rbd的一些属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph --show-config|grep rbd|grep features </span></span><br><span class="line">rbd_default_features = <span class="number">61</span></span><br></pre></td></tr></table></figure></p>
<p>RBD属性表：<br><img src="http://static.zybuluo.com/zphj1987/1amw9dopkoohq90dfwme7z6p/%E5%B1%9E%E6%80%A7.png" alt="此处输入图片的描述"><br>61的意思是上面图中的bit码相加得到的值<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用虚拟化环境虚拟nvme盘]]></title>
    <link href="http://www.zphj1987.com/2016/06/04/%E5%88%A9%E7%94%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E8%99%9A%E6%8B%9Fnvme%E7%9B%98/"/>
    <id>http://www.zphj1987.com/2016/06/04/利用虚拟化环境虚拟nvme盘/</id>
    <published>2016-06-03T16:02:50.000Z</published>
    <updated>2016-06-03T16:21:05.955Z</updated>
    <content type="html"><![CDATA[<h2 id="前情介绍">前情介绍</h2><h3 id="SPDK">SPDK</h3><p>SPDK的全称为Storage Performance Development Kit ，是Intel发起的一个开源驱动项目，这个是一个开发套件，可以让应用程序在用户态去访问存储资源，具体做能做什么可以去官网看一下 <a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk" target="_blank" rel="external">SPDK官网</a></p>
<h3 id="NVME">NVME</h3><p>NVMe其实与AHCI一样都是逻辑设备接口标准，NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p>
<h3 id="Bluestore">Bluestore</h3><p>BlueStore 是用来存储ceph的数据的地方，提供了一种在块设备上直接写入方式的存储。这个是因为之前ceph社区尝试做了一个kvstore，但是性能达不到想要的效果，然后基于rocksdb的原型，重新开发了一套存储系统，BlueStore直接消耗原始分区。还有一个分区是存储元数据的，实际上就是一个RocksDB键/值数据库存储，这个比之前的filestore最大的优势就是去掉了journal，从而提供了更平滑的IO</p>
<a id="more"></a>
<h3 id="SPDK+NVME+Bluestore能产生什么化学反应">SPDK+NVME+Bluestore能产生什么化学反应</h3><p>目前这一块走的比较前沿的就是xsky了，这块的最初的推动力量是Intel，NVME的硬件的推出，需要一个很好的催化剂，传统的内核中断式的访问磁盘的方式，已经不能最大化发挥NVME的性能了，因此推出了SPDK的套件，可以在用户态的去访问磁盘数据，Bluestore按照这个标准就可以去以最大化的跑出磁盘的性能了，从而给上层提供一个非常强悍的IO性能，目前来说这几项都是很新的东西，如果没有特别强的技术，或者找Intel做技术支持话，用好还是需要再等一段时间</p>
<h2 id="开篇">开篇</h2><h3 id="本篇文章做什么">本篇文章做什么</h3><p>之前有篇文章已经实现了bluestore的配置，这个配置并不难，并且用普通的硬盘就能实现，这里是讲怎么弄出来NVME磁盘，因为NVME的磁盘很贵，并不是每个人都能有环境的，这里是用虚拟化的方式虚拟出nvme，以供以后进行相关的功能验证</p>
<h3 id="准备工作">准备工作</h3><p>准备好kvm虚拟化的环境，这个地方就不在这里赘述了，本环境采用的是ubuntu的宿主机，如果是centos需要另做改动，如果有需要欢迎留言</p>
<h3 id="安装操作系统">安装操作系统</h3><h4 id="创建两个磁盘">创建两个磁盘</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">qemu-img create <span class="operator">-f</span> raw /mnt/localdisk.raw  <span class="number">40</span>G</span><br><span class="line">qemu-img create <span class="operator">-f</span> raw  /mnt/nvme.raw <span class="number">50</span>G</span><br></pre></td></tr></table></figure>
<h4 id="执行安装操作系统">执行安装操作系统</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virt-install --name nvmetest --ram <span class="number">4096</span> --vcpus=<span class="number">2</span> --disk path=/mnt/localdisk.raw --network bridge=br0 --cdrom /mnt/CentOS-<span class="number">7</span>-x86_64-DVD-<span class="number">1503</span>-<span class="number">01</span>.iso --vnclisten=<span class="number">192.168</span>.<span class="number">8.107</span> --vncport=<span class="number">7000</span> --vnc --autostart</span><br></pre></td></tr></table></figure>
<p>上面的iso文件需要提前准备，vnclisten就用宿主机的IP即可</p>
<p>都安装好了以后，先停止虚拟机，需要对配置文件做一些改动，因为virsh管理的时候有一些参数是不支持的，这个需要自己做一个  qemu:commandline 的改动</p>
<h4 id="停止掉虚拟机">停止掉虚拟机</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh destroy nvmetest</span><br></pre></td></tr></table></figure>
<h4 id="编辑配置文件">编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh edit nvmetest</span><br></pre></td></tr></table></figure>
<p>内容如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;domain <span class="built_in">type</span>=<span class="string">'kvm'</span>&gt;</span><br><span class="line">····</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;domain <span class="built_in">type</span>=<span class="string">'kvm'</span> xmlns:qemu=<span class="string">'http://libvirt.org/schemas/domain/qemu/1.0'</span>&gt;</span><br><span class="line">····</span><br><span class="line">  &lt;qemu:commandline&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'-drive'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'file=/mnt/nvme.raw,if=none,id=D22,format=raw'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'-device'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'nvme,drive=D22,serial=1235'</span>/&gt;</span><br><span class="line">  &lt;/qemu:commandline&gt;</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这个地方一定要注意加入的这个固定格式一定要写到最后的位置，否则不生效</p>
</blockquote>
<h3 id="检查虚拟机的磁盘是否生成">检查虚拟机的磁盘是否生成</h3><h4 id="启动测试的虚拟机">启动测试的虚拟机</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh destroy nvmetest</span><br></pre></td></tr></table></figure>
<p>使用vnc连接登陆刚刚的那个vnc的端口<br>登陆上机器后就可以fdisk -l,<br><img src="http://static.zybuluo.com/zphj1987/nc79igpyb1hgxny09d5w9nk0/nvmedisk.png" alt="nvme"><br>lspci看到的如下<br><img src="http://static.zybuluo.com/zphj1987/8klt54xfe4w05mdkr41rspwk/lspci.png" alt="lspci.png-1.5kB"></p>
<h4 id="结束">结束</h4><p>本次实践当中还有一部分是对spdk的代码进行编译的，编译没有问题，并且可以根据测试脚本加载驱动，将nvme磁盘排它性的从内核态移出，但是无法找到如何使用这个用户态的磁盘，在ceph的代码分支中已经包含了spdk部分的代码，在ceph中应该默认可以直接使用这个驱动，使用的方式是 spdk：sdasdasdasd (disk SN) ，但是配置当中如何使用还是无从得知，这一块如果资料会第一时间分析，目前xsky应该能够配置出环境来，本篇涉及的几个东西都是比较新的一些东西，在未来将会极大的提高性能的，目前这个阶段还处于开发阶段</p>
<h4 id="异常处理">异常处理</h4><p>执行virsh start nvmetest的时候会提示nvme.raw的磁盘没有访问权限，这个地方卡了很久，不清楚在ubuntu下面居然还有个apparmor的权限问题，是调看系统日志才发现的，下面是处理办法:<br>执行下面的命令为libvirt禁用 apparmor:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ln <span class="operator">-s</span> /etc/apparmor.d/usr.sbin.libvirtd  /etc/apparmor.d/<span class="built_in">disable</span>/</span><br><span class="line">ln <span class="operator">-s</span> /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper  /etc/apparmor.d/<span class="built_in">disable</span>/</span><br><span class="line">apparmor_parser -R  /etc/apparmor.d/usr.sbin.libvirtd</span><br><span class="line">apparmor_parser -R  /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper</span><br></pre></td></tr></table></figure></p>
<p>/etc/libvirt/qemu.conf去掉认证的,修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">security_driver = <span class="string">"none"</span></span><br></pre></td></tr></table></figure></p>
<p>重启libvirt服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/libvirt-bin restart</span><br></pre></td></tr></table></figure></p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前情介绍">前情介绍</h2><h3 id="SPDK">SPDK</h3><p>SPDK的全称为Storage Performance Development Kit ，是Intel发起的一个开源驱动项目，这个是一个开发套件，可以让应用程序在用户态去访问存储资源，具体做能做什么可以去官网看一下 <a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk">SPDK官网</a></p>
<h3 id="NVME">NVME</h3><p>NVMe其实与AHCI一样都是逻辑设备接口标准，NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p>
<h3 id="Bluestore">Bluestore</h3><p>BlueStore 是用来存储ceph的数据的地方，提供了一种在块设备上直接写入方式的存储。这个是因为之前ceph社区尝试做了一个kvstore，但是性能达不到想要的效果，然后基于rocksdb的原型，重新开发了一套存储系统，BlueStore直接消耗原始分区。还有一个分区是存储元数据的，实际上就是一个RocksDB键/值数据库存储，这个比之前的filestore最大的优势就是去掉了journal，从而提供了更平滑的IO</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mon到底能坏几个]]></title>
    <link href="http://www.zphj1987.com/2016/05/26/mon%E5%88%B0%E5%BA%95%E8%83%BD%E5%9D%8F%E5%87%A0%E4%B8%AA/"/>
    <id>http://www.zphj1987.com/2016/05/26/mon到底能坏几个/</id>
    <published>2016-05-25T16:28:21.000Z</published>
    <updated>2016-06-03T17:29:11.446Z</updated>
    <content type="html"><![CDATA[<p>如果是在做ceph的配置，我们会经常遇到这几个问题</p>
<ol>
<li>问：ceph需要配置几个mon<br>答：配置一个可以，但是坏了一个就不行了，需要配置只是三个mon，并且需要是奇数个</li>
<li>问：ceph的mon能跟osd放在一起么，需要配置很好么？<br>答：能跟放在一起，但是建议在环境允许的情况下一定独立机器，并且mon的配置能好尽量好，能上ssd就上ssd</li>
</ol>
<p>这两个问题的答案不能说是错的，但是为什么这么说，这么说有没有问题，这篇文章将根据实际的数据来告诉你，到底mon的极限在哪里，为什么都说要奇数，偶数难道就不行么</p>
<a id="more"></a>
<h3 id="前言">前言</h3><p>本篇将从真实的实践中，让你更能够理解mon的故障极限，本次测试的场景数据样本足够大，最大的一个测试使用了10个mon，我想目前就算PB基本的ceph集群里也没有人会超过10个mon，所以足够覆盖大部分的场景，先来一个数据图看下10个mon的集群长什么样<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e10: <span class="number">10</span> mons at &#123;<span class="number">10</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">0</span>,<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">58</span>, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span></span><br><span class="line">     osdmap e7: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v13: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">34268</span> kB used, <span class="number">274</span> GB / <span class="number">274</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>mon的地方可以看到10个mon了</p>
<h3 id="测试结论">测试结论</h3><p><img src="http://static.zybuluo.com/zphj1987/jz15y76o7yytmxksk2ps4eyi/1.png" alt="mondown"></p>
<p>ceph的mon能够正常情况需要保证，当前剩余的mon的个数需要大于总mon个数的一半，例如10个mon，mon个数一半就是5个，那么大于5个就是6个，也就是最少需要6个，上面的测试结论也符合这个规则，为什么不去偶数个，是因为当mon的个数为偶数个的时候，允许down的mon的个数与少一个mon的情况下的mon的个数允许的个数是一样的，所以要么多两个，多一个增加不了可靠性，并不是不允许</p>
<h3 id="测试过程的数据">测试过程的数据</h3><h4 id="10个mon集群">10个mon集群</h4><p>10个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">   health HEALTH_WARN</span><br><span class="line">          <span class="number">4</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span></span><br><span class="line">   monmap e10: <span class="number">10</span> mons at &#123;<span class="number">10</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">0</span>,<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.10</span></span><br></pre></td></tr></table></figure></p>
<p>10个mon关闭4个没问题，关闭5个就卡死</p>
<h4 id="9个mon集群">9个mon集群</h4><p>9个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">4</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">  monmap e11: <span class="number">9</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>9个mon关闭4个没问题，关闭5个就卡死</p>
<h4 id="8个mon集群">8个mon集群</h4><p>8个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">3</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">  monmap e12: <span class="number">8</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>8个mon关闭3个没问题，关闭4个就卡死</p>
<h4 id="7个mon集群">7个mon集群</h4><p>7个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">   health HEALTH_WARN</span><br><span class="line">          <span class="number">3</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">   monmap e13: <span class="number">7</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>7个mon关闭3个没问题，关闭4个就卡死</p>
<h4 id="6个mon集群">6个mon集群</h4><p>6个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">2</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">  monmap e14: <span class="number">6</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>6个mon关闭2个没问题，关闭3个就卡死</p>
<h4 id="5个mon集群">5个mon集群</h4><p>5个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">2</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8107,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line">  monmap e15: <span class="number">5</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>5个mon关闭2个没问题，关闭3个就卡死</p>
<h4 id="4个mon集群">4个mon集群</h4><p>4个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8107,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line">  monmap e16: <span class="number">4</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>4个mon关闭1个没问题，关闭2个就卡死</p>
<h4 id="3个mon集群">3个mon集群</h4><p>3个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span> lab8107,<span class="number">2</span></span><br><span class="line">  monmap e17: <span class="number">3</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>3个mon关闭1个没问题，关闭2个就卡死</p>
<h3 id="测试结束">测试结束</h3><p>下面为自己玩的一个动态图，10个mon正常，down 4个还是好的，down 5个就无法使用了</p>
<p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/10mon.gif" alt=""></p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>如果是在做ceph的配置，我们会经常遇到这几个问题</p>
<ol>
<li>问：ceph需要配置几个mon<br>答：配置一个可以，但是坏了一个就不行了，需要配置只是三个mon，并且需要是奇数个</li>
<li>问：ceph的mon能跟osd放在一起么，需要配置很好么？<br>答：能跟放在一起，但是建议在环境允许的情况下一定独立机器，并且mon的配置能好尽量好，能上ssd就上ssd</li>
</ol>
<p>这两个问题的答案不能说是错的，但是为什么这么说，这么说有没有问题，这篇文章将根据实际的数据来告诉你，到底mon的极限在哪里，为什么都说要奇数，偶数难道就不行么</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[磨磨自动问答系统上线]]></title>
    <link href="http://www.zphj1987.com/2016/05/21/%E7%A3%A8%E7%A3%A8%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%BA%BF/"/>
    <id>http://www.zphj1987.com/2016/05/21/磨磨自动问答系统上线/</id>
    <published>2016-05-20T16:22:38.000Z</published>
    <updated>2016-05-20T16:49:07.273Z</updated>
    <content type="html"><![CDATA[<p>新上线磨磨自动问答系统，系统也是在自我学习当中，欢迎大家踊跃提问，生命不息折腾不止</p>
<p>本问答系统支持自动回答，有时会在线回答</p>
<p>测试阶段，欢迎吐槽</p>
<a id="more"></a>
<p>使用方法：</p>
<blockquote>
<p>戳图片下面的链接</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/ki000574lo1su5fnvqtzsshq/dribbbb.gif" alt=""></p>
<p><a href="http://v3.faqrobot.org/robot/faqrobot.html?sysNum=146375952481212447" target="_blank" rel="external">磨磨自动问答系统</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>新上线磨磨自动问答系统，系统也是在自我学习当中，欢迎大家踊跃提问，生命不息折腾不止</p>
<p>本问答系统支持自动回答，有时会在线回答</p>
<p>测试阶段，欢迎吐槽</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的jewel新支持的rbd-nbd]]></title>
    <link href="http://www.zphj1987.com/2016/05/19/ceph%E7%9A%84jewel%E6%96%B0%E6%94%AF%E6%8C%81%E7%9A%84rbd-nbd/"/>
    <id>http://www.zphj1987.com/2016/05/19/ceph的jewel新支持的rbd-nbd/</id>
    <published>2016-05-19T04:41:11.000Z</published>
    <updated>2016-05-20T12:56:25.438Z</updated>
    <content type="html"><![CDATA[<p>jewel版本新增加了一个驱动NBD，允许librbd实现一个内核级别的rbd</p>
<p>NBD相比较于kernel rbd：</p>
<ul>
<li>rbd-ko是根据内核主线走的，升级kernel</li>
<li>rbd需要升级到相应的内核，改动太大</li>
<li>rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd</li>
</ul>
<p>rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障<br><a id="more"></a></p>
<h3 id="怎么理解用户态和内核态？">怎么理解用户态和内核态？</h3><ul>
<li>librbd就是用户态，一般的kvm对接的就是librbd的</li>
<li>kernel rbd就是内核态，这个是一个内核模块，是内核直接与osd交互的，一般来说内核态的性能会优于用户态</li>
</ul>
<h2 id="下面来做下基本的操作：">下面来做下基本的操作：</h2><h3 id="1、创建一个image">1、创建一个image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd create testnbdrbd -s 10G</span></span><br></pre></td></tr></table></figure>
<h3 id="2、映射这个image">2、映射这个image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd map rbd/testnbdrbd</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure>
<h3 id="3、查询已经映射的nbd">3、查询已经映射的nbd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd list-mapped</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure>
<p>上面说了这么多，那么来点直观的认识,nbd带来的好处<br>查询下image的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info testnbdrbd</span></span><br><span class="line">rbd image <span class="string">'testnbdrbd'</span>:</span><br><span class="line">	size <span class="number">10240</span> MB <span class="keyword">in</span> <span class="number">2560</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10</span>ad2ae8944a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>jewel版本默认开启了features: layering, exclusive-lock, object-map, fast-diff, deep-flatten这么多的属性，而这些属性是kernel-rbd还不支持的</p>
</blockquote>
<p>所以做rbd map的时候就会出现下面的问题:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map  testnbdrbd</span></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature <span class="built_in">set</span> mismatch. You can <span class="built_in">disable</span> features unsupported by the kernel with <span class="string">"rbd feature disable"</span>.</span><br><span class="line">In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">"dmesg | tail"</span> or so.</span><br><span class="line">rbd: map failed: (<span class="number">6</span>) No such device or address</span><br></pre></td></tr></table></figure></p>
<p>如果非要用，就默认禁用掉这些属性，在配置文件增加</p>
<blockquote>
<p>rbd_default_features = 3</p>
</blockquote>
<p>那么现在开启属性还行想用块设备方式怎么用，就可以用nbd了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd map rbd/testnbdrbd</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure></p>
<p>这样就可以用了。不用担心接口的问题了,因为只要librbd支持的属性，nbd就默认支持了</p>
<p>rbd几种常用的模式和新模式图：<br><img src="http://static.zybuluo.com/zphj1987/33s6u70sw4qhgqvfhvwe5d5b/nbd.png" alt="nbd.png-40.5kB"></p>
<h3 id="本篇ceph版本">本篇ceph版本</h3><blockquote>
<p>ceph version 10.2.1 (3a66dd4f30852819c1bdaa8ec23c795d4ad77269)</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>jewel版本新增加了一个驱动NBD，允许librbd实现一个内核级别的rbd</p>
<p>NBD相比较于kernel rbd：</p>
<ul>
<li>rbd-ko是根据内核主线走的，升级kernel</li>
<li>rbd需要升级到相应的内核，改动太大</li>
<li>rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd</li>
</ul>
<p>rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的分答-付费语音回答问题]]></title>
    <link href="http://www.zphj1987.com/2016/05/18/%E6%88%91%E7%9A%84%E5%88%86%E7%AD%94-%E4%BB%98%E8%B4%B9%E8%AF%AD%E9%9F%B3%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98/"/>
    <id>http://www.zphj1987.com/2016/05/18/我的分答-付费语音回答问题/</id>
    <published>2016-05-17T17:34:06.000Z</published>
    <updated>2016-05-17T17:37:29.900Z</updated>
    <content type="html"><![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 10，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>
<a id="more"></a>
<center><br><img src="http://static.zybuluo.com/zphj1987/qxfdnyf1c6o1hd4irdaz7xp4/liantu.png" alt="微信扫一扫"><br></center>




]]></content>
    <summary type="html">
    <![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 10，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[支持jewel版本的calamari]]></title>
    <link href="http://www.zphj1987.com/2016/05/16/%E6%94%AF%E6%8C%81jewel%E7%89%88%E6%9C%AC%E7%9A%84calamari/"/>
    <id>http://www.zphj1987.com/2016/05/16/支持jewel版本的calamari/</id>
    <published>2016-05-15T16:48:46.000Z</published>
    <updated>2016-05-15T16:57:48.821Z</updated>
    <content type="html"><![CDATA[<p>之前测试了下，发现calamari不支持jewel版本的，是因为接口了有了一些变化，在提出这个问题后，作者给出了回答，说肯定会支持的，并且做了一点小的改动，就可以支持了，这个作者merge了到了github的一些分支当中，但是还没有merge到最新的1.4的分支合master分支当中，这个可能是因为1.4还在做一些功能的开发</p>
<p>我使用作者的修改好的分支打好了包，直接可以使用，测试了ubuntu14.04 和centos 7的版本都可以使用，下面是百度云的链接，欢迎使用和测试</p>
<a id="more"></a>
<h3 id="ubuntu版本下载地址：">ubuntu版本下载地址：</h3><p><a href="http://pan.baidu.com/s/1cmKVoi" target="_blank" rel="external">ubuntu-jewel-calamari</a><br>密码：jpra</p>
<h3 id="centos7版本下载地址">centos7版本下载地址</h3><p><a href="http://pan.baidu.com/s/1eSa15B0" target="_blank" rel="external">centos-jewel-calamari</a><br>密码：5514</p>
<blockquote>
<p>如果您觉得本篇资源对您有用，欢迎通过下面的打赏通道进行打赏</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>之前测试了下，发现calamari不支持jewel版本的，是因为接口了有了一些变化，在提出这个问题后，作者给出了回答，说肯定会支持的，并且做了一点小的改动，就可以支持了，这个作者merge了到了github的一些分支当中，但是还没有merge到最新的1.4的分支合master分支当中，这个可能是因为1.4还在做一些功能的开发</p>
<p>我使用作者的修改好的分支打好了包，直接可以使用，测试了ubuntu14.04 和centos 7的版本都可以使用，下面是百度云的链接，欢迎使用和测试</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph卡在active+remapped状态]]></title>
    <link href="http://www.zphj1987.com/2016/05/14/ceph%E5%8D%A1%E5%9C%A8active-remapped%E7%8A%B6%E6%80%81/"/>
    <id>http://www.zphj1987.com/2016/05/14/ceph卡在active-remapped状态/</id>
    <published>2016-05-13T16:12:26.000Z</published>
    <updated>2016-05-13T16:15:13.519Z</updated>
    <content type="html"><![CDATA[<p>最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常</p>
<p>这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd</p>
<ul>
<li>一台机器上面的磁盘的容量不一样，有的3T，有的1T</li>
<li>两台主机上面的OSD个数不一样，有的5个，有的2个<a id="more"></a>
</li>
</ul>
<p>这样会造成主机的crush 的weight的差别很大的问题，以及分布算法上的不平衡问题，建议对于一个存储池来说，它所映射的osd至少需要是磁盘大小一致和个数一致的</p>
<p>这个问题我在我的环境下做了复现，确实有卡在remapped的问题</p>
<h3 id="出现这个情况一般是什么操作引起的？">出现这个情况一般是什么操作引起的？</h3><p>做osd的reweight的操作引起的，这个因为一般在做reweight的操作的时候，根据算法，这个上面的pg是会尽量分布在这个主机上的，而crush reweight不变的情况下，去修改osd 的reweight的时候，可能算法上会出现无法映射的问题</p>
<h3 id="怎么解决这个问题？">怎么解决这个问题？</h3><p>直接做osd crush reweigh的调整即可避免这个问题，这个straw算法里面还是有点小问题的，在调整某个因子的时候会引起整个因子的变动</p>
<blockquote>
<p>之前看到过sage在回复这种remapped问题的时候，都是不把这个归到bug里面去的，这个我也认为是配置问题引起的极端的问题，正常情况下都能避免的</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常</p>
<p>这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd</p>
<ul>
<li>一台机器上面的磁盘的容量不一样，有的3T，有的1T</li>
<li>两台主机上面的OSD个数不一样，有的5个，有的2个]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何开启内网的外网访问]]></title>
    <link href="http://www.zphj1987.com/2016/05/09/%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E5%86%85%E7%BD%91%E7%9A%84%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE/"/>
    <id>http://www.zphj1987.com/2016/05/09/如何开启内网的外网访问/</id>
    <published>2016-05-09T14:27:28.000Z</published>
    <updated>2016-05-09T14:52:45.469Z</updated>
    <content type="html"><![CDATA[<h3 id="为什么不要用QQ远程">为什么不要用QQ远程</h3><p>很多人在涉及到需要ssh远程的时候，第一印象应该就是QQ远程，这个是一般的普通用户都知道的方式，这个方式我为什么不推荐呢</p>
<ul>
<li>QQ是图形界面的远程，远程的操作在网络不好的时候很容易卡顿</li>
<li>窗口大小无法自适应</li>
<li>无法传递一些按键，比如需要Esc退出vim，变成了退出远程窗口的最大化</li>
<li>QQ远程适合帮助家里人处理一些界面上或者操作上的小问题</li>
</ul>
<a id="more"></a>
<h3 id="为什么使用Teamviver">为什么使用Teamviver</h3><p>那么还有什么替代么，相信有几年上班经验的人或者有过远程运维经验的人，都知道Teamviver，这个软件的出现，减少了多少的路途奔波，为什么推荐这个</p>
<ul>
<li>远程画面自适应</li>
<li>方便传递文件</li>
<li>方便开启守护模式，随时可以连接，无需对方有人</li>
<li>画面延时非常小</li>
</ul>
<p><a href="http://www.teamviewer.com/zhCN/" target="_blank" rel="external">Teamviver</a>下载链接<br>界面如下：<br><img src="http://static.zybuluo.com/zphj1987/f2k1n6th65unbijxpfj5dyvx/teamviver.png" alt="Teamviver"><br>适合远程的windows机器上正好有需要调试的应用的情况</p>
<h3 id="还有谁？">还有谁？</h3><p>其实对于这种远程的软件，需要的是连接数，需要的流量并不大，国内在这一刻唯一有所作为的也就是花生壳了，在提供免费的同时也提供更高级的企业级别服务，当然一般情况下免费的也能满足我们的需求</p>
<h4 id="长啥样？">长啥样？</h4><p><img src="http://static.zybuluo.com/zphj1987/tj157zs3zyiek4i697btrnvr/haushengke.png" alt="花生壳"><br>这个是最新的3.0版本的，安装包非常小，操作非常简单，比起以前刚开始用的时候优化了太多<br>下载地址：<a href="http://www.oray.com/activity/160302/?ici=act_160302&amp;icn=hsk_home-banner" target="_blank" rel="external">花生壳</a><br>点击内网映射就可以开启内网的端口映射了，根据注册的账号提供的免费域名和一个随机端口，就可以ssh到内网的指定的机器了，非常方便远程linux服务器的调试</p>
<p>上面介绍了3种远程的软件，一般情况下，需要操作界面的就推荐使用teamviver,需要调试服务器的就推荐使用花生壳做端口映射，或者有固定的IP的时候，也可以直接做端口映射，这个看具体的需求了</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>



]]></content>
    <summary type="html">
    <![CDATA[<h3 id="为什么不要用QQ远程">为什么不要用QQ远程</h3><p>很多人在涉及到需要ssh远程的时候，第一印象应该就是QQ远程，这个是一般的普通用户都知道的方式，这个方式我为什么不推荐呢</p>
<ul>
<li>QQ是图形界面的远程，远程的操作在网络不好的时候很容易卡顿</li>
<li>窗口大小无法自适应</li>
<li>无法传递一些按键，比如需要Esc退出vim，变成了退出远程窗口的最大化</li>
<li>QQ远程适合帮助家里人处理一些界面上或者操作上的小问题</li>
</ul>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[calamari不支持ceph的jewel版本]]></title>
    <link href="http://www.zphj1987.com/2016/05/07/calamari%E4%B8%8D%E6%94%AF%E6%8C%81ceph%E7%9A%84jewel%E7%89%88%E6%9C%AC/"/>
    <id>http://www.zphj1987.com/2016/05/07/calamari不支持ceph的jewel版本/</id>
    <published>2016-05-07T09:51:07.000Z</published>
    <updated>2016-05-07T10:03:52.278Z</updated>
    <content type="html"><![CDATA[<p>之前已经在0.94.x系列下进行了calamari的相关的打包和测试，基本没有太多问题，最近看到ceph发布了jewel的版本，想尝试下是否兼容，测试的结果是不兼容的</p>
<p>从调试的信息来看，ceph在jewel版本下对输出做了很大的改动，其中第一条，配置完成后，面板都无法显示，这个里面是因为ceph把原来的mdsmap这个字段在输出的时候改成了fsmap，造成里面的接口识别不到，这个改动以后，又发现restapi接口里面的osd地方又报错了</p>
<a id="more"></a>
<p>这个是管理接口的一些错误，系统信息的收集的地方diamond的接口也有改变</p>
<p>所以在目前环境下，要么自己对calamari做大量的改动去兼容新版本，要么就是等待发布以后的新版本，从版本的发布频率来看，红帽并不重视calamari这一块了，似乎是为后面出的统一平台做准备，之前calamari是作为红帽官方版本发布出去的</p>
<p>calmari在hammer版本下的使用还是不错了</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前已经在0.94.x系列下进行了calamari的相关的打包和测试，基本没有太多问题，最近看到ceph发布了jewel的版本，想尝试下是否兼容，测试的结果是不兼容的</p>
<p>从调试的信息来看，ceph在jewel版本下对输出做了很大的改动，其中第一条，配置完成后，面板都无法显示，这个里面是因为ceph把原来的mdsmap这个字段在输出的时候改成了fsmap，造成里面的接口识别不到，这个改动以后，又发现restapi接口里面的osd地方又报错了</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[性能数据的可视化]]></title>
    <link href="http://www.zphj1987.com/2016/04/27/%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://www.zphj1987.com/2016/04/27/性能数据的可视化/</id>
    <published>2016-04-27T15:25:03.000Z</published>
    <updated>2016-04-27T16:04:32.585Z</updated>
    <content type="html"><![CDATA[<p>在工作当中，很多时候我们在去分析一个性能的时候，会产生大量的数据，面对数据的时候我们一般应该会有以下几个处理过程</p>
<ul>
<li>直接肉眼看<br>这个属于第一个级别，比如监控系统负载的时候去用top观察，这个方法是我最开始经常使用的一种方法，这个适合异常的时候使用，但是实际上获取的数据是有偏差的</li>
<li>有监控系统<br>使用数据监控系统对需要监控的数据进行监控，这个前提是有一个监控系统，并且方便的去增加数据，可以根据需求去设定数据，这个监控系统有很多，能可视化的也很多，这篇文章就不做介绍</li>
<li>使用监控脚本采集数据，采用excel进行可视化<br>使用脚本收集大量的数据，然后将数据导入到excel当中，然后显示出来，这个是我们公司测试人员采用的方法，也是比较容易实现的一个方式</li>
</ul>
<a id="more"></a>
<p>也可能还有其他的方法，总之一图胜千言，通过图形来展示数据，会获取到更多的信息，我也一直在寻找一些方案来方便的展示数据，从目前的监控系统来看，一般的实现方法都是</p>
<ul>
<li>数据采集到数据库</li>
<li>使用图形展示数据库中间的问题</li>
</ul>
<p>我现在需要的一个功能就是，使用一个采集工具将数据收集起来，然后直接将数据输出为图片，这个图片的渲染是可以根据我的需要进行定制的，最近在研究web自动化测试的发现可以有办法对html进行渲染成图片，想到这个地方可以跟这个地方进行结合</p>
<p>这里的思路是使用html+highchart方式进行数据的渲染，然后将页面导出成图片，最终做成一个简单的数据展示工具，并且为其他地方的提供数据图片</p>
<p>下面是这个工具渲染的图片效果：</p>
<p><img src="http://static.zybuluo.com/zphj1987/ixyicn1b5ps5m3x380nsbpz1/chart.png" alt=""></p>
<p>可以根据自己的需要去显示数据，后续会记录方法，这里暂时只记录一个思路</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在工作当中，很多时候我们在去分析一个性能的时候，会产生大量的数据，面对数据的时候我们一般应该会有以下几个处理过程</p>
<ul>
<li>直接肉眼看<br>这个属于第一个级别，比如监控系统负载的时候去用top观察，这个方法是我最开始经常使用的一种方法，这个适合异常的时候使用，但是实际上获取的数据是有偏差的</li>
<li>有监控系统<br>使用数据监控系统对需要监控的数据进行监控，这个前提是有一个监控系统，并且方便的去增加数据，可以根据需求去设定数据，这个监控系统有很多，能可视化的也很多，这篇文章就不做介绍</li>
<li>使用监控脚本采集数据，采用excel进行可视化<br>使用脚本收集大量的数据，然后将数据导入到excel当中，然后显示出来，这个是我们公司测试人员采用的方法，也是比较容易实现的一个方式</li>
</ul>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mon的稳定性问题]]></title>
    <link href="http://www.zphj1987.com/2016/04/25/mon%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/"/>
    <id>http://www.zphj1987.com/2016/04/25/mon的稳定性问题/</id>
    <published>2016-04-25T15:36:28.000Z</published>
    <updated>2016-04-25T15:51:59.949Z</updated>
    <content type="html"><![CDATA[<h3 id="MON的稳定性问题：">MON的稳定性问题：</h3><ul>
<li>mon的选举风暴影响客户端IO</li>
<li>LevelDB的暴涨</li>
<li>频繁的客户端请求的DDOS</li>
</ul>
<h4 id="mon选举风暴：">mon选举风暴：</h4><p>monmap会因为mon之间或者mon与客户端之间网络的影响或者消息传递的异常发生变化,从而触发选举<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="LevelDB的暴涨：">LevelDB的暴涨：</h4><p>LevelDB的大小会涨到几十GB然后影响了osd的请求<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="频繁的客户端请求的DDOS：">频繁的客户端请求的DDOS：</h4><p>mon的响应因为levelDB变慢或者选举风暴，都会造成客户端发出大量的消息流<br>让客户端操作失效，包括卷创建，rbd的连接</p>
<a id="more"></a>
<h3 id="解决办法：">解决办法：</h3><h4 id="LevelDB的暴涨的问题解决办法">LevelDB的暴涨的问题解决办法</h4><p>升级ceph的版本，这个在0.94.6版本解决了这个问题<br><img src="http://static.zybuluo.com/zphj1987/yjdvldb3xyxl84bwlc3cjaxw/leveldb.png" alt="leveldb.png-21.9kB"></p>
<h4 id="选举风暴问题解决办法">选举风暴问题解决办法</h4><blockquote>
<p>[mon]<br>mon_lease = 20 (default = 5)<br>mon_lease_renew_interval = 12 (default 3)<br>mon_lease_ack_timeout = 40 (default 10)<br>mon_accept_timeout = 40 (default 10)<br>[client]<br>mon_client_hunt_interval = 40 (defaiult 3)</p>
</blockquote>
<p>将mon的数据放置在ssd上，因为LevelDB存储了集群的 metadata,包括 osdmap, pgmap, monmap,clientID, authID etc 等等，很大的leveldb会有更长的查询时间，更长的IO等待，然后就是更慢的客户端请求</p>
<p>这个地方是增长了mon之间判断需要切换的时间，降低客户端的请求的频率，使用ssd加快查询的速度</p>
<hr>
<p>这个问题是一个不太容易发觉的问题，有时候就是ceph -s反应的很慢，但是很多时候可能体现的就是客户端出现请求缓慢，然后还找不到原因，所以说硬件的隔离是非常有必要的，不要为了省成本然后影响了整个环境的稳定性和性能，对于很大的环境mon需要用三台独立的机器，这个机器需要一定的内存和cpu，磁盘使用ssd，1U服务器就可以了，上面可以运行一些其他类似管理平台，或者一些监控服务什么的，混合在osd的机器上的时候，一旦OSD出现大量的数据迁移的时候，或者大量的请求的时候，会阻塞了消息，这个就是做方案的时候需要考虑的问题，当然在性能要求不那么高的时候将mon混合在osd上使用也是可以的，这个时候尽量有更多的内存和更好的磁盘性能也能减少一些负担</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="MON的稳定性问题：">MON的稳定性问题：</h3><ul>
<li>mon的选举风暴影响客户端IO</li>
<li>LevelDB的暴涨</li>
<li>频繁的客户端请求的DDOS</li>
</ul>
<h4 id="mon选举风暴：">mon选举风暴：</h4><p>monmap会因为mon之间或者mon与客户端之间网络的影响或者消息传递的异常发生变化,从而触发选举<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="LevelDB的暴涨：">LevelDB的暴涨：</h4><p>LevelDB的大小会涨到几十GB然后影响了osd的请求<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="频繁的客户端请求的DDOS：">频繁的客户端请求的DDOS：</h4><p>mon的响应因为levelDB变慢或者选举风暴，都会造成客户端发出大量的消息流<br>让客户端操作失效，包括卷创建，rbd的连接</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[backfill和recovery的最优值]]></title>
    <link href="http://www.zphj1987.com/2016/04/24/backfill%E5%92%8Crecovery%E7%9A%84%E6%9C%80%E4%BC%98%E5%80%BC/"/>
    <id>http://www.zphj1987.com/2016/04/24/backfill和recovery的最优值/</id>
    <published>2016-04-24T15:37:21.000Z</published>
    <updated>2016-04-24T15:43:36.247Z</updated>
    <content type="html"><![CDATA[<p>ceph在增加osd的时候会触发backfill，让数据得到平均，触发数据的迁移<br>ceph在移除osd的时候需要在节点上进行数据的恢复，也有数据的迁移和生成</p>
<p>只要是集群里面有数据的变动就会有网卡流量，cpu，内存等资源的占用，并且最重要的是还有磁盘的占用，这个客户端也是需要对磁盘进行访问的，当请求出现碰撞的时候，肯定会比正常的情况下要慢很多，而且还有可能因为资源方面的原因而引起机器down机等异常状况的出现</p>
<p>主要引起的问题可能：</p>
<ul>
<li>在peering的时候 block 了IO请求</li>
<li>在backfill的引起了slow requests</li>
<li>上面的两个情况会引起客户端的降速和出现soft lockup</li>
</ul>
<p>这个在一般情况下会出现不同的需求：</p>
<ol>
<li>慢点可以一定不能出问题，不能中断业务</li>
<li>越快迁移完越好，早点结束维护服务</li>
<li>需要又快又不能影响业务</li>
</ol>
<a id="more"></a>
<p>这个需要根据自己可以掌控的程度来进行控制，首先环境的不同，影响不同，迁移数据量，网卡的带宽都是重要的影响因素，从整体上可以根据自己的环境按照上面的三个要求中的一个进行控制</p>
<p>上面的三种情况：<br>第一个慢点迁移不能出问题，这个处理方式比较简单，直接将相关参数控制到最低的值，这个能保证业务的影响最低，但是带来的影响就是迁移需要很久的时间，可能长达几十个小时</p>
<p>第二个越快越好就是用默认的参数或者加大参数，然后观察这个迁移过程中的资源的占用情况</p>
<p>第三个就是需要在自己的环境下进行多测试验证这个参数，本篇主要就是根据思科的测试出来的参数进行分析</p>
<p>下面的参数是思科测试出来的值：</p>
<blockquote>
<p>osd recovery max active = 3    （default    : 15)<br>osd    recovery op priority = 3    (default : 10)<br>osd    max    backfills = 1 (default : 10)    </p>
</blockquote>
<h3 id="测试过程的数据图">测试过程的数据图</h3><p><img src="http://static.zybuluo.com/zphj1987/u6iiuglqxdx47nuogt9r6a0g/back%E5%BD%B1%E5%93%8D.png" alt="back影响.png-58.2kB"></p>
<p>这个图开始的时候我也没太明白，后来多看下就理解了，实际上在很多情况下，一个因素的变化是会引起其他两个因素的变化，而这两个因素是一个正面的因素和一个负面的因素，而找到这个平衡值就是最优的情况，在这里的因素包括：<br>max-backfill和max-recovery :迁移相关参数<br>MTTR（mean time to recovery）:失效恢复时间，也就是迁移完成<br>Soft Lockup:前面虚拟机出现的soft lockup，也可以理解为对前端的影响</p>
<p>测试环境一致，都是 down 掉10%的osd进行恢复：<br>在迁移参数最低的时候，没有出现soft lockup ，也就是最低迁移参数的时候，影响最小，恢复使用了45分钟<br>随着迁移相关参数调大的时候，迁移的时间的曲线是先降低，在到达一定的值后又开始增加（这个地方可能是迁移过大出现了前端io锁住，然后影响了迁移速度）<br>随着迁移相关参数的调大，出现soft lockup的情况是增加的</p>
<p>从测试的曲线来看，在2-6之间是出现的最优值，也就是出现异常的情况概率最低，并且迁移速度最快，最终选择了一组最优的值 ：</p>
<blockquote>
<p>osd recovery max active = 3    （default    : 15)<br>osd    recovery op priority = 3    (default : 10)<br>osd    max    backfills = 1 (default : 10)</p>
</blockquote>
<p>这个值是思科的测试出来的值，这个值可以根据自己的需要进行取用，大概的情况是这样</p>
<ul>
<li>完全无法把控就把参数调整到最低</li>
<li>使用思科的推荐值</li>
<li>根据自己的环境测出自己环境的最优值</li>
</ul>
<p>很多参数是别人根据自己的环境测试出来的，很多情况并不是通用的，得到别人测试的思路是最重要的，然后消化后自己根据自己的需要得出自己的值</p>
<hr>
<h3 id="说点自己最近的感想">说点自己最近的感想</h3><p>根据自己的观察和自己的经验，所有的知识都是需要自己主动去获取主动的去消化，然后去实践的，在任何地方没有说通过传授知识，你就能学会了，公司的程序员的技术也是自己主动的去学习的，所谓的经验也只能是告诉你一些方法，而且你也没办法要求任何人与你一样的努力，一样的对你所做的东西感兴趣，认同你的观点，很多时候需要的是技术的碰撞，在一家公司需要的是员工能够完成你的事情，所以我们要尊重努力的员工，这类员工非常努力，但是可能无法达到你的要求，这个需要鼓励，还有一种是效率非常高的员工，这类员工能够轻松的完成任何事情，这类员工可以给与充分的自由，最终以时间以及结果双向评估员工对公司的贡献</p>
<p>最近tinyfool老师在进行ios开发的一个分享的时候，一堆想获取的干活的人去听，而tinyfool老师在这个分享会上通知了自己公司破产解散的事情，宣告再次的失败，而一些来想获取干货的人却开喷了，说没有获取到任何干货，这些人想获取的干货就是拿来直接干的货，而tinyfool老师分享的经验，包括在最后宣告解散的时候准备开源自己的东西，极力的推荐自己的员工是多么的优秀，还有其他的一些东西，这些其实都是干货，引用高春辉老师对这件事的看法的一段话：</p>
<blockquote>
<p>很多人可能相比之下，觉得代码语言这些硬技能最重要。其实我和我周边的朋友基本都认为，软技能才最重要，其实人的智商都差不多，再笨也不会笨很多，但如何待人接物，如何对待同事和朋友，如何高效利用时间，内心对成功的渴望，还有是否有责任心和荣誉感还有成就感这些，包括个人兴趣以及性格，这些是很难在进入社会之后再改变的了，除非有重大事情发生，否则很难很难改变。所以多数情况下，十年后的你的境遇，其实是十年前就已经决定的了  —高春辉</p>
</blockquote>
<p>任何公司任何员工都不可能十全十美，其实有时候换位思考一下，站在企业角度想一下，公司需要什么样的员工，站在员工的角度想一下，想要公司怎么的为自己保证最大利益，如果能够做到相互的价值观一致，就能处于一个和谐的状态了</p>
<blockquote>
<p>今后会写更多文字来分享一些东西，这些也不再仅限于ceph，这也是一个二次消化的过程，欢迎大家点评讨论</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>ceph在增加osd的时候会触发backfill，让数据得到平均，触发数据的迁移<br>ceph在移除osd的时候需要在节点上进行数据的恢复，也有数据的迁移和生成</p>
<p>只要是集群里面有数据的变动就会有网卡流量，cpu，内存等资源的占用，并且最重要的是还有磁盘的占用，这个客户端也是需要对磁盘进行访问的，当请求出现碰撞的时候，肯定会比正常的情况下要慢很多，而且还有可能因为资源方面的原因而引起机器down机等异常状况的出现</p>
<p>主要引起的问题可能：</p>
<ul>
<li>在peering的时候 block 了IO请求</li>
<li>在backfill的引起了slow requests</li>
<li>上面的两个情况会引起客户端的降速和出现soft lockup</li>
</ul>
<p>这个在一般情况下会出现不同的需求：</p>
<ol>
<li>慢点可以一定不能出问题，不能中断业务</li>
<li>越快迁移完越好，早点结束维护服务</li>
<li>需要又快又不能影响业务</li>
</ol>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[福昕pdf加载慢崩溃的解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/04/23/%E7%A6%8F%E6%98%95pdf%E5%8A%A0%E8%BD%BD%E6%85%A2%E5%B4%A9%E6%BA%83%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/04/23/福昕pdf加载慢崩溃的解决办法/</id>
    <published>2016-04-23T03:01:48.000Z</published>
    <updated>2016-04-23T03:09:23.612Z</updated>
    <content type="html"><![CDATA[<p>以前使用福昕pdf阅读器一直没有问题，但是最近打开网上下载的各种pdf都出现了崩溃的情况，并且我的办公电脑和家里的电脑都出现了情况，只能说软件开发的越来越大，最基本的功能反而保证不了了，那么就自己来找解决办法了，通过网上的一些搜寻，终于找到了办法，这里做下记录以备不时之需，对于其他做软件的都有这个问题，我们不要忘了我们为了什么出发，一切都是为了解决问题</p>
<a id="more"></a>
<p>福昕阅读器启动速度变慢的罪魁祸首就是加载项，可以删除4个文件夹：</p>
<ul>
<li>plugins（变慢的主要文件夹）</li>
<li>ShellExtensions（扩展功能）</li>
<li>Advertisement（广告文件夹）</li>
<li>FoxitCloud（福昕云服务，如果用户使用他们家的这个服务，就不要删）。</li>
</ul>
<p>删除之后，再去试着打开PDF文件，速度要快了很多，也没有崩溃的问题了</p>
<p>打开福昕阅读器—&gt;文件—&gt;偏好设置—&gt;常规，最下面一行把“显示启动页面”和“显示广告条”的打钩都去掉</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>以前使用福昕pdf阅读器一直没有问题，但是最近打开网上下载的各种pdf都出现了崩溃的情况，并且我的办公电脑和家里的电脑都出现了情况，只能说软件开发的越来越大，最基本的功能反而保证不了了，那么就自己来找解决办法了，通过网上的一些搜寻，终于找到了办法，这里做下记录以备不时之需，对于其他做软件的都有这个问题，我们不要忘了我们为了什么出发，一切都是为了解决问题</p>]]>
    
    </summary>
    
      <category term="其他" scheme="http://www.zphj1987.com/tags/%E5%85%B6%E4%BB%96/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[inkscope完整安装配置]]></title>
    <link href="http://www.zphj1987.com/2016/04/19/inkscope%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.zphj1987.com/2016/04/19/inkscope完整安装配置/</id>
    <published>2016-04-19T05:52:34.000Z</published>
    <updated>2016-05-19T04:01:49.441Z</updated>
    <content type="html"><![CDATA[<h3 id="准备centos7基础系统">准备centos7基础系统</h3><p>首先安装基础系统centos7 在安装选项那里选择base web server ，选择其他的也可以，选择mini安装会缺很多常用的软件包，后续需要一个个安装比较麻烦</p>
<h3 id="关闭防火墙相关">关闭防火墙相关</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># setenforce 0</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># systemctl disable firewalld</span></span><br></pre></td></tr></table></figure>
<h3 id="更新源相关的">更新源相关的</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># rm -rf /etc/yum.repos.d/*.repo</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>修改里面的系统版本为7.2.1511,当前用的centos的版本的的yum源可能已经清空了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo</span></span><br></pre></td></tr></table></figure></p>
<p>添加ceph源<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># vim /etc/yum.repos.d/ceph.repo</span></span><br></pre></td></tr></table></figure></p>
<p>添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/x86_64/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/noarch/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h3 id="准备inkscope的相关安装包">准备inkscope的相关安装包</h3><p>下载相关软件包的脚本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/flake8-<span class="number">2.3</span>.<span class="number">0</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-admviz-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-cephprobe-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-cephrestapi-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-common-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-monitoring-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/inkscope-sysprobe-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/pep8-<span class="number">1.5</span>.<span class="number">7</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/pyflakes-<span class="number">0.8</span>.<span class="number">1</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-babel-<span class="number">0.9</span>.<span class="number">6</span>-<span class="number">8</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-boto-<span class="number">2.34</span>.<span class="number">0</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-bson-<span class="number">2.5</span>.<span class="number">2</span>-<span class="number">2</span>.el7.x86_64.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-flask-<span class="number">0.10</span>.<span class="number">1</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-flask-doc-<span class="number">0.10</span>.<span class="number">1</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-itsdangerous-<span class="number">0.23</span>-<span class="number">2</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-jinja2-<span class="number">2.7</span>.<span class="number">2</span>-<span class="number">2</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-markupsafe-<span class="number">0.11</span>-<span class="number">10</span>.el7.x86_64.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-pip-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-psutil-<span class="number">2.2</span>.<span class="number">0</span>-<span class="number">1</span>.x86_64.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-pymongo-<span class="number">2.5</span>.<span class="number">2</span>-<span class="number">2</span>.el7.x86_64.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-rsa-<span class="number">3.1</span>.<span class="number">1</span>-<span class="number">5</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-simplejson-<span class="number">3.3</span>.<span class="number">3</span>-<span class="number">1</span>.el7.x86_64.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-werkzeug-<span class="number">0.9</span>.<span class="number">1</span>-<span class="number">2</span>.el7.noarch.rpm</span><br><span class="line">wget https://raw.githubusercontent.com/inkscope/inkscope-packaging/master/RPMS/python-werkzeug-doc-<span class="number">0.9</span>.<span class="number">1</span>-<span class="number">2</span>.el7.noarch.rpm</span><br><span class="line">wget http://copr-be.cloud.fedoraproject.org/results/merlinthp/el7-mypa/epel-<span class="number">7</span>-x86_64/python-flask-login-<span class="number">0.2</span>.<span class="number">11</span>-<span class="number">1</span>.el7.centos/python-flask-login-<span class="number">0.2</span>.<span class="number">11</span>-<span class="number">1</span>.el7.centos.noarch.rpm</span><br></pre></td></tr></table></figure></p>
<p>已经离线下载好了，可以直接用下面的百度云里面的安装包<br>链接：<a href="http://pan.baidu.com/s/1czANCi" target="_blank" rel="external">http://pan.baidu.com/s/1czANCi</a> 密码：qw3k</p>
<h3 id="软件包安装的位置">软件包安装的位置</h3><ul>
<li>集群的mon节点 cephprobe ceph-rest-api </li>
<li>集群的osd节点 sysprobe inkscope-common</li>
<li>inkscope管理控制台    inkscope-admviz inkscope-monitor mongodb</li>
</ul>
<h3 id="开始安装inkscope管理节点">开始安装inkscope管理节点</h3><h4 id="安装下ceph(非必选建议安装下)">安装下ceph(非必选建议安装下)</h4><p>这个地方可以选择安装也可以选择不安装，这个方便查看，也有可能管理节点就在ceph的某个节点上，这个自己随意<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># yum install ceph ceph-radosgw</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># yum install python-ceph</span></span><br></pre></td></tr></table></figure></p>
<p>我的环境是想单独一台机器做管理平台控制节点</p>
<p>那么把ceph集群中的这两个文件拷贝到这个管理节点的/etc/ceph/下面</p>
<ul>
<li>ceph.client.admin.keyring </li>
<li>ceph.conf</li>
</ul>
<p>如果你管理节点本身就在ceph集群当中就不需要做了<br>检查ceph -s是否有输出，有集群输出就是正常的</p>
<h4 id="安装依赖包">安装依赖包</h4><p>安装apache2和其它几个包(用于配置web服务器)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># yum install httpd -y</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># yum install python-setuptools</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># yum install lshw</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># yum install mod_wsgi</span></span><br></pre></td></tr></table></figure></p>
<h4 id="安装MongoDb(用于存储收集到的数据的)">安装MongoDb(用于存储收集到的数据的)</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># yum install mongodb -y</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># yum install mongodb-server -y</span></span><br></pre></td></tr></table></figure>
<p>修改配置文件，让mongdb可以远程访问<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># vim /etc/mongod.conf</span></span><br></pre></td></tr></table></figure></p>
<p>bind_ip = 127.0.0.1修改为 bind_ip = 0.0.0.0</p>
<h4 id="启动mongodb服务">启动mongodb服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># systemctl start mongod.service</span></span><br></pre></td></tr></table></figure>
<h4 id="检查服务">检查服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope bao]<span class="comment"># netstat -tunlp|grep mongod</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">27017</span>           <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">11836</span>/mongod</span><br></pre></td></tr></table></figure>
<h4 id="安装inkscope相关的包">安装inkscope相关的包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">安装（有些安装ceph的时候已经安装了）</span><br><span class="line">rpm -ivh flake8-<span class="number">2.3</span>.<span class="number">0</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">rpm -ivh pep8-<span class="number">1.5</span>.<span class="number">7</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">rpm -ivh pyflakes-<span class="number">0.8</span>.<span class="number">1</span>-<span class="number">1</span>.noarch.rpm</span><br><span class="line">rpm -ivh python-rsa-<span class="number">3.1</span>.<span class="number">1</span>-<span class="number">5</span>.el7.noarch.rpm</span><br><span class="line">rpm -ivh python-boto-<span class="number">2.34</span>.<span class="number">0</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">rpm -ivh python-bson-<span class="number">2.5</span>.<span class="number">2</span>-<span class="number">2</span>.el7.x86_64.rpm</span><br><span class="line">rpm -ivh python-flask-doc-<span class="number">0.10</span>.<span class="number">1</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">rpm -ivh python-pip-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">4</span>.el7.noarch.rpm</span><br><span class="line">rpm -ivh python-psutil-<span class="number">2.2</span>.<span class="number">0</span>-<span class="number">1</span>.x86_64.rpm</span><br><span class="line">rpm -ivh python-pymongo-<span class="number">2.5</span>.<span class="number">2</span>-<span class="number">2</span>.el7.x86_64.rpm</span><br><span class="line">rpm -ivh python-simplejson-<span class="number">3.3</span>.<span class="number">3</span>-<span class="number">1</span>.el7.x86_64.rpm</span><br><span class="line">rpm -ivh python-werkzeug-doc-<span class="number">0.9</span>.<span class="number">1</span>-<span class="number">2</span>.el7.noarch.rpm </span><br><span class="line">rpm -ivh inkscope-common-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">rpm -ivh inkscope-sysprobe-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">rpm -ivh inkscope-monitoring-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">rpm -ivh inkscope-cephrestapi-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm</span><br><span class="line">rpm -ivh inkscope-cephprobe-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm </span><br><span class="line">rpm -ivh inkscope-admviz-<span class="number">1.3</span>.<span class="number">1</span>-<span class="number">2</span>.noarch.rpm </span><br><span class="line"><span class="comment">##rpm -ivh --nodeps inkscope-admviz-1.3.1-2.noarch.rpm</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>这个里面因为python-ceph在10.x已经更名了，所以在确保其他已经安装好的情况下使用忽略依赖进行安装，ceph0.94没有这个问题</p>
</blockquote>
<h4 id="配置权限">配置权限</h4><p>需要创建一个client.restapi的用户 拥有权限 [mds] allow, [mon] allow <em> , [osd] allow </em><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth get-or-create client.restapi mds <span class="string">'allow'</span> osd <span class="string">'allow *'</span> mon <span class="string">'allow *'</span> &gt; /etc/ceph/ceph.client.restapi.keyring</span><br><span class="line"></span><br><span class="line">chmod <span class="number">644</span> /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">chmod <span class="number">644</span> /etc/ceph/ceph.client.restapi.keyring</span><br></pre></td></tr></table></figure></p>
<p>在/etc/ceph/ceph.conf配置文件里面添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.restapi]</span><br><span class="line">    <span class="built_in">log</span>_file = /dev/null</span><br><span class="line">    keyring = /etc/ceph/ceph.client.restapi.keyring</span><br></pre></td></tr></table></figure></p>
<h4 id="配置httpd">配置httpd</h4><p> /etc/httpd/conf/httpd.conf 中间添加一条</p>
<blockquote>
<p>Listen 8080</p>
</blockquote>
<p>因为inkscope的web 默认采用虚拟主机的方式使用了8080端口</p>
<h3 id="给目录访问权限(存储日志使用)">给目录访问权限(存储日志使用)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># chmod 777 -R /var/log/ceph/</span></span><br></pre></td></tr></table></figure>
<p>修改ceph-rest-api的地址<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/httpd/conf.d/inkScope.conf</span><br></pre></td></tr></table></figure></p>
<p>修改为：</p>
<blockquote>
<p>ProxyPass /ceph-rest-api/ <a href="http://192.168.222.100:8080/ceph_rest_api/api/v0.1/" target="_blank" rel="external">http://192.168.222.100:8080/ceph_rest_api/api/v0.1/</a></p>
</blockquote>
<p>这个地方是写的这台管理节点的地址和端口，因为本机实现了wsgi的方式的rest-api的接口，不是用的集群的5000的端口，而是直接使用web配置的8080的端口</p>
<h4 id="启动httpd服务">启动httpd服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># systemctl restart httpd</span></span><br></pre></td></tr></table></figure>
<p>检查ceph-rest-api是否能访问，地址是 <a href="http://192.168.222.100:8080/ceph_rest_api/api/v0.1/" target="_blank" rel="external">http://192.168.222.100:8080/ceph_rest_api/api/v0.1/</a></p>
<p><img src="http://static.zybuluo.com/zphj1987/4s2rmktfiwp1qahs0f1z7hf7/rest-api.png" alt="rest-api.png-22.5kB"></p>
<p>修改/opt/inkscope/etc/inkscope.conf配置文件，<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">"ceph_rest_api"</span>: <span class="string">"192.168.222.100:8080"</span>,</span><br><span class="line"><span class="string">"ceph_rest_api_subfolder"</span>: <span class="string">"ceph_rest_api"</span>,</span><br><span class="line"><span class="string">"mongodb_host"</span> : <span class="string">"192.168.222.100"</span>,</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意上面的地址不要在ip和地址前面加<a href="http://否则获取不到信息的，注意使用的是inkscope的web端口8080，也就是上面配置好的rest-api的端口" target="_blank" rel="external">http://否则获取不到信息的，注意使用的是inkscope的web端口8080，也就是上面配置好的rest-api的端口</a></p>
</blockquote>
<h4 id="启动cephprobe_服务">启动cephprobe 服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># /etc/init.d/cephprobe restart</span></span><br></pre></td></tr></table></figure>
<p>现在就可以访问<br><a href="http://192.168.222.100:8080/inkscopeViz/index.html" target="_blank" rel="external">http://192.168.222.100:8080/inkscopeViz/index.html</a></p>
<p>这个是没有用户名密码的，我们为了安全采用以下用户名密码的方式,需要安装flask-login<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># rpm -ivh python-flask-login-0.2.11-1.el7.centos.noarch.rpm</span></span><br></pre></td></tr></table></figure></p>
<h4 id="重启httpd服务">重启httpd服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># systemctl restart httpd</span></span><br></pre></td></tr></table></figure>
<p> <img src="http://static.zybuluo.com/zphj1987/hasskxlkvn11atsgz5gbcym3/denglu.png" alt="denglu.png-35.5kB"></p>
<p>再次访问就需要用户名密码了</p>
<ul>
<li>默认用户名:admin  </li>
<li>默认密码:admin</li>
</ul>
<p>到这里最基本的管理平台配置就完成了</p>
<h3 id="配置sysprobe">配置sysprobe</h3><p>sysprobe是获取集群节点的主机的信息的</p>
<h4 id="安装">安装</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># rpm -ivh inkscope-common-1.3.1-2.noarch.rpm</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># rpm -ivh inkscope-sysprobe-1.3.1-2.noarch.rpm</span></span><br></pre></td></tr></table></figure>
<p>将主监控节点的inkscope配置文件拷贝到节点上<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># scp /opt/inkscope/etc/inkscope.conf 192.168.8.106:/opt/inkscope/etc/</span></span><br></pre></td></tr></table></figure></p>
<p>在osd节点启动sysprobe服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># /etc/init.d/sysprobe restart</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/wjjk9wy4krx7i6elpoxucjpu/probe.png" alt="probe.png-49.6kB"><br>正常情况就用上面这个页面检查配置的状况</p>
<p>cephprobe是用来或者集群的相关信息和操作的<br>sysprobe是获取节点的磁盘分区等相关信息的</p>
<p>基本节点的软件包配置完毕了，一些扩展功能也配置一下</p>
<h3 id="创建rgw相关的">创建rgw相关的</h3><h4 id="配置rgw网关">配置rgw网关</h4><p>在/etc/ceph/ceph.conf<br>添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw.gateway]</span><br><span class="line">host = inkscope</span><br><span class="line">rgw <span class="built_in">print</span> <span class="built_in">continue</span> = <span class="literal">false</span></span><br><span class="line">debug_rgw = <span class="number">0</span></span><br><span class="line">user = root</span><br></pre></td></tr></table></figure></p>
<h4 id="启动radosgw服务">启动radosgw服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># /etc/init.d/ceph-radosgw restart</span></span><br></pre></td></tr></table></figure>
<p>检查端口是否启动，默认是7480<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7480</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">113493</span>/radosgw</span><br></pre></td></tr></table></figure></p>
<h4 id="创建rgw使用的存储池并且添加到rgw">创建rgw使用的存储池并且添加到rgw</h4><p>rados mkpool .rgw.buckets 1024 1024<br>radosgw-admin pool add —pool .rgw.buckets</p>
<p>执行完后检查存储池情况,自动会创建了一些存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    <span class="number">548</span>G      <span class="number">548</span>G       <span class="number">70080</span>k          <span class="number">0.01</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd              <span class="number">0</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">0</span> </span><br><span class="line">    zp               <span class="number">1</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">0</span> </span><br><span class="line">    .rgw.root        <span class="number">2</span>       <span class="number">848</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">3</span> </span><br><span class="line">    .rgw.control     <span class="number">3</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">8</span> </span><br><span class="line">    .rgw             <span class="number">4</span>        <span class="number">24</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">1</span> </span><br><span class="line">    .rgw.gc          <span class="number">5</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G          <span class="number">32</span> </span><br><span class="line">    .users.uid       <span class="number">6</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">0</span> </span><br><span class="line">    .rgw.buckets     <span class="number">7</span>         <span class="number">0</span>         <span class="number">0</span>          <span class="number">274</span>G           <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h3 id="创建rgw的用户">创建rgw的用户</h3><p>这个用户是管理员用户，需要给很多权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># radosgw-admin user create --uid=inkscope --display-name="inkscope" --access-key=inkscope --secret=inkscope --access=full --caps="metadata=*;users=*;buckets=*"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="修改配置文件">修改配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># vim /opt/inkscope/etc/inkscope.conf</span></span><br></pre></td></tr></table></figure>
<p>修改下面的几项</p>
<pre><code class="bash"><span class="string">"radosgw_url"</span>: <span class="string">"http://192.168.222.100:7480"</span>,
<span class="string">"radosgw_admin"</span>: <span class="string">"admin"</span>,
<span class="string">"radosgw_key"</span>: <span class="string">"inkscope"</span>,
<span class="string">"radosgw_secret"</span>: <span class="string">"inkscope"</span>
</code></pre>
<p>radosgw_url为rgw的访问地址<br>radosgw_admin字段不更改就是admin<br>radosgw_key，radosgw_secret就是上面创建的那个密钥</p>
<h3 id="配置结束">配置结束</h3><p>基本按照上面的做法就可以配置完毕了，并且可以正常使用，之前搞错了一个地方就是那个restapi，这个地方可能是最开始的时候，这个地方是需要调用的原始的那个5000端口的api，然后基本操作都是可以做的，一些新开发的功能需要用到新的接口，就按照新的配置即可，inkscope从我开始关注到现在已经改进了很多，添加了sanky chart来显示pg的分布，里面的api接口也更加的丰富。并且提供了友好的安装方式，应该是目前最成熟的一种管理平台了</p>
<h3 id="展示">展示</h3><p>sanky chart显示pg<br><img src="http://static.zybuluo.com/zphj1987/xlcu9uj1syk40tgbug07e0dp/sankey.png" alt="sankey.png-500.1kB"></p>
<h4 id="视频展示">视频展示</h4><div class="video-container"> <object><br><param name="allowFullScreen" value="true"><param name="flashVars" value="id=23750026 "><param name="movie" value="http://i7.imgs.letv.com/player/swfPlayer.swf?autoplay=0"><embed src="http://i7.imgs.letv.com/player/swfPlayer.swf?autoplay=0" flashvars="id=23750026" allowfullscreen="true" type="application/x-shockwave-flash"></object><br></div>


<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="准备centos7基础系统">准备centos7基础系统</h3><p>首先安装基础系统centos7 在安装选项那里选择base web server ，选择其他的也可以，选择mini安装会缺很多常用的软件包，后续需要一个个安装比较麻烦</p>
<h3 id="关闭防火墙相关">关闭防火墙相关</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># setenforce 0</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># systemctl disable firewalld</span></span><br></pre></td></tr></table></figure>
<h3 id="更新源相关的">更新源相关的</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@inkscope ~]<span class="comment"># rm -rf /etc/yum.repos.d/*.repo</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@inkscope ~]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记一次不完全成功到成功的失效恢复(20160412)]]></title>
    <link href="http://www.zphj1987.com/2016/04/13/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%88%90%E5%8A%9F%E5%88%B0%E6%88%90%E5%8A%9F%E7%9A%84%E5%A4%B1%E6%95%88%E6%81%A2%E5%A4%8D-20160412/"/>
    <id>http://www.zphj1987.com/2016/04/13/记一次不完全成功到成功的失效恢复-20160412/</id>
    <published>2016-04-12T16:04:56.000Z</published>
    <updated>2016-04-17T14:58:54.474Z</updated>
    <content type="html"><![CDATA[<h3 id="更新：">更新：</h3><p>在经历了好几天后，失效的环境最终变成了可用状态，只能说有的时候不放弃还真是有点用的</p>
<hr>
<p>在不久前处理了一个故障恢复以后，又碰上一个群友的集群出现了严重故障，本篇将记录这个中间大致处理的过程，一些细节在以后会补充</p>
<p>首先看到给出的截图显示的是大量的pg处于异常的状态，从经验上判断，环境要么处于down机的边缘，或者是刚经历了一次大量的重启，这个时候集群可以说是前端的访问肯定全断的，这个故障的时候资源一般会比较紧张，所以在启动的过程中也要注意不要触发更大面积的down机，对于集群来说是会有连带效应的</p>
<p>在启动了部分osd后，集群还是有大量的pg出现的是down+peering的状态，而发现down的osd实际全部在一台服务器上的，这个从ceph的架构来说是不应该出现这个状态的，这个可能是在down机过程中，频繁的pg的状态变化造成了pg的状态停留在之前的down的状态上，而pg出现锁死的状况，这个在之前的那位群友的环境中出现过一次，那个是多机有osd出现异常的情况，这次是单机出现的情况</p>
<a id="more"></a>
<p>尝试加大日志级别，从几个osd里面看日志出现两类的异常，从后面的处理的情况来看，实际这个是触发了两个bug，第一个问题出现是部分的数据丢失，这个在进行处理以后，再次启动的时候，几个osd出现了同样的错误，在询问了我们的研发大牛后，基本能判断这个是一个0.94.x的bug，并且在邮件列表里面已经解决</p>
<p>然后尝试对其中的一台进行升级，这次升级直接升级到了10.1.1，然后启动osd，确实可以启动了，具体的怎么触发这个bug，就不是太清楚里面的过程，这个环境是经历了一个比较复杂的状态变化</p>
<p>启动了部分osd后，发现还是osd无法启动，一检查，发现居然是这个机器的5个磁盘都有文件系统的错误，之前的部分数据丢失，也可能是文件系统错误引起的，很有可能是异常后造成了大面积的异常，这个地方只是推断，因为没有看到监控中间的过程，在修复了文件系统以后，osd都能起来了，只是又碰上另外的一个问题，<br>incomplete状态，就是事情没做完，在检查了里面的数据，发现数据没问题</p>
<p>尝试做修复，使用cephobjecttool导出数据再导入另外的pg，状态还是无法变化，然后根据之前另外一个国外的处理经验，将pg导入到非pg映射的osd，然后让其自动backfiill，发现还是无法生效，pg仍然停留在imcomplete状态</p>
<p>在询问了对方里面的数据情况后得知只有一个镜像比较重要，果断尝试后端的修复，大概思路就是将img镜像所对应的数据全部拷贝到一个目录，然后进行拼接的操作，这个在我之前的测试环境测试过没问题，这次在这个环境上进行了操作，因为环境的对象大小经过了修改，所以脚本也要对应修改，最后合成看了一个raw文件，在经过验证后，能够启动，数据基本是算是恢复了</p>
<p>然后做了pg repair osd repair deep-scrub等操作都是无法改变状态</p>
<p>环境还停留在不可用的状态，尝试做最后的修复，将pg数据进行备份后，强制创建pg，这个在我自己的测试环境下是可行的方案，但是这个环境在停留在creating状态比较久后，还是会进入imcomplete状态，尝试几次还是不行，开始怀疑是这两个osd问题，然后将osd out以后，在重新分布的osd上进行了创建pg操作，还是creating后进入imcomplete状态，到此，基本判断环境无法恢复了，数据算是保住了</p>
<p>这个是国内一个比较牛的cepher也碰到的情况 <a href="http://m.oschina.net/blog/360274" target="_blank" rel="external">osd盘崩溃的总结</a>，他这个环境也是最终无法救回来<br>这是他查询到的国外的一个人写的情况：</p>
<blockquote>
<p>查了一圈无果。一个有同样遭遇的人的一段话：<br>I already tried “ceph pg repair 4.77”, stop/start OSDs, “ceph osd lost”, “ceph pg force_create_pg 4.77”.<br>Most scary thing is “force_create_pg” does not work. At least it should be a way to wipe out a incomplete PG without destroying a whole pool.</p>
</blockquote>
<p>这个地方出故障的环境做一个总结：</p>
<ul>
<li>环境做了比较极端的优化，这里就不说了，ceph的journal这一层就是防止down机出现数据不一致做replay的，做了极端的环境优化需要做多次整机down机测试，这个down机是无法完全避免的，所以要考虑</li>
<li>磁盘出现了多个同时的损坏，这个没有办法，文件系统的损坏有可能是主机系统出现比较特殊的异常造成磁盘数据异常，这个单机多磁盘损坏的可能是有的，最怕就是部分损坏</li>
<li>ceph有部分bug是在比较极端的情况下出现的，并不是没有，所以不能想着完全避免bug，多想想真出问题了，怎样把损失降低到最小，我的底线是数据回来</li>
<li>ceph集群的副本只能保证系统内的高可用，系统级别的高可用，只能是双系统，能搭两套一定两套，哪怕非实时定期备份也好</li>
<li>随着ceph使用者越多，出现问题的情况会越来越多的，特别是在使用的越久，概率就越大，磁盘也是有寿命的，集群呢？还是早做防范措施</li>
</ul>
<hr>
<h3 id="后续：">后续：</h3><p>事情本以为就这么完结了，因为已经达到了最低的标准，数据的恢复，但实际上对于我自己来说，还是觉得有点遗憾的，毕竟环境是处于一个无法使用的状态，并且，环境中实际也只有部分数据的损坏，但是因为pg的状态不对，那些虚拟机实际是无法写入的，变相的这个环境就是一个僵住的状态了，虽然想了好几天，但是并没有更好的办法，有一个办法是将整个的数据导出再导入，这个时间周期会很长，如果里面数据很多都是重要的，这个是不得不走的一步了，正好这个环境重要数据只有一个，也就没去尝试了</p>
<p>我有一个翻译的计划的，已经停滞了很久，但是说实话，我之前的想法是一章章的细细的研究，细细的翻译，然后写出自己的想法，但是迫于时间原因，以及最近事情比较多，暂时处于停滞状态，这个后期会跟进的，目前已经购买的书友，以及支持的朋友，我尽量的是对你提出的问题或者困惑给出我个人的见解，总之一个事情的处理方式有多种，我从来都是告诉你我会怎么做，然后告诉你，你可以根据你的想法来，正是因为想到自己最近没时间翻译，自己干脆把这本书过一遍，果然还是多读书好，根据书里面的一个提示，我就去尝试做另外一个操作</p>
<p>在有想法以后，联系了群友，正好环境还在，没有做推倒重来的操作，这个也感谢ceph群友的信任和支持，在隔了几天再次登录环境以后，根据提示，我将这个pg的数据进行了删除，这次的删除不是之前的暴力的直接rm，而是使用ceph内部的工具进行的删除，主副本停止osd后同时做的操作，我怀疑是不是还有哪里的元数据被锁住了，在删除以后再次起来，再次创建pg的时候，环境还是处于一个异常的状态，因为书中描述了是我之前没见过的操作，当时想想是不是有其他的不清楚的操作方式，在一番查询以后，真的有我没用过的操作，然后直接尝试，果然整个集群正常了，然后把之前的pg数据进行导入操作，然后用rados直接get那个异常的pg里面的对象，果然能读取了，然后用rados ls也能够列出所有的对象了，环境终于能够正常了，环境是强制的改变状态变成可正常，数据也能够读写了，我个人的建议如果真是有很多重要数据，还是把数据倒出来再导入进去，集群正常情况下的导出导入操作逻辑和时间比后台的导出逻辑要简单非常多</p>
<p>好了，到了这里终于将一个环境变成了正常的状态了，对于我自己来说，对ceph的控制又提高了一点，之前认为数据盘在，我就能把数据恢复，倒出来，但是原集群的恢复，没有太多的保证，现在基本上只要盘符不被格式化掉，环境我也能有很大的概率去恢复正常，总之保底恢复方式的越多，越有信心去恢复它</p>
<p>这次的经历让我再一次感觉，不要放弃，不要放弃，有的时候真的会有转机，同时感谢群友能够提供环境给我，也欢迎有更多的朋友在出现问题的时候可以找我探讨一下</p>
<blockquote>
<p>by 运维-武汉-磨渣<br>2016年04月12日夜<br>更新于2016年04月17日夜</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="更新：">更新：</h3><p>在经历了好几天后，失效的环境最终变成了可用状态，只能说有的时候不放弃还真是有点用的</p>
<hr>
<p>在不久前处理了一个故障恢复以后，又碰上一个群友的集群出现了严重故障，本篇将记录这个中间大致处理的过程，一些细节在以后会补充</p>
<p>首先看到给出的截图显示的是大量的pg处于异常的状态，从经验上判断，环境要么处于down机的边缘，或者是刚经历了一次大量的重启，这个时候集群可以说是前端的访问肯定全断的，这个故障的时候资源一般会比较紧张，所以在启动的过程中也要注意不要触发更大面积的down机，对于集群来说是会有连带效应的</p>
<p>在启动了部分osd后，集群还是有大量的pg出现的是down+peering的状态，而发现down的osd实际全部在一台服务器上的，这个从ceph的架构来说是不应该出现这个状态的，这个可能是在down机过程中，频繁的pg的状态变化造成了pg的状态停留在之前的down的状态上，而pg出现锁死的状况，这个在之前的那位群友的环境中出现过一次，那个是多机有osd出现异常的情况，这次是单机出现的情况</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记一次ceph的故障修复(20160408)]]></title>
    <link href="http://www.zphj1987.com/2016/04/11/%E8%AE%B0%E4%B8%80%E6%AC%A1ceph%E7%9A%84%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D-20160408/"/>
    <id>http://www.zphj1987.com/2016/04/11/记一次ceph的故障修复-20160408/</id>
    <published>2016-04-10T17:24:35.000Z</published>
    <updated>2016-04-12T16:18:22.737Z</updated>
    <content type="html"><![CDATA[<p>ceph的在正常运行的时候基本不会出现故障，出现故障一般在变动的时候，具体有下面几种可能出现的情形</p>
<ul>
<li>软件升级</li>
<li>增加存储节点</li>
<li>减少存储节点</li>
<li>调整副本数目</li>
<li>调整pg数目</li>
<li>磁盘出现损坏</li>
<li>节点网络出现异常</li>
</ul>
<p>以上这些操作过程中是最可能出现异常的情形，并不是一定会出问题，以上问题除了网络和磁盘问题出现的异常是基本无法避免外，其他出现的时候，一般是非正常操作引起的，也就是我通常认为的人为事故，这个一般出现在操作的不谨慎上</p>
<a id="more"></a>
<p>本篇记录了一次故障的修复过程，这个故障不是出现在我们公司的产品上，是看到一个ceph社区群里有一个成员在里面问到一个异常是否能解决，这个不同于普通的问题，从他贴出的信息来看，集群已经是非常严重的状态了</p>
<p>正好看到是周五，周六还可以休息下，所以即使快到了晚上12点了，我还是联系了一下那哥们，从简短的几句交流后，基本可以判断对方对于ceph基本处于刚接触的阶段，在询问是否有其他人能协助他做一些比较有难度的操作的时候，他说没有，就他一个人，我想在目前中国很多公司，都是让一个并不太熟悉ceph的运维人员，或者完全就是开发人员维护着存储着非常宝贵的数据的云存储环境，上面运行的应该都是客户的数据，想想我们自己的电脑在硬盘损坏后，自己有多么不爽，而对于企业来说，一个运行环境的损坏有多么严重，一方面损失了数据，另一方面，基本不会再选择这个服务的提供商了，而这些都是一个定时炸弹，运行在中国的开源存储网络环境当中，而且基本都是初创小企业，大企业会有专门的专业的相关人员，而一个数据损失基本会对这些初创企业带来巨大的损失，这些都是需要企业的boss多关注的，这也是我一直持有的一个观点，越来越多的企业是用ceph，也意味着存储需要修复的出现几率就越大，其实我们也是一个小企业，我个人是非常关注数据恢复这一块的，这个比调优更加的重要，大环境的吐槽就到这里，下面开始讲下具体的经过</p>
<h3 id="首先找对方要了一个ssh登陆环境">首先找对方要了一个ssh登陆环境</h3><p>这个对方正好有这个环境允许我的登陆，虽然中间经过了堡垒机，虽然运行命令比较卡顿，但好歹能上去，这个是我个人非常支持的一种做法，不管怎样，是VPN也好，代理也好，一定留一个外网的ssh端口能够让连上机器，这个能允许随时随地能上去处理问题，等你运维人员到达现场，真是黄花菜都凉了，对于比较保密的环境，最好也能够有一个在紧急情况下开启远程允许环境的条件，这个具体花费，一个上网卡，一台破旧的笔记本就基本能实现了，在需要远程操作的时候能够连上去处理，目前已经协助了几个朋友处理了一些简单的问题，基本都是ssh连过去的，而没有远程环境的，我也是无能为力的</p>
<h3 id="检查环境">检查环境</h3><p>登陆上去以后，检查环境发现提示的是2个pg的状态imcomplete，这个是pg的数据不一致的提示，而在检查了对应的osd上的这个pg的数据的时候，发现映射计算到的3个上面有两个是没有数据的，有一个是有数据的，在询问对方做过的操作后，对方是做了一个删除osd的操作，并且是多台机器上面都做过删除，到这里我询问了一下对方，对方是按照一些通用的操作去做的删除操作，命令肯定是没有问题的，这个在后面我处理完后，基本能判断出对方是人为的操作失误引起的</p>
<h3 id="尝试修复">尝试修复</h3><p>开始想起之前做过的一次模拟修复，本来以为这个可以把环境弄好了，基本想法就是如下流程：</p>
<ul>
<li>停止pg对应的3个osd</li>
<li>导出有数据的pg</li>
<li>在无数据的osd上进行pg的数据导入</li>
<li>启动三个osd</li>
</ul>
<p>在进行到数据的导入的时候提示了pg is blocked，这个在我之前的做的测试中是没有遇到过的，后来进行pg的状态查询时候，发现是pg的显示的数据全是0，也就是集群认为这个pg是没有数据的，并且被几个已经删除了的osd blocked,而且做ceph osd  lost 也是无法操作的，提示没有osd，这个应该是pg状态不一致，也就是这个pg状态完全异常了，并且还无法导入了</p>
<h3 id="思考解决办法">思考解决办法</h3><p>到这里我个人判断基本是回天无力了，再次跟对方确认删除的过程，发现对方好在数据盘都保留了，并且还插在机器上，只是有部分osd在进行增加的时候还占用了删除的osd的id</p>
<p>到这里我基本想出来两种方法：</p>
<ul>
<li>最不济，也是终极解决办法就是把后台缺失的数据拼起来，这个耗时巨大，操作难度大，基本上只能作为最后终极挽回的方法，这个只有在客户已经觉得数据可能要丢了，然后去做最后的终极挽回大法了，客户的容忍度是会随着你问题严重性而改变的，相信我数据还在都好说</li>
<li>就是将删除的数据盘给加进来，这个操作在我几年ceph生涯中也是从未做过的，也想不出什么场景下需要这种操作，好吧，不管多么特殊的操作，总有它的存在的意义，我也不能确定ceph是否支持这种操作，那就试试这种</li>
</ul>
<p>这个集群之所以能挽回，有几个特殊点正好都在，缺一不可 </p>
<ol>
<li>删除的数据盘居然没被格式化，或者搞掉，这个如果弄没了，数据必丢</li>
<li>删除的数据盘的盘位部分被新加的节点占用了，部分还没有被占用，而这个缺失数据的pg的数据所删除的osd正好又没有被占用（所以以后替换osd的时候最好是用新的编号，老的盘和编号保留着）</li>
</ol>
<h3 id="开始恢复的操作">开始恢复的操作</h3><p>之前我加节点的操作都是用的ceph-deploy，可以说基本没有遇到过手动能做的ceph-deploy无法完成的，好吧这次我知道了还是有无法完成的，手动的还是多学学比较好，好在我比较熟悉，就按步骤去做</p>
<h4 id="1、增加认证">1、增加认证</h4><p>我们在删除osd的最后一步的时候操作都是ceph auth del<br>我就反向的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth add osd.<span class="number">0</span> osd <span class="string">'allow *'</span> mon <span class="string">'allow rwx'</span> -i /var/lib/ceph/osd0/keyring</span><br></pre></td></tr></table></figure></p>
<p>这个对应keyring就是在删除那个osd上面有，每个osd上面都有的<br>这一步操作完成后auth里面就有osd.0了</p>
<h4 id="2、创建osd">2、创建osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd create</span><br></pre></td></tr></table></figure>
<p>这个步骤也是之前没有做过的，之前准备直接加crush 直接启动发现都是无法启动，提示没有osd<br>这一步相对于删除里面的操作应该就是 ceph osd rm 的操作了</p>
<h4 id="3、增加crush">3、增加crush</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush add osd.<span class="number">0</span> <span class="number">0.9</span> host=node1</span><br></pre></td></tr></table></figure>
<p>这个就是加入到crush里面去</p>
<h4 id="4、启动osd">4、启动osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph start osd.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="检查现在环境状况">检查现在环境状况</h3><p>在检查的时候发现osd真的就加进来了，然后在添加了另一个被block的osd后，集群状态就没有imcomplete了就是active+其他的一些恢复状态什么的，只需要等待恢复，集群即可恢复正常了，到这个时候已经凌晨三点了，事情能够完满解决是最开心的事情</p>
<h3 id="后记">后记</h3><p>集群的删除操作随意，集群的信息基本无记录，环境的基础记录都没有，这个是这个事故的最大原因，再往上走就是对于数据操作这块，公司没有一个重视的态度，上面的boss永远不会关心你运维做了什么操作，而运维人员也可以说是我按标准流程操作的，也没法去定谁的责任，丢了就是丢了，运维最多也就是丢了工作，而企业损失应该就是以万为单位的损失加客户的流失了<br>到这里也许这家公司的头并不知道发生了什么，也许只是认为是一个小的业务中断，但真的某一天出事了，这就是大事了，所以一定要重视系统的监控和系统操作的谨慎，boss不需要关心，你cto，研发的头，运维的头，总有一个人需要重视这个问题，而大部分应该是散养状态，也是就是在集群交付以后基本是没有运维监控的状态，因为即使在我们的环境下，即使我有这个意识，但是也无法推动这个事情的</p>
<p>之前我总在跟别人说欢迎打赏，有时能收到几十，目前最高就是小白的100块，其实我知道大家都是上着班，为公司而工作，为了公司的事情需要自己再去掏钱，似乎也没有太多理由，所以对于一般的情况，别人要感谢，要请吃饭，我也就是说着好好，心意我领了，我的收入在我所在的城市其实已经可以了，只是最近因为有一些需要用钱的地方，所以又是录课程，又是翻译书籍，为了赚取10块钱也是费尽心机啊，其实大家不打赏也无所谓的，能认识一些朋友，在某天，记得我有帮过你就行了，我认识的一个中科院的博士，那个时候还是2011年，他是我的老师的同学，即使现在他linux或者服务器有什么问题，需要帮助的时候，我有时间也会帮处理他，举手之劳就能帮别人解决问题，对于自己也是很开心的</p>
<p>好了，就到这里，如果你的集群是生产环境，相信我话，需要帮助的话，留个ssh，有时间的时候我可以帮忙处理，绝对不会出现要给钱才弄的情况，当然你如果财力雄厚，事后给点劳务费，我也不会拒绝的，测试环境话，就自己研究下为好，特别不好解决的问题也可以沟通一下，当然我也有我的工作，需要我有时间的时候</p>
<blockquote>
<p>by 运维-武汉-磨渣<br>2016年04月11日夜</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>ceph的在正常运行的时候基本不会出现故障，出现故障一般在变动的时候，具体有下面几种可能出现的情形</p>
<ul>
<li>软件升级</li>
<li>增加存储节点</li>
<li>减少存储节点</li>
<li>调整副本数目</li>
<li>调整pg数目</li>
<li>磁盘出现损坏</li>
<li>节点网络出现异常</li>
</ul>
<p>以上这些操作过程中是最可能出现异常的情形，并不是一定会出问题，以上问题除了网络和磁盘问题出现的异常是基本无法避免外，其他出现的时候，一般是非正常操作引起的，也就是我通常认为的人为事故，这个一般出现在操作的不谨慎上</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph在centos7下一个不容易发现的改变]]></title>
    <link href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/"/>
    <id>http://www.zphj1987.com/2016/03/31/ceph在centos7下一个不容易发现的改变/</id>
    <published>2016-03-31T08:19:27.000Z</published>
    <updated>2016-03-31T08:20:38.371Z</updated>
    <content type="html"><![CDATA[<p>在centos6以及以前的osd版本，在启动osd的时候，回去根据ceph.conf的配置文件进行挂载osd，然后进行进程的启动，这个格式是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[osd.<span class="number">0</span>]</span><br><span class="line">host = hostname</span><br><span class="line">devs=/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>启动的时候就会把sdb1盘符挂载到0的目录里面去了</p>
<p>然后在centos7的版本的时候，发现居然不写配置文件也能够自动挂载启动，这个地方是什么地方发生了变化，在做了一些日志的查询以后，发现centos7下居然做了一个改变</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl list-unit-files |grep ceph-disk</span></span><br><span class="line">ceph-disk@.service                          static</span><br></pre></td></tr></table></figure>
<p>可以看到有这个服务</p>
<a id="more"></a>
<h3 id="我们来验证下这个服务">我们来验证下这个服务</h3><h4 id="先停止服务">先停止服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="umount挂载点">umount挂载点</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">umount /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在已经没有挂载点了</p>
<h4 id="现在执行下面的服务（我的sdc1是刚刚的osd-1）">现在执行下面的服务（我的sdc1是刚刚的osd.1）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl start ceph-disk@/dev/sdc1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-disk@/dev/sdc1</span></span><br><span class="line">● ceph-disk@-dev-sdc1.service - Ceph disk activation: /dev/sdc1</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-disk@.service; static; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-detect-init --default sysvinit</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: activate: Marking with init system systemd</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: activate: ceph osd.<span class="number">1</span> data dir is ready at /var/lib/ceph/tmp/mnt.<span class="number">3</span>a8xNK</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: move_mount: Moving mount to final location...</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /bin/mount -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /bin/umount <span class="operator">-l</span> -- /var/lib/ceph/tmp/mnt.<span class="number">3</span>a8xNK</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: start_daemon: Starting ceph osd.<span class="number">1</span>...</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl <span class="built_in">enable</span> ceph-osd@<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl start ceph-osd@<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph disk activation: /dev/sdc1.</span><br></pre></td></tr></table></figure>
<p>执行完检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># df -h |grep ceph-1</span></span><br><span class="line">/dev/sdc1       <span class="number">275</span>G   <span class="number">35</span>M  <span class="number">275</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到已经挂载好，并且启动了服务<br>可以看到我没有使用任何配置情况下，没有告诉集群sdc1就是要挂载到<code>/var/lib/ceph/osd/ceph-1</code>这个目录的，自动挂载好了，这个是集群自己先mount到一个临时目录根据磁盘里面的信息来判断了这个osd真实的数据，根据这个数据来mount到一个挂载点，这个做法是非常好的做法</p>
<p><strong>如果觉得我的文章对您有用，欢迎打赏。您的支持将鼓励我继续创作！10元足矣！</strong></p>
<center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>在centos6以及以前的osd版本，在启动osd的时候，回去根据ceph.conf的配置文件进行挂载osd，然后进行进程的启动，这个格式是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[osd.<span class="number">0</span>]</span><br><span class="line">host = hostname</span><br><span class="line">devs=/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>启动的时候就会把sdb1盘符挂载到0的目录里面去了</p>
<p>然后在centos7的版本的时候，发现居然不写配置文件也能够自动挂载启动，这个地方是什么地方发生了变化，在做了一些日志的查询以后，发现centos7下居然做了一个改变</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl list-unit-files |grep ceph-disk</span></span><br><span class="line">ceph-disk@.service                          static</span><br></pre></td></tr></table></figure>
<p>可以看到有这个服务</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
