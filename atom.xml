<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[但行好事，莫问前程]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-12-13T08:45:45.190Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Cephfs的快照功能]]></title>
    <link href="http://www.zphj1987.com/2016/12/13/Cephfs%E7%9A%84%E5%BF%AB%E7%85%A7%E5%8A%9F%E8%83%BD/"/>
    <id>http://www.zphj1987.com/2016/12/13/Cephfs的快照功能/</id>
    <published>2016-12-13T08:26:35.000Z</published>
    <updated>2016-12-13T08:45:45.190Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/snapshot.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>Cephfs的快照功能在官网都很少提及，因为即使开发了很多年，但是由于cephfs的复杂性，功能一直没能达到稳定，这里，只是介绍一下这个功能，怎么使用，并且建议不要在生产中使用，因为搞不好是会丢数据的</p>
<a id="more"></a>
<h2 id="功能介绍">功能介绍</h2><p>首先这个功能是默认关闭的，所以需要开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph fs set ceph  allow_new_snaps 1</span></span><br><span class="line">Error EPERM: Warning! This feature is experimental.It may cause problems up to and including data loss.Consult the documentation at ceph.com, and <span class="keyword">if</span> unsure, <span class="keyword">do</span> not proceed.Add --yes-i-really-mean-it <span class="keyword">if</span> you are certain.</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph fs set ceph  allow_new_snaps 1  --yes-i-really-mean-it</span></span><br></pre></td></tr></table></figure></p>
<p>从提示上可以看到，还是不要在生产上使用</p>
<p>开发者的话：</p>
<blockquote>
<p>In Jewel ceph fs snapshots are still experimental. Does someone has a clue when this would become stable, or how experimental this is ?<br>We’re not sure yet. Probably it will follow stable multi-MDS; we’re thinking about redoing some of the core snapshot pieces still. :/<br>It’s still pretty experimental in Jewel. Shen had been working on this and I think it often works, but tends to fall apart under the failure of other components (eg, restarting an MDS while snapshot work is happening).<br>-Greg</p>
</blockquote>
<p>挂载集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mount -t ceph 192.168.8.106:/ /mnt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cd /mnt/</span></span><br></pre></td></tr></table></figure></p>
<p>快照是对目录创建的<br>所以我们来看下<br><figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line">[<span class="literal">root</span>@lab8106 mnt]<span class="comment"># ll</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> <span class="literal">root</span> <span class="literal">root</span> <span class="number">51341</span> <span class="constant">Dec</span> <span class="number">13</span> <span class="number">11</span>:<span class="number">41</span> test1</span><br><span class="line">drwxr-xr-x <span class="number">1</span> <span class="literal">root</span> <span class="literal">root</span> <span class="number">51591</span> <span class="constant">Dec</span> <span class="number">13</span> <span class="number">16</span>:<span class="number">18</span> test2</span><br><span class="line">drwxr-xr-x <span class="number">1</span> <span class="literal">root</span> <span class="literal">root</span> <span class="number">30951</span> <span class="constant">Dec</span> <span class="number">13</span> <span class="number">15</span>:<span class="number">19</span> test3</span><br></pre></td></tr></table></figure></p>
<p>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># mkdir .snap/snap1</span></span><br><span class="line">[root@lab8106 mnt]<span class="comment"># ll .snap/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">133883</span> Dec <span class="number">13</span> <span class="number">14</span>:<span class="number">21</span> snap1</span><br><span class="line">[root@lab8106 mnt]<span class="comment"># ll .snap/snap1</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">51341</span> Dec <span class="number">13</span> <span class="number">11</span>:<span class="number">41</span> <span class="built_in">test</span>1</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">51591</span> Dec <span class="number">13</span> <span class="number">16</span>:<span class="number">18</span> <span class="built_in">test</span>2</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">30951</span> Dec <span class="number">13</span> <span class="number">15</span>:<span class="number">19</span> <span class="built_in">test</span>3</span><br></pre></td></tr></table></figure></p>
<p>创建快照很简单，就是在需要做快照的目录下面执行 <code>mkdir .snap/snapname</code> 后面接快照的名称</p>
<p>快照的速度非常快，秒级别的</p>
<p>恢复快照数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># cp -ra .snap/snap1/* ./</span></span><br></pre></td></tr></table></figure></p>
<p>删除快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># rmdir .snap/snap1</span></span><br></pre></td></tr></table></figure></p>
<p>删除快照需要用rmdir命令</p>
<h2 id="总结">总结</h2><p>本篇简单的介绍了下cephfs快照的相关的操作，自己很久没搞，命令都找不到了，供参考</p>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/snapshot.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>Cephfs的快照功能在官网都很少提及，因为即使开发了很多年，但是由于cephfs的复杂性，功能一直没能达到稳定，这里，只是介绍一下这个功能，怎么使用，并且建议不要在生产中使用，因为搞不好是会丢数据的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[查询Ceph的OSD占用内存]]></title>
    <link href="http://www.zphj1987.com/2016/12/08/%E6%9F%A5%E8%AF%A2Ceph%E7%9A%84OSD%E5%8D%A0%E7%94%A8%E5%86%85%E5%AD%98/"/>
    <id>http://www.zphj1987.com/2016/12/08/查询Ceph的OSD占用内存/</id>
    <published>2016-12-08T14:15:19.000Z</published>
    <updated>2016-12-09T02:09:14.892Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/Memory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前写过一篇关于查询OSD的运行的CPU的情况的分享，本篇是讲的获取内存占用的，代码包括两种输出，一种是直接的表格，一种是可以方便解析的json<br><a id="more"></a></p>
<h2 id="代码">代码</h2><p>直接上代码，python才用不久，所以可能代码实现比较低级，主要是看实现的方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">import psutil</span><br><span class="line">import commands</span><br><span class="line">from prettytable import PrettyTable</span><br><span class="line">def main():</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">1</span>:</span><br><span class="line">        printosdmemtable(<span class="string">"table"</span>)</span><br><span class="line">    <span class="keyword">elif</span> sys.argv[<span class="number">1</span>] == <span class="string">'json'</span>:</span><br><span class="line">        printosdmemtable(<span class="string">"json"</span>)</span><br><span class="line"></span><br><span class="line">def printosdmemtable(chosse):</span><br><span class="line">        data_dic = &#123;&#125;</span><br><span class="line">        osd_list=&#123;&#125;</span><br><span class="line">        row = PrettyTable()</span><br><span class="line">        row.header = True</span><br><span class="line">        memlist = [<span class="string">"OSD\MEM"</span>]</span><br><span class="line">        memchose = [ <span class="string">'VIRT'</span>,<span class="string">'RES'</span>]</span><br><span class="line">        <span class="keyword">for</span> meminfo <span class="keyword">in</span> memchose:</span><br><span class="line">            memlist.append(<span class="string">"%s"</span> %meminfo )</span><br><span class="line">        row.field_names = memlist</span><br><span class="line">        <span class="keyword">for</span> root, <span class="built_in">dirs</span>, files <span class="keyword">in</span> os.walk(<span class="string">'/var/run/ceph/'</span>):</span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">"osd"</span>  <span class="keyword">in</span> name and <span class="string">"pid"</span> <span class="keyword">in</span> name :</span><br><span class="line">                    osdlist = []</span><br><span class="line">                    osdthlist=[]</span><br><span class="line">                    <span class="keyword">for</span> osdmem <span class="keyword">in</span> range(len(memchose)):</span><br><span class="line">                        osdlist.append(<span class="string">" "</span>)</span><br><span class="line">                    pidfile=root+ name</span><br><span class="line">                    osdid=commands.getoutput(<span class="string">'ls  %s|cut -d "." -f 2 2&gt;/dev/null'</span>  %pidfile )</span><br><span class="line">                    osdpid = commands.getoutput(<span class="string">'cat %s  2&gt;/dev/null'</span> %pidfile)</span><br><span class="line">                    osd_runmemvsz = commands.getoutput(<span class="string">'ps -p %s  -o vsz |grep -v VSZ 2&gt;/dev/null'</span> %osdpid)</span><br><span class="line">                    osd_runmemrsz = commands.getoutput(<span class="string">'ps -p %s  -o rsz |grep -v RSZ 2&gt;/dev/null'</span> %osdpid)</span><br><span class="line">                    osdname=<span class="string">"osd."</span>+osdid</span><br><span class="line">                    osdlist.insert(<span class="number">0</span>,osdname)</span><br><span class="line">                    osdlist[<span class="number">1</span>] = str(int(osd_runmemvsz)/<span class="number">1024</span>)+<span class="string">"KB"</span></span><br><span class="line">                    osdlist[<span class="number">2</span>] = str(int(osd_runmemrsz)/<span class="number">1024</span>)+<span class="string">"KB"</span></span><br><span class="line">                    vm_dic = &#123;&#125;</span><br><span class="line">                    vm_dic[<span class="string">'VSZ'</span>]= str(int(osd_runmemvsz)/<span class="number">1024</span>)+<span class="string">"KB"</span></span><br><span class="line">                    vm_dic[<span class="string">'RSZ'</span>]= str(int(osd_runmemrsz)/<span class="number">1024</span>)+<span class="string">"KB"</span></span><br><span class="line">                    osd_list[osdname] = vm_dic</span><br><span class="line">                    data_dic[<span class="string">'osdmemused'</span>] = osd_list</span><br><span class="line">                    <span class="keyword">if</span> chosse == <span class="string">"table"</span>:</span><br><span class="line">                        row.add_row(osdlist)</span><br><span class="line">                    <span class="keyword">elif</span> chosse == <span class="string">"json"</span>:</span><br><span class="line">                        row = json.dumps(data_dic,separators=(<span class="string">','</span>, <span class="string">':'</span>))</span><br><span class="line">        <span class="built_in">print</span> row</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h2 id="运行脚本">运行脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 getmem]<span class="comment"># python getmem.py </span></span><br><span class="line">+---------+-------+------+</span><br><span class="line">| OSD\MEM |  VIRT | RES  |</span><br><span class="line">+---------+-------+------+</span><br><span class="line">|  osd.<span class="number">0</span>  | <span class="number">825</span>KB | <span class="number">43</span>KB |</span><br><span class="line">|  osd.<span class="number">1</span>  | <span class="number">826</span>KB | <span class="number">43</span>KB |</span><br><span class="line">+---------+-------+------+</span><br><span class="line">[root@lab8106 getmem]<span class="comment"># python getmem.py json</span></span><br><span class="line">&#123;<span class="string">"osdmemused"</span>:&#123;<span class="string">"osd.1"</span>:&#123;<span class="string">"VSZ"</span>:<span class="string">"826KB"</span>,<span class="string">"RSZ"</span>:<span class="string">"43KB"</span>&#125;,<span class="string">"osd.0"</span>:&#123;<span class="string">"VSZ"</span>:<span class="string">"825KB"</span>,<span class="string">"RSZ"</span>:<span class="string">"43KB"</span>&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/Memory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前写过一篇关于查询OSD的运行的CPU的情况的分享，本篇是讲的获取内存占用的，代码包括两种输出，一种是直接的表格，一种是可以方便解析的json<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于发行版本内核打造自己的内核]]></title>
    <link href="http://www.zphj1987.com/2016/12/07/%E5%9F%BA%E4%BA%8E%E5%8F%91%E8%A1%8C%E7%89%88%E6%9C%AC%E5%86%85%E6%A0%B8%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84%E5%86%85%E6%A0%B8/"/>
    <id>http://www.zphj1987.com/2016/12/07/基于发行版本内核打造自己的内核/</id>
    <published>2016-12-07T07:55:38.000Z</published>
    <updated>2016-12-07T07:59:05.158Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/kernel.png" alt="此处输入图片的描述"><br></center>

<p>Linux当中最核心的部分就是内核，这个也是最基础，最可能被忽视的一部分，随便找一个刚入职的运维，学习个两三天，网上找些资料也能能自己安装编译内核了，很多运维的初期培训就是做的这些学习，为什么在网上已经有这么多文章的情况下，还要写一篇关于内核的文章，这是因为，我想讲的是如何去选择内核<br><a id="more"></a><br>一般来说，找内核的时候都会去下面这个网站进行选择</p>
<blockquote>
<p><a href="https://www.kernel.org/" target="_blank" rel="external">https://www.kernel.org/</a></p>
</blockquote>
<p>很多人在问我的时候，都会问，我该怎么去选择哪个版本的内核，一般来说我的回答是这样的</p>
<blockquote>
<p>选取最后一个长期支持版本，或者最后一个稳定的版本</p>
</blockquote>
<p>一般来说,选择这两个版本基本不会出太大的问题，并且即使有问题，后面做小版本的升级也不是很难的事情，当然这是基于你对自定义内核很有兴趣，或者需要自己去裁剪，增加一些东西的时候，用我上面说的两个版本都没有问题，下面是一个其他的选择</p>
<p>最近把linus的just for fun看完了，也基本上了解了linux大概的发展历程，linux走向成功也有一定的原因是围绕在其周围的一些商业公司，红帽是其中最成功的一个公司，当然还有其他各种发型版本，开源版本和商业版本的最大区别在于服务上面，商业公司能够提供专业的服务，开源并不意味着免费，其中很大一部分是学习成本，然后其次就是包装和推广了，最终才是一个完整的产品</p>
<p>开源有开源的规矩，当然这个规矩在中国不一定行得通，大部分公司不会将开源修改的东西回馈回去，而能够回馈回去的，基本都是技术非常领先的公司，这些公司核心在于自己的技术，以及对产品的把控，所以也就不介意源代码的开源了，并且乐意去引领行业的发展</p>
<p>当然这个对于红帽这样级别的公司，代码当然是会开源的，而其发行版本的内核，其实都是经过了一些修改的，并且这些修改也都是会开源出来的，只是大部分时候我们并没有去关注它，这就是本篇的重点</p>
<h2 id="获取源代码">获取源代码</h2><blockquote>
<p><a href="http://vault.centos.org/7.2.1511/os/Source/SPackages/" target="_blank" rel="external">http://vault.centos.org/7.2.1511/os/Source/SPackages/</a><br>centos版本</p>
</blockquote>
<p>红帽的内核源码之前托管在ftp上的，现在全部放到了订阅中心了，这里进入红帽订阅中心，进行rpm包的搜索，找到需要的部分，选择下载即可</p>
<blockquote>
<p><a href="https://access.redhat.com/downloads/content/kernel/3.10.0-514.el7/x86_64/fd431d51/package" target="_blank" rel="external">https://access.redhat.com/downloads/content/kernel/3.10.0-514.el7/x86_64/fd431d51/package</a></p>
</blockquote>
<center><br><img src="http://static.zybuluo.com/zphj1987/n8cgirh3357s5ky5bq0tqnsx/image_1b39jfbj2178udi87sdptj13809.png" alt="image_1b39jfbj2178udi87sdptj13809.png-137.5kB"><br></center>

<p>这里我们是要选择的是源码包，因为可能需要自己加些内核模块进去<br>安装源码包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ivh kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">514</span>.el7.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>安装后默认会放到下面的目录下面，如果你有自定义的目录，也可以直接解压rpm，解压的方法是,下面命令默认会将文件解压到当前目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm2cpio kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">514</span>.el7.src.rpm |cpio -div</span><br></pre></td></tr></table></figure></p>
<p>检查文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll  /root/rpmbuild/SOURCES/</span></span><br><span class="line">total <span class="number">82804</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root     <span class="number">3118</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> check-kabi</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">150</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> cpupower.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">294</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> cpupower.service</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root        <span class="number">0</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> extra_certificates</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">121660</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-ppc64.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">121951</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-ppc64-debug.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">121229</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-ppc64le.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">121531</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-ppc64le-debug.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">58278</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span><span class="operator">-s</span>390x.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">57895</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span><span class="operator">-s</span>390x-debug.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">30834</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span><span class="operator">-s</span>390x-kdump.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">137690</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-x86_64.config</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root   <span class="number">137991</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> kernel-<span class="number">3.10</span>.<span class="number">0</span>-x86_64-debug.config</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> root root     <span class="number">8582</span> Oct <span class="number">19</span> <span class="number">22</span>:<span class="number">19</span> kernel-abi-whitelists-<span class="number">514</span>.tar.bz2</span><br><span class="line">-rw-rw-r-- <span class="number">1</span> root root <span class="number">83660860</span> Oct <span class="number">19</span> <span class="number">22</span>:<span class="number">19</span> linux-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">514</span>.el7.tar.xz</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root        <span class="number">0</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> linux-kernel-test.patch</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root     <span class="number">1757</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> Makefile.common</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">34277</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> Module.kabi_ppc64</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">34277</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> Module.kabi_ppc64le</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">31748</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> Module.kabi_s390x</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root    <span class="number">36881</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> Module.kabi_x86_64</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root     <span class="number">1198</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> rheldup3.x509</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root     <span class="number">1176</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> rhelkpatch1.x509</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">977</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> securebootca.cer</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">899</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> secureboot.cer</span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root      <span class="number">507</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> sign-modules</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">361</span> Oct <span class="number">19</span> <span class="number">23</span>:<span class="number">20</span> x509.genkey</span><br></pre></td></tr></table></figure></p>
<h2 id="打包内核">打包内核</h2><p>如果需要修改默认的内核选项，就修改这个文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /root/rpmbuild/SOURCES/kernel-<span class="number">3.10</span>.<span class="number">0</span>-x86_64.config</span><br></pre></td></tr></table></figure></p>
<p>然后开始编译内核rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpmbuild -ba /root/rpmbuild/SPECS/kernel.spec</span><br></pre></td></tr></table></figure></p>
<p>然后内核包就生成了，在下面目录当中取rpm包即可</p>
<blockquote>
<p>/root/rpmbuild/RPMS/x86_64/</p>
</blockquote>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/kernel.png" alt="此处输入图片的描述"><br></center>

<p>Linux当中最核心的部分就是内核，这个也是最基础，最可能被忽视的一部分，随便找一个刚入职的运维，学习个两三天，网上找些资料也能能自己安装编译内核了，很多运维的初期培训就是做的这些学习，为什么在网上已经有这么多文章的情况下，还要写一篇关于内核的文章，这是因为，我想讲的是如何去选择内核<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[The Dos and Don'ts for Ceph for OpenStack]]></title>
    <link href="http://www.zphj1987.com/2016/11/29/The-Dos-and-Don-ts-for-Ceph-for-OpenStack/"/>
    <id>http://www.zphj1987.com/2016/11/29/The-Dos-and-Don-ts-for-Ceph-for-OpenStack/</id>
    <published>2016-11-29T15:59:07.000Z</published>
    <updated>2016-11-30T03:10:40.805Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dos-and-dons.png" alt=""><br></center>

<p>Ceph和OpenStack是一个非常有用和非常受欢迎的组合。 不过，部署Ceph / OpenStack经常会有一些容易避免的缺点 - 我们将帮助你解决它们</p>
<a id="more"></a>
<h2 id="使用_show_image_direct_url_and_the_Glance_v2_API">使用 show_image_direct_url and the Glance v2 API</h2><p>使用ceph的RBD（RADOS Block Device）,你可以创建克隆,你可以将克隆理解为可写的快照（快照通常是只读的）。克隆只会为相对于父快照变化的部分创建对象，这意味着：</p>
<ol>
<li>可以节省空间。这是显而易见的，但是这并不能很有说服力，毕竟存储是分布式系统当中最便宜的部分</li>
<li>克隆中没有修改的部分还是由原始卷提供。这很重要，因为很容易命中相同的RADOS 对象，相同的osd，不论是用的哪个克隆。而且这意味着，这些对象是从OSD的页面缓存进行响应，换句话说，是RAM提供。RAM比任何存储访问方式速度都快，所以从内存当中提供大量的读取是很好的。正因为这样，从克隆的卷提供数据读取，要比相同数据全拷贝的情况下速度要快一些</li>
</ol>
<p>Cinder（当从image创建一个卷）和Nova(从ceph提供临时磁盘)都能够使用ceph的后端的RBD image的克隆，并且是自动的，但这个只有在glance-api.conf中设置了show_image_direct_url=true 才会使用，并且配置使用 Glance v2 API进行连接Glance。<a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#any-openstack-version" target="_blank" rel="external">参考官网</a></p>
<h2 id="设置_libvirt/images_type_=_rbd_on_Nova_compute_nodes">设置 libvirt/images_type = rbd on Nova compute nodes</h2><p>在NOVA中（使用libvirt的KVM计算驱动），有几个存储临时镜像的配置，不从Cinder卷启动的情况。你可以设置 nova‑compute.conf 的[libvirt]当中的images_type：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">images_<span class="built_in">type</span> = &lt;<span class="built_in">type</span>&gt;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>默认的类型是磁盘，这意味着你启动一个新的vm的时候，将会发生下面的事：<br>nova-compute在你的虚拟机管理节点上链接到Glance API,查找所需要的image，下载这个image到你的计算节点，默认在/var/lib/nova/instances/_base路径下</li>
<li>然后会创建一个qcow2文件，使用下载的这个image做它的backing file</li>
</ul>
<p>这个过程在计算节点上会占用大量的空间，并且会一旦这个镜像没有提前在计算节点上下载好，就会需要等很久才能启动虚拟机，这也使得这样的vm不可能实时的迁移到另外一台主机而不产生宕机时间</p>
<p>将images_types设置为rbd后意味着disk是存储在rbd的后端的，是原始镜像的克隆，并且是立即创建的，没有延时启动，没有浪费空间，可以获得所有克隆的好处，<a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#id2" target="_blank" rel="external">参考文档</a></p>
<h2 id="在Nova计算节点上启用RBD缓存">在Nova计算节点上启用RBD缓存</h2><p>librbd是支持Qemu / KVM RBD存储驱动程序的ceph的库，可以使用虚拟化主机的RAM进行磁盘的缓存。你应该使用这个。</p>
<p>是的，它是一个可以安全使用的缓存。 一方面，virtio-blk与Qemu RBD 驱动程序的组合将正确地实现磁盘刷新。 也就是说，当虚拟机中的应用程序显示“我现在想在磁盘上存储此数据”时，virtio-blk，Qemu和Ceph将一起工作，只有在写入完成时才会报告</p>
<ul>
<li>写入主OSD</li>
<li>复制到可用的副本OSD</li>
<li>只是写入所有的osd journal才会acknowledged</li>
</ul>
<p>此外，Ceph RBD具有一个智能保护：即使它被配置为write-back缓存，它也将拒绝这样做（这意味着它将 write-through模式操作），直到它接收到用户的第一次flush请求。 因此，如果你运行一个永远不会这样做的虚拟机，因为它被错误配置或者它的客户操作系统很老的，那么RBD将固执地拒绝缓存任何写入。 相应的RBD选项称为 rbd cache writethrough until flush，它默认为true，你不应该禁用它。</p>
<p>你可以通过修改nova-compute 配置文件的下面选项开启writeback caching<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">images_<span class="built_in">type</span> = rbd</span><br><span class="line">...</span><br><span class="line">disk_cachemodes=<span class="string">"network=writeback"</span></span><br></pre></td></tr></table></figure></p>
<p>你应该这样去做</p>
<h2 id="为images,_volumes,_and_ephemeral_disks使用单独的池">为images, volumes, and ephemeral disks使用单独的池</h2><p>现在你已经在Glance开启了enabled show_image_direct_url=true，配置Cinder and nova-compute与Glance交互 的时候使用 v2 API, 配置 nova-compute使用 libvirt/images_type=rbd，所有的VMs和volumes都使用rbd克隆，克隆可以跨存储进行，意味着你可以创建RBD image(已经快照)在一个存储池，然后它的克隆在另外一个存储池<br>你应该这样做，有几个原因：</p>
<ul>
<li><p>单独的池意味着您可以分别控制对这些池的访问。 这只是一个标准的缓解危险方法：如果您的nova-compute节点被攻破，并且攻击者可以损坏或删除临时磁盘，那么这是坏的 - 但如果他们也可能损坏您的Glance图像那将会更糟。</p>
</li>
<li><p>单独池也意味着您可以有不同的池设置，例如size或pg_num的设置。</p>
</li>
<li><p>最重要的是，单独的池可以使用单独的crush_ruleset设置。 下面我们会做介绍</p>
</li>
</ul>
<p>通常有三个不同的池：一个用于Glance图像（通常命名为glance或图像），一个用于Cinder卷（cinder或卷），一个用于VM（nova-compute或vms）。</p>
<h2 id="不需要使用SSD作为你的Ceph_OSD_journal">不需要使用SSD作为你的Ceph OSD journal</h2><p>在这篇文章的建议中，这一个可能是最令人感觉到奇怪和不认可的。 当然，传统的情况下都会认为，你应该总是把你的OSD journal在更快的设备上，并且你应该以1：4到1：6的比例部署ssd和普通磁盘，对吧？</p>
<p>让我们来看看。 假设你是按1：6的配比方法，你的SATA转盘能够以100 MB/s的速度写。 6个OSD，每个OSD使用企业SSD分区上的分区作为。进一步假设SSD能够以500MB/s写入。  </p>
<p>恭喜你，在那种情况下，你刚刚使你的SSD成为瓶颈。虽然你的OSDs聚合带宽支持600 MB / s，你的SSD限制你大约83％的性能。</p>
<p>在这种情况下，你实际上可以用1：4的比例，但使你的聚合带宽只快了一点点，SSD的没有很大的优势</p>
<p>现在，当然，考虑另一种选择：如果你把你的journal放在OSD相同的设备上，那么你只能有效地使用一半的驱动器的标称带宽，平均来说，因为你写两次到同一设备。 所以这意味着没有SSD，你的有效单个osd带宽只有大约50 MB/s，所以你从6个驱动器中得到的总带宽更像是300 MB/s，对此，500MB/ s仍然是一个实质性的改进。</p>
<p>所以你需要将自己的配比匹配到上面的计算当中，并对价格和性能进行自己的评估。 只是不要认为SSD journal将是万灵药，也许使用ssd算是一个好主意，关键在于比较</p>
<h2 id="使用all-flash_OSDs">使用all-flash OSDs</h2><p>有一件事要注意，你的SSD journal不会提高读。 那么，怎样利用SSD的提高读取呢？</p>
<p>使用ssd做OSD。 也就是说，不是OSD journal，而是具有文件存储和journal的OSD。 这样的ssd的OSD不仅仅是写入速度快，而且读取也会快。</p>
<h2 id="将_all-flash_OSDs_放入独立的CRUSH_root">将 all-flash OSDs 放入独立的CRUSH root</h2><p>假设你不是在全闪存硬件上运行，而是运行一个经济高效的混合集群，其中一些OSD是普通的，而其他是SSD（或NVMe设备或其他），你显然需要单独处理这些OSD。 最简单和容易的方法就是，除了正常配置的默认根之外再创建一个单独的CRUSH根。</p>
<p>例如，您可以按如下所示设置CRUSH层次结构：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ID WEIGHT  TYPE NAME         UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">- </span><br><span class="line">-<span class="number">1</span> <span class="number">4.85994</span> root default</span><br><span class="line">-<span class="number">2</span> <span class="number">1.61998</span>     host elk</span><br><span class="line"> <span class="number">0</span> <span class="number">0.53999</span>         osd.<span class="number">0</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.53999</span>         osd.<span class="number">1</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.53999</span>         osd.<span class="number">2</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">3</span> <span class="number">1.61998</span>     host moose</span><br><span class="line"> <span class="number">3</span> <span class="number">0.53999</span>         osd.<span class="number">3</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">4</span> <span class="number">0.53999</span>         osd.<span class="number">4</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">5</span> <span class="number">0.53999</span>         osd.<span class="number">5</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">4</span> <span class="number">1.61998</span>     host reindeer</span><br><span class="line"> <span class="number">6</span> <span class="number">0.53999</span>         osd.<span class="number">6</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">7</span> <span class="number">0.53999</span>         osd.<span class="number">7</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">8</span> <span class="number">0.53999</span>         osd.<span class="number">8</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br><span class="line">-<span class="number">5</span> <span class="number">4.85994</span> root highperf</span><br><span class="line">-<span class="number">6</span> <span class="number">1.61998</span>     host elk-ssd</span><br><span class="line"> <span class="number">9</span> <span class="number">0.53999</span>         osd.<span class="number">9</span>          up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">10</span> <span class="number">0.53999</span>         osd.<span class="number">10</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">11</span> <span class="number">0.53999</span>         osd.<span class="number">11</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">7</span> <span class="number">1.61998</span>     host moose-ssd</span><br><span class="line"><span class="number">12</span> <span class="number">0.53999</span>         osd.<span class="number">12</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">13</span> <span class="number">0.53999</span>         osd.<span class="number">13</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">14</span> <span class="number">0.53999</span>         osd.<span class="number">14</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">8</span> <span class="number">1.61998</span>     host reindeer-ssd</span><br><span class="line"><span class="number">15</span> <span class="number">0.53999</span>         osd.<span class="number">15</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">16</span> <span class="number">0.53999</span>         osd.<span class="number">16</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">17</span> <span class="number">0.53999</span>         osd.<span class="number">17</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure></p>
<p>在上面的示例中，OSDs 0-8分配到默认根，而OSDs 9-17（我们的SSD）属于根highperf。 我们现在可以创建两个单独的CRUSH rule：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rule replicated_ruleset &#123;</span><br><span class="line">    ruleset <span class="number">0</span></span><br><span class="line">    <span class="built_in">type</span> replicated</span><br><span class="line">    min_size <span class="number">1</span></span><br><span class="line">    max_size <span class="number">10</span></span><br><span class="line">    step take default</span><br><span class="line">    step chooseleaf firstn <span class="number">0</span> <span class="built_in">type</span> host</span><br><span class="line">    step emit</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rule highperf_ruleset &#123;</span><br><span class="line">    ruleset <span class="number">1</span></span><br><span class="line">    <span class="built_in">type</span> replicated</span><br><span class="line">    min_size <span class="number">1</span></span><br><span class="line">    max_size <span class="number">10</span></span><br><span class="line">    step take highperf</span><br><span class="line">    step chooseleaf firstn <span class="number">0</span> <span class="built_in">type</span> host</span><br><span class="line">    step emit</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>默认crush rule 是replicated_ruleset，从默认根选择OSD，而step take highperf在highperf_ruleset当中意味着它只会选择在highperf根的OSD。</p>
<h2 id="为存储池池指定all-flash_rule">为存储池池指定all-flash rule</h2><p>将单个池分配给新的CRUSH crule（并因此分配给不同的OSD集），使用一个命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> &lt;name&gt; crush_ruleset &lt;number&gt;</span><br></pre></td></tr></table></figure></p>
<p>…其中<name>是池的名称，<number>是您的CRUSH RULE的ID。 你可以在线执行此操作，而客户端正在访问其数据 - 当然会有很多remapped和backfill，因此您的整体性能会受到一些影响。</number></name></p>
<p>现在，假设你的环境普通存储比SSD存储更多。 因此，您将需要为all-flash OSD选择单独的池。 这里有一些池可以先迁移到 all-flash。 您可以将以下列表解释为优先级列表：在向群集添加更多SSD容量时，可以逐个将池移动到全闪存存储。</p>
<ul>
<li>Nova ephemeral RBD池（vms，nova-compute）</li>
<li>radosgw bucket indexes .rgw.buckets.index and friends） - 如果你使用radosgw替换你OpenStack Swift</li>
<li>Cinder volume pools (cinder, volumes)</li>
<li>radosgw data pools (.rgw.buckets and friends)  - 如果您需要在Swift存储上进行低延迟读取和写入</li>
<li>Glance image pools (glance, images)</li>
<li>Cinder backup pools (cinder-backup)  - 通常是这是最后一个转换为 all-flash 的池。</li>
</ul>
<h2 id="配置一些具有低延迟本地存储的非Ceph计算主机">配置一些具有低延迟本地存储的非Ceph计算主机</h2><p>现在，毫无疑问，有一些应用场景，Ceph不会产生你所需要的延迟。 也许任何基于网络的存储都无法满足。 这只是存储和网络技术最近发展的直接结果。</p>
<p>就在几年前，对块设备的单扇区非缓存写入的平均延迟大约为毫秒或1000微秒（μs）。 相比之下，在承载512字节（1扇区）有效载荷的TCP分组上引起的延迟大约为50μs，这使得100μs的往返行程。 总而言之，从网络写入设备（而不是本地写入）所产生的额外延迟约为10％。</p>
<p>在过渡期间，对于相同价格的器件的单扇区写入本身约为100μs，顶级的，一些价格还是合理的设备下降到约40μs。 相比之下，网络延迟并没有改变那么多 - 从千兆以太网到10 GbE下降约20％。</p>
<p>因此，即使通过网络访问单个未复制的SSD设备，现在的延迟将为40 + 80 = 120μs，而本地仅为40μs。 这不是10％的开销了，这是一个惊人的三倍</p>
<p>使用Ceph，这变得更糟。 Ceph多次写入数据，首先到主OSD，然后（并行）写入所有副本。 因此，与40μs的单扇区写操作相比，我们现在至少有两次写操作的延迟，再加上两次网络往返，即40×2 + 80×2 =240μs，是本地写延迟的6倍 </p>
<p>好消息是，大多数应用程序不关心这种延迟开销，因为它们延迟不是关键的。 坏消息是，有些非常在意。</p>
<p>所以，你应该放弃Ceph因为这样吗？ 不。 但是请考虑添加一些未使用libvirt / images_type = rbd配置的计算节点，而是使用本地磁盘映像。 将这些主机进行主机聚合，并将它们映射到指定的flavor。 建议您的用户，他们选择这种flavor来跑低延迟的应用程序。</p>
<h2 id="引用">引用</h2><p><a href="https://www.hastexo.com/resources/hints-and-kinks/dos-donts-ceph-openstack/index.html" target="_blank" rel="external">本篇英文原文</a></p>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dos-and-dons.png" alt=""><br></center>

<p>Ceph和OpenStack是一个非常有用和非常受欢迎的组合。 不过，部署Ceph / OpenStack经常会有一些容易避免的缺点 - 我们将帮助你解决它们</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday武汉20161119分享回顾]]></title>
    <link href="http://www.zphj1987.com/2016/11/29/Cephday%E6%AD%A6%E6%B1%8920161119%E5%88%86%E4%BA%AB%E5%9B%9E%E9%A1%BE/"/>
    <id>http://www.zphj1987.com/2016/11/29/Cephday武汉20161119分享回顾/</id>
    <published>2016-11-29T04:29:37.000Z</published>
    <updated>2016-11-29T07:43:18.135Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/wuhan-china.png" alt="image_1b2ne8h4m1vfq1f7b1qgr262dfn9.png-405.3kB"><br></center>

<h2 id="活动地点">活动地点</h2><p>这次的分享活动是由武汉烽火进行赞助的，在烽火创新谷谷咖啡进行的，武汉的高校很多，光一本重点学校就有十所，武大，华科的学生的科研水平在全国也是非常强的，但是与之相匹配的大型企业却不是很多，本土大型企业更是少，这个地方还是一个相对封闭的地方，技术的交流远没有达到北上广深等地的开放程度，光谷也是在近十年成为一个科技飞速发展的地方，这与东湖高新的大力支持以及一些高校参与到企业当中来有很大的关系，未来的发展还是值得期待的<br><a id="more"></a></p>
<p>在这样的大环境下，烽火作为一个传统企业能以开放的姿态来支持开源社区的活动，还是非常好的一件事情</p>
<h2 id="分享内容">分享内容</h2><p>本次活动的分享有六个演讲，分别是：</p>
<ul>
<li>CEPH RBD MIRRORING-烽火李海静 </li>
<li>新的RADOS接口-多对象原子修改操作-麒麟云汪黎 </li>
<li>When Ceph Meets SPDK-xsky张和泉 </li>
<li>Ceph-based FC SAN-中兴宋柏森、付波 </li>
<li>基于Ceph的云存储备份系统的设计与实现-烽火丁刚</li>
<li>使用Bcache为Ceph OSD加速的具体实践-花瑞</li>
</ul>
<h2 id="分享解析">分享解析</h2><p>之前在深圳站结束以后我做了一次简单的解析，对于这些分享，我是根据自己所理解的给出自己的解析，这些分享都非常的好，每一个点都值得去挖掘更多的东西</p>
<h3 id="分享一：CEPH_RBD_MIRRORING-烽火李海静">分享一：CEPH RBD MIRRORING-烽火李海静</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/01_CEPH%20RBD%20MIRRORING%20by_%E6%9D%8E%E6%B5%B7%E9%9D%99.pdf" width="850" height="455"></center>

<p>eph的rbd mirroring功能是在Jewel版本才新引入的一个功能，主要目的是对ceph的rbd进行灾备的作用，在原有的写入模型之上又引入了一个mirror,在实际场景当中采用的是回滚记录的方式实现的数据一致性，这个在mysql数据的主备模式当中也是类似的处理方式，在ceph当中这个回滚所需要的带宽和io相对数据库场景来说，还是有一定的差别的，数据库的本地的io是非常的大，但是数据量很小，在进行一个合并二进制后，很小的传输量就能实现主备数据的一致，而在rbd的使用场景当中，本来就大量的读写，再加一层读写，这个量还是有很大影响的，当然这是提供了一种功能，具体就要看使用场景了</p>
<p>分享者是李海静，也是这次分享者里面唯一的女生，之前有简单的交流过一些ceph的问题，她就是ceph社区群里面的存储-西安-杀破狼，也是Ceph社区的活跃参与者之一，本篇分享开始详细的介绍了rbd mirroring的相关的配置，中间比较深入的就是在这个功能与openstack相结合以后，在openstack端需要做的一些改动，以及存在的一些问题，有需要这个功能的可以根据文档当中的提示进行实践</p>
<p>关于这个功能，个人觉得目前还是不适合上生产的功能，生产环境要求的是稳定性，在需要灾备的场景，尽量去做系统级别的灾备，也就是整个系统的备份，备份做的好的情况就是增量备份，并且制定好周期，尽量不去对生产环境有影响，否则可能为了上一个备份系统，反而造成了系统的不稳定，这个是架构师需要去深入研究和定夺的问题，关于rbd 的备份方案，已经就基于快照的增量备份，那个我还是比较推荐的一种方式，这个不在这里展开</p>
<h3 id="分享二：新的RADOS接口-多对象原子修改操作-麒麟云汪黎">分享二：新的RADOS接口-多对象原子修改操作-麒麟云汪黎</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/02_%E6%96%B0%E7%9A%84RADOS%E6%8E%A5%E5%8F%A3-%E5%A4%9A%E5%AF%B9%E8%B1%A1%E5%8E%9F%E5%AD%90%E4%BF%AE%E6%94%B9%E6%93%8D%E4%BD%9C%20by_%E6%B1%AA%E9%BB%8E.pdf" width="850" height="460"></center>

<p>汪黎老师这次带来的演讲是源码相关的演讲，汪老师是ceph的开发者之一，他们的团队就是kylin-cloud团队，他本身也是国防科大博士</p>
<p>这次的分享是他们提出的一个pr，这个会对性能有一定的提升，实现的是对象的原子性操作，这样能够实现并发的去操作对象，并且能在程序中实现一致性，将目前的一致性的保证挪到了raods层面，这个是一个很复杂的开发，目前还在开发阶段，有兴趣的同学可以跟汪黎老师进行交流，共同完成这个功能，对于源码不太会，这里就不做过多的解析</p>
<h3 id="分享三：When_Ceph_Meets_SPDK_-张和泉">分享三：When Ceph Meets SPDK -张和泉</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/03_When%20Ceph%20Meets%20SPDK%20by_%E5%BC%A0%E5%92%8C%E6%B3%89.pdf" width="850" height="530"></center>

<p>本篇来自的是Xsky的张和泉的分享，主要内容讲的是ceph中在加入了spdk以后的好处，这个由于spdk目前来说只是一个开发套件，并且这个里面的一些驱动的开发都是Xsky在引导在，目前大范围的配置还是不能实现的，并且目前来说一些驱动还是私有的，所以我们只能是看到是一个很好的未来，在稳定以后，不知道Intel是否会进行一些标准化的工作，从而能让更改人使用，目前具备开发能力的cepher可以尝试一下</p>
<p>spdk和dpdk都是比较新的开发套件，实现的功能是将存储的io和网络的io从目前的内核态剥离出来，变成用户态的，这样在高速存储的环境下能够最大化的发挥出硬件的性能，这个在最近的Intel的分享当中可以看到相关的资料，这一块目前我也只能做到将驱动载入，但是跟ceph怎么结合，还目前还是不知道从哪入手</p>
<h3 id="分享四:Ceph-based_FC_SAN_-宋柏森、付波">分享四:Ceph-based FC SAN -宋柏森、付波</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/04_Ceph-based%20FC%20SAN%20by_%E5%AE%8B%E6%9F%8F%E6%A3%AE%E3%80%81%E4%BB%98%E6%B3%A2.pdf" width="850" height="455"></center>

<p>本篇的分享来自中兴，实现的是基于RBD做的FC SAN,这个方案还是比较有特点的，整个是基于LIO来实现的，LIO是内核态的iscsi类方案，跟stgt还是有区别的，方案采用的是内核态的rbd，这个就屏蔽掉了cache的问题，因为内核的rbd是没有rbd cache的，性能会有一定的下降，在客户端采用多路径的工具来实现高可用和流量负载均衡，从而提高了带宽，提高了可用性，管理上加入了一个管理平台进行控制，配置文件采用的是对象存储存储到集群当中，保证了版本的一致性，这些都是一些很好的点，值得深入实践</p>
<p>国内ceph研发当中中兴还是比较低调的，背后应该还是做了很多事情的</p>
<h3 id="分享五：基于Ceph的云存储备份系统的设计与实现-丁刚">分享五：基于Ceph的云存储备份系统的设计与实现-丁刚</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/05_%E5%9F%BA%E4%BA%8ECeph%E7%9A%84%E4%BA%91%E5%AD%98%E5%82%A8%E5%A4%87%E4%BB%BD%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%20by_%E4%B8%81%E5%88%9A.pdf" width="850" height="455"></center>

<p>这篇来自烽火大数据的丁刚的演讲，备份场景是我觉得ceph最适合的一种场景了，一方面对延时没有那么高，另一方面ceph好扩展，并发好，这些都是适用于备份场景的，ceph的io路径较长，本篇就是讲的ceph在大数据下的使用场景</p>
<p>整个备份系统的处理是中间有一个拉取推送的系统，这是使用的C来写的，并且在拉取数据和推送数据的地方都采用了负载均衡，和高可用，系统的健壮性非常好</p>
<h3 id="分享六：使用Bcache为Ceph_OSD加速的具体实践-花瑞">分享六：使用Bcache为Ceph OSD加速的具体实践-花瑞</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/06_%E4%BD%BF%E7%94%A8Bcache%E4%B8%BACeph%20OSD%E5%8A%A0%E9%80%9F%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E8%B7%B5%20by_%E8%8A%B1%E7%91%9E.pdf" width="850" height="460"></center>

<p>本篇来自杉岩数据的花瑞做的分享，加速方案采用的是bcache，这个方案之前在邮件列表里面有看到国外的cepher有提到他的千兆ceph环境很容易出block，在使用加速方案以后得到了很好的改善，加速方案有很多，bcache，flashcache，EnhanceIO,dm-cache,原理都是相似的，都是采用一个ssd设备进行一个缓存，从而达到加速的，本篇详细的介绍了bcache的优点和原理，这个方案我个人也是很推荐的一种加速方案，选择加速方案的时候，需要进行几种软件的功能点比较，然后选择适合自己使用环境的，缓存方案主要是面向随机写的场景，顺序下大文件的场景，SSD可能看不出很大的优势</p>
<h2 id="总结">总结</h2><p>本次武汉的社区活动偏方案的要多一些，这些其实对于企业来说都是很好的分享点，面对各种解决方案，如何找到适合自己场景的方案才是最重要的</p>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/wuhan/wuhan-china.png" alt="image_1b2ne8h4m1vfq1f7b1qgr262dfn9.png-405.3kB"><br></center>

<h2 id="活动地点">活动地点</h2><p>这次的分享活动是由武汉烽火进行赞助的，在烽火创新谷谷咖啡进行的，武汉的高校很多，光一本重点学校就有十所，武大，华科的学生的科研水平在全国也是非常强的，但是与之相匹配的大型企业却不是很多，本土大型企业更是少，这个地方还是一个相对封闭的地方，技术的交流远没有达到北上广深等地的开放程度，光谷也是在近十年成为一个科技飞速发展的地方，这与东湖高新的大力支持以及一些高校参与到企业当中来有很大的关系，未来的发展还是值得期待的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[fio测试ceph的filestore]]></title>
    <link href="http://www.zphj1987.com/2016/11/23/fio%E6%B5%8B%E8%AF%95ceph%E7%9A%84filestore/"/>
    <id>http://www.zphj1987.com/2016/11/23/fio测试ceph的filestore/</id>
    <published>2016-11-23T15:04:53.000Z</published>
    <updated>2016-11-23T15:47:38.431Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fio-banner.jpg" alt="fio"><br></center>

<h2 id="前言">前言</h2><p>fio是一个适应性非常强的软件，基本上能够模拟所有的IO请求，是目前最全面的一款测试软件，之前在看德国电信的一篇分享的时候，里面就提到了，如果需要测试存储性能，尽量只用一款软件，这样从上层测试到底层去，才能更好的去比较差别</p>
<p>fio对于ceph来说，可以测试文件系统，基于文件系统之上测试，可以测试内核rbd，将rbdmap到本地格式化以后进行测试，或者基于librbd直接对rbd进行测试，这个是目前都已经有的测试场景，这些不在本篇的讨论的范围内，今天讲的是一种新的测试场景，直接对ceph的底层存储进行测试</p>
<a id="more"></a>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fiotest.png" alt=""><br></center>



<p>为什么会有这个，因为在以前，如果你要测试一块磁盘是不是适合ceph使用的时候，都是直接对挂载的磁盘进行一些测试，这个是基于文件系统的，并没有真正的模拟到ceph自己的写入模型，所以在开发人员的努力下，模拟对象的写入的驱动已经完成了，这就是本篇需要讲述的内容</p>
<h2 id="实践过程">实践过程</h2><p>fio engine for objectstore 这个是在ceph的11.0.2这个版本才正式发布出来的，可以看这个pr(<a href="https://github.com/ceph/ceph/pull/10267" target="_blank" rel="external">pr10267</a>),11.0.2是ceph第一个公开释放的KRAKEN版本的，也说明Jewel版本即将进入比较稳定的情况，新的功能可能会尽量在K版本进行开发</p>
<h3 id="下载相关代码">下载相关代码</h3><p>创建一个目录用于存储代码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /root/newceph</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cd /root/newceph/</span></span><br></pre></td></tr></table></figure></p>
<h4 id="下载fio的代码">下载fio的代码</h4><p>[root@lab8106 newceph]# git clone git://git.kernel.dk/fio.git</p>
<h4 id="下载ceph的代码">下载ceph的代码</h4><p>下载代码并且切换到指定的11.0.2分支，不要用master分支，里面还没有合进去，并且还有bug<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cd /root/newceph/</span></span><br><span class="line">[root@lab8106 newceph]<span class="comment"># git clone git://github.com/ceph/ceph.git</span></span><br><span class="line">[root@lab8106 newceph]<span class="comment"># cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#git checkout -b myfenzhi v11.0.2</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#git submodule update --init --recursive</span></span><br></pre></td></tr></table></figure></p>
<h4 id="创建一个cmake编译的目录并且编译">创建一个cmake编译的目录并且编译</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /root/newceph/build</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cd /root/newceph/build</span></span><br><span class="line">[root@lab8106 build]<span class="comment"># cmake -DWITH_FIO=ON -DFIO_INCLUDE_DIR=/root/newceph/fio/ -DCMAKE_BUILD_TYPE=Release /root/newceph/ceph </span></span><br><span class="line">[root@lab8106 build]<span class="comment"># make install -j 16</span></span><br></pre></td></tr></table></figure>
<p>安装完成检查是不是生成了这个库文件,fio就是利用这个库作为写入引擎的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 build]<span class="comment"># ll lib/libfio_ceph_objectstore.so</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">59090338</span> Nov <span class="number">23</span> <span class="number">22</span>:<span class="number">17</span> lib/libfio_ceph_objectstore.so</span><br></pre></td></tr></table></figure></p>
<p>将库路径让系统识别<br>export LD_LIBRARY_PATH=/root/newceph/build/lib/</p>
<p>编译fio<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cd /root/newceph/fio/</span></span><br><span class="line">[root@lab8106 fio]<span class="comment">#./configure</span></span><br><span class="line">[root@lab8106 fio]<span class="comment"># make</span></span><br></pre></td></tr></table></figure></p>
<p>如果显示下面的，就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 fio]<span class="comment"># ./fio --enghelp=libfio_ceph_objectstore.so</span></span><br><span class="line">conf                    : Path to a ceph configuration file</span><br></pre></td></tr></table></figure></p>
<h3 id="配置测试">配置测试</h3><p>下面需要准备两个配置文件，一个是ceph自身的，一个是fio配置文件，我们看下我的环境下这个配置文件如何写的<br>写fio的测试文件<br>vim filestore.fio<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">ioengine=libfio_ceph_objectstore.so <span class="comment"># must be found in your LD_LIBRARY_PATH</span></span><br><span class="line"></span><br><span class="line">conf=/etc/ceph/ceph-filestore.conf <span class="comment"># must point to a valid ceph configuration file</span></span><br><span class="line">directory=/var/lib/ceph/osd/ceph-<span class="number">8</span> <span class="comment"># directory for osd_data</span></span><br><span class="line"></span><br><span class="line">rw=randwrite</span><br><span class="line">iodepth=<span class="number">16</span></span><br><span class="line"></span><br><span class="line">time_based=<span class="number">1</span></span><br><span class="line">runtime=<span class="number">20</span>s</span><br><span class="line"></span><br><span class="line">[filestore]</span><br><span class="line">nr_files=<span class="number">64</span></span><br><span class="line">size=<span class="number">256</span>m</span><br><span class="line">bs=<span class="number">64</span>k</span><br></pre></td></tr></table></figure></p>
<p>上面的指定了一个配置文件和一个目录，这个目录是你需要测试的集群的存储的目录，里面不需要数据<br>写ceph的配置文件<br>vim /etc/ceph/ceph-filestore.conf<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">    debug filestore = <span class="number">0</span>/<span class="number">0</span></span><br><span class="line">    debug journal = <span class="number">0</span>/<span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># spread objects over 8 collections</span></span><br><span class="line">    osd pool default pg num = <span class="number">8</span></span><br><span class="line">    <span class="comment"># increasing shards can help when scaling number of collections</span></span><br><span class="line">    osd op num shards = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    filestore fd cache size = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">[osd]</span><br><span class="line">    osd objectstore = filestore</span><br><span class="line"></span><br><span class="line">    <span class="comment"># use directory= option from fio job file</span></span><br><span class="line">    osd data =  /var/lib/ceph/osd/ceph-<span class="number">8</span>/</span><br><span class="line"></span><br><span class="line">    <span class="comment"># journal inside fio_dir</span></span><br><span class="line">    osd journal =  /var/lib/ceph/osd/ceph-<span class="number">8</span>/journal</span><br><span class="line">    osd journal size = <span class="number">5000</span></span><br><span class="line">    journal force aio = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>配置文件指定数据目录，和journal路径</p>
<p>开始测试</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 fio]<span class="comment"># ./fio  filestore.fio </span></span><br><span class="line">filestore: (g=<span class="number">0</span>): rw=randwrite, bs=<span class="number">64</span>K-<span class="number">64</span>K/<span class="number">64</span>K-<span class="number">64</span>K/<span class="number">64</span>K-<span class="number">64</span>K, ioengine=ceph-os, iodepth=<span class="number">16</span></span><br><span class="line">fio-<span class="number">2.15</span>-<span class="number">11</span>-g42f1</span><br><span class="line">Starting <span class="number">1</span> process</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.713473</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> filestore(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) backend xfs (magic <span class="number">0</span>x58465342)</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.804601</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> filestore(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) backend xfs (magic <span class="number">0</span>x58465342)</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.805003</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> genericfilestorebackend(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) detect_features: FIEMAP ioctl is disabled via <span class="string">'filestore fiemap'</span> config option</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.805018</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> genericfilestorebackend(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) detect_features: SEEK_DATA/SEEK_HOLE is disabled via <span class="string">'filestore seek data hole'</span> config option</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.805020</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> genericfilestorebackend(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) detect_features: splice() is disabled via <span class="string">'filestore splice'</span> config option</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.864962</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> genericfilestorebackend(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) detect_features: syncfs(<span class="number">2</span>) syscall fully supported (by glibc and kernel)</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.865056</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> xfsfilestorebackend(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) detect_feature: extsize is disabled by conf</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.865643</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> filestore(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) start omap initiation</span><br><span class="line"><span class="number">2016</span>-<span class="number">11</span>-<span class="number">23</span> <span class="number">22</span>:<span class="number">32</span>:<span class="number">17.926589</span> <span class="number">7</span>f7536d56780  <span class="number">0</span> filestore(/var/lib/ceph/osd/ceph-<span class="number">8</span>/) mount: enabling WRITEAHEAD journal mode: checkpoint is not enabled</span><br></pre></td></tr></table></figure>
<p>可以看到，已经开始以对象存储的IO模型去生成测试了，根据自己的需要对不同的存储设备和组合进行测试就可以了</p>
<h2 id="总结">总结</h2><p>作为一个新的测试模型的出现，更加完善了ceph的整体体系，也给磁盘的选型增加更好的测试工具</p>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fio-banner.jpg" alt="fio"><br></center>

<h2 id="前言">前言</h2><p>fio是一个适应性非常强的软件，基本上能够模拟所有的IO请求，是目前最全面的一款测试软件，之前在看德国电信的一篇分享的时候，里面就提到了，如果需要测试存储性能，尽量只用一款软件，这样从上层测试到底层去，才能更好的去比较差别</p>
<p>fio对于ceph来说，可以测试文件系统，基于文件系统之上测试，可以测试内核rbd，将rbdmap到本地格式化以后进行测试，或者基于librbd直接对rbd进行测试，这个是目前都已经有的测试场景，这些不在本篇的讨论的范围内，今天讲的是一种新的测试场景，直接对ceph的底层存储进行测试</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[查询OSD运行在哪些cpu上]]></title>
    <link href="http://www.zphj1987.com/2016/11/16/%E6%9F%A5%E8%AF%A2OSD%E8%BF%90%E8%A1%8C%E5%9C%A8%E5%93%AA%E4%BA%9Bcpu%E4%B8%8A/"/>
    <id>http://www.zphj1987.com/2016/11/16/查询OSD运行在哪些cpu上/</id>
    <published>2016-11-16T08:58:58.000Z</published>
    <updated>2016-11-16T15:08:55.857Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cpu.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在看CPU相关的文章的时候，想起来之前有文章讨论是否要做CPU绑定，这个有说绑定的也有说不绑定的，然后就想到一个问题，有去观测这些OSD到底运行在哪些CPU上面么,有问题就好解决了，现在就是要查下机器上的OSD运行在哪些CPU上</p>
<a id="more"></a>
<h2 id="代码">代码</h2><p>这里直接上代码了，最近学习python在，就用python来实现<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"><span class="keyword">import</span> commands</span><br><span class="line"><span class="keyword">from</span> prettytable <span class="keyword">import</span> PrettyTable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) == <span class="number">1</span>:</span><br><span class="line">        printosdcputable(<span class="string">"process"</span>)</span><br><span class="line">    <span class="keyword">elif</span> sys.argv[<span class="number">1</span>] == <span class="string">'t'</span>:</span><br><span class="line">        printosdcputable(<span class="string">"thread"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printosdcputable</span><span class="params">(choose)</span>:</span></span><br><span class="line">    <span class="keyword">print</span> choose</span><br><span class="line">    row = PrettyTable()</span><br><span class="line">    row.header = <span class="keyword">True</span></span><br><span class="line">    cpulist = [<span class="string">"OSD\CPU"</span>]</span><br><span class="line">    corelist=[<span class="string">"Core ID"</span>]</span><br><span class="line">    phylist = [<span class="string">"Physical ID"</span>]</span><br><span class="line">    emplist=[<span class="string">"-----------"</span>]</span><br><span class="line">    <span class="keyword">for</span> cpupro <span class="keyword">in</span> range(psutil.cpu_count()):</span><br><span class="line">        cpulist.append(<span class="string">"%s"</span> %cpupro )</span><br><span class="line"></span><br><span class="line">        coreid=commands.getoutput(<span class="string">'egrep \'processor|physical id|core id\' /proc/cpuinfo | cut -d : -f 2 | paste - - -  | awk  \'$1==%s &#123;print $3 &#125;\''</span> %cpupro)</span><br><span class="line">        corelist.append(<span class="string">"%s"</span> %coreid)</span><br><span class="line"></span><br><span class="line">        phyid = commands.getoutput(<span class="string">'egrep \'processor|physical id|core id\' /proc/cpuinfo | cut -d : -f 2 | paste - - -  | awk  \'$1==%s &#123;print $2 &#125;\''</span> % cpupro)</span><br><span class="line">        phylist.append(<span class="string">"%s"</span> %phyid)</span><br><span class="line">        emplist.append(<span class="string">"--"</span>)</span><br><span class="line"></span><br><span class="line">    row.field_names = cpulist</span><br><span class="line">    row.add_row(corelist)</span><br><span class="line">    row.add_row(phylist)</span><br><span class="line">    row.add_row(emplist)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(<span class="string">'/var/run/ceph/'</span>):</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"osd"</span>  <span class="keyword">in</span> name <span class="keyword">and</span> <span class="string">"pid"</span> <span class="keyword">in</span> name :</span><br><span class="line">                osdlist = []</span><br><span class="line">                osdthlist=[]</span><br><span class="line">                <span class="keyword">for</span> osdcpu <span class="keyword">in</span> range(psutil.cpu_count()):</span><br><span class="line">                    osdlist.append(<span class="string">" "</span>)</span><br><span class="line">                    osdthlist.append(<span class="string">"0"</span>)</span><br><span class="line">                pidfile=root+ name</span><br><span class="line">                osdid=commands.getoutput(<span class="string">'ls  %s|cut -d "." -f 2 2&gt;/dev/null'</span>  %pidfile )</span><br><span class="line">                osdpid = commands.getoutput(<span class="string">'cat %s  2&gt;/dev/null'</span> %pidfile)</span><br><span class="line">                osd_runcpu = commands.getoutput(<span class="string">'ps -o  psr -p %s |grep -v PSR 2&gt;/dev/null'</span> %osdpid)</span><br><span class="line">                th_list = commands.getoutput(<span class="string">'ps -o  psr -L  -p %s |grep -v PSR|awk \'gsub(/^ *| *$/,"")\'  2&gt;/dev/null'</span> % osdpid)</span><br><span class="line"></span><br><span class="line">                osdname=<span class="string">"osd."</span>+osdid</span><br><span class="line">                osdlist[int(osd_runcpu)]=<span class="string">"+"</span></span><br><span class="line">                <span class="keyword">for</span> osdth <span class="keyword">in</span> th_list.split(<span class="string">'\n'</span>):</span><br><span class="line">                    osdthlist[int(osdth)] = int(osdthlist[int(osdth)])+<span class="number">1</span></span><br><span class="line">                osdlist.insert(<span class="number">0</span>,osdname)</span><br><span class="line">                osdthlist.insert(<span class="number">0</span>,osdname)</span><br><span class="line">                <span class="keyword">if</span> choose == <span class="string">"process"</span>:</span><br><span class="line">                    row.add_row(osdlist)</span><br><span class="line">                <span class="keyword">elif</span> choose == <span class="string">"thread"</span>:</span><br><span class="line">                    row.add_row(osdthlist)</span><br><span class="line">    <span class="keyword">print</span> row</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>运行脚本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch python getosdcpu.py</span><br></pre></td></tr></table></figure></p>
<p>或者监控线程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch python getosdcpu.py t</span><br></pre></td></tr></table></figure></p>
<p>运行效果如下：</p>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/osdcpu.png" alt=""><br></center>

<p>线程的情况</p>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/thread.png" alt=""><br></center>


<p>看上去确实有些CPU上面运行了多个OSD，这里不讨论CPU绑定的好坏，只是展示现象，具体有什么效果，是需要用数据取分析的，这个以后再看下</p>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cpu.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在看CPU相关的文章的时候，想起来之前有文章讨论是否要做CPU绑定，这个有说绑定的也有说不绑定的，然后就想到一个问题，有去观测这些OSD到底运行在哪些CPU上面么,有问题就好解决了，现在就是要查下机器上的OSD运行在哪些CPU上</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[处理stale的pg]]></title>
    <link href="http://www.zphj1987.com/2016/11/14/%E5%A4%84%E7%90%86stale%E7%9A%84pg/"/>
    <id>http://www.zphj1987.com/2016/11/14/处理stale的pg/</id>
    <published>2016-11-14T12:32:27.000Z</published>
    <updated>2016-11-14T12:58:58.506Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/wremch.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在某些场景下Ceph集群会出现stale的情况，也就是ceph集群PG的僵死状态，这个状态实际上是无法处理新的请求了，新的请求过来只会block，那么我们如何去恢复环境</p>
<h2 id="实践过程">实践过程</h2><p>首先模拟stale环境，这个比较好模拟</p>
<blockquote>
<p>设置副本2，然后同时关闭两个OSD（不同故障域上），然后删除这两个OSD</p>
</blockquote>
<p>集群这个时候就会出现stale的情况了，因为两份数据都丢了，在一些环境下，数据本身就是临时的或者不是那么重要的，比如存储日志，这样的环境下，只需要快速的恢复环境即可，而不担心数据的丢失</p>
<a id="more"></a>
<h3 id="处理过程">处理过程</h3><p>首先用ceph pg dump|grep stale 找出所有的stale的pg</p>
<p>然后用 ceph force_create_pg  pg_id</p>
<p>如果做到这里，可以看到之前的stale的状态的PG，现在已经是creating状态的了，这个时候一个关键的步骤需要做下</p>
<p>就是重启整个集群的OSD，在重启完成了以后，集群的状态就会恢复正常了，也能够正常的写入新的数据了，对于老的数据，做下清理即可</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-14</td>
</tr>
</tbody>
</table>
<h2 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/wremch.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在某些场景下Ceph集群会出现stale的情况，也就是ceph集群PG的僵死状态，这个状态实际上是无法处理新的请求了，新的请求过来只会block，那么我们如何去恢复环境</p>
<h2 id="实践过程">实践过程</h2><p>首先模拟stale环境，这个比较好模拟</p>
<blockquote>
<p>设置副本2，然后同时关闭两个OSD（不同故障域上），然后删除这两个OSD</p>
</blockquote>
<p>集群这个时候就会出现stale的情况了，因为两份数据都丢了，在一些环境下，数据本身就是临时的或者不是那么重要的，比如存储日志，这样的环境下，只需要快速的恢复环境即可，而不担心数据的丢失</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[yum安装Ceph指定Jewel版本]]></title>
    <link href="http://www.zphj1987.com/2016/11/14/yum%E5%AE%89%E8%A3%85Ceph%E6%8C%87%E5%AE%9AJewel%E7%89%88%E6%9C%AC/"/>
    <id>http://www.zphj1987.com/2016/11/14/yum安装Ceph指定Jewel版本/</id>
    <published>2016-11-14T10:04:16.000Z</published>
    <updated>2016-11-14T12:58:47.384Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yum-logo.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>通过yum安装指定的rpm包，这个一般是 <code>yum --showduplicates list ceph | expand</code>,然后去通过yum安装指定的版本即可，这个在hammer下是没有问题的，但是在Jewel下进行安装的时候却出现了问题，我们来看下怎么解决这个问题的</p>
<h2 id="实践过程">实践过程</h2><p>我们需要安装 <code>ceph-10.2.0-0.el7</code> 这个版本的，根据之前的方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum install  ceph-10.2.0-0.el7</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">base                                                                            | <span class="number">3.6</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">ceph                                                                            | <span class="number">2.9</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">ceph-noarch                                                                     | <span class="number">2.9</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">epel                                                                            | <span class="number">4.3</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">···</span><br><span class="line">Error: Package: <span class="number">1</span>:ceph-base-<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br><span class="line">           Requires: librados2 = <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7</span><br><span class="line">           Removing: <span class="number">1</span>:librados2-<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7.x86_64 (@ceph)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7</span><br><span class="line">           Updated By: <span class="number">1</span>:librados2-<span class="number">10.2</span>.<span class="number">3</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">3</span>-<span class="number">0</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">0.8</span>.el7.x86_64 (epel)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">0.8</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">3</span>.el7.x86_64 (base)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">3</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">10.1</span>.<span class="number">0</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>可以看到我们指定了ceph-10.2.0,但是这个rpm包的依赖却自动的去升级到了librados2-10.2.3，然后这个10.2.3又会跟准备安装的ceph-10.2.0冲突了，然后就会提示无法安装了</p>
<p>问题已经找到了，我们如何解决这个问题，第一想法就是应该把版本限制住，在参阅了一些资料以后，发现yum确实可以支持这个需求的，我们来限制下版本</p>
<h3 id="限制yum版本">限制yum版本</h3><p>vim /etc/yum.conf<br>在[main]当中,添加下面的内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">exclude=*<span class="number">10.2</span>.<span class="number">3</span>* *<span class="number">10.2</span>.<span class="number">2</span>* *<span class="number">10.2</span>.<span class="number">1</span>*</span><br></pre></td></tr></table></figure></p>
<p>为什么写了三个，因为在10.2.0之上有三个版本的，这个地方进行全匹配的方式进行限制</p>
<h3 id="安装ceph-10-2-0">安装ceph-10.2.0</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum install  ceph-10.2.0-0.el7</span></span><br><span class="line">Dependencies Resolved</span><br><span class="line"></span><br><span class="line">==================================================================================================================================================================================================================</span><br><span class="line"> Package                                                 Arch                                          Version                                                  Repository                                   Size</span><br><span class="line">==================================================================================================================================================================================================================</span><br><span class="line">Updating:</span><br><span class="line"> ceph                                                    x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">3.1</span> k</span><br><span class="line">Installing <span class="keyword">for</span> dependencies:</span><br><span class="line"> ceph-base                                               x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">4.2</span> M</span><br><span class="line"> ceph-mds                                                x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">2.8</span> M</span><br><span class="line"> ceph-mon                                                x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">2.8</span> M</span><br><span class="line"> ceph-osd                                                x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">9.0</span> M</span><br><span class="line"> ceph-selinux                                            x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                         <span class="number">20</span> k</span><br><span class="line"> libradosstriper1                                        x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">1.8</span> M</span><br><span class="line"> librgw2                                                 x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">2.8</span> M</span><br><span class="line">Updating <span class="keyword">for</span> dependencies:</span><br><span class="line"> ceph-common                                             x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                         <span class="number">15</span> M</span><br><span class="line"> libcephfs1                                              x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">1.8</span> M</span><br><span class="line"> librados2                                               x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">1.9</span> M</span><br><span class="line"> librbd1                                                 x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">2.4</span> M</span><br><span class="line"> python-cephfs                                           x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                         <span class="number">67</span> k</span><br><span class="line"> python-rados                                            x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                        <span class="number">146</span> k</span><br><span class="line"> python-rbd                                              x86_64                                        <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7                                           ceph                                         <span class="number">62</span> k</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">==================================================================================================================================================================================================================</span><br><span class="line">Install             ( <span class="number">7</span> Dependent packages)</span><br><span class="line">Upgrade  <span class="number">1</span> Package  (+<span class="number">7</span> Dependent packages)</span><br></pre></td></tr></table></figure>
<p>可以正确的安装了</p>
<h3 id="总结">总结</h3><p>通过yum去指定版本，然后去过滤高的版本的方式，来安装了指定的版本的ceph</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-14</td>
</tr>
</tbody>
</table>
<h3 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h3><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yum-logo.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>通过yum安装指定的rpm包，这个一般是 <code>yum --showduplicates list ceph | expand</code>,然后去通过yum安装指定的版本即可，这个在hammer下是没有问题的，但是在Jewel下进行安装的时候却出现了问题，我们来看下怎么解决这个问题的</p>
<h2 id="实践过程">实践过程</h2><p>我们需要安装 <code>ceph-10.2.0-0.el7</code> 这个版本的，根据之前的方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum install  ceph-10.2.0-0.el7</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">base                                                                            | <span class="number">3.6</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">ceph                                                                            | <span class="number">2.9</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">ceph-noarch                                                                     | <span class="number">2.9</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>     </span><br><span class="line">epel                                                                            | <span class="number">4.3</span> kB  <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">···</span><br><span class="line">Error: Package: <span class="number">1</span>:ceph-base-<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br><span class="line">           Requires: librados2 = <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">0</span>-<span class="number">0</span>.el7</span><br><span class="line">           Removing: <span class="number">1</span>:librados2-<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7.x86_64 (@ceph)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7</span><br><span class="line">           Updated By: <span class="number">1</span>:librados2-<span class="number">10.2</span>.<span class="number">3</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">3</span>-<span class="number">0</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">0.8</span>.el7.x86_64 (epel)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">0.8</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">3</span>.el7.x86_64 (base)</span><br><span class="line">               librados2 = <span class="number">1</span>:<span class="number">0.80</span>.<span class="number">7</span>-<span class="number">3</span>.el7</span><br><span class="line">           Available: <span class="number">1</span>:librados2-<span class="number">10.1</span>.<span class="number">0</span>-<span class="number">0</span>.el7.x86_64 (ceph)</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[预估Ceph集群恢复时间]]></title>
    <link href="http://www.zphj1987.com/2016/11/10/%E9%A2%84%E4%BC%B0Ceph%E9%9B%86%E7%BE%A4%E6%81%A2%E5%A4%8D%E6%97%B6%E9%97%B4/"/>
    <id>http://www.zphj1987.com/2016/11/10/预估Ceph集群恢复时间/</id>
    <published>2016-11-10T06:52:46.000Z</published>
    <updated>2016-11-10T09:26:14.117Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/time-and-money.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本章很简单，就是预估集群恢复的时间,这个地方是简单的通过计算来预估需要恢复的实际，动态的显示<br><a id="more"></a></p>
<h2 id="二、代码">二、代码</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import commands</span><br><span class="line">import json</span><br><span class="line">def main():</span><br><span class="line">    gettime()</span><br><span class="line">def conversecs(sec):</span><br><span class="line">    d = sec/<span class="number">86400</span></span><br><span class="line">    h = sec%<span class="number">86400</span>/<span class="number">3600</span></span><br><span class="line">    m = sec%<span class="number">3600</span>/<span class="number">60</span></span><br><span class="line">    s = sec%<span class="number">60</span></span><br><span class="line">    <span class="built_in">return</span> <span class="string">"remain time:%s day %s hour %s min %s sec"</span> %(d,h,m,s)</span><br><span class="line">def gettime():</span><br><span class="line">    try:</span><br><span class="line">        recover_time = commands.getoutput(<span class="string">'timeout 10 ceph -s -f json 2&gt;/dev/null'</span>)</span><br><span class="line">        json_str = json.loads(recover_time)</span><br><span class="line">        <span class="keyword">if</span> json_str[<span class="string">"pgmap"</span>].has_key(<span class="string">'degraded_objects'</span>) == True:</span><br><span class="line">            degraded_objects = json_str[<span class="string">"pgmap"</span>][<span class="string">"degraded_objects"</span>]</span><br><span class="line">            <span class="keyword">if</span> json_str[<span class="string">"pgmap"</span>].has_key(<span class="string">'recovering_objects_per_sec'</span>) == True and json_str[<span class="string">"pgmap"</span>][<span class="string">"recovering_objects_per_sec"</span>] != <span class="number">0</span>:</span><br><span class="line">                recovering_objects_per_sec = json_str[<span class="string">"pgmap"</span>][<span class="string">"recovering_objects_per_sec"</span>]</span><br><span class="line">                resec=degraded_objects/recovering_objects_per_sec</span><br><span class="line">                <span class="built_in">print</span>  <span class="string">"recovery  objects: %s"</span> %(degraded_objects)</span><br><span class="line">                <span class="built_in">print</span>   <span class="string">"recovery speed :%s"</span> %(recovering_objects_per_sec)</span><br><span class="line">                <span class="built_in">print</span>  conversecs(resec)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                resec=degraded_objects/<span class="number">1</span></span><br><span class="line">                <span class="built_in">print</span>  <span class="string">"recovery  objects: %s"</span> %(degraded_objects)</span><br><span class="line">                <span class="built_in">print</span>   <span class="string">"recovery speed :0"</span></span><br><span class="line">                <span class="built_in">print</span>  conversecs(resec)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span> <span class="string">"recover all  done!"</span></span><br><span class="line">    except:</span><br><span class="line">        <span class="built_in">print</span> <span class="string">"Ceph Cluster health？try ceph -s"</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="执行">执行</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch python  recoverytime.py</span><br></pre></td></tr></table></figure>
<h2 id="三、效果">三、效果</h2><center><br><img src="http://static.zybuluo.com/zphj1987/5cxdv72kdsd718joyfolg0db/image_1b16geboc1nok1iif73n6uuij719.png" alt=""><br></center>


<h2 id="四、进度">四、进度</h2><p>目前只统计了恢复的，还要考虑backfill的，后续增加</p>
<h3 id="五、变更记录">五、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-10</td>
</tr>
</tbody>
</table>
<h3 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h3><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/time-and-money.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本章很简单，就是预估集群恢复的时间,这个地方是简单的通过计算来预估需要恢复的实际，动态的显示<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue4]]></title>
    <link href="http://www.zphj1987.com/2016/11/09/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue4/"/>
    <id>http://www.zphj1987.com/2016/11/09/Ceph用户邮件列表Vol45-Issue4/</id>
    <published>2016-11-09T03:37:34.000Z</published>
    <updated>2016-11-12T12:25:26.892Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayfour.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_4">ceph Vol 45 Issue 4</h2><h3 id="1-upgrade_from_v0-94-6_or_lower_and_‘failed_to_encode_map_X_with_expected_crc’">1.upgrade from v0.94.6 or lower and ‘failed to encode map X with expected crc’</h3><blockquote>
<p>f user upgrades the cluster from a prior release to v0.94.7 or up by<br>following the steps:</p>
<ol>
<li>upgrade the monitors first,</li>
<li>and then the OSDs.</li>
</ol>
<p>It is expected that the cluster log will be flooded with messages like:</p>
<p>2016-07-12 08:42:42.1234567 osd.1234 [WRN] failed to encode map e4321<br>with expected crc</p>
</blockquote>
<p>这个是开发者kefu chai发出来的邮件，是提醒用户注意一个升级的问题的，先介绍下这个问题<br><a id="more"></a></p>
<p>因为在ceph的hammer的0.94.7版本开始采用了一种新的osdmap的编码方式，在更新了以后，mon会用新的编码方式发送新的增量osdmap到其他osd，但是老的osd上还是老的编码方式，就会产生CRC错误，提示不匹配，然后OSD就会向MON请求全量的osdmap<br>对于一个很大的ceph集群就会有下面的问题</p>
<p>1、mon会因为这个clog产生大量消息flood<br>2、mon因为需要发送全量的osdmap增加负载<br>3、网络会被大量的osdmap的全量的消息占用<br>4、因为osdmap更新和网络的大量请求，客户端出现slow request</p>
<p>对于已经升级的了集群解决办法是：<br>先降低到之前的版本</p>
<ul>
<li>升级OSD的机器到新的版本</li>
<li>升级MON的机器到新的版本</li>
</ul>
<p>如果准备计划升级的集群</p>
<ul>
<li>先升级OSD的机器到新的版本</li>
<li>再升级MON的机器到新的版本</li>
</ul>
<p>目前社区准备解决这个问题（<a href="http://tracker.ceph.com/issues/17386" target="_blank" rel="external">Issue</a>）</p>
<p>目前可以用上面的方法避免</p>
<h3 id="问题重现方法">问题重现方法</h3><p>配置一个0.94.6或者以下的集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">6</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure></p>
<p>配置好了后升级<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">7</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure></p>
<p>升级以后如果不重启进程实际上是没更新的，根据官方的建议先重启mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart mon</span><br></pre></td></tr></table></figure></p>
<p>然后重启osd，然后查看ceph -w<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart osd</span><br></pre></td></tr></table></figure></p>
<p>可以看到failed to encode map e4321 with expected crc</p>
<h3 id="换一种升级方式">换一种升级方式</h3><p>先降级，这里用yum的方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum <span class="built_in">history</span> list</span><br></pre></td></tr></table></figure></p>
<p>找到刚刚upgrade的编号<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum <span class="built_in">history</span> undo <span class="number">289</span></span><br></pre></td></tr></table></figure></p>
<p>就可以降级到0.94.6了</p>
<h3 id="新的操作">新的操作</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">7</span>-<span class="number">0</span>.el7</span><br></pre></td></tr></table></figure>
<p>先重启OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart osd</span><br></pre></td></tr></table></figure></p>
<p>再重启mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph restart mon</span><br></pre></td></tr></table></figure></p>
<p>再观察<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -w</span><br></pre></td></tr></table></figure></p>
<p>已经没有提示了</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-09</td>
</tr>
</tbody>
</table>
<h3 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h3><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayfour.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_4">ceph Vol 45 Issue 4</h2><h3 id="1-upgrade_from_v0-94-6_or_lower_and_‘failed_to_encode_map_X_with_expected_crc’">1.upgrade from v0.94.6 or lower and ‘failed to encode map X with expected crc’</h3><blockquote>
<p>f user upgrades the cluster from a prior release to v0.94.7 or up by<br>following the steps:</p>
<ol>
<li>upgrade the monitors first,</li>
<li>and then the OSDs.</li>
</ol>
<p>It is expected that the cluster log will be flooded with messages like:</p>
<p>2016-07-12 08:42:42.1234567 osd.1234 [WRN] failed to encode map e4321<br>with expected crc</p>
</blockquote>
<p>这个是开发者kefu chai发出来的邮件，是提醒用户注意一个升级的问题的，先介绍下这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue3]]></title>
    <link href="http://www.zphj1987.com/2016/11/08/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue3/"/>
    <id>http://www.zphj1987.com/2016/11/08/Ceph用户邮件列表Vol45-Issue3/</id>
    <published>2016-11-08T09:26:01.000Z</published>
    <updated>2016-11-10T07:14:26.966Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daythree.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_3">ceph Vol 45 Issue 3</h2><h3 id="1-Crash_in_ceph_readdir-">1.Crash in ceph_readdir.</h3><blockquote>
<p>Hello, </p>
<p>I’ve been investigating the following crash with cephfs:<br>···<br>According to the state of the ceph_inoide_info this means that<br>ceph_dir_is_complete_ordered would return true and the second condition<br>should also be true since ptr_pos is held in r12 and the dir size is 26496.<br>So the dentry being passed should be the 2953 % 512 = 393 in the<br>cache_ctl.dentries array.<br>Unfortunately my crashdump excldues the page cache pages and I cannot really see<br>what are the contents of the dentries array. </p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>Could you provide any info on how to further debug this </p>
</blockquote>
<p>作者在使用cephfs的时候遇上了崩溃的情况，readdir的操作</p>
<p>Yan, Zheng已经对这个bug进行了修复</p>
<p><a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=af5e5eb574776cdf1b756a27cc437bff257e22fe" target="_blank" rel="external">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=af5e5eb574776cdf1b756a27cc437bff257e22fe</a><br><a href="https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a3d714c33632ef6bfdfaacc74ae6ba297b4c5820" target="_blank" rel="external">https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=a3d714c33632ef6bfdfaacc74ae6ba297b4c5820</a></p>
<p>但是这个是提交到Linux kernel的4.6的分支里面去了的，所以目前从官方版本来说是4.6或者更新的版本才会解决</p>
<p>这个问题只能是说遇到了再升级内核了</p>
<h3 id="2-Can’t_activate_OSD">2.Can’t activate OSD</h3><blockquote>
<p>Hello all,</p>
<p>Over the past few weeks I’ve been trying to go through the Quick Ceph Deploy<br>tutorial at:</p>
<p>ceph-deploy osd activate ceph02:/dev/sdc ceph03:/dev/sdc</p>
<p>part. It never actually seems to activate the OSD and eventually times out:</p>
<p>[ceph03][INFO  ] Running command: sudo /usr/sbin/ceph-disk -v activate  —mark-init systemd —mount /dev/sdc<br>[ceph03][WARNIN] main_activate: path = /dev/sdc<br>[ceph03][WARNIN] No data was received after 300 seconds, disconnecting…</p>
</blockquote>
<p>作者在部署osd的时候出现无法激活osd的问题，最后在别人的帮助下发现了问题，在交换机上创建了 VLAN ，但没允许jumbo packets，所以出现了问题</p>
<p>另外一个人也出现了类似的问题，通过升级了parted解决问题（from 3.1 from the CentOS7 base）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -Uhv ftp://<span class="number">195.220</span>.<span class="number">108.108</span>/linux/fedora/linux/updates/<span class="number">22</span>/x86_64/p/parted-<span class="number">3.2</span>-<span class="number">16</span>.fc22.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>这个一般没什么问题，确实定位到这里再升级了，一般情况下很少出现不能activate osd的情况</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
<tr>
<td style="text-align:center">完成Issue 3</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
<h3 id="我的公众号-磨磨谈">我的公众号-磨磨谈</h3><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/qrcode_for_gh_6998a54d68f7_430.jpg" alt=""><br></center>]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daythree.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_3">ceph Vol 45 Issue 3</h2><h3 id="1-Crash_in_ceph_readdir-">1.Crash in ceph_readdir.</h3><blockquote>
<p>Hello, </p>
<p>I’ve been investigating the following crash with cephfs:<br>···<br>According to the state of the ceph_inoide_info this means that<br>ceph_dir_is_complete_ordered would return true and the second condition<br>should also be true since ptr_pos is held in r12 and the dir size is 26496.<br>So the dentry being passed should be the 2953 % 512 = 393 in the<br>cache_ctl.dentries array.<br>Unfortunately my crashdump excldues the page cache pages and I cannot really see<br>what are the contents of the dentries array. </p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue2]]></title>
    <link href="http://www.zphj1987.com/2016/11/07/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue2/"/>
    <id>http://www.zphj1987.com/2016/11/07/Ceph用户邮件列表Vol45-Issue2/</id>
    <published>2016-11-07T05:26:01.000Z</published>
    <updated>2016-11-09T14:35:02.794Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daytwo.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_2">ceph Vol 45 Issue 2</h2><h3 id="1-CephFS:_No_space_left_on_device">1.CephFS: No space left on device</h3><blockquote>
<p>After upgrading to 10.2.3 we frequently see messages like</p>
<p>‘rm: cannot remove ‘…’: No space left on device</p>
<p>The folders we are trying to delete contain approx. 50K files 193 KB each.</p>
<p>The cluster state and storage available are both OK:</p>
<p>   cluster 98d72518-6619-4b5c-b148-9a781ef13bcb<br>     health HEALTH_WARN<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>     monmap e1: 1 mons at {000-s-ragnarok=XXX.XXX.XXX.XXX:6789/0}<br>            election epoch 11, quorum 0 000-s-ragnarok<br>      fsmap e62643: 1/1/1 up {0=000-s-ragnarok=up:active}<br>     osdmap e20203: 16 osds: 16 up, 16 in<br>            flags sortbitwise<br>      pgmap v15284654: 1088 pgs, 2 pools, 11263 GB data, 40801 kobjects<br>            23048 GB used, 6745 GB / 29793 GB avail<br>                1085 active+clean<br>                   2 active+clean+scrubbing<br>                   1 active+clean+scrubbing+deep</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>Has anybody experienced this issue so far?</p>
</blockquote>
<p>这个问题是作者在升级了一个集群以后（jewel 10.2.3），做删除的时候，发现提示了 No space left on device，按正常的理解做删除不会出现提示空间不足</p>
<p>这个地方的原因是，有一个参数会对目录的entry做一个最大值的控制<code>mds_bal_fragment_size_max</code>,而这个参数实际上在做删除操作的时候，当文件被unlink的时候，被放入待删除区的时候，这个也是被限制住的，所以需要调整这个参数，如果有上百万的文件被等待删除的时候，可能就会出现这个情况,并且出现 <code>failing to respond to cache pressure</code> 我们根据自己的需要去设置这个值</p>
<p>默认的 mds_bal_fragment_size_max=100000，也就是单个文件10万文件，如果不调整，单目录写入10万文件就能出现上面的问题</p>
<p>这个地方可以用命令来监控mds的当前状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># ceph daemonperf mds.lab8106</span></span><br><span class="line">-----mds------ --mds_server-- ---objecter--- -----mds_cache----- ---mds_<span class="built_in">log</span>---- </span><br><span class="line">rlat inos caps|hsr  hcs  hcr |writ <span class="built_in">read</span> actv|recd recy stry purg|segs evts subm|</span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |  <span class="number">0</span>    <span class="number">0</span>   <span class="number">36</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k   <span class="number">0</span> | <span class="number">33</span>   <span class="number">29</span>k   <span class="number">0</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |  <span class="number">6</span>    <span class="number">0</span>   <span class="number">34</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k   <span class="number">6</span> | <span class="number">33</span>   <span class="number">29</span>k   <span class="number">6</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> | <span class="number">24</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k  <span class="number">24</span> | <span class="number">32</span>   <span class="number">29</span>k  <span class="number">24</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">163</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> | <span class="number">42</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">145</span>k  <span class="number">42</span> | <span class="number">32</span>   <span class="number">29</span>k  <span class="number">42</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">972</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">144</span>k <span class="number">970</span> | <span class="number">33</span>   <span class="number">27</span>k <span class="number">971</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">905</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">143</span>k <span class="number">905</span> | <span class="number">31</span>   <span class="number">28</span>k <span class="number">906</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">969</span>    <span class="number">0</span>   <span class="number">32</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">142</span>k <span class="number">969</span> | <span class="number">32</span>   <span class="number">29</span>k <span class="number">970</span> </span><br><span class="line">  <span class="number">0</span>  <span class="number">159</span>k   <span class="number">5</span> |  <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span> |<span class="number">601</span>    <span class="number">0</span>   <span class="number">31</span> |  <span class="number">0</span>    <span class="number">0</span>  <span class="number">141</span>k <span class="number">601</span> | <span class="number">33</span>   <span class="number">29</span>k <span class="number">602</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方还有一个硬链接删除以后没有释放stry的问题，最新版的master里面已经合进去了代码（<a href="https://github.com/ukernel/ceph/commit/edc84d905a1f0e3c504f427cc4693c7a98561e7c" target="_blank" rel="external">scan_link</a>）</p>
<p>修复过程如下<br>执行flush MDS journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph daemon mds.xxx flush journal</span><br></pre></td></tr></table></figure></p>
<p>停止掉所有mds<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">stop all mds</span><br></pre></td></tr></table></figure></p>
<p>执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephfs-data-scan scan_links</span><br></pre></td></tr></table></figure></p>
<p>重启mds<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">restart mds</span><br></pre></td></tr></table></figure></p>
<p>执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph daemon mds.x scrub_path / recursive repair</span><br></pre></td></tr></table></figure></p>
<p>执行完了以后去对目录进行一次ll，可以看到mds_cache的stry的就会被清理干净了</p>
<p>这个问题就可以解决了,实际测试中在换了新版本以后，重启后然后进行目录的ll，也能清空stry</p>
<h3 id="2-_Blog_post_about_Ceph_cache_tiers_-_feedback_welcome">2. Blog post about Ceph cache tiers - feedback welcome</h3><blockquote>
<p>Hi all,</p>
<p>as it took quite a while until we got our Ceph cache working (and we’re still hit but some unexpected things, see the thread Ceph with cache pool - disk usage / cleanup), I thought it might be good to write a summary of what I (believe) to know up to this point.</p>
<p>Any feedback, especially corrections is highly welcome!</p>
<p><a href="http://maybebuggy.de/post/ceph-cache-tier/" target="_blank" rel="external">http://maybebuggy.de/post/ceph-cache-tier/</a></p>
<p>Greetings<br>-Sascha-</p>
</blockquote>
<p>这是一篇分享文，作者因为最近想深入研究下ceph的cache pool，作者写的文章非常的好，这里先直接翻译这篇文章，然后再加入我自己的相关数据</p>
<h3 id="blog原文"><a href="http://maybebuggy.de/post/ceph-cache-tier/" target="_blank" rel="external">blog原文</a></h3><p>作者想启动blog写下自己的Openstack和Ceph的相关经验，第一个话题就选择了<code>Ceph cache tiering</code>, 作者的使用场景为短时间的虚拟机，用来跑测试的，这种场景他们准备用Nvme做一个缓冲池来加速的虚拟机</p>
<p>cache 相关的一些参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">target_max_bytes</span><br><span class="line">target_max_objects</span><br><span class="line">cache_target_dirty_ratio</span><br><span class="line">cache_target_full_ratio</span><br><span class="line">cache_min_flush_age</span><br><span class="line">cache_min_evict_age</span><br></pre></td></tr></table></figure></p>
<p>Jewel版本还新加入了一个参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cache_target_dirty_high_ratio</span><br></pre></td></tr></table></figure></p>
<p>作者的想法是先把数据写入到缓冲池当中，等后面某个时刻再写入到真实的存储池的当中</p>
<p>Flushing vs. Evicting<br>Flushing是将缓冲池中的数据刷到真实的存储池当中去，但是并不去删除缓冲池里面缓存的数据，只有clean的数据才能被evic，如果是dirty的数据做evic，那么先要flush到真实存储池，然后再删除掉</p>
<p>Cache 调整</p>
<p>Ceph的是不能够自动确定缓存池的大小，所以这里需要配置一个缓冲池的绝对大小，flush/evic将无法工作。</p>
<p>设置了上限以后，相关的参数就是cache_target_full_ratio和cache_target_dirty_ratio。这些参数是控制什么时候进行flush和evic的</p>
<p>这个dirty ratio是比较难设置的值，需要根据场景进行相关的调整</p>
<p>新版本里面到了dirty_high_ratio才开始下刷</p>
<p>还有cache_min_flush_age和cache_min_evict_age这个控制，这个一般来说到了设定的阀值前，这些对象的留存时间应该是要够老的，能够被触发清理掉的</p>
<p>通过ceph df detail 可以观测你的存储池的数据的情况</p>
<p>里面会有一些0字节对象的，缓冲池的0字节对象是数据已经被删除了，防止刷新的时候又要操作对象。在真实存储池中的0字节对象是数据已经在缓冲池当中，但没有刷新到缓冲池</p>
<h3 id="案例测试">案例测试</h3><p>基于上面的控制，下面我们来具体看下这些参数的实际效果是怎样的，这样我们才能真正在实际场景当中做到精准的控制</p>
<p>首先我们要对参数分类</p>
<ul>
<li>缓冲池的总大小，这个大小分成两类一个对象个数控制，一个大小的控制</li>
<li>flush和evic的百分比，这个百分比既按照大小进行控制，也按照对象进行控制</li>
<li>flush和evic的时间控制</li>
</ul>
<p>分好类以后，我们就开始我们的测试，基于对象的数目的控制，比较容易观察，我们就用对象控制来举例子</p>
<h3 id="创建一个缓冲池的环境">创建一个缓冲池的环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create testpool <span class="number">24</span> <span class="number">24</span> </span><br><span class="line">ceph osd pool create cachepool <span class="number">24</span> <span class="number">24</span></span><br><span class="line">ceph osd tier add  testpool cachepool</span><br><span class="line">ceph osd tier cache-mode  cachepool writeback</span><br><span class="line">ceph osd tier <span class="built_in">set</span>-overlay  testpool cachepool</span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_<span class="built_in">type</span> bloom</span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_count <span class="number">1</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool hit_<span class="built_in">set</span>_period <span class="number">3600</span></span><br></pre></td></tr></table></figure>
<p>上面的操作是基本的一些操作、我们现在做参数相关的调整<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool  target_max_bytes <span class="number">1000000000000</span></span><br></pre></td></tr></table></figure></p>
<p>为了排除干扰，我们把 target_max_bytes设置成了1T，我们的测试数据很少，肯定不会触发这个大小</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool target_max_objects <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p>设置缓冲池的对象max为1000<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_target_dirty_ratio <span class="number">0.4</span></span><br></pre></td></tr></table></figure></p>
<p>设置dirty_ratio为0.4，也就是0.4为判断为dirty的阀值<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_target_full_ratio <span class="number">0.8</span></span><br></pre></td></tr></table></figure></p>
<p>设置cache_target_full_ratio为0.8，即超过80%的时候需要evic<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_flush_age <span class="number">600</span></span><br><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_evict_age <span class="number">1800</span></span><br></pre></td></tr></table></figure></p>
<p>设置两个flush和evic的时间，这个时间周期比我写入的数据的时间周期大很多，这个等下会调整这个</p>
<p>开启一个终端动态观察存储池的对象变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># watch ceph df</span></span><br><span class="line">Every <span class="number">2.0</span>s: ceph df                                                                    </span><br><span class="line"></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED</span><br><span class="line">    <span class="number">834</span>G      <span class="number">833</span>G         <span class="number">958</span>M          <span class="number">0.11</span></span><br><span class="line">POOLS:</span><br><span class="line">    NAME          ID     USED       %USED     MAX AVAIL     OBJECTS</span><br><span class="line">    rbd           <span class="number">0</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">277</span>G           <span class="number">0</span></span><br><span class="line">    metadata	  <span class="number">1</span>	  <span class="number">61953</span>k      <span class="number">0.01</span>          <span class="number">416</span>G          <span class="number">39</span></span><br><span class="line">    data          <span class="number">2</span>	  <span class="number">50500</span>k      <span class="number">0.01</span>          <span class="number">416</span>G       <span class="number">50501</span></span><br><span class="line">    testpool	  <span class="number">5</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">416</span>G           <span class="number">0</span></span><br><span class="line">    cachepool     <span class="number">6</span>           <span class="number">0</span>         <span class="number">0</span>          <span class="number">416</span>G           <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>尝试写入数据并且观察，到了1000左右的时候停止<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p testpool bench <span class="number">100</span> write  -b <span class="number">4</span>K --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>可以观察到cachepool的对象数目大概在1100-1200之间，一直写也会是这个数字，在停止写以后，观察cachepool的对象数目在960左右，我们设置的 target_max_objects 为1000，在超过了这个值以后，并且写停止的情况下，系统会把这个cache pool的对象控制在比target_max少50左右，现在我们修改下<code>cache_min_evict_age</code>这个参数，看下会发生些什么</p>
<p>我们把这个参数调整为30<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> cachepool cache_min_evict_age <span class="number">30</span></span><br></pre></td></tr></table></figure></p>
<p>设置完了以后，可以看到cache pool的对象数目在 744左右，现在再写入数据，然后等待，看下会是多少，还是756，如果按我们设置的<code>cache_target_full_ratio</code>0.8就正好是800，我们尝试再次调整大cache_min_evict_age看下情况，对象维持在960左右，根据这个测试，基本上可以看出来是如何控制缓存的数据了，下面用一张图来看下这个问题</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/cache.png" alt=""></p>
<p>来总结一下：</p>
<ul>
<li>如果cache pool对象到了 target_max_objects，那么会边flush，边evic，然后因为前面有客户端请求，这个时候实际是会阻塞的</li>
<li>如果停止了写请求，系统会自动将cache pool的对象控制在比 target_max_objects 少一点点</li>
<li>如果时间周期到了cache_min_evict_age，那么系统会自动将cache pool的对象控制在比 cache_target_full_ratio 少一点点</li>
<li>同理如果到了cache_min_flush_age，那么会将对象往真实的存储池flush到 cache_target_dirty_ratio 少一点点</li>
</ul>
<p>也就是ratio是给定了一个比例，然后时间到了就去将缓存控制到指定的ratio，这个地方就需要根据需要去控制缓冲池数据是留有多少的缓存余地的</p>
<p>使用命令清空缓冲池的数据，会将数据flush到真实存储池，然后将数据evic掉</p>
<p>关于缓冲池的就写这么多了，实际环境是要根据自己的使用场景去制定这些值的，从而能保证缓冲池能真正起到作用，上面的例子是基于对象的控制的，基于大小的控制是一样的，只是将对象数的设置换成了大小即可，然后尽量去放大对象的控制</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p cachepool cache-try-flush-evict-all</span><br></pre></td></tr></table></figure>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-07</td>
</tr>
<tr>
<td style="text-align:center">完成缓冲池相关</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/daytwo.jpg" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_2">ceph Vol 45 Issue 2</h2><h3 id="1-CephFS:_No_space_left_on_device">1.CephFS: No space left on device</h3><blockquote>
<p>After upgrading to 10.2.3 we frequently see messages like</p>
<p>‘rm: cannot remove ‘…’: No space left on device</p>
<p>The folders we are trying to delete contain approx. 50K files 193 KB each.</p>
<p>The cluster state and storage available are both OK:</p>
<p>   cluster 98d72518-6619-4b5c-b148-9a781ef13bcb<br>     health HEALTH_WARN<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>            mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure<br>     monmap e1: 1 mons at {000-s-ragnarok=XXX.XXX.XXX.XXX:6789/0}<br>            election epoch 11, quorum 0 000-s-ragnarok<br>      fsmap e62643: 1/1/1 up {0=000-s-ragnarok=up:active}<br>     osdmap e20203: 16 osds: 16 up, 16 in<br>            flags sortbitwise<br>      pgmap v15284654: 1088 pgs, 2 pools, 11263 GB data, 40801 kobjects<br>            23048 GB used, 6745 GB / 29793 GB avail<br>                1085 active+clean<br>                   2 active+clean+scrubbing<br>                   1 active+clean+scrubbing+deep</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue1]]></title>
    <link href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/"/>
    <id>http://www.zphj1987.com/2016/11/04/Ceph用户邮件列表Vol45-Issue1/</id>
    <published>2016-11-04T03:57:25.000Z</published>
    <updated>2016-11-09T07:52:50.245Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayone.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html" target="_blank" rel="external">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br><a id="more"></a><br>I tried to mark the PG as complete using ceph-objectstore-tool but it did not help as the PG is in fact complete but for some reason blocks.</p>
<p>I tried recreating an empty osd.87 and importing the PG exported from other replica but it did not help.</p>
<p>Can someone help me please? This is really important.</p>
</blockquote>
<p>这个问题是作者一个集群中(ceph 0.94.5)出现了一个磁盘损坏以后造成了一些对象的丢失，然后在做了一定的处理以后，集群状态已经正常了，但是还是新的请求会出现block的状态，这个情况下如何处理才能让集群正常，作者贴出了pg dump，ceph -s,ceph osd dump相关信息，当出现异常的时候，需要人协助的时候，应该提供这些信息方便其他人定位问题，最后这个问题作者自己给出了自己的解决办法，出现的时候影响是当时的流量只有正常情况下的10%了，影响还是很大的</p>
<h3 id="复现问题过程">复现问题过程</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd put testremove testremove</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd map rbd testremove</span></span><br><span class="line">osdmap e85 pool <span class="string">'rbd'</span> (<span class="number">0</span>) object <span class="string">'testremove'</span> -&gt; pg <span class="number">0</span>.eaf226a7 (<span class="number">0.27</span>) -&gt; up ([<span class="number">1</span>,<span class="number">0</span>], p1) acting</span><br></pre></td></tr></table></figure>
<p>写入文件,找到文件，然后去后台删除对象<br>然后停止掉其中一个OSD，这里选择停掉主OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">1</span></span><br><span class="line">ceph osd out <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看状态pg被锁住状态active+degrade，不会迁移完整,并且会检测到了有数据unfound了</p>
<p>然后向这个对象发起get请求<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd get testremove testfile</span></span><br></pre></td></tr></table></figure></p>
<p>前端rados请求会卡住，后端出现 requests are blocked</p>
<p>看下如何处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph pg <span class="number">0.27</span> mark_unfound_lost delete</span><br></pre></td></tr></table></figure></p>
<p>邮件列表作者的环境，这个命令也无法执行，直接卡死，后来发现有个执行窗口，就是这个对象所在的PG的OSD在启动过程中还是可以接受命令的，就在这个执行窗口执行这个命令就可以解决了</p>
<p>执行了以后可以执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados  -p rbd  get testremove  a</span></span><br><span class="line">error getting rbd/testremove: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>这个时候查询集群的状态可以看到，集群已经正常的恢复了，不会因为一个对象的丢失造成集群的PG状态卡在待迁移状态</p>
<p>可以看到请求是失败的但是不会像之前一样卡死的状态，卡死是比失败更严重的一种状态</p>
<p>如果不想看到老的 slow request ,那么就重启这个卡住的PG所在的osd，如果本来就正常了，那么这个异常状态就会消失</p>
<p>这个是一个需要人工干预的状态，实际上模拟的就是对象丢失的场景，什么情况下会对象丢失，一般来说，底层磁盘的故障，写下去的对象当时记录着有，正好写入完成又准备写副本的时候，磁盘坏了，这个就有比较高的概率出现，所以出现了坏盘要尽早更换</p>
<p>本系列是只会对列表的当天的非re进行一个汇总，这样保持了一个问题的追踪都在一篇里面，所以这一天只有这一个问题</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-04</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/number/dayone.png" alt=""><br></center>

<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph日历格子之2016年10月(Vol45)]]></title>
    <link href="http://www.zphj1987.com/2016/11/03/Ceph%E6%97%A5%E5%8E%86%E6%A0%BC%E5%AD%90%E4%B9%8B2016%E5%B9%B410%E6%9C%88-Vol45/"/>
    <id>http://www.zphj1987.com/2016/11/03/Ceph日历格子之2016年10月-Vol45/</id>
    <published>2016-11-03T09:51:33.000Z</published>
    <updated>2016-11-08T10:01:03.670Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br><a id="more"></a></p>
<h2 id="二、内容">二、内容</h2><p>Ceph邮件列表2016年10月Issue格子   </p>
<h3 id="Vol_45">Vol 45</h3><table>
<thead>
<tr>
<th style="text-align:center">一</th>
<th style="text-align:center">二</th>
<th style="text-align:center">三</th>
<th style="text-align:center">四</th>
<th style="text-align:center">五</th>
<th style="text-align:center">六</th>
<th style="text-align:center">日</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center">-</td>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/" target="_blank" rel="external">Issue 1</a></td>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/07/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue2/" target="_blank" rel="external">Issue2</a></td>
</tr>
<tr>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/08/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue3/" target="_blank" rel="external">Issue 3</a></td>
<td style="text-align:center">Issue 4</td>
<td style="text-align:center">Issue 5</td>
<td style="text-align:center">Issue 6</td>
<td style="text-align:center">Issue 7</td>
<td style="text-align:center">Issue 8</td>
<td style="text-align:center">Issue 9</td>
</tr>
<tr>
<td style="text-align:center">Issue 10</td>
<td style="text-align:center">Issue 11</td>
<td style="text-align:center">Issue12</td>
<td style="text-align:center">Issue 13</td>
<td style="text-align:center">Issue 14</td>
<td style="text-align:center">Issue 15</td>
<td style="text-align:center">Issue 16</td>
</tr>
<tr>
<td style="text-align:center">Issue 17</td>
<td style="text-align:center">Issue 18</td>
<td style="text-align:center">Issue 19</td>
<td style="text-align:center">Issue 20</td>
<td style="text-align:center">Issue 21</td>
<td style="text-align:center">Issue 22</td>
<td style="text-align:center">Issue 23</td>
</tr>
<tr>
<td style="text-align:center">Issue 24</td>
<td style="text-align:center">Issue 25</td>
<td style="text-align:center">Issue 26</td>
<td style="text-align:center">Issue 27</td>
<td style="text-align:center">Issue 28</td>
<td style="text-align:center">Issue 29</td>
<td style="text-align:center">Issue 30</td>
</tr>
<tr>
<td style="text-align:center">Issue 31</td>
</tr>
</tbody>
</table>
<h2 id="三、本月主要问题如下：">三、本月主要问题如下：</h2><p>如何处理对象丢失引起的PG状态不对的问题(Issue1)<br>处理Cephfs的No space left on device(Issue2)<br>处理mds0: Client XXX.XXX.XXX.XXX failing to respond to cache pressure(Issue2)<br>处理stry not clean(Issue2)<br>关于缓冲池相关参数的控制（Issue2）<br>Cephfs readdir出现crash的问题（Issue3）<br>无法 activate OSD（Issue3）</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-03</td>
</tr>
<tr>
<td style="text-align:center">增加Issue2</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-07</td>
</tr>
<tr>
<td style="text-align:center">完成Issue2,完成Issue3</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph部署的时候修改默认权重]]></title>
    <link href="http://www.zphj1987.com/2016/11/02/Ceph%E9%83%A8%E7%BD%B2%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%9D%83%E9%87%8D/"/>
    <id>http://www.zphj1987.com/2016/11/02/Ceph部署的时候修改默认权重/</id>
    <published>2016-11-02T07:46:07.000Z</published>
    <updated>2016-11-02T09:49:40.801Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br><a id="more"></a></p>
<h2 id="二、修改">二、修改</h2><p>如果统一的初始值，那么直接添加参数即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_crush_initial_weight</span><br></pre></td></tr></table></figure></p>
<p>如果想自己添加算法，那么就根据下面的去做就可以了</p>
<h3 id="2-1_centos+jewel">2.1 centos+jewel</h3><p>修改：<br>/usr/lib/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k <span class="variable">$data</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/107374182 ; r = sprintf("%.4f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改这个地方的值就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">2</span>`</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2_centos+hammer">2.2 centos+hammer</h3><p>修改 /etc/init.d/ceph<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(df -P -k $osd_data/. | tail -1 | awk '&#123; print sprintf("%.2f",$2/1073741824)</span> &#125;')"</span></span><br></pre></td></tr></table></figure></p>
<p>修改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(echo 5)</span>"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-3_ubuntu+hammer">2.3 ubuntu+hammer</h3><p>由于ubuntu用initctl控制服务，不是用的/etc/init.d/ceph/,所以要修改另外的一个路径<br>修改/usr/libexec/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k /var/lib/ceph/osd/<span class="variable">$&#123;cluster:-ceph&#125;</span>-<span class="variable">$id</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/1073741824 ; r = sprintf("%.2f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">8</span>`</span><br></pre></td></tr></table></figure></p>
<h2 id="三、总结">三、总结</h2><p>这个比较简单，通过修改取值就可以改变默认配置了,上面的可以根据自己的需求加入算法即可</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday深圳20161029总结]]></title>
    <link href="http://www.zphj1987.com/2016/10/31/Cephday%E6%B7%B1%E5%9C%B320161029%E6%80%BB%E7%BB%93/"/>
    <id>http://www.zphj1987.com/2016/10/31/Cephday深圳20161029总结/</id>
    <published>2016-10-31T04:34:15.000Z</published>
    <updated>2016-10-31T07:31:46.461Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br><a id="more"></a></p>
<h2 id="二、PPT内容">二、PPT内容</h2><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/01_%E4%BC%81%E4%B8%9A%E7%BA%A7Ceph%E4%B9%8B%E8%B7%AF%20-%20iSCSI%E5%AE%9E%E8%B7%B5%E4%B8%8E%E4%BC%98%E5%8C%96%20by_%E9%82%B1%E5%B0%9A%E9%AB%98.pdf" width="850" height="530"></center>


<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/02_YY%E4%BA%91%E5%B9%B3%E5%8F%B0Ceph%E5%AE%9E%E8%B7%B5%20by_%E6%88%9A%E6%98%B1.pdf" width="850" height="690"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/03_%E4%B8%AD%E5%85%B4%E4%BA%91%E5%AD%98%E5%82%A8%E4%B9%8B%E8%B7%AFby_%E9%AA%86%E7%A7%91%E5%AD%A6&%E4%BB%BB%E7%84%95%E6%96%87.pdf" width="850" height="540"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/04_Swift%20Using%20Ceph%20Backend%20by_%E4%BA%8E%E8%88%AA.pdf" width="850" height="530"></center>

<h2 id="三、总结">三、总结</h2><p>第一篇来自杉岩数据的分享，主要集中在ceph在iscsi方面的实现，从开源解决方案来说，有一些弊端，或者高级的属性无法支持，在这个基础上，杉岩数据给出了自己的企业级解决方案，增加了企业级的支持，从功能来说还是增加了不少功能，如果能开源当然就更好了</p>
<p>第二篇来自YY云平台的分享，讲述了ceph在YY平台中的实践，这里面比较有用的是超多osd的线程过载问题，以及ubuntu下的系统检查触发的坑，这个都是很有价值的分享，ubuntu下面有几个类似的检查服务我曾经使用的时候也踩过类似的坑</p>
<p>第三篇来自中兴的分享，中兴基于ceph有一套商业存储，这里面比较有用的分享是mon数据的恢复问题，这个功能一开始的出来的时候我就进行了测试，触发了一个bug，然后有个人进行更深入的测试，从这次分享来看，深入测试的那个人应该来自中兴的，这里面提出了多种方案，其中的基于map的重建是之前没见过的，其他两种都是实践过，没有问题，再一个分享就是文件系统的配额的，这个给出的两种方案也是可行的，一种基于内核态的，一种基于用户态的，这个都是比较好的分享</p>
<p>第四篇来自奥思数据分享，主要集中在swift对象存储的分享，这个接触不多，研究对象存储的同学可以看看有什么可以借鉴的没</p>
<p>从上面的几篇分享来看，都是很好的分享，分享者自己都提出问题给出了解决方案，或者指明了一些方向，在技术相对封闭了环境下也是不可多得资料，希望类似的活动越来越多，多进行交流，提高行业整体水平，在ceph布道过程中，ceph中国社区在背后默默做了很多的工作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-31</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix实现自定义自动发现的流程]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix实现自定义自动发现的流程/</id>
    <published>2016-10-28T05:56:24.000Z</published>
    <updated>2016-11-01T03:56:27.977Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>这里我的宏就是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="comment">#POOLNAME&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>然后要创建一个监控项原型：<br>也是一个名称和一个键值：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- 名称：<span class="built_in">test</span> on <span class="variable">$1</span></span><br><span class="line">- 键值：ceph.pools.used[&#123;<span class="comment">#POOLNAME&#125;]</span></span><br></pre></td></tr></table></figure>
<p>这个地方名称可以用参数形式，包含的就是下面的那个键值中的参数对应的位置的值</p>
<p>然后需要去写一个这样的键值的收集<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">UserParameter=ceph.pools, python /sbin/ceph-status.py pools</span><br><span class="line">UserParameter=ceph.pools.used[*], python /sbin/ceph-status.py pool_used <span class="variable">$1</span></span><br></pre></td></tr></table></figure></p>
<p>测试下效果：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools.used["rbd"]</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方，上面的是去做的自动发现，下面的键值就是根据上面的自动发现返回的值去作为一个参数进行新的查询，后面的$1就是将参数传到收集的脚本里面去的，这样就能根据自动发现的不同的名称返回来不同的值，从而添加不同的监控项目，而不需要自己一个个添加了</p>
<h2 id="三、总结">三、总结</h2><p>自动发现实际上就是需要首先去获得需要监控的值，然后将这个值作为一个新的参数传递到另外一个收集数据的item里面去，这样就可以了，这里是做的最基本的单项数据的获取，后面应该会遇到多重变量的情况，就需要赋值多重变量，到时需要用到再记录下</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix自动发现的python方式数据生成]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84python%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix自动发现的python方式数据生成/</id>
    <published>2016-10-27T17:11:08.000Z</published>
    <updated>2016-10-27T17:29:38.340Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br><a id="more"></a></p>
<p>我们同样的来看看ceph原生的命令的json接口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s -f json</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"health"</span>:&#123;<span class="string">"health"</span>:&#123;<span class="string">"health_services"</span>:[&#123;<span class="string">"mons"</span>:[&#123;<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"kb_total"</span>:<span class="number">52403200</span>,<span class="string">"kb_used"</span>:<span class="number">32905432</span>,<span class="string">"kb_avail"</span>:<span class="number">19497768</span>,<span class="string">"avail_percent"</span>:<span class="number">37</span>,<span class="string">"last_updated"</span>:<span class="string">"2016-10-28 01:15:29.431854"</span>,<span class="string">"store_stats"</span>&#123;<span class="string">"bytes_total"</span>:<span class="number">20206814</span>,<span class="string">"bytes_sst"</span>:<span class="number">16929998</span>,<span class="string">"bytes_log"</span>:<span class="number">3080192</span>,<span class="string">"bytes_misc"</span>:<span class="number">196624</span>,<span class="string">"last_updated"</span>:<span class="string">"0.000000"</span>&#125;,<span class="string">"health"</span>:<span class="string">"HEALTH_OK"</span>&#125;]&#125;]&#125;,<span class="string">"timechecks"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">4</span>,<span class="string">"round"</span>:<span class="number">0</span>,<span class="string">"round_status"</span>:<span class="string">"finished"</span>&#125;,<span class="string">"summary"</span>:[],<span class="string">"overall_status"</span>:<span class="string">"HEALTH_OK"</span>,<span class="string">"detail"</span>:[]&#125;,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"election_epoch"</span>:<span class="number">4</span>,<span class="string">"quorum"</span>:[<span class="number">0</span>],<span class="string">"quorum_names"</span>:[<span class="string">"lab8106"</span>],<span class="string">"monmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">1</span>,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"modified"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"created"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"mons"</span>:[&#123;<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"addr"</span>:<span class="string">"192.168.8.106:6789\/0"</span>&#125;]&#125;,<span class="string">"osdmap"</span>:&#123;<span class="string">"osdmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">63</span>,<span class="string">"num_osds"</span>:<span class="number">2</span>,<span class="string">"num_up_osds"</span>:<span class="number">2</span>,<span class="string">"num_in_osds"</span>:<span class="number">2</span>,<span class="string">"full"</span>:<span class="literal">false</span>,<span class="string">"nearfull"</span>:<span class="literal">false</span>,<span class="string">"num_remapped_pgs"</span>:<span class="number">0</span>&#125;&#125;,<span class="string">"pgmap"</span>:&#123;<span class="string">"pgs_by_state"</span>:[&#123;<span class="string">"state_name"</span>:<span class="string">"active+clean"</span>,<span class="string">"count"</span>:<span class="number">80</span>&#125;],<span class="string">"version"</span>:<span class="number">19174</span>,<span class="string">"num_pgs"</span>:<span class="number">80</span>,<span class="string">"data_bytes"</span>:<span class="number">45848191333</span>,<span class="string">"bytes_used"</span>:<span class="number">45966077952</span>,<span class="string">"bytes_avail"</span>:<span class="number">551592390656</span>,<span class="string">"bytes_total"</span>:<span class="number">597558468608</span>&#125;,<span class="string">"fsmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">5</span>,<span class="string">"id"</span>:<span class="number">1</span>,<span class="string">"up"</span>:<span class="number">1</span>,<span class="string">"in"</span>:<span class="number">1</span>,<span class="string">"max"</span>:<span class="number">1</span>,<span class="string">"by_rank"</span>:[&#123;<span class="string">"filesystem_id"</span>:<span class="number">1</span>,<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"status"</span>:<span class="string">"up:active"</span>&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样也是这个类型的数据，好了，这里直接上代码：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def get_cluster_pools():</span><br><span class="line">    try:</span><br><span class="line">        pool_list=[]</span><br><span class="line">        data_dic = &#123;&#125;</span><br><span class="line">        cluster_pools = commands.getoutput(<span class="string">'timeout 10 ceph osd pool ls -f json 2&gt;/dev/null'</span>)</span><br><span class="line">        json_str = json.loads(cluster_pools)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_str:</span><br><span class="line">            pool_dic = &#123;&#125;</span><br><span class="line">            pool_dic[<span class="string">'&#123;#POOLNAME&#125;'</span>] = str(item)</span><br><span class="line">            pool_list.append(pool_dic)</span><br><span class="line">        data_dic[<span class="string">'data'</span>] = pool_list</span><br><span class="line">        <span class="built_in">return</span> json.dumps(data_dic,separators=(<span class="string">','</span>, <span class="string">':'</span>))</span><br><span class="line">    except:</span><br><span class="line">        <span class="built_in">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>输出如下<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;&#34;data&#34;:[&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;rbd&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;metedata&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;data&#34;&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>跟上面的格式一样了，关键在对字典进行赋值的处理，然后进行一个空格处理就完成了</p>
<h2 id="三、总结">三、总结</h2><p>还是接触的太少，造成简单的处理都需要花费比较久的时间</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br>]]>
    
    </summary>
    
      <category term="zabbix" scheme="http://www.zphj1987.com/tags/zabbix/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker与Ceph的分与合]]></title>
    <link href="http://www.zphj1987.com/2016/10/19/Docker%E4%B8%8ECeph%E7%9A%84%E5%88%86%E4%B8%8E%E5%90%88/"/>
    <id>http://www.zphj1987.com/2016/10/19/Docker与Ceph的分与合/</id>
    <published>2016-10-19T15:46:12.000Z</published>
    <updated>2016-10-19T15:48:24.553Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br><a id="more"></a></p>
<h2 id="二、实践">二、实践</h2><h3 id="2-1、配置docker的基础环境">2.1、配置docker的基础环境</h3><p>拉取基础镜像<br>这个是拉取的灵雀云的docker仓库的centos<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull index.alauda.cn/library/centos</span><br></pre></td></tr></table></figure></p>
<p>启动docker进程,并且设置自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure></p>
<p>查询当前机器上面的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">index.alauda.cn/library/centos   latest              <span class="number">904</span>d6c400333        <span class="number">4</span> months ago        <span class="number">196.7</span> MB</span><br></pre></td></tr></table></figure></p>
<p>我们先对我们的镜像做一些基本的设置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -itd --name=cephbase --net=host --pid=host index.alauda.cn/library/centos /bin/bash</span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker attach cephbase</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 /]<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                                                                     Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/docker-<span class="number">8</span>:<span class="number">2</span>-<span class="number">83216</span>-dd340d1f6a68b6849b9500c4e6f9b7fb1901c3c0cb1ce0d7336f5104a1ef4a10   <span class="number">10</span>G  <span class="number">240</span>M  <span class="number">9.8</span>G   <span class="number">3</span>% /</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda2                                                                                       <span class="number">50</span>G   <span class="number">31</span>G   <span class="number">20</span>G  <span class="number">62</span>% /etc/hosts</span><br><span class="line">shm</span><br></pre></td></tr></table></figure></p>
<p>可以看到我们已经进入了容器内部了，下面需要做的事情，就是将ceph运行需要的一些软件装上去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install wget --nogpgcheck</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># rm -rf /etc/yum.repos.d/*.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># vi /etc/yum.repos.d/ceph.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install ceph ceph-deploy</span></span><br></pre></td></tr></table></figure></p>
<p>检查软件版本装对了没<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">10.2</span>.<span class="number">3</span> (ecc23778eb545d8dd55e2e4735b53cc93f92e65b)</span><br><span class="line">[root@lab8106 /]<span class="comment"># ceph-deploy --version</span></span><br><span class="line"><span class="number">1.5</span>.<span class="number">36</span></span><br></pre></td></tr></table></figure></p>
<p>可以退出了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>查看之前的容器的ID<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE                            COMMAND             CREATED             STATUS                      PORTS               NAMES</span><br><span class="line"><span class="number">48420</span>c9955b5        index.alauda.cn/library/centos   <span class="string">"/bin/bash"</span>         About an hour ago   Exited (<span class="number">0</span>) <span class="number">14</span> seconds ago                       cephbase</span><br></pre></td></tr></table></figure></p>
<p>将容器保存为一个新的镜像，cephbase<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker commit 48420c9955b5 cephbase</span></span><br><span class="line">sha256:ffe236ee2bb61d2809bf1f4c03596f83b9c0e8a6<span class="built_in">fc</span>2eb9013a81abb25be833e9</span><br></pre></td></tr></table></figure></p>
<p>查看当前的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">cephbase                         latest              ffe236ee2bb6        About a minute ago   <span class="number">1.39</span> GB</span><br></pre></td></tr></table></figure></p>
<p>基础镜像就完成，包括了ceph运行需要的软件</p>
<p>我们来创建mon的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=monnode --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph  -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>进入到容器当中去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>在容器当中执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 deploy]<span class="comment"># ceph-deploy mon create lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>我们来创建osd的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=osd0 --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph -v /var/lib/ceph/osd/ceph-<span class="number">0</span>:/var/lib/ceph/osd/ceph-<span class="number">0</span> -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>我们将网络映射到主机上，也就是容器和主机公用网络和主机名，然后把本地的一个数据盘的目录映射进去用于osd的部署，这里都是使用-v进行映射</p>
<p>这个地方因为是centos7，所以systemctl内部是无法使用的，而ceph是需要这个来控制服务的，所以需要提权，并且把入口改为/sbin/init</p>
<h2 id="三、回顾流程">三、回顾流程</h2><ul>
<li>下载centos基础镜像 </li>
<li>修改镜像的内容并提交为新的镜像</li>
<li>基于新的镜像启动容器（采用host映射，目录映射，所有数据都是留在物理机）</li>
<li>进入容器进行ceph的部署 </li>
<li>进入容器启动相关进程</li>
</ul>
<p>这样ceph是运行到了docker中，即使把docker容器销毁掉，因为基于主机名和网络的配置跟宿主机是一致的，所以直接在宿主机上也是能马上启动起来的</p>
<h2 id="四、为何用容器">四、为何用容器</h2><p>基于容器的技术是最近几年开始火起来的，目前的云计算还处于火热期，openstack还是显得比较重型的，很多时候我们只需要的是一个能够运行我们web服务的环境，然后容器技术就应运而生了，直接启动一个容器，就能实现，这个对于宿主机来说方便的只是启动一个进程那么简单</p>
<p>对于庞大复杂的服务来说，如何做到环境一致也是一直很难做到的，一排物理机，因为各种各样的原因，升级，重装系统，很难保证整套系统基础环境的一致性，而基于docker的环境就能很方便的实现这个，相当于把整个运行环境打了一个包，所有的宿主机能够很方便的统一到相同的环境，即使重装了宿主机，也能方便的用一两条命令将环境部署到统一，比如上面所说的ceph，升级了基础镜像内的软件包，然后将所有的运行进程进行一次重启，就相当于运行了一个新的环境</p>
<p>容器还能够做的事情就是能够很便捷的把一个复杂环境运行起来，特别对于web类的服务，一台机器上可以跑一排的对外服务，即使出了问题，也能很快的再运行起来，这个对于传统的环境来说就是很难实现的，这里讲一下calamari，这个监控系统不是很复杂，但是因为依赖的软件的问题，造成很多人无法正常运行起来，这个后面我会出一个集成好calamari的docker环境，实现一键运行</p>
<p>在低版本的os上能够运行高版本的服务，比如在centos6上运行centos7的docker环境</p>
<h2 id="五、总结">五、总结</h2><p>本篇的文章的标题为docker与ceph的分与合，一套系统除了自身需要稳定性以外，系统自身最好不要受制于其他系统，需要在设计初期就能保证，各个模块都能轻松的剥离，否则很容易受制于另外一套系统，所以基于上面的方案来说，docker和ceph既是合在一起的，也是分开的,本篇只是讲了一个框架，实际部署ceph的过程当中还是有一些小问题需要具体处理的，不是很难，权限问题，目录问题</p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-19</td>
</tr>
</tbody>
</table>
<h2 id="附录：">附录：</h2><p>docker的常用操作<br>查询镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br></pre></td></tr></table></figure></p>
<p>查询容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br></pre></td></tr></table></figure></p>
<p>删除容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rm 64f617dfada5</span></span><br></pre></td></tr></table></figure></p>
<p>删除镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rmi node</span></span><br></pre></td></tr></table></figure></p>
<p>进入容器内部<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>让容器执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec monnode uptime</span></span><br></pre></td></tr></table></figure></p>
<p>退出容器,不停止容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ctrl+p然后ctrl+q</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
