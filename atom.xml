<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-04-17T14:58:54.474Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[记一次不完全成功到成功的失效恢复(20160412)]]></title>
    <link href="http://www.zphj1987.com/2016/04/13/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%88%90%E5%8A%9F%E5%88%B0%E6%88%90%E5%8A%9F%E7%9A%84%E5%A4%B1%E6%95%88%E6%81%A2%E5%A4%8D-20160412/"/>
    <id>http://www.zphj1987.com/2016/04/13/记一次不完全成功到成功的失效恢复-20160412/</id>
    <published>2016-04-12T16:04:56.000Z</published>
    <updated>2016-04-17T14:58:54.474Z</updated>
    <content type="html"><![CDATA[<h3 id="更新：">更新：</h3><p>在经历了好几天后，失效的环境最终变成了可用状态，只能说有的时候不放弃还真是有点用的</p>
<hr>
<p>在不久前处理了一个故障恢复以后，又碰上一个群友的集群出现了严重故障，本篇将记录这个中间大致处理的过程，一些细节在以后会补充</p>
<p>首先看到给出的截图显示的是大量的pg处于异常的状态，从经验上判断，环境要么处于down机的边缘，或者是刚经历了一次大量的重启，这个时候集群可以说是前端的访问肯定全断的，这个故障的时候资源一般会比较紧张，所以在启动的过程中也要注意不要触发更大面积的down机，对于集群来说是会有连带效应的</p>
<p>在启动了部分osd后，集群还是有大量的pg出现的是down+peering的状态，而发现down的osd实际全部在一台服务器上的，这个从ceph的架构来说是不应该出现这个状态的，这个可能是在down机过程中，频繁的pg的状态变化造成了pg的状态停留在之前的down的状态上，而pg出现锁死的状况，这个在之前的那位群友的环境中出现过一次，那个是多机有osd出现异常的情况，这次是单机出现的情况</p>
<a id="more"></a>
<p>尝试加大日志级别，从几个osd里面看日志出现两类的异常，从后面的处理的情况来看，实际这个是触发了两个bug，第一个问题出现是部分的数据丢失，这个在进行处理以后，再次启动的时候，几个osd出现了同样的错误，在询问了我们的研发大牛后，基本能判断这个是一个0.94.x的bug，并且在邮件列表里面已经解决</p>
<p>然后尝试对其中的一台进行升级，这次升级直接升级到了10.1.1，然后启动osd，确实可以启动了，具体的怎么触发这个bug，就不是太清楚里面的过程，这个环境是经历了一个比较复杂的状态变化</p>
<p>启动了部分osd后，发现还是osd无法启动，一检查，发现居然是这个机器的5个磁盘都有文件系统的错误，之前的部分数据丢失，也可能是文件系统错误引起的，很有可能是异常后造成了大面积的异常，这个地方只是推断，因为没有看到监控中间的过程，在修复了文件系统以后，osd都能起来了，只是又碰上另外的一个问题，<br>incomplete状态，就是事情没做完，在检查了里面的数据，发现数据没问题</p>
<p>尝试做修复，使用cephobjecttool导出数据再导入另外的pg，状态还是无法变化，然后根据之前另外一个国外的处理经验，将pg导入到非pg映射的osd，然后让其自动backfiill，发现还是无法生效，pg仍然停留在imcomplete状态</p>
<p>在询问了对方里面的数据情况后得知只有一个镜像比较重要，果断尝试后端的修复，大概思路就是将img镜像所对应的数据全部拷贝到一个目录，然后进行拼接的操作，这个在我之前的测试环境测试过没问题，这次在这个环境上进行了操作，因为环境的对象大小经过了修改，所以脚本也要对应修改，最后合成看了一个raw文件，在经过验证后，能够启动，数据基本是算是恢复了</p>
<p>然后做了pg repair osd repair deep-scrub等操作都是无法改变状态</p>
<p>环境还停留在不可用的状态，尝试做最后的修复，将pg数据进行备份后，强制创建pg，这个在我自己的测试环境下是可行的方案，但是这个环境在停留在creating状态比较久后，还是会进入imcomplete状态，尝试几次还是不行，开始怀疑是这两个osd问题，然后将osd out以后，在重新分布的osd上进行了创建pg操作，还是creating后进入imcomplete状态，到此，基本判断环境无法恢复了，数据算是保住了</p>
<p>这个是国内一个比较牛的cepher也碰到的情况 <a href="http://m.oschina.net/blog/360274" target="_blank" rel="external">osd盘崩溃的总结</a>，他这个环境也是最终无法救回来<br>这是他查询到的国外的一个人写的情况：</p>
<blockquote>
<p>查了一圈无果。一个有同样遭遇的人的一段话：<br>I already tried “ceph pg repair 4.77”, stop/start OSDs, “ceph osd lost”, “ceph pg force_create_pg 4.77”.<br>Most scary thing is “force_create_pg” does not work. At least it should be a way to wipe out a incomplete PG without destroying a whole pool.</p>
</blockquote>
<p>这个地方出故障的环境做一个总结：</p>
<ul>
<li>环境做了比较极端的优化，这里就不说了，ceph的journal这一层就是防止down机出现数据不一致做replay的，做了极端的环境优化需要做多次整机down机测试，这个down机是无法完全避免的，所以要考虑</li>
<li>磁盘出现了多个同时的损坏，这个没有办法，文件系统的损坏有可能是主机系统出现比较特殊的异常造成磁盘数据异常，这个单机多磁盘损坏的可能是有的，最怕就是部分损坏</li>
<li>ceph有部分bug是在比较极端的情况下出现的，并不是没有，所以不能想着完全避免bug，多想想真出问题了，怎样把损失降低到最小，我的底线是数据回来</li>
<li>ceph集群的副本只能保证系统内的高可用，系统级别的高可用，只能是双系统，能搭两套一定两套，哪怕非实时定期备份也好</li>
<li>随着ceph使用者越多，出现问题的情况会越来越多的，特别是在使用的越久，概率就越大，磁盘也是有寿命的，集群呢？还是早做防范措施</li>
</ul>
<hr>
<h3 id="后续：">后续：</h3><p>事情本以为就这么完结了，因为已经达到了最低的标准，数据的恢复，但实际上对于我自己来说，还是觉得有点遗憾的，毕竟环境是处于一个无法使用的状态，并且，环境中实际也只有部分数据的损坏，但是因为pg的状态不对，那些虚拟机实际是无法写入的，变相的这个环境就是一个僵住的状态了，虽然想了好几天，但是并没有更好的办法，有一个办法是将整个的数据导出再导入，这个时间周期会很长，如果里面数据很多都是重要的，这个是不得不走的一步了，正好这个环境重要数据只有一个，也就没去尝试了</p>
<p>我有一个翻译的计划的，已经停滞了很久，但是说实话，我之前的想法是一章章的细细的研究，细细的翻译，然后写出自己的想法，但是迫于时间原因，以及最近事情比较多，暂时处于停滞状态，这个后期会跟进的，目前已经购买的书友，以及支持的朋友，我尽量的是对你提出的问题或者困惑给出我个人的见解，总之一个事情的处理方式有多种，我从来都是告诉你我会怎么做，然后告诉你，你可以根据你的想法来，正是因为想到自己最近没时间翻译，自己干脆把这本书过一遍，果然还是多读书好，根据书里面的一个提示，我就去尝试做另外一个操作</p>
<p>在有想法以后，联系了群友，正好环境还在，没有做推倒重来的操作，这个也感谢ceph群友的信任和支持，在隔了几天再次登录环境以后，根据提示，我将这个pg的数据进行了删除，这次的删除不是之前的暴力的直接rm，而是使用ceph内部的工具进行的删除，主副本停止osd后同时做的操作，我怀疑是不是还有哪里的元数据被锁住了，在删除以后再次起来，再次创建pg的时候，环境还是处于一个异常的状态，因为书中描述了是我之前没见过的操作，当时想想是不是有其他的不清楚的操作方式，在一番查询以后，真的有我没用过的操作，然后直接尝试，果然整个集群正常了，然后把之前的pg数据进行导入操作，然后用rados直接get那个异常的pg里面的对象，果然能读取了，然后用rados ls也能够列出所有的对象了，环境终于能够正常了，环境是强制的改变状态变成可正常，数据也能够读写了，我个人的建议如果真是有很多重要数据，还是把数据倒出来再导入进去，集群正常情况下的导出导入操作逻辑和时间比后台的导出逻辑要简单非常多</p>
<p>好了，到了这里终于将一个环境变成了正常的状态了，对于我自己来说，对ceph的控制又提高了一点，之前认为数据盘在，我就能把数据恢复，倒出来，但是原集群的恢复，没有太多的保证，现在基本上只要盘符不被格式化掉，环境我也能有很大的概率去恢复正常，总之保底恢复方式的越多，越有信心去恢复它</p>
<p>这次的经历让我再一次感觉，不要放弃，不要放弃，有的时候真的会有转机，同时感谢群友能够提供环境给我，也欢迎有更多的朋友在出现问题的时候可以找我探讨一下</p>
<blockquote>
<p>by 运维-武汉-磨渣<br>2016年04月12日夜<br>更新于2016年04月17日夜</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="更新：">更新：</h3><p>在经历了好几天后，失效的环境最终变成了可用状态，只能说有的时候不放弃还真是有点用的</p>
<hr>
<p>在不久前处理了一个故障恢复以后，又碰上一个群友的集群出现了严重故障，本篇将记录这个中间大致处理的过程，一些细节在以后会补充</p>
<p>首先看到给出的截图显示的是大量的pg处于异常的状态，从经验上判断，环境要么处于down机的边缘，或者是刚经历了一次大量的重启，这个时候集群可以说是前端的访问肯定全断的，这个故障的时候资源一般会比较紧张，所以在启动的过程中也要注意不要触发更大面积的down机，对于集群来说是会有连带效应的</p>
<p>在启动了部分osd后，集群还是有大量的pg出现的是down+peering的状态，而发现down的osd实际全部在一台服务器上的，这个从ceph的架构来说是不应该出现这个状态的，这个可能是在down机过程中，频繁的pg的状态变化造成了pg的状态停留在之前的down的状态上，而pg出现锁死的状况，这个在之前的那位群友的环境中出现过一次，那个是多机有osd出现异常的情况，这次是单机出现的情况</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记一次ceph的故障修复(20160408)]]></title>
    <link href="http://www.zphj1987.com/2016/04/11/%E8%AE%B0%E4%B8%80%E6%AC%A1ceph%E7%9A%84%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D-20160408/"/>
    <id>http://www.zphj1987.com/2016/04/11/记一次ceph的故障修复-20160408/</id>
    <published>2016-04-10T17:24:35.000Z</published>
    <updated>2016-04-12T16:18:22.737Z</updated>
    <content type="html"><![CDATA[<p>ceph的在正常运行的时候基本不会出现故障，出现故障一般在变动的时候，具体有下面几种可能出现的情形</p>
<ul>
<li>软件升级</li>
<li>增加存储节点</li>
<li>减少存储节点</li>
<li>调整副本数目</li>
<li>调整pg数目</li>
<li>磁盘出现损坏</li>
<li>节点网络出现异常</li>
</ul>
<p>以上这些操作过程中是最可能出现异常的情形，并不是一定会出问题，以上问题除了网络和磁盘问题出现的异常是基本无法避免外，其他出现的时候，一般是非正常操作引起的，也就是我通常认为的人为事故，这个一般出现在操作的不谨慎上</p>
<a id="more"></a>
<p>本篇记录了一次故障的修复过程，这个故障不是出现在我们公司的产品上，是看到一个ceph社区群里有一个成员在里面问到一个异常是否能解决，这个不同于普通的问题，从他贴出的信息来看，集群已经是非常严重的状态了</p>
<p>正好看到是周五，周六还可以休息下，所以即使快到了晚上12点了，我还是联系了一下那哥们，从简短的几句交流后，基本可以判断对方对于ceph基本处于刚接触的阶段，在询问是否有其他人能协助他做一些比较有难度的操作的时候，他说没有，就他一个人，我想在目前中国很多公司，都是让一个并不太熟悉ceph的运维人员，或者完全就是开发人员维护着存储着非常宝贵的数据的云存储环境，上面运行的应该都是客户的数据，想想我们自己的电脑在硬盘损坏后，自己有多么不爽，而对于企业来说，一个运行环境的损坏有多么严重，一方面损失了数据，另一方面，基本不会再选择这个服务的提供商了，而这些都是一个定时炸弹，运行在中国的开源存储网络环境当中，而且基本都是初创小企业，大企业会有专门的专业的相关人员，而一个数据损失基本会对这些初创企业带来巨大的损失，这些都是需要企业的boss多关注的，这也是我一直持有的一个观点，越来越多的企业是用ceph，也意味着存储需要修复的出现几率就越大，其实我们也是一个小企业，我个人是非常关注数据恢复这一块的，这个比调优更加的重要，大环境的吐槽就到这里，下面开始讲下具体的经过</p>
<h3 id="首先找对方要了一个ssh登陆环境">首先找对方要了一个ssh登陆环境</h3><p>这个对方正好有这个环境允许我的登陆，虽然中间经过了堡垒机，虽然运行命令比较卡顿，但好歹能上去，这个是我个人非常支持的一种做法，不管怎样，是VPN也好，代理也好，一定留一个外网的ssh端口能够让连上机器，这个能允许随时随地能上去处理问题，等你运维人员到达现场，真是黄花菜都凉了，对于比较保密的环境，最好也能够有一个在紧急情况下开启远程允许环境的条件，这个具体花费，一个上网卡，一台破旧的笔记本就基本能实现了，在需要远程操作的时候能够连上去处理，目前已经协助了几个朋友处理了一些简单的问题，基本都是ssh连过去的，而没有远程环境的，我也是无能为力的</p>
<h3 id="检查环境">检查环境</h3><p>登陆上去以后，检查环境发现提示的是2个pg的状态imcomplete，这个是pg的数据不一致的提示，而在检查了对应的osd上的这个pg的数据的时候，发现映射计算到的3个上面有两个是没有数据的，有一个是有数据的，在询问对方做过的操作后，对方是做了一个删除osd的操作，并且是多台机器上面都做过删除，到这里我询问了一下对方，对方是按照一些通用的操作去做的删除操作，命令肯定是没有问题的，这个在后面我处理完后，基本能判断出对方是人为的操作失误引起的</p>
<h3 id="尝试修复">尝试修复</h3><p>开始想起之前做过的一次模拟修复，本来以为这个可以把环境弄好了，基本想法就是如下流程：</p>
<ul>
<li>停止pg对应的3个osd</li>
<li>导出有数据的pg</li>
<li>在无数据的osd上进行pg的数据导入</li>
<li>启动三个osd</li>
</ul>
<p>在进行到数据的导入的时候提示了pg is blocked，这个在我之前的做的测试中是没有遇到过的，后来进行pg的状态查询时候，发现是pg的显示的数据全是0，也就是集群认为这个pg是没有数据的，并且被几个已经删除了的osd blocked,而且做ceph osd  lost 也是无法操作的，提示没有osd，这个应该是pg状态不一致，也就是这个pg状态完全异常了，并且还无法导入了</p>
<h3 id="思考解决办法">思考解决办法</h3><p>到这里我个人判断基本是回天无力了，再次跟对方确认删除的过程，发现对方好在数据盘都保留了，并且还插在机器上，只是有部分osd在进行增加的时候还占用了删除的osd的id</p>
<p>到这里我基本想出来两种方法：</p>
<ul>
<li>最不济，也是终极解决办法就是把后台缺失的数据拼起来，这个耗时巨大，操作难度大，基本上只能作为最后终极挽回的方法，这个只有在客户已经觉得数据可能要丢了，然后去做最后的终极挽回大法了，客户的容忍度是会随着你问题严重性而改变的，相信我数据还在都好说</li>
<li>就是将删除的数据盘给加进来，这个操作在我几年ceph生涯中也是从未做过的，也想不出什么场景下需要这种操作，好吧，不管多么特殊的操作，总有它的存在的意义，我也不能确定ceph是否支持这种操作，那就试试这种</li>
</ul>
<p>这个集群之所以能挽回，有几个特殊点正好都在，缺一不可 </p>
<ol>
<li>删除的数据盘居然没被格式化，或者搞掉，这个如果弄没了，数据必丢</li>
<li>删除的数据盘的盘位部分被新加的节点占用了，部分还没有被占用，而这个缺失数据的pg的数据所删除的osd正好又没有被占用（所以以后替换osd的时候最好是用新的编号，老的盘和编号保留着）</li>
</ol>
<h3 id="开始恢复的操作">开始恢复的操作</h3><p>之前我加节点的操作都是用的ceph-deploy，可以说基本没有遇到过手动能做的ceph-deploy无法完成的，好吧这次我知道了还是有无法完成的，手动的还是多学学比较好，好在我比较熟悉，就按步骤去做</p>
<h4 id="1、增加认证">1、增加认证</h4><p>我们在删除osd的最后一步的时候操作都是ceph auth del<br>我就反向的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth add osd.<span class="number">0</span> osd <span class="string">'allow *'</span> mon <span class="string">'allow rwx'</span> -i /var/lib/ceph/osd0/keyring</span><br></pre></td></tr></table></figure></p>
<p>这个对应keyring就是在删除那个osd上面有，每个osd上面都有的<br>这一步操作完成后auth里面就有osd.0了</p>
<h4 id="2、创建osd">2、创建osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd create</span><br></pre></td></tr></table></figure>
<p>这个步骤也是之前没有做过的，之前准备直接加crush 直接启动发现都是无法启动，提示没有osd<br>这一步相对于删除里面的操作应该就是 ceph osd rm 的操作了</p>
<h4 id="3、增加crush">3、增加crush</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush add osd.<span class="number">0</span> <span class="number">0.9</span> host=node1</span><br></pre></td></tr></table></figure>
<p>这个就是加入到crush里面去</p>
<h4 id="4、启动osd">4、启动osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/ceph start osd.<span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="检查现在环境状况">检查现在环境状况</h3><p>在检查的时候发现osd真的就加进来了，然后在添加了另一个被block的osd后，集群状态就没有imcomplete了就是active+其他的一些恢复状态什么的，只需要等待恢复，集群即可恢复正常了，到这个时候已经凌晨三点了，事情能够完满解决是最开心的事情</p>
<h3 id="后记">后记</h3><p>集群的删除操作随意，集群的信息基本无记录，环境的基础记录都没有，这个是这个事故的最大原因，再往上走就是对于数据操作这块，公司没有一个重视的态度，上面的boss永远不会关心你运维做了什么操作，而运维人员也可以说是我按标准流程操作的，也没法去定谁的责任，丢了就是丢了，运维最多也就是丢了工作，而企业损失应该就是以万为单位的损失加客户的流失了<br>到这里也许这家公司的头并不知道发生了什么，也许只是认为是一个小的业务中断，但真的某一天出事了，这就是大事了，所以一定要重视系统的监控和系统操作的谨慎，boss不需要关心，你cto，研发的头，运维的头，总有一个人需要重视这个问题，而大部分应该是散养状态，也是就是在集群交付以后基本是没有运维监控的状态，因为即使在我们的环境下，即使我有这个意识，但是也无法推动这个事情的</p>
<p>之前我总在跟别人说欢迎打赏，有时能收到几十，目前最高就是小白的100块，其实我知道大家都是上着班，为公司而工作，为了公司的事情需要自己再去掏钱，似乎也没有太多理由，所以对于一般的情况，别人要感谢，要请吃饭，我也就是说着好好，心意我领了，我的收入在我所在的城市其实已经可以了，只是最近因为有一些需要用钱的地方，所以又是录课程，又是翻译书籍，为了赚取10块钱也是费尽心机啊，其实大家不打赏也无所谓的，能认识一些朋友，在某天，记得我有帮过你就行了，我认识的一个中科院的博士，那个时候还是2011年，他是我的老师的同学，即使现在他linux或者服务器有什么问题，需要帮助的时候，我有时间也会帮处理他，举手之劳就能帮别人解决问题，对于自己也是很开心的</p>
<p>好了，就到这里，如果你的集群是生产环境，相信我话，需要帮助的话，留个ssh，有时间的时候我可以帮忙处理，绝对不会出现要给钱才弄的情况，当然你如果财力雄厚，事后给点劳务费，我也不会拒绝的，测试环境话，就自己研究下为好，特别不好解决的问题也可以沟通一下，当然我也有我的工作，需要我有时间的时候</p>
<blockquote>
<p>by 运维-武汉-磨渣<br>2016年04月11日夜</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>ceph的在正常运行的时候基本不会出现故障，出现故障一般在变动的时候，具体有下面几种可能出现的情形</p>
<ul>
<li>软件升级</li>
<li>增加存储节点</li>
<li>减少存储节点</li>
<li>调整副本数目</li>
<li>调整pg数目</li>
<li>磁盘出现损坏</li>
<li>节点网络出现异常</li>
</ul>
<p>以上这些操作过程中是最可能出现异常的情形，并不是一定会出问题，以上问题除了网络和磁盘问题出现的异常是基本无法避免外，其他出现的时候，一般是非正常操作引起的，也就是我通常认为的人为事故，这个一般出现在操作的不谨慎上</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph在centos7下一个不容易发现的改变]]></title>
    <link href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/"/>
    <id>http://www.zphj1987.com/2016/03/31/ceph在centos7下一个不容易发现的改变/</id>
    <published>2016-03-31T08:19:27.000Z</published>
    <updated>2016-03-31T08:20:38.371Z</updated>
    <content type="html"><![CDATA[<p>在centos6以及以前的osd版本，在启动osd的时候，回去根据ceph.conf的配置文件进行挂载osd，然后进行进程的启动，这个格式是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[osd.<span class="number">0</span>]</span><br><span class="line">host = hostname</span><br><span class="line">devs=/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>启动的时候就会把sdb1盘符挂载到0的目录里面去了</p>
<p>然后在centos7的版本的时候，发现居然不写配置文件也能够自动挂载启动，这个地方是什么地方发生了变化，在做了一些日志的查询以后，发现centos7下居然做了一个改变</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl list-unit-files |grep ceph-disk</span></span><br><span class="line">ceph-disk@.service                          static</span><br></pre></td></tr></table></figure>
<p>可以看到有这个服务</p>
<a id="more"></a>
<h3 id="我们来验证下这个服务">我们来验证下这个服务</h3><h4 id="先停止服务">先停止服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="umount挂载点">umount挂载点</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">umount /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>现在已经没有挂载点了</p>
<h4 id="现在执行下面的服务（我的sdc1是刚刚的osd-1）">现在执行下面的服务（我的sdc1是刚刚的osd.1）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl start ceph-disk@/dev/sdc1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-disk@/dev/sdc1</span></span><br><span class="line">● ceph-disk@-dev-sdc1.service - Ceph disk activation: /dev/sdc1</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-disk@.service; static; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-detect-init --default sysvinit</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: activate: Marking with init system systemd</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: activate: ceph osd.<span class="number">1</span> data dir is ready at /var/lib/ceph/tmp/mnt.<span class="number">3</span>a8xNK</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: move_mount: Moving mount to final location...</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /bin/mount -o noatime,inode64 -- /dev/sdc1 /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /bin/umount <span class="operator">-l</span> -- /var/lib/ceph/tmp/mnt.<span class="number">3</span>a8xNK</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: start_daemon: Starting ceph osd.<span class="number">1</span>...</span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl <span class="built_in">enable</span> ceph-osd@<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 sh[<span class="number">17847</span>]: <span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl start ceph-osd@<span class="number">1</span></span><br><span class="line">Mar <span class="number">31</span> <span class="number">16</span>:<span class="number">11</span>:<span class="number">37</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph disk activation: /dev/sdc1.</span><br></pre></td></tr></table></figure>
<p>执行完检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># df -h |grep ceph-1</span></span><br><span class="line">/dev/sdc1       <span class="number">275</span>G   <span class="number">35</span>M  <span class="number">275</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到已经挂载好，并且启动了服务<br>可以看到我没有使用任何配置情况下，没有告诉集群sdc1就是要挂载到<code>/var/lib/ceph/osd/ceph-1</code>这个目录的，自动挂载好了，这个是集群自己先mount到一个临时目录根据磁盘里面的信息来判断了这个osd真实的数据，根据这个数据来mount到一个挂载点，这个做法是非常好的做法</p>
<p><strong>如果觉得我的文章对您有用，欢迎打赏。您的支持将鼓励我继续创作！10元足矣！</strong></p>
<center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>在centos6以及以前的osd版本，在启动osd的时候，回去根据ceph.conf的配置文件进行挂载osd，然后进行进程的启动，这个格式是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[osd.<span class="number">0</span>]</span><br><span class="line">host = hostname</span><br><span class="line">devs=/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>启动的时候就会把sdb1盘符挂载到0的目录里面去了</p>
<p>然后在centos7的版本的时候，发现居然不写配置文件也能够自动挂载启动，这个地方是什么地方发生了变化，在做了一些日志的查询以后，发现centos7下居然做了一个改变</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl list-unit-files |grep ceph-disk</span></span><br><span class="line">ceph-disk@.service                          static</span><br></pre></td></tr></table></figure>
<p>可以看到有这个服务</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd-mirror新功能]]></title>
    <link href="http://www.zphj1987.com/2016/03/30/rbd-mirror%E6%96%B0%E5%8A%9F%E8%83%BD/"/>
    <id>http://www.zphj1987.com/2016/03/30/rbd-mirror新功能/</id>
    <published>2016-03-30T15:55:31.000Z</published>
    <updated>2016-03-30T16:11:01.486Z</updated>
    <content type="html"><![CDATA[<p>RBD 的 mirroring 功能将会在下一个稳定版本Jewel中实现，这个Jewel版本已经发布了第一个版本10.1.0,这个功能已经在这个发布的版本中实现了</p>
<p>一、基本原理<br>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p>
<p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p>
<ul>
<li>灾难恢复</li>
<li>全球块设备分布（跨地理位置）</li>
</ul>
<p>二、内部的实现</p>
<p><img src="http://static.zybuluo.com/zphj1987/owvwxjz172f2aokh699fd008/%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p>
<p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p>
<a id="more"></a>
<p>二、一个新的进程<br>一个新的守护程序：rbd-mirror 将会负责将一个镜像从一个集群同步到另一个，rbd-mirror需要在两个集群上都配置，它会同时连接本地和远程的集群。在jewel版本中还是一对一的方式，在以后的版本中会实现一对多的，所以在以后的版本可以配置一对多的备份</p>
<p>作为起点，这个功能讲使用配置文件连接集群，使用用户和密钥。使用admin用户就可以了，使用的验证方式就是默认的cephx的方式</p>
<p>为了相互识别，两个集群都需要相互注册使用 <code>rbd mirror pool peer add</code>命令， 这个在下面会实践</p>
<p>二、镜像<br><img src="http://static.zybuluo.com/zphj1987/60ehzzmtrvrf8rhdl9yelyqw/ceph-rbd-mirror-inside.png" alt="ceph-rbd-mirror-inside.png-80.8kB"><br>The RBD mirroring 依赖两个新的rbd的属性</p>
<ul>
<li>journaling: 启动后会记录image的事件</li>
<li>mirroring: 明确告诉rbd-mirror需要复制这个镜像</li>
</ul>
<p>也有命令可以禁用单独的某个镜像。journaling可以看做是另一个rbd的image（一些rados对象），一般情况下，先写日志，然后返回客户端，然后被写入底层的rbd的image，出于性能考虑，这个journal可以跟它的镜像不在一个存储池当中，目前是一个image一个journal，最近应该会沿用这个策略，直到ceph引入一致性组。关于一致性组的概念就是一组卷，然后用的是一个RBD image。可以在所有的组中执行快照操作，有了一致性的保证，所有的卷就都在一致的状态。当一致性组实现的时候，我们就可以用一个journal来管理所有的RBD的镜像</p>
<p>可以给一个已经存在image开启journal么，可以的，ceph将会将你的镜像做一个快照，然后对快照做一个复制，然后开启journal，这都是后台执行的一个任务</p>
<p>可以启用和关闭单个镜像或者存储池的mirror功能，如果启用了journal功能，那么每个镜像将会被复制</p>
<p>可以使用 rbd mirror pool enable启用它</p>
<p>三、灾难恢复<br>交叉同步复制是可以的，默认的就是这个方式，这意味着<strong>两个地方的存储池名称需要相同的</strong>这个会带来两个问题</p>
<ul>
<li>使用相同的存储做备份做使用会影响性能的</li>
<li>相同的池名称在进行恢复的时候也更容易。openstack里面只需要记录卷ID即可</li>
</ul>
<p>每个image都有 mirroring_directory 记录当前active的地方。在本地镜像提示为 primary的时候，是可写的并且远程的站点上就会有锁，这个image就是不可写的。只有在primary镜像降级，备份的点升级就可以了，demoted 和 promoted来控制这里，这就是为什么引入了等级制度，一旦备份的地方升级了，那么主的就自动降级了，这就意味着同步的方向就会发生变化了</p>
<p>如果出现脑裂的情况，那么rbd-mirror将会停止同步，你自己需要判断哪个是最新的image，然后手动强制去同步<code>rbd mirror image resync</code></p>
<p>上面基本参照的是sebastien翻译的，原文只是做了简短的说明，下面是我的实践部分</p>
<h2 id="下面在我的环境下进行实践">下面在我的环境下进行实践</h2><p>下面的环境是在两个集群上进行的，集群分别为：</p>
<ul>
<li>机器lab8106 </li>
<li>机器lab8107</li>
</ul>
<p><a href="https://www.zybuluo.com/zphj1987/note/328708" target="_blank" rel="external">实践详细文档</a></p>
<p>感谢一直在支持我的朋友，技术是无价的，也是免费的，天下没有免费的午餐，关键是你自己觉得划算不</p>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！10元足矣！</strong></p>
<center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>RBD 的 mirroring 功能将会在下一个稳定版本Jewel中实现，这个Jewel版本已经发布了第一个版本10.1.0,这个功能已经在这个发布的版本中实现了</p>
<p>一、基本原理<br>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p>
<p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p>
<ul>
<li>灾难恢复</li>
<li>全球块设备分布（跨地理位置）</li>
</ul>
<p>二、内部的实现</p>
<p><img src="http://static.zybuluo.com/zphj1987/owvwxjz172f2aokh699fd008/%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p>
<p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph Bluestore首测]]></title>
    <link href="http://www.zphj1987.com/2016/03/24/Ceph-Bluestore%E9%A6%96%E6%B5%8B/"/>
    <id>http://www.zphj1987.com/2016/03/24/Ceph-Bluestore首测/</id>
    <published>2016-03-24T14:07:43.000Z</published>
    <updated>2016-03-24T14:18:41.233Z</updated>
    <content type="html"><![CDATA[<p>Bluestore 作为 Ceph Jewel 版本推出的一个重大的更新，提供了一种之前没有的存储形式，一直以来ceph的存储方式一直是以filestore的方式存储的，也就是对象是以文件方式存储在osd的磁盘上的，pg是以目录的方式存在于osd的磁盘上的<br>在发展过程中，中间出现了kvstore，这个还是存储在文件系统之上，以leveldb或者rocksdb的方式存储对象数据，这个也没有推广开来，性能上没有太大的改观，在某些情况下性能还低于filestore<br>最终在sage的大力支持下，ceph社区准备撸一个新的文件系统，这个系统类似于rocksdb，但是数据是可以直接存储到裸设备上去的，也就是存储对象数据的地方是没有传统意义上的文件系统的，并且解决了一种被抱怨的写双份数据的问题，在filestore中，数据需要先写入journal再入磁盘，对于磁盘来说实际就是双份写了</p>
<p>在这里不做过多的探讨技术上的细节，bluestore处于开发阶段，在最新的版本的ceph中，发现已经集成了这个，虽然还是实验阶段，但是还是体现出其未来巨大的价值</p>
<a id="more"></a>
<h3 id="环境准备">环境准备</h3><p>由于没有测试大量的设备，就在一个小环境下进行性能的验证，基准的性能不需要大量的机器，至于数据可靠性，就靠自己去判断了</p>
<h4 id="硬件环境：">硬件环境：</h4><p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/blue%E7%A1%AC%E4%BB%B6.png" alt=""></p>
<h4 id="软件环境：">软件环境：</h4><p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/blue%E8%BD%AF%E4%BB%B6.png" alt=""></p>
<h3 id="一、先测试Filestore">一、先测试Filestore</h3><p>ceph-disk有个update_partition的bug，部署过程需要处理一下，后期发版本应该会解决</p>
<h4 id="1、4K随机写200G的rbd测试时间300s">1、4K随机写200G的rbd测试时间300s</h4><p><img src="http://static.zybuluo.com/zphj1987/5azadvfqzqny89fduyodbwpw/filestorefio%E8%BF%87%E7%A8%8B.png" alt="filestorefio过程.png-132.1kB"><br>测试的io的抖动的情况</p>
<p><img src="http://static.zybuluo.com/zphj1987/455zwwxtc16zs4m49wfd5qhq/filestoreresul.png" alt="filestoreresul.png-89.4kB"><br>测试的FIO结果的页面</p>
<h4 id="2、4M顺序写200G的rbd测试时间300s">2、4M顺序写200G的rbd测试时间300s</h4><p><img src="http://static.zybuluo.com/zphj1987/89bnflxt1imyzld02ur78qum/filestorefio%E8%BF%87%E7%A8%8B4M.png" alt="filestorefio过程4M.png-166.9kB"><br>测试的io的抖动的情况<br><img src="http://static.zybuluo.com/zphj1987/taicqvht6iz757w0ioo6ww82/filestoreresul4M.png" alt="filestoreresul4M.png-88kB"></p>
<h3 id="二、测试bluestore">二、测试bluestore</h3><h4 id="1、4K随机写200G的rbd测试时间300s-1">1、4K随机写200G的rbd测试时间300s</h4><p><img src="http://static.zybuluo.com/zphj1987/iqy1lk80fa07aq8f71dug2xh/blueresult4K.png" alt="blueresult4K.png-99.6kB"><br><img src="http://static.zybuluo.com/zphj1987/xbr4sjr3u8ahmf2sfcr2x72l/bluefinal4K.png" alt="bluefinal4K.png-87.9kB"></p>
<h4 id="2、4M顺序写200G的rbd测试时间300s-1">2、4M顺序写200G的rbd测试时间300s</h4><p><img src="http://static.zybuluo.com/zphj1987/o7r57lpqf49rlpmfmchfwrrt/blueresult4M.png" alt="blueresult4M.png-133.6kB"><br><img src="http://static.zybuluo.com/zphj1987/7kkhhmnxy55jwwseljw767e1/blueiostatfinal4M.png" alt="blueiostatfinal4M.png-90.9kB"></p>
<p>以上为测试过程的数据记录，下面为对比的</p>
<p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/blue%E6%AF%94%E8%BE%83.png" alt=""></p>
<p>整个测试来看改进非常的大，数据的曲线比之前要平滑很多，延时也变得更小，但是还是开发阶段，估计bug还是很多，不可控因素太多，并且暂时还没有修复工具，作为对未来ceph发展的一种期待吧，肯定会越来越好</p>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！</strong></p>
<div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"><br></div>

]]></content>
    <summary type="html">
    <![CDATA[<p>Bluestore 作为 Ceph Jewel 版本推出的一个重大的更新，提供了一种之前没有的存储形式，一直以来ceph的存储方式一直是以filestore的方式存储的，也就是对象是以文件方式存储在osd的磁盘上的，pg是以目录的方式存在于osd的磁盘上的<br>在发展过程中，中间出现了kvstore，这个还是存储在文件系统之上，以leveldb或者rocksdb的方式存储对象数据，这个也没有推广开来，性能上没有太大的改观，在某些情况下性能还低于filestore<br>最终在sage的大力支持下，ceph社区准备撸一个新的文件系统，这个系统类似于rocksdb，但是数据是可以直接存储到裸设备上去的，也就是存储对象数据的地方是没有传统意义上的文件系统的，并且解决了一种被抱怨的写双份数据的问题，在filestore中，数据需要先写入journal再入磁盘，对于磁盘来说实际就是双份写了</p>
<p>在这里不做过多的探讨技术上的细节，bluestore处于开发阶段，在最新的版本的ceph中，发现已经集成了这个，虽然还是实验阶段，但是还是体现出其未来巨大的价值</p>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph查询rbd的使用容量（快速）]]></title>
    <link href="http://www.zphj1987.com/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/"/>
    <id>http://www.zphj1987.com/2016/03/24/ceph查询rbd的使用容量（快速）/</id>
    <published>2016-03-24T13:42:59.000Z</published>
    <updated>2016-03-24T13:53:57.127Z</updated>
    <content type="html"><![CDATA[<p>ceph在Infernalis加入了一个功能是查询rbd的块设备的使用的大小，默认是可以查询的，但是无法快速查询，那么我们来看看这个功能是怎么开启的</p>
<h3 id="ceph版本">ceph版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">9.2</span>.<span class="number">0</span> (bb2ecea240f3a1d525bcb35670cb07bd1f0ca299)</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="创建RBD设备">创建RBD设备</h3><p>我们先来创建一个rbd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd create test --size 4000</span></span><br><span class="line">root@lab8107:~/ceph<span class="comment"># rbd info test</span></span><br><span class="line">rbd image <span class="string">'test'</span>:</span><br><span class="line">	size <span class="number">4000</span> MB <span class="keyword">in</span> <span class="number">1000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10305695</span>d26a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<h3 id="进行RBD容量使用查询">进行RBD容量使用查询</h3><p>我们来试一下rbd du命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd du test</span></span><br><span class="line">warning: fast-diff map is not enabled <span class="keyword">for</span> test. operation may be slow.</span><br><span class="line">NAME PROVISIONED USED </span><br><span class="line"><span class="built_in">test</span>       <span class="number">4000</span>M    <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到有个提示需要开启fast-diff的属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd --help</span></span><br><span class="line">···</span><br><span class="line">Supported image features:</span><br><span class="line">  layering (+), striping (+), exclusive-lock (*), object-map (*), fast-diff (*), deep-flatten</span><br><span class="line"></span><br><span class="line">  (*) supports enabling/disabling on existing images</span><br><span class="line">  (+) enabled by default <span class="keyword">for</span> new images <span class="keyword">if</span> features are not specified</span><br></pre></td></tr></table></figure></p>
<p>可以看到默认开启了  layering striping 属性，后面属性没有开启<br>我们看一下rbd的man page<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># man rbd</span></span><br><span class="line">···</span><br><span class="line">--image-feature feature-name</span><br><span class="line">    Specifies <span class="built_in">which</span> RBD format <span class="number">2</span> feature should be enabled when creating an image. Multiple features can be enabled by repeating  this  option  multiple times. The following features are supported:</span><br><span class="line"></span><br><span class="line">    · layering: layering support</span><br><span class="line">    · striping: striping v2 support</span><br><span class="line">    · exclusive-lock: exclusive locking support</span><br><span class="line">    · object-map: object map support (requires exclusive-lock)</span><br><span class="line">    · fast-diff: fast diff calculations (requires object-map)</span><br><span class="line">    · deep-flatten: snapshot flatten support</span><br></pre></td></tr></table></figure></p>
<h3 id="开启RBD属性">开启RBD属性</h3><p>可以看到开启fast-diff 需要开启<code>exclusive-lock</code>和 <code>object-map</code> 属性<br>那么依次开启就好了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd  feature enable test  exclusive-lock</span></span><br><span class="line">root@lab8107:~/ceph<span class="comment"># rbd  feature enable test  object-map</span></span><br><span class="line">root@lab8107:~/ceph<span class="comment"># rbd  feature enable test  fast-diff</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">17</span>:<span class="number">23.822720</span> <span class="number">7</span>f241a5447c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">17</span>:<span class="number">23.823191</span> <span class="number">7</span>f241a5447c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>来查看下 rbd info<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd info test</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">18</span>:<span class="number">37.972235</span> <span class="number">7</span>f9918a7d7c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br><span class="line">rbd image <span class="string">'test'</span>:</span><br><span class="line">	size <span class="number">4000</span> MB <span class="keyword">in</span> <span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">18</span>:<span class="number">37.972900</span> <span class="number">7</span>f9918a7d7c0 -<span class="number">1</span> <span class="number">1000</span>librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory objects</span><br><span class="line"></span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10305695</span>d26a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff</span><br><span class="line">	flags: object map invalid, fast diff invalid</span><br></pre></td></tr></table></figure></p>
<p>我们可以看到又报错了，这个是因为是后开启object map，需要重建一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd  object-map rebuild  test  </span></span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">20</span>:<span class="number">05.488515</span> <span class="number">7</span>fa0141917c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">20</span>:<span class="number">05.489142</span> <span class="number">7</span>fa0141917c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br><span class="line"><span class="number">2016</span>-<span class="number">03</span>-<span class="number">24</span> <span class="number">21</span>:<span class="number">20</span>:<span class="number">05.530344</span> <span class="number">7</span>fa0141917c0 -<span class="number">1</span> librbd::ObjectMap: error refreshing object map: (<span class="number">2</span>) No such file or directory</span><br><span class="line">Object Map Rebuild: <span class="number">100</span>% complete...done.</span><br></pre></td></tr></table></figure></p>
<p>再次查看下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd info test</span></span><br><span class="line">rbd image <span class="string">'test'</span>:</span><br><span class="line">	size <span class="number">4000</span> MB <span class="keyword">in</span> <span class="number">1000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10305695</span>d26a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>已经可以了，我们来试下这个功能</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd du test</span></span><br><span class="line">NAME PROVISIONED USED </span><br><span class="line"><span class="built_in">test</span>       <span class="number">4000</span>M    <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>好了，这个功能已经开启了，这个是对已经创建好的rbd，然后开启这个属性，那么如果不想这么麻烦，默认就开启，创建的时候就开启，有什么方法么，当然是有的</p>
<h3 id="默认开启RBD容量快速查询的方法">默认开启RBD容量快速查询的方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># ceph --show-config|grep rbd_default_features</span></span><br><span class="line">rbd_default_features = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>查看下默认配置，这个是3，那么3是什么意思，3=1+2，这个是属性中常用的一种做法，给属性设置一个bit码，在配置的时候，只需要设置加起来的和<br>在RBD的属性里面：<br><img src="http://static.zybuluo.com/zphj1987/1amw9dopkoohq90dfwme7z6p/%E5%B1%9E%E6%80%A7.png" alt="属性.png-14.1kB"></p>
<p>我们要开启 前五个属性那么就是 31<br>在ceph.conf中添加配置</p>
<blockquote>
<p>rbd_default_features = 31</p>
</blockquote>
<p>创建后不做任何操作直接查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># rbd create test1 --size 1000</span></span><br><span class="line">root@lab8107:~/ceph<span class="comment"># rbd info test1</span></span><br><span class="line">rbd image <span class="string">'test1'</span>:</span><br><span class="line">	size <span class="number">1000</span> MB <span class="keyword">in</span> <span class="number">250</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">103</span>c29f2280d</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff</span><br><span class="line">	flags: </span><br><span class="line">root@lab8107:~/ceph<span class="comment"># rbd du test1</span></span><br><span class="line">NAME  PROVISIONED USED </span><br><span class="line"><span class="built_in">test</span>1       <span class="number">1000</span>M    <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到默认就把几个属性都开启好了，关于这个属性的开启就记录到这里，之前已经测试了一次</p>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！</strong></p>
<div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"><br></div>

]]></content>
    <summary type="html">
    <![CDATA[<p>ceph在Infernalis加入了一个功能是查询rbd的块设备的使用的大小，默认是可以查询的，但是无法快速查询，那么我们来看看这个功能是怎么开启的</p>
<h3 id="ceph版本">ceph版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">9.2</span>.<span class="number">0</span> (bb2ecea240f3a1d525bcb35670cb07bd1f0ca299)</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的书]]></title>
    <link href="http://www.zphj1987.com/2016/03/12/ceph%E7%9A%84%E4%B9%A6/"/>
    <id>http://www.zphj1987.com/2016/03/12/ceph的书/</id>
    <published>2016-03-12T15:50:34.000Z</published>
    <updated>2016-03-24T13:43:56.452Z</updated>
    <content type="html"><![CDATA[<p>以下是最新的一本ceph英文书的目录，我会学习里面的内容，并且翻译成中文的读书笔记，接触ceph也有几年了，还记得那个夏天毕业的时候辞掉了一份月薪接近4K的工作，转而换了一份月薪差不多1K的工作，然后就接触了现在非常火的ceph，有时候就是这样的，如果没有当时进入这个行业，也就没有今天的我<br>现在在接触的越多的相关知识后，越发发现自己还需要学的东西很多，只有掌握的越多，越能对你所做的事情能后有确定的信心，所以抓住一切能成长的机会，在技术上，我是很喜欢遇到问题的，遇到的问题越多，能够学到的东西越多，所以在ceph的中国社区里面，在一段时间内，我都尽力去回答每一个我知道的问题，但是我们毕竟是一家商业的公司，自己的技术不是都能够去无私的告诉别人，所以在某些时候，还是需要有所保留的。在公司没有涉及的领域，技术方面是可以共同学习成长的，这个也能够认识一些技术圈的朋友，之前有录过几段小的视频教程，之所以停止下来，发现那个太耗时间了，并且能够之前传递的知识面比较小，在这里感谢之前支持的朋友<br>在ceph这套系统里面，在跟一些朋友有过技术的交流后，发现真的是不知者不惧，在没有深入了解运行机制的情况下，做着一些非常危险的操作，很多操作可能会直接弄崩掉集群，我们应该对未知的东西充满敬畏之心，小心小心再小心<br>在任何情况下，如果有条件允许，ceph的集群一定要部署两套，一个作为备份，这个是我一直坚持的观点，因为系统级别的故障是你永远无法预料的，副本只能解决系统内的故障，你的修复能力也只能将损失降到最低</p>
<p>好吧那么开始下面的学习之旅了！<br><a id="more"></a></p>
<p><img src="http://i.imgur.com/mVe4yqv.png" alt=""></p>
<h4 id="ceph_cookbook">ceph cookbook</h4><h4 id="参与人员_(原文)_(翻译)_(评论)">参与人员  (<a href="https://www.zybuluo.com/zphj1987/note/311834" title="原文" target="_blank" rel="external">原文</a>) (<a href="https://www.zybuluo.com/zphj1987/note/311760" title="翻译" target="_blank" rel="external">翻译</a>) (<a href="https://www.zybuluo.com/zphj1987/note/311840" title="评论" target="_blank" rel="external">评论</a>)</h4><h4 id="前言(原文)(翻译)">前言(<a href="https://www.zybuluo.com/zphj1987/note/311844" title="原文" target="_blank" rel="external">原文</a>)(<a href="https://www.zybuluo.com/zphj1987/note/312602" target="_blank" rel="external">翻译</a>)</h4><h4 id="关于作者(原文)(翻译)">关于作者(<a href="https://www.zybuluo.com/zphj1987/note/312750" target="_blank" rel="external">原文</a>)(<a href="https://www.zybuluo.com/zphj1987/note/313284" target="_blank" rel="external">翻译</a>)</h4><h4 id="关于审稿（原文)(翻译）">关于审稿（<a href="https://www.zybuluo.com/zphj1987/note/313371" target="_blank" rel="external">原文</a>)(<a href="https://www.zybuluo.com/zphj1987/note/313352" target="_blank" rel="external">翻译</a>）</h4><h4 id="序言(原文)(翻译)">序言(<a href="https://www.zybuluo.com/zphj1987/note/313383" target="_blank" rel="external">原文</a>)(<a href="https://www.zybuluo.com/zphj1987/note/313402" target="_blank" rel="external">翻译</a>)</h4><ul>
<li>你需要为这本书准备什么</li>
<li>这本书为谁写的</li>
<li>章节结构</li>
<li>约定</li>
<li>读者反馈</li>
<li>客户支持<h3 id="1-ceph-介绍以及其它">1.ceph-介绍以及其它</h3></li>
<li>ceph-一个新的时代开启</li>
<li>RAID-一个时代的结束</li>
<li>ceph-架构的概述</li>
<li>ceph的部署的规划</li>
<li>创建一个虚拟的环境</li>
<li>安装和配置ceph</li>
<li>扩展你的集群</li>
<li>集群操作的实践<h3 id="2-Ceph_Block_Device_相关(原文)">2.Ceph Block Device 相关(<a href="https://www.zybuluo.com/zphj1987/note/315050" target="_blank" rel="external">原文</a>)</h3></li>
<li>ceph Block Device 相关（<a href="https://www.zybuluo.com/zphj1987/note/315086" target="_blank" rel="external">原文</a>）</li>
<li>配置ceph客户端（<a href="https://www.zybuluo.com/zphj1987/note/315090" target="_blank" rel="external">原文</a>）</li>
<li>创建 ceph block device（<a href="https://www.zybuluo.com/zphj1987/note/315110" target="_blank" rel="external">原文</a>）</li>
<li>映射 ceph block Device（<a href="https://www.zybuluo.com/zphj1987/note/315118" target="_blank" rel="external">原文</a>）</li>
<li>调整 ceph rbd 大小（<a href="https://www.zybuluo.com/zphj1987/note/315127" target="_blank" rel="external">原文</a>）</li>
<li>RBD 快照相关(<a href="https://www.zybuluo.com/zphj1987/note/315133" target="_blank" rel="external">原文</a>)</li>
<li>RBD 克隆相关(<a href="https://www.zybuluo.com/zphj1987/note/315587" target="_blank" rel="external">原文</a>)</li>
<li>快速浏览一下openstack</li>
<li>ceph-openstack 的最佳组合</li>
<li>配置 openstack</li>
<li>配置 glance 的ceph 后端</li>
<li>配置 cinder 的ceph 后端</li>
<li>配置 nova 连接ceph RBD</li>
<li>配置 nova 从ceph rbd启动实例<h3 id="3-ceph_对象存储相关">3.ceph 对象存储相关</h3></li>
<li>理解 ceph 对象存储</li>
<li>RADOS网关的标准设置,安装和配置</li>
<li>创建一个 radosgw 用户</li>
<li>使用 s3 API 访问ceph对象存储</li>
<li>使用 swift 访问 ceph对象存储</li>
<li>RADOS 网关与 openstack keystone的整合</li>
<li>ceph 联合网关配置</li>
<li>测试 ceph 联合网关配置</li>
<li>基于RGW构建文件同步和共享服务<h3 id="4-ceph_文件系统相关">4.ceph 文件系统相关</h3></li>
<li>理解 ceph 文件系统 和 MDS</li>
<li>部署 ceph MDS</li>
<li>使用内核驱动访问cephfs</li>
<li>使用 fuse 客户端访问cephfs</li>
<li>使用nfs作为cephfs的接口</li>
<li>cephfs的windows客户端 ceph-dokan</li>
<li>cephfs 直接替代 hdfs<h3 id="5-使用_calamari_监控ceph_集群">5.使用 calamari 监控ceph 集群</h3></li>
<li>ceph 集群的监控-经典方法</li>
<li>监控ceph集群</li>
<li>介绍 ceph calamari</li>
<li>打包 calamari server </li>
<li>打包 calamari client </li>
<li>配置 calamari master 服务器</li>
<li>添加 ceph 节点到 calamari</li>
<li>通过calamari 图形界面监控ceph集群</li>
<li>calamari 故障排查<h3 id="6-操作和管理_ceph_集群">6.操作和管理 ceph 集群</h3></li>
<li>理解 ceph 服务管理</li>
<li>管理 集群的配置文件</li>
<li>使用 SYSvinit 运行ceph</li>
<li>使用 service 运行ceph</li>
<li>横向扩展 与纵向扩展</li>
<li>横向扩展你的集群</li>
<li>横向缩小你的集群</li>
<li>替换集群一个损坏的盘</li>
<li>升级你的集群</li>
<li>维护一个集群<h3 id="7-ceph的内部运作">7.ceph的内部运作</h3></li>
<li>ceph 可扩展性和高可用性</li>
<li>理解crush 的运行机制</li>
<li>crush map 内部</li>
<li>ceph 的 cluster map</li>
<li>minitors的高可用</li>
<li>ceph 认证和授权</li>
<li>ceph 集群动态管理</li>
<li>ceph placement group</li>
<li>placement group 状态</li>
<li>在指定osd上创建存储池<h3 id="8-生产环境的规划和性能调优">8.生产环境的规划和性能调优</h3></li>
<li>容量、性能、以及成本的动态变化</li>
<li>选择ceph的硬件和软件组件</li>
<li>ceph 建议和性能调优</li>
<li>ceph 纠删码</li>
<li>创建一个纠删码存储池</li>
<li>ceph 的cache tiering</li>
<li>创建cache tiering的存储池</li>
<li>创建cache tiering</li>
<li>配置cache tier</li>
<li>测试cache tier<h3 id="9-ceph的virtual_storage_manager（intel的vsm）">9.ceph的virtual storage manager（intel的vsm）</h3></li>
<li>理解 vsm 的架构</li>
<li>配置 vsm 的环境</li>
<li>vsm的准备工作</li>
<li>安装vsm</li>
<li>使用vsm创建一个集群</li>
<li>探索 vsm 的图形界面</li>
<li>使用 vsm 升级一个集群</li>
<li>vsm 的roadmap</li>
<li>vsm 的资源<h3 id="10-更多关于ceph">10.更多关于ceph</h3></li>
<li>ceph 集群的基准测试</li>
<li>磁盘性能基准</li>
<li>网络性能的基准</li>
<li>ceph 的rados bench</li>
<li>rados load-gen 负载生成</li>
<li>ceph block device的基准测试</li>
<li>使用fio做rbd的基准测试(<a href="https://www.zybuluo.com/zphj1987/note/313152" target="_blank" rel="external">原文</a>)（<a href="https://www.zybuluo.com/zphj1987/note/313159" target="_blank" rel="external">翻译</a>）（评论）</li>
<li>ceph 的admin socket</li>
<li>使用ceph tell 命令</li>
<li>ceph rest api</li>
<li>分析 ceph 内存</li>
<li>使用 ansible 部署集群</li>
<li>ceph-objectstore 工具</li>
</ul>
<hr>
<p>更新时间：2016-03-15</p>
<ul>
<li>序言章节完成</li>
<li>需要补充gfio测试rbd内容</li>
</ul>
<p>更新时间：2016-03-14</p>
<ul>
<li>使用fio做rbd的基准测试章节完成，会根据从中挑选出部分先写</li>
</ul>
<p>更新时间：2016-03-13</p>
<ul>
<li>完成参与人员章节翻译与评论</li>
<li>内容以链接+密码的方式发布在作业部落上，并购买会员提供更好的访问体验</li>
</ul>
<p>更新时间：2016-03-12</p>
<ul>
<li>创建book的目录</li>
</ul>
<hr>
<p><a href="https://zphj1987.gitbooks.io/calamaribook/content/" title="我之前的calamari书" target="_blank" rel="external">我之前的calamari书</a></p>
<p>后面的评论会对翻译的内容做补充的，可能由于是出版书的原因，作者不会在某一个点上讲的太深，让人意犹未尽，这个作者只是一个引子，关键还是靠自己摸索，这个部分我会给出自己所理解的，以及补充内容</p>
<p>会创建一个读者群用于交流本书</p>
<p><strong>如果觉得我的文章对您有用，请联系我（qq:199383004）。您的支持将鼓励我继续创作！</strong></p>
<p><div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"></div></p>
<div style="text-align:center">


<hr>
<p>关于本书的讨论群：</p>
<p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""></p>
<p>感谢支持人员：<br>运维-北京-struggle<br>研发-北京-solar<br>运维-北京-小白<br>研发-北京-猪呆呆</p>
</div>]]></content>
    <summary type="html">
    <![CDATA[<p>以下是最新的一本ceph英文书的目录，我会学习里面的内容，并且翻译成中文的读书笔记，接触ceph也有几年了，还记得那个夏天毕业的时候辞掉了一份月薪接近4K的工作，转而换了一份月薪差不多1K的工作，然后就接触了现在非常火的ceph，有时候就是这样的，如果没有当时进入这个行业，也就没有今天的我<br>现在在接触的越多的相关知识后，越发发现自己还需要学的东西很多，只有掌握的越多，越能对你所做的事情能后有确定的信心，所以抓住一切能成长的机会，在技术上，我是很喜欢遇到问题的，遇到的问题越多，能够学到的东西越多，所以在ceph的中国社区里面，在一段时间内，我都尽力去回答每一个我知道的问题，但是我们毕竟是一家商业的公司，自己的技术不是都能够去无私的告诉别人，所以在某些时候，还是需要有所保留的。在公司没有涉及的领域，技术方面是可以共同学习成长的，这个也能够认识一些技术圈的朋友，之前有录过几段小的视频教程，之所以停止下来，发现那个太耗时间了，并且能够之前传递的知识面比较小，在这里感谢之前支持的朋友<br>在ceph这套系统里面，在跟一些朋友有过技术的交流后，发现真的是不知者不惧，在没有深入了解运行机制的情况下，做着一些非常危险的操作，很多操作可能会直接弄崩掉集群，我们应该对未知的东西充满敬畏之心，小心小心再小心<br>在任何情况下，如果有条件允许，ceph的集群一定要部署两套，一个作为备份，这个是我一直坚持的观点，因为系统级别的故障是你永远无法预料的，副本只能解决系统内的故障，你的修复能力也只能将损失降到最低</p>
<p>好吧那么开始下面的学习之旅了！<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[丢了ceph.mon.keying解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/03/03/%E4%B8%A2%E4%BA%86ceph-mon-kerying%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/03/03/丢了ceph-mon-kerying解决办法/</id>
    <published>2016-03-03T15:38:47.000Z</published>
    <updated>2016-03-24T13:45:53.340Z</updated>
    <content type="html"><![CDATA[<p>在linux操作系统下，可能因为一些很小的误操作，都会造成非常重要的文件的丢失，而文件的备份并不是每时每刻都会注意到，一般是等到文件丢失了才会去想办法，这里讲下ceph.mon.keyring丢失的解决办法</p>
<h3 id="1、没有启用部署认证的">1、没有启用部署认证的</h3><blockquote>
<p>auth_cluster_required =none</p>
</blockquote>
<p>在进行部署的时候 ceph-deploy new 以后会生成ceph.mon.keyring文件,内容如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mon.]</span><br><span class="line">key = AQBnutdWAAAAABAAPmLaAd9CeKaCRj1CIrztyA==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>这个keyring在增加mon的时候，mon之间加密会用到的，如果在未开启认证的情况下，只需要在部署目录下面创建一个同名文件，里面填入一个格式相同的任意内容即可（需要有这个文件，不然无法部署新的mon)，在没有开启认证的时候是没有生成这个文件的 <code>/var/lib/ceph/mon/ceph-lab8106/keyring</code></p>
<p>一般是在系统盘损坏的时候容易丢失这个部署目录,并且没有做备份</p>
<h3 id="2、启用了部署认证">2、启用了部署认证</h3><blockquote>
<p>auth_cluster_required =cephx</p>
</blockquote>
<p>开启了以后，在进行mon的部署以后，是会生成/var/lib/ceph/mon/ceph-lab8106/keyring这个文件的，这个文件的内容跟部署的目录下面的keyring是一样的，所以丢失了部署目录以后，去mon的路径下面查看文件，然后写入一个同名文件里面即可<br>如下所示</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># cat /var/lib/ceph/mon/ceph-lab8106/keyring </span></span><br><span class="line">[mon.]</span><br><span class="line">key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==</span><br><span class="line">caps mon = <span class="string">"allow *"</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cat ceph.mon.keyring </span></span><br><span class="line">[mon.]</span><br><span class="line">key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure>
<p>这个是个简单的操作即可避免无法新加mon的问题，当然做好预备工作是最好的，建议做好下面的两个工作：</p>
<ul>
<li>1、直接备份部署目录里面的文件，最好备份到自己的电脑上</li>
<li>2、将这个keyring的信息备份到集群里面去,记录到认证信息中<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph auth import -i ceph.mon.keyring</span></span><br><span class="line">imported keyring</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph auth get mon.</span></span><br><span class="line">exported keyring <span class="keyword">for</span> mon.</span><br><span class="line">[mon.]</span><br><span class="line">key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==</span><br><span class="line">caps mon = <span class="string">"allow *"</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cat ceph.mon.keyring </span></span><br><span class="line">[mon.]</span><br><span class="line">key = AQC4V9hWAAAAABAArPTf+Az43NWsoU88okI+Mg==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure>
</li>
</ul>
<hr>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！</strong></p>
<div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"><br></div>


]]></content>
    <summary type="html">
    <![CDATA[<p>在linux操作系统下，可能因为一些很小的误操作，都会造成非常重要的文件的丢失，而文件的备份并不是每时每刻都会注意到，一般是等到文件丢失了才会去想办法，这里讲下ceph.mon.keyring丢失的解决办法</p>
<h3 id="1、没有启用部署认证的">1、没有启用部署认证的</h3><blockquote>
<p>auth_cluster_required =none</p>
</blockquote>
<p>在进行部署的时候 ceph-deploy new 以后会生成ceph.mon.keyring文件,内容如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mon.]</span><br><span class="line">key = AQBnutdWAAAAABAAPmLaAd9CeKaCRj1CIrztyA==</span><br><span class="line">caps mon = allow *</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[快速增加osdmap的epoch]]></title>
    <link href="http://www.zphj1987.com/2016/03/03/%E5%BF%AB%E9%80%9F%E5%A2%9E%E5%8A%A0osdmap%E7%9A%84epoch/"/>
    <id>http://www.zphj1987.com/2016/03/03/快速增加osdmap的epoch/</id>
    <published>2016-03-03T13:57:26.000Z</published>
    <updated>2016-03-12T17:12:26.119Z</updated>
    <content type="html"><![CDATA[<p>最近因为一个实验需要用到一个功能，需要快速的增加 ceph 的 osdmap 的 epoch 编号</p>
<h3 id="查询osd的epoch编号">查询osd的epoch编号</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># ceph osd stat</span></span><br><span class="line">     osdmap e4686: <span class="number">8</span> osds: <span class="number">8</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure>
<p>上面显示的 <code>e4686</code> 即为osdmap的epoch的编号</p>
<h3 id="增加epoch">增加epoch</h3><p>现在需要增加1000<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd thrash <span class="number">1000</span></span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>执行完了后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd <span class="built_in">stat</span></span><br><span class="line">osdmap e5687: <span class="number">8</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span>; <span class="number">232</span> remapped pgs</span><br></pre></td></tr></table></figure></p>
<p>很快就增加了1000的编号，这个命令执行完了后，osd weight 会变成0，这个做下恢复即可，节点会down，调整下weight，恢复下状态就可以了</p>
<hr>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！</strong></p>
<div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"><br></div>

]]></content>
    <summary type="html">
    <![CDATA[<p>最近因为一个实验需要用到一个功能，需要快速的增加 ceph 的 osdmap 的 epoch 编号</p>
<h3 id="查询osd的epoch编号">查询osd的epoch编号</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># ceph osd stat</span></span><br><span class="line">     osdmap e4686: <span class="number">8</span> osds: <span class="number">8</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure>
<p>上面显示的 <code>e4686</code> 即为osdmap的epoch的编号</p>
<h3 id="增加epoch">增加epoch</h3><p>现在需要增加1000<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd thrash <span class="number">1000</span></span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[yum安装指定版本ceph包]]></title>
    <link href="http://www.zphj1987.com/2016/03/01/yum%E5%AE%89%E8%A3%85%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%ACceph%E5%8C%85/"/>
    <id>http://www.zphj1987.com/2016/03/01/yum安装指定版本ceph包/</id>
    <published>2016-03-01T04:21:56.000Z</published>
    <updated>2016-03-01T05:17:28.224Z</updated>
    <content type="html"><![CDATA[<p>安装ceph包的方式有很多，这里讲的是从官网直接通过yum源的安装方式进行安装</p>
<p>yum源对应的地址为<br><a href="http://download.ceph.com/rpm-hammer/el6/x86_64/" target="_blank" rel="external">http://download.ceph.com/rpm-hammer/el6/x86_64/</a></p>
<p>怎么配置ceph源就不在这里赘述了</p>
<p>下图为ceph官网的yum源里面的文件列表：</p>
<a id="more"></a>
<p><img src="http://7xo9we.com1.z0.glb.clouddn.com/16-3-1/80906036.jpg" alt=""></p>
<p>可以看到有多个版本的，默认的会安装最新的版本的</p>
<p>这样就会有个问题：<br>安装了一个老版本的包，需要安装一个附属的包，安装的版本就会是最新版本的，而不是已经安装的版本的附属的包，会引起版本错乱</p>
<p>解决办法：</p>
<h3 id="1、查询源里面的包有多少个版本">1、查询源里面的包有多少个版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@zhongbo ~]<span class="comment"># yum --showduplicates list ceph | expand</span></span><br><span class="line">Loaded plugins: security</span><br><span class="line">Available Packages</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>-<span class="number">0</span>.el6                            ceph</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">1</span>-<span class="number">0</span>.el6                          ceph</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">2</span>-<span class="number">0</span>.el6                          ceph</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">3</span>-<span class="number">0</span>.el6                          ceph</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">4</span>-<span class="number">0</span>.el6                          ceph</span><br><span class="line">ceph.x86_64                         <span class="number">1</span>:<span class="number">0.94</span>.<span class="number">5</span>-<span class="number">0</span>.el6                          ceph</span><br></pre></td></tr></table></figure>
<h3 id="2、安装指定版本的包">2、安装指定版本的包</h3><p>假如现在需要安装的是1:0.94.4-0.el6这个版本<br>安装的格式为：</p>
<pre><code>yum install &lt;package <span class="property">name</span>&gt;-&lt;<span class="property">version</span> info&gt;
</code></pre><p>对应到这里<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-<span class="number">0.94</span>.<span class="number">4</span>-<span class="number">0</span>.el6</span><br></pre></td></tr></table></figure></p>
<p>注意名称规则是前面的名称，中间的版本去掉 1: 然后就可以安装指定版本的包了</p>
<hr>
<h3 id="My_daily">My daily</h3><p>今天的午饭不错<br>天气也很好<br><img src="http://7xo9we.com1.z0.glb.clouddn.com/16-3-1/67526735.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>安装ceph包的方式有很多，这里讲的是从官网直接通过yum源的安装方式进行安装</p>
<p>yum源对应的地址为<br><a href="http://download.ceph.com/rpm-hammer/el6/x86_64/">http://download.ceph.com/rpm-hammer/el6/x86_64/</a></p>
<p>怎么配置ceph源就不在这里赘述了</p>
<p>下图为ceph官网的yum源里面的文件列表：</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[一步一步打造集群监控系统(持续更新中-最后更新20160301)]]></title>
    <link href="http://www.zphj1987.com/2016/02/24/%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%89%93%E9%80%A0%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/"/>
    <id>http://www.zphj1987.com/2016/02/24/一步一步打造集群监控系统/</id>
    <published>2016-02-24T12:28:00.000Z</published>
    <updated>2016-03-01T09:46:26.912Z</updated>
    <content type="html"><![CDATA[<p>目前的开源的ceph的监控平台有几个，inkscope，calamari，vsm，ceph-dash，这几个软件各有各的特色各有个的特点，按需取用，下面先简单介绍下这几个软件的运行模式</p>
<ul>
<li>inkscope自己封装了一套api，在集群数据方面直接调用的ceph的自己的ceph-rest-api,节点硬件数据通过一个自带的插件收取数据，里面的osd的pg显示非常有特色，使用了sankey chart来显示关联关系</li>
<li>calamari 是 redhat官方提供的一个开源监控，监控基于的graphite，diamond收集各个节点的数据发送给管理平台的机器，然后显示，目前结构稳定，基本没太多变化</li>
<li>vsm 是intel出的一个监控加管理的平台，可以进行部署和监控，现在还处于开发阶段，版本在一直发布，与openstack有结合</li>
<li>ceph-dash配置很简单，直接运行即可，可以获取到一些基本的数据</li>
</ul>
<p>以上软件都非常不错，如果需求不是太高，直接取用就行</p>
<p>本篇将根据我自己的经验，根据现有的软件，打造一个自己需要的监控系统，本篇不会具体写细节如何实现，主要是记录，在监控一套系统的时候，需要去关注什么数据，数据以什么形式的展示，整个过程会比较久，会持续更新，欢迎探讨</p>
<a id="more"></a>
<h3 id="一、监控组成">一、监控组成</h3><p>监控采用了grafana+graphite+diamond的方式，需要开发量的主要在diamond的数据的获取方面，这个部分会有单独的文章介绍，页面采用grafana，这个数据展示是目前比较关注的地方，也是监控系统的难点</p>
<h3 id="二、监控区块">二、监控区块</h3><h4 id="2-1、健康状态的监控">2.1、健康状态的监控</h4><p><img src="http://www.zphj1987.com/assets/blogImg/jiankong/health.png" alt="healthstatus"><br>ceph -s 可以看到集群的状态，这个状态有三个:</p>
<ul>
<li>health_ok</li>
<li>health_warn</li>
<li>health_err</li>
</ul>
<p>第一个监控区块实现的功能如下：</p>
<ul>
<li>OK为绿色，WARN为黄色，ERR为红色</li>
<li>区块的堆叠图记录了集群的状态，OK为值2，WARN为值1，ERR为值0，基于时间线</li>
</ul>
<p>这样做的好处可以方便的分辨出状态的变化，可以从下面的堆叠图看到是否有短时的状态异常的情况</p>
<h4 id="2-2、服务状态监控和pg状态监控">2.2、服务状态监控和pg状态监控</h4><p>监控从底层往上面走，集群的服务的监控包括了mon的状态的监控，和osd的监控，这里的监控设计是满足监控状态的时候为绿色，不满足的时候为黄色，显示数目</p>
<p>监控pg的状态是各种状态都监控，active+clean为健康状态的，显示各个状态的数目，这里因为状态太多，后期需要优化下显示，提取重要的状态进行显示</p>
<h2 id=""><img src="http://7xo9we.com1.z0.glb.clouddn.com/16-3-1/32340024.jpg" alt=""></h2><p>最后更新时间：<br>2016-03-01 </p>
<p><strong>如果觉得我的文章对您有用，请随意打赏。您的支持将鼓励我继续创作！</strong></p>
<div style="text-align:center"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="支付宝" title="支付宝扫一扫"><br></div>]]></content>
    <summary type="html">
    <![CDATA[<p>目前的开源的ceph的监控平台有几个，inkscope，calamari，vsm，ceph-dash，这几个软件各有各的特色各有个的特点，按需取用，下面先简单介绍下这几个软件的运行模式</p>
<ul>
<li>inkscope自己封装了一套api，在集群数据方面直接调用的ceph的自己的ceph-rest-api,节点硬件数据通过一个自带的插件收取数据，里面的osd的pg显示非常有特色，使用了sankey chart来显示关联关系</li>
<li>calamari 是 redhat官方提供的一个开源监控，监控基于的graphite，diamond收集各个节点的数据发送给管理平台的机器，然后显示，目前结构稳定，基本没太多变化</li>
<li>vsm 是intel出的一个监控加管理的平台，可以进行部署和监控，现在还处于开发阶段，版本在一直发布，与openstack有结合</li>
<li>ceph-dash配置很简单，直接运行即可，可以获取到一些基本的数据</li>
</ul>
<p>以上软件都非常不错，如果需求不是太高，直接取用就行</p>
<p>本篇将根据我自己的经验，根据现有的软件，打造一个自己需要的监控系统，本篇不会具体写细节如何实现，主要是记录，在监控一套系统的时候，需要去关注什么数据，数据以什么形式的展示，整个过程会比较久，会持续更新，欢迎探讨</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[内核参数比较]]></title>
    <link href="http://www.zphj1987.com/2016/02/23/%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E6%AF%94%E8%BE%83/"/>
    <id>http://www.zphj1987.com/2016/02/23/内核参数比较/</id>
    <published>2016-02-23T05:14:19.000Z</published>
    <updated>2016-02-29T06:02:28.455Z</updated>
    <content type="html"><![CDATA[<p>使用sysctl -a &gt; file1.log<br>取出系统的内核参数，拷贝到同一个目录下面<br>然后用下面的脚本打印出不同的值<br>然后分析差异<br>diff只能按行比较，而这个文件有错行，所以用一个简单的办法实现<br><a id="more"></a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line"><span class="comment"># the output is the different from file1 to file2 ,so you must change the position of the file1name of file1 and file2 ;</span></span><br><span class="line">file1=centos.log</span><br><span class="line">file2=centos2.log</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> `cat <span class="variable">$file1</span> |awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">va1=`cat <span class="variable">$file1</span> |grep <span class="variable">$key</span>|cut <span class="operator">-f</span> <span class="number">2</span> <span class="operator">-d</span> <span class="string">"="</span>`</span><br><span class="line">va2=`cat <span class="variable">$file2</span> |grep <span class="variable">$key</span>|cut <span class="operator">-f</span> <span class="number">2</span> <span class="operator">-d</span> <span class="string">"="</span>`</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"<span class="variable">$va1</span>"</span> != <span class="string">"<span class="variable">$va2</span>"</span> ];<span class="keyword">then</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$key</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$file1</span>"</span>:<span class="variable">$va1</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$file2</span>"</span>:<span class="variable">$va2</span></span><br><span class="line"><span class="built_in">echo</span> ======================</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<script type="text/javascript">
var sogou_ad_id=547108;
var sogou_ad_height=90;
var sogou_ad_width=728;
</script>
<script type="text/javascript" src="http://images.sohu.com/cs/jsfile/js/c.js"></script>]]></content>
    <summary type="html">
    <![CDATA[<p>使用sysctl -a &gt; file1.log<br>取出系统的内核参数，拷贝到同一个目录下面<br>然后用下面的脚本打印出不同的值<br>然后分析差异<br>diff只能按行比较，而这个文件有错行，所以用一个简单的办法实现<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[nginx状态监控统计]]></title>
    <link href="http://www.zphj1987.com/2016/02/22/nginx%E7%8A%B6%E6%80%81%E7%9B%91%E6%8E%A7%E7%BB%9F%E8%AE%A1/"/>
    <id>http://www.zphj1987.com/2016/02/22/nginx状态监控统计/</id>
    <published>2016-02-22T15:58:02.000Z</published>
    <updated>2016-02-22T16:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>nginx是一款很优秀的web服务器软件，很多地方都有接触和使用到他，大部分的场景压力还没达到需要调优的地步，而调优的难点其实不在于调，而在于各项状态的监控，能够很快的找到资源在什么时候出现问题，调整前后出现的变化，如果都不知道变化在哪里所做的调优只能是凭感觉的</p>
<p>之前看到有技术人员用nginx作为rgw的前端的时候，通过优化去实现将nginx的并发提高到很大，而不出现4xx等问题，nginx的access.log里面是有记录访问的状态码的，而这个日志的分析如果是一次次的去看，这样的分析是无法用精确的数据去展示的</p>
<p>最开始的想法是想根据时间点去统计时间点的状态码，后来发现这样做既复杂，又无法输出到一些数据展示软件当中，实际上我只需要统计一定时间的总的状态值，然后定期去取这个值，然后在数据展示的时候，就可以看到一个数值的曲线图，增量即为这个时间区间所产生的状态值</p>
<p>下面就是我的实现，一个脚本就可以统计了，这个是最初的版本，纯统计状态码，还没有区分读写分离的情况，这个在后面会加入分离的情况</p>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">### BEGIN INIT INFO</span></span><br><span class="line"><span class="comment"># Provides:          nginxstatus</span></span><br><span class="line"><span class="comment"># Required-Start:    $nginx</span></span><br><span class="line"><span class="comment"># Short-Description: nginxstatus</span></span><br><span class="line"><span class="comment"># Description: collectstatus of nginx</span></span><br><span class="line"><span class="comment">### END INIT INFO</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># pidfile: /var/run/nginx/nginxstatus.pid</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Source function library.</span></span><br><span class="line"><span class="comment">##########################################</span></span><br><span class="line"><span class="comment">#状态码一般分为1xx,2xx,3xx,4xx,5xx,total</span></span><br><span class="line">statucode=<span class="string">"2 3 4 5"</span></span><br><span class="line"><span class="comment">##check intervel setting </span></span><br><span class="line">interval=<span class="number">2</span></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"><span class="comment">#check the nginxstatus pid dir if exist</span></span><br><span class="line"><span class="keyword">if</span> [ ! <span class="operator">-d</span> /var/run/nginxstatus/ ];<span class="keyword">then</span></span><br><span class="line">mkdir  /var/run/nginxstatus/</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">##check the status of nginx access </span></span><br><span class="line"><span class="function"><span class="title">check</span></span>()&#123;</span><br><span class="line"><span class="keyword">for</span> code <span class="keyword">in</span> <span class="variable">$statucode</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$code</span>"</span>xx:`cat /var/<span class="built_in">log</span>/nginx/access.log |awk <span class="string">'&#123;if( substr($9,0,1) == '</span><span class="string">''</span><span class="variable">$code</span><span class="string">''</span><span class="string">' )  print $9&#125;'</span>  |wc <span class="operator">-l</span>` &gt; /var/<span class="built_in">log</span>/nginx/<span class="string">"<span class="variable">$code</span>"</span>xx.log</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">sleep <span class="variable">$interval</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#start nginx status</span></span><br><span class="line"><span class="function"><span class="title">start</span></span>() &#123;</span><br><span class="line"><span class="built_in">echo</span> <span class="operator">-e</span> Starting nginxstatus:                              <span class="string">"\033[32m [  OK  ] \033[0m"</span></span><br><span class="line"><span class="keyword">while</span> [ <span class="number">2</span> &gt; <span class="number">1</span> ]</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">check</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span> &amp;</span><br><span class="line">pid=`ps ax | grep -i <span class="string">'nginxstatus'</span> | head -n <span class="number">1</span>|awk <span class="string">'&#123;print $1&#125;'</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="operator">-e</span>  pid is  <span class="string">"\033[33m  $! \033[0m"</span> </span><br><span class="line"><span class="built_in">echo</span> $! &gt;&gt; /var/run/nginxstatus/nginxstatus.pid</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#stop nginx </span></span><br><span class="line"><span class="function"><span class="title">stop</span></span>() &#123;</span><br><span class="line"><span class="built_in">echo</span> <span class="operator">-e</span> stop nginxstatus collect  <span class="string">"\033[32m  [  OK  ] \033[0m"</span> </span><br><span class="line"></span><br><span class="line">pid=`cat /var/run/nginxstatus/nginxstatus.pid  <span class="number">2</span>&gt;/dev/null`</span><br><span class="line"><span class="built_in">kill</span> -<span class="number">10</span> <span class="variable">$pid</span>  <span class="number">2</span>&gt;/dev/null</span><br><span class="line"><span class="comment">#killall nginxstatus</span></span><br><span class="line">rm -rf /var/run/nginxstatus/nginxstatus.pid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">status</span></span>() &#123;</span><br><span class="line"><span class="keyword">for</span> code <span class="keyword">in</span> <span class="variable">$statucode</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">cat /var/<span class="built_in">log</span>/nginx/<span class="string">"<span class="variable">$code</span>"</span>xx.log</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">clean</span></span> () &#123;</span><br><span class="line"><span class="keyword">for</span> code <span class="keyword">in</span> <span class="variable">$statucode</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">""</span> &gt;  /var/<span class="built_in">log</span>/nginx/<span class="string">"<span class="variable">$code</span>"</span>xx.log</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">""</span> &gt; /var/<span class="built_in">log</span>/nginx/access.log</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"clean /var/log/nginx/access.log"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="operator">-e</span> <span class="string">"clean /var/log/nginx/access.log"</span> <span class="string">"\033[32m  [  OK  ] \033[0m"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="string">"<span class="variable">$1</span>"</span> <span class="keyword">in</span></span><br><span class="line">        start)</span><br><span class="line">                start  &amp;&amp; <span class="built_in">exit</span> <span class="number">0</span></span><br><span class="line">                ;;</span><br><span class="line">        stop)</span><br><span class="line">                stop || <span class="built_in">exit</span> <span class="number">0</span></span><br><span class="line">                ;;</span><br><span class="line">        status)</span><br><span class="line">                status</span><br><span class="line">                ;;</span><br><span class="line">        clean)</span><br><span class="line">                clean</span><br><span class="line">                ;;</span><br><span class="line">        *)</span><br><span class="line">                <span class="built_in">echo</span> $<span class="string">"Usage: <span class="variable">$0</span> &#123;start|stop|status|clean&#125;"</span></span><br><span class="line">                <span class="built_in">exit</span> <span class="number">2</span></span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"><span class="built_in">exit</span> $?</span><br></pre></td></tr></table></figure>
<p>使用方法：</p>
<h3 id="1、启动进程">1、启动进程</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@zhongbo ~]<span class="comment"># /etc/init.d/nginxstatus start</span></span><br><span class="line">Starting nginxstatus:  [  OK  ] </span><br><span class="line">pid is   <span class="number">166534</span></span><br></pre></td></tr></table></figure>
<p>会生成下面的状态文件，周期为2s一更新<br>[root@zhongbo ~]# ll /var/log/nginx/*xx.log<br>-rw-r—r— 1 root root 7 Feb 23 00:25 /var/log/nginx/2xx.log<br>-rw-r—r— 1 root root 6 Feb 23 00:25 /var/log/nginx/3xx.log<br>-rw-r—r— 1 root root 7 Feb 23 00:25 /var/log/nginx/4xx.log<br>-rw-r—r— 1 root root 6 Feb 23 00:25 /var/log/nginx/5xx.log</p>
<h3 id="2、当前nginx的状态查询">2、当前nginx的状态查询</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@zhongbo ~]<span class="comment"># /etc/init.d/nginxstatus status</span></span><br><span class="line"><span class="number">2</span>xx:<span class="number">21</span></span><br><span class="line"><span class="number">3</span>xx:<span class="number">1</span></span><br><span class="line"><span class="number">4</span>xx:<span class="number">10</span></span><br><span class="line"><span class="number">5</span>xx:<span class="number">0</span></span><br></pre></td></tr></table></figure>
<h3 id="3、停止nginxstatus进程">3、停止nginxstatus进程</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@zhongbo ~]<span class="comment"># /etc/init.d/nginxstatus stop</span></span><br><span class="line">stop nginxstatus collect   [  OK  ]</span><br></pre></td></tr></table></figure>
<h3 id="4、清理历史数据">4、清理历史数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@zhongbo ~]<span class="comment"># /etc/init.d/nginxstatus clean</span></span><br><span class="line">clean /var/<span class="built_in">log</span>/nginx/access.log   [  OK  ]</span><br></pre></td></tr></table></figure>
<p>这个操作会清空/var/log/nginx/access.log日志的内容重新统计</p>
<p>这个会在后期根据需求进行优化</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>nginx是一款很优秀的web服务器软件，很多地方都有接触和使用到他，大部分的场景压力还没达到需要调优的地步，而调优的难点其实不在于调，而在于各项状态的监控，能够很快的找到资源在什么时候出现问题，调整前后出现的变化，如果都不知道变化在哪里所做的调优只能是凭感觉的</p>
<p>之前看到有技术人员用nginx作为rgw的前端的时候，通过优化去实现将nginx的并发提高到很大，而不出现4xx等问题，nginx的access.log里面是有记录访问的状态码的，而这个日志的分析如果是一次次的去看，这样的分析是无法用精确的数据去展示的</p>
<p>最开始的想法是想根据时间点去统计时间点的状态码，后来发现这样做既复杂，又无法输出到一些数据展示软件当中，实际上我只需要统计一定时间的总的状态值，然后定期去取这个值，然后在数据展示的时候，就可以看到一个数值的曲线图，增量即为这个时间区间所产生的状态值</p>
<p>下面就是我的实现，一个脚本就可以统计了，这个是最初的版本，纯统计状态码，还没有区分读写分离的情况，这个在后面会加入分离的情况</p>]]>
    
    </summary>
    
      <category term="nginx" scheme="http://www.zphj1987.com/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[diamond收集插件的自定义]]></title>
    <link href="http://www.zphj1987.com/2016/02/21/diamond%E6%94%B6%E9%9B%86%E6%8F%92%E4%BB%B6%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89/"/>
    <id>http://www.zphj1987.com/2016/02/21/diamond收集插件的自定义/</id>
    <published>2016-02-21T11:23:55.000Z</published>
    <updated>2016-02-21T12:55:14.000Z</updated>
    <content type="html"><![CDATA[<p>diamond是与graphite配合使用的一个数据收集的软件，关于这个配置的资料很多，使用起来也比较简单，详细的安装和配置会在后面的关于整套监控系统的文章里面写到，本篇是专门讲解怎么自定义这个数据收集的插件</p>
<p>diamond的结构比较简单:</p>
<ul>
<li>Collector 数据采集的模块</li>
<li>handlers 数据发送的模块</li>
</ul>
<p>这里主要讲解的是Collector部分的插件的编写，diamond自身带了非常丰富的插件，可以很方便的使用自带的插件进行监控，包括ceph和cephstats这两个可以用来监控ceph的插件，弄清楚怎么去写插件会方便很多，并且能扩展原来插件所没有的数据,calamari里面的数据的收集就是通过的diamond的</p>
<p>本例将讲解怎么写一个监控ceph的健康状态的插件</p>
<a id="more"></a>
<h3 id="1、diamond软件的安装">1、diamond软件的安装</h3><p>通过github上下载代码然后安装在服务器上即可</p>
<h3 id="2、收集数据的py的编写">2、收集数据的py的编写</h3><p>收集数据的collect的路径：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/share/diamond/collectors/</span><br></pre></td></tr></table></figure></p>
<h4 id="2-1_创建一个目录">2.1 创建一个目录</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /usr/share/diamond/collectors/cephhealth/</span><br></pre></td></tr></table></figure>
<h4 id="2-2_创建采集的py文件">2.2 创建采集的py文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /usr/share/diamond/collectors/cephhealth/cephhealth.py</span><br></pre></td></tr></table></figure>
<p>添加下面的内容：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="string">""</span><span class="string">"</span><br><span class="line">本插件用于采集ceph的健康状态</span><br><span class="line">"</span><span class="string">""</span></span><br><span class="line">import diamond.collector</span><br><span class="line">import json</span><br><span class="line">import os</span><br><span class="line">class cephhealthCollector(diamond.collector.Collector):</span><br><span class="line"></span><br><span class="line">    def get_default_config_<span class="built_in">help</span>(self):</span><br><span class="line">        config_<span class="built_in">help</span> = super(cephhealthCollector, self).get_default_config_<span class="built_in">help</span>()</span><br><span class="line">        config_help.update(&#123;</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="built_in">return</span> config_<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">    def get_default_config(self):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span><br><span class="line">        Returns the default collector settings</span><br><span class="line">        "</span><span class="string">""</span></span><br><span class="line">        config = super(cephhealth, self).get_default_config()</span><br><span class="line">        config.update(&#123;</span><br><span class="line">            <span class="string">'path'</span>:     <span class="string">'ceph'</span></span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="built_in">return</span> config</span><br><span class="line"></span><br><span class="line">    def collect(self):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span><br><span class="line">        Overrides the Collector.collect method</span><br><span class="line">        "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set Metric Name</span></span><br><span class="line">        metric_name = <span class="string">"my.cephhealth.metric"</span></span><br><span class="line">        data = os.popen(<span class="string">'ceph health -f json'</span>).read()</span><br><span class="line">        ddata = json.loads(data)</span><br><span class="line">        status = ddata[<span class="string">'overall_status'</span>]</span><br><span class="line">        <span class="keyword">if</span> status == <span class="string">'HEALTH_ERR'</span>:</span><br><span class="line">                statuscode = <span class="number">10</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">                statuscode = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set Metric Value</span></span><br><span class="line">        metric_value = statuscode</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Publish Metric</span></span><br><span class="line">        self.publish(metric_name, metric_value)</span><br></pre></td></tr></table></figure></p>
<p>以上插件注意：<br>cephhealthCollector 为这个插件的名称，也是diamond.conf里面设置的时候设置的值<br>‘path’:     ‘ceph’ 这个是在最后输出结果中会显示这个名称<br>其他部分就是注意输出一个名称 metric_name，和最后的 metric_value 即可<br>中间的部分可以自己去用python去获取数值即可</p>
<h4 id="3、修改diamond配置文件">3、修改diamond配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/diamond/diamond.conf</span><br></pre></td></tr></table></figure>
<p>在[collectors]下面插件配置的区域添加<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">[[cephhealthCollector]]&#10;enabled = true</span><br></pre></td></tr></table></figure></p>
<h4 id="4、重启diamond进程">4、重启diamond进程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/diamond restart</span><br></pre></td></tr></table></figure>
<h4 id="5、检查输出的数值">5、检查输出的数值</h4><p>可以把其他插件全部关闭，然后查看文件<br>/var/log/diamond/archive.log的内容</p>
<p>上面的插件的输出为：<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">servers.grafana.ceph.my.cephhealth.metric 10 1456057146</span><br></pre></td></tr></table></figure></p>
<p>这个结果的格式为:</p>
<ul>
<li>path_prefix = servers    （diamond.conf中配置）</li>
<li>hostname = grafana        （diamond.conf中配置）</li>
<li>get_default_config(self): path = ceph   (插件py中配置)</li>
<li>metric_name = “my.cephhealth.metric”     （插件py中配置）</li>
</ul>
<p>到这来插件就完成了，写起来还是比较简单方便的，上面的地方因为ceph里面的输出的是字符串，而grafana里面的显示状态的地方使用的是数字的，所以在这里可以通过字符串转数字，然后在web界面上使用数字字符的匹配来显示这个状态</p>
<hr>
<p>diamond的性能是很牛的，1000 台服务器每分钟总共300万个监控数值的压力也能扛下来，对于我们普通级别的使用时绰绰有余的，关于这个规模的有人写了文章，优化的时候可以参考（<a href="https://answers.launchpad.net/graphite/+question/178969" title="1000台监控" target="_blank" rel="external">1000台监控</a>）</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>diamond是与graphite配合使用的一个数据收集的软件，关于这个配置的资料很多，使用起来也比较简单，详细的安装和配置会在后面的关于整套监控系统的文章里面写到，本篇是专门讲解怎么自定义这个数据收集的插件</p>
<p>diamond的结构比较简单:</p>
<ul>
<li>Collector 数据采集的模块</li>
<li>handlers 数据发送的模块</li>
</ul>
<p>这里主要讲解的是Collector部分的插件的编写，diamond自身带了非常丰富的插件，可以很方便的使用自带的插件进行监控，包括ceph和cephstats这两个可以用来监控ceph的插件，弄清楚怎么去写插件会方便很多，并且能扩展原来插件所没有的数据,calamari里面的数据的收集就是通过的diamond的</p>
<p>本例将讲解怎么写一个监控ceph的健康状态的插件</p>]]>
    
    </summary>
    
      <category term="监控" scheme="http://www.zphj1987.com/tags/%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ubuntu安装软件自动交互]]></title>
    <link href="http://www.zphj1987.com/2016/01/27/ubuntu%E5%AE%89%E8%A3%85%E8%BD%AF%E4%BB%B6%E8%87%AA%E5%8A%A8%E4%BA%A4%E4%BA%92/"/>
    <id>http://www.zphj1987.com/2016/01/27/ubuntu安装软件自动交互/</id>
    <published>2016-01-27T08:26:30.000Z</published>
    <updated>2016-01-27T08:38:59.526Z</updated>
    <content type="html"><![CDATA[<p>在ubuntu下安装软件过程中可能会出现需要你输入密码或者其他的一些交互类的操作，这样在脚本安装的时候就可能出现阻断，这个在ubuntu里面已经考虑到了这个情况，以前我在安装这个的时候，通过的是脚本传递参数的方式，这里介绍的是原生的控制方式，这个方式更好</p>
<p>以安装mariadb-server-5.5为例</p>
<h3 id="1、查询需要应答的问题">1、查询需要应答的问题</h3><p>首先通过命令查询这个软件需要问答什么问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@mytest:/var/cache/apt/archives<span class="comment"># debconf-show mariadb-server-5.5 </span></span><br><span class="line">* mysql-server/root_password: (password omitted)</span><br><span class="line">* mysql-server/root_password_again: (password omitted)</span><br><span class="line">  mysql-server/password_mismatch:</span><br><span class="line">  mysql-server/error_setting_password:</span><br><span class="line">* mariadb-server/oneway_migration: <span class="literal">true</span></span><br><span class="line">  mysql-server-<span class="number">5.5</span>/nis_warning:</span><br><span class="line">  mysql-server-<span class="number">5.5</span>/postrm_remove_databases: <span class="literal">false</span></span><br><span class="line">  mariadb-server-<span class="number">5.5</span>/really_downgrade: <span class="literal">false</span></span><br><span class="line">  mysql-server/no_upgrade_when_using_ndb:</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>这里可以看到需要处理的是密码的问题</p>
<h3 id="2、执行命令传递参数进去">2、执行命令传递参数进去</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">debconf-set-selections &lt;&lt;&lt; <span class="string">'mariadb-server-5.5 mysql-server/root_password password 123456'</span></span><br><span class="line">debconf-set-selections &lt;&lt;&lt; <span class="string">'mariadb-server-5.5 mysql-server/root_password_again password 123456'</span></span><br></pre></td></tr></table></figure>
<p>这个地方实际是把这个值记录到了这个地方,如果要修改可以覆盖或者删除即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@mytest:/var/cache/apt/archives<span class="comment"># cat /var/cache/debconf/passwords.dat</span></span><br><span class="line">Name: mysql-server/root_password</span><br><span class="line">Template: mysql-server/root_password</span><br><span class="line">Value: <span class="number">123456</span></span><br><span class="line">Owners: mariadb-server-<span class="number">5.5</span></span><br><span class="line">Flags: seen</span><br><span class="line"></span><br><span class="line">Name: mysql-server/root_password_again</span><br><span class="line">Template: mysql-server/root_password_again</span><br><span class="line">Value: <span class="number">123456</span></span><br><span class="line">Owners: mariadb-server-<span class="number">5.5</span></span><br><span class="line">Flags: seen</span><br></pre></td></tr></table></figure></p>
<p>然后执行安装的操作：<br>修改环境变量（这一步不做也没看到有问题）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> DEBIAN_FRONTEND=noninteractive</span><br></pre></td></tr></table></figure></p>
<h3 id="3、安装相应的包">3、安装相应的包</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dpkg -i mariadb-server-<span class="number">5.5</span>_5.<span class="number">5.46</span>-<span class="number">1</span>ubuntu0.<span class="number">14.04</span>.<span class="number">2</span>_amd64.deb</span><br></pre></td></tr></table></figure>
<p>以上即为ubuntu下的deb包的自动应答的处理</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在ubuntu下安装软件过程中可能会出现需要你输入密码或者其他的一些交互类的操作，这样在脚本安装的时候就可能出现阻断，这个在ubuntu里面已经考虑到了这个情况，以前我在安装这个的时候，通过的是脚本传递参数的方式，这里介绍的是原生的控制方式，这个方式更好</p>
<p>以安装mariadb-server-5.5为例</p>
<h3 id="1、查询需要应答的问题">1、查询需要应答的问题</h3><p>首先通过命令查询这个软件需要问答什么问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@mytest:/var/cache/apt/archives<span class="comment"># debconf-show mariadb-server-5.5 </span></span><br><span class="line">* mysql-server/root_password: (password omitted)</span><br><span class="line">* mysql-server/root_password_again: (password omitted)</span><br><span class="line">  mysql-server/password_mismatch:</span><br><span class="line">  mysql-server/error_setting_password:</span><br><span class="line">* mariadb-server/oneway_migration: <span class="literal">true</span></span><br><span class="line">  mysql-server-<span class="number">5.5</span>/nis_warning:</span><br><span class="line">  mysql-server-<span class="number">5.5</span>/postrm_remove_databases: <span class="literal">false</span></span><br><span class="line">  mariadb-server-<span class="number">5.5</span>/really_downgrade: <span class="literal">false</span></span><br><span class="line">  mysql-server/no_upgrade_when_using_ndb:</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ubuntu" scheme="http://www.zphj1987.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[磨磨谈 ceph-users 邮件列表之Vol-1-Issue-1(完结)]]></title>
    <link href="http://www.zphj1987.com/2016/01/25/%E7%A3%A8%E7%A3%A8%E8%B0%88-ceph-users-%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8%E4%B9%8BVol-1-Issue-1/"/>
    <id>http://www.zphj1987.com/2016/01/25/磨磨谈-ceph-users-邮件列表之Vol-1-Issue-1/</id>
    <published>2016-01-24T16:06:58.000Z</published>
    <updated>2016-02-24T13:46:45.097Z</updated>
    <content type="html"><![CDATA[<p>关于归档方式本来开始准备采用邮件列表的vol和issue的方式，但是因为issue里面是以天为单位，而里面的问题可能是重复的，这个不好进行规整了，那么在我这里，准备以Thread的方式，每个Issue里面包含5个问题，每10个issue为一个vol，进行规整，以2016年开始为起点，前后去收集，会记录问题提问者和发起问题的时间</p>
<h2 id="前言：">前言：</h2><p>一个技术的邮件列表其实是个大宝藏，看你怎么去挖掘它了，里面有太多的灵光一现，太多的大牛的引导，而我等也只能在某些出现问题的时候，去搜索的时候，才发现这些问题是很多人遇到过的，最近看到一个技术人说的一句话深有同感,意思是说，很多时候我们花精力重复解决问题</p>
<ul>
<li>一流的公司在从整体上去解决一批的问题</li>
<li>二流的公司在一次又一次解决相同的问题</li>
</ul>
<p>与其等待别人去整理，还不如自己去做这个事情，没有压力，没有人催促，最后自己还能有所收获，何乐而不为，这个系列我会去将邮件列表的里面的内容进行整理，提出我个人的处理办法或者方式，或者总结一下里面别人的经验，这个系列将以现在的时间线往前和往后的方式进行整理，这个会是一个很长的过程，希望尽量的获取更多的东西，文档格式尽量保持一致</p>
<a id="more"></a>
<h3 id="一、今天的_Topics:">一、今天的 Topics:</h3><ul>
<li>journal encryption with dmcrypt （Date: Fri, 22 Jan 2016 20:35:46 +0000）</li>
<li>CephFS</li>
<li>ceph fuse closing stale session while still operable</li>
<li>inkscope version 1.3.1  </li>
<li>Ceph monitors 100% full filesystem, refusing start</li>
</ul>
<hr>
<h3 id="二、涉及问题：">二、涉及问题：</h3><ul>
<li>osd分区加密</li>
<li>文件系统加密挂载问题</li>
<li>ceph-fuse写入僵住的问题</li>
<li>inkscope发布了新版本</li>
<li>monitor的文件系统磁盘满了无法启动的问题</li>
</ul>
<h3 id="三、问题解析">三、问题解析</h3><h4 id="问题一：">问题一：</h4><p>journal encryption with dmcrypt （Reno Rainz）</p>
<h5 id="问题原文：">问题原文：</h5><p>I’m trying to setup a cluster with encryption on osd data and journal.<br>To do  that I use ceph-deploy with this 2 options —dmcrypt<br>—dmcrypt-key-dir on /dev/sdc disk.<br>……</p>
<h5 id="分析：">分析：</h5><p>问题的提出者试图在部署osd的时候使用 encryption 对 osd 进行加密,在用 ceph-deploy 的时候，部署的时候出现了失败</p>
<h5 id="总结：">总结：</h5><p>这个地方是因为 ceph-deploy 在进行 activate 操作的时候，把这个加密分区当做了 crypto_LUKS 分区格式进行了 mount 操作，这个肯定是不能成功的，因为这个加密盘是需要进行映射操作的，这里缺少了这个操作，不清楚是需要加其他的参数还是怎样，这个地方可以通过其他方式进行处理</p>
<p>在进行 ceph-deploy osd prepare 操作的时候，可以查看看到有一行这个，这个中间的 f6244401-c848-42d1-9096-9a3ee5a136e9 即为 osd 的 fsid<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Running <span class="built_in">command</span>: /usr/sbin/cryptsetup --batch-mode --key-file /root/keydir/f6244401-c848-<span class="number">42</span>d1-<span class="number">9096</span>-<span class="number">9</span>a3ee5a136e9.luks.key luksFormat /dev/sdd1</span><br></pre></td></tr></table></figure></p>
<p>等待osd prepare 操作做完了以后，就进行下面的操作</p>
<p>1、进行磁盘的映射<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup --key-file /root/keydir/f6244401-c848-<span class="number">42</span>d1-<span class="number">9096</span>-<span class="number">9</span>a3ee5a136e9.luks.key luksOpen /dev/sdd1 f6244401-c848-<span class="number">42</span>d1-<span class="number">9096</span>-<span class="number">9</span>a3ee5a136e9</span><br></pre></td></tr></table></figure></p>
<p>2、进行osd的激活<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd activate /dev/mapper/f6244401-c848-<span class="number">42</span>d1-<span class="number">9096</span>-<span class="number">9</span>a3ee5a136e9</span><br></pre></td></tr></table></figure></p>
<p>这样就可以了</p>
<p>另外：<br>取消映射的操作是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup remove f6244401-c848-<span class="number">42</span>d1-<span class="number">9096</span>-<span class="number">9</span>a3ee5a136e9</span><br></pre></td></tr></table></figure></p>
<h4 id="问题二：">问题二：</h4><p>ceph fuse closing stale session while still   operable （Oliver Dzombic）</p>
<h5 id="问题原文：-1">问题原文：</h5><p>Hi,<br>i am testing on centos 6 x64 minimal install.<br>i am mounting successfully:<br>ceph-fuse -m 10.0.0.1:6789,10.0.0.2:6789,10.0.0.3:6789,10.0.0.4:6789<br>/ceph-storage/<br>……</p>
<h5 id="分析：-1">分析：</h5><p>问题的提出者在使用ceph-fuse去挂载集群的时候，写入一个大文件的时候出现无法写入的问题，在mds的日志当中可以看到<br>closing stale session client.21176728 10.0.0.91:0/1635 after 301.302291 的日志信息</p>
<p>从日志检查过程看<br>ceph -s 出现了  62 requests are blocked &gt; 32 sec<br>问题提出者在认证的时候，出现了语法错误 ceph auth list showed 可以检查，后经修改，还是问题一样</p>
<p>查看客户端的请求：<br>ceph daemon /var/run/ceph/ceph-client.admin.asok objecter_requests<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"ops"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"tid"</span>: <span class="number">12</span>,</span><br><span class="line">            <span class="string">"pg"</span>: <span class="string">"6.7230bd94"</span>,</span><br><span class="line">            <span class="string">"osd"</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">"object_id"</span>: <span class="string">"10000000017.00000004"</span>,</span><br><span class="line">            <span class="string">"object_locator"</span>: <span class="string">"@6"</span>,</span><br><span class="line">            <span class="string">"target_object_id"</span>: <span class="string">"10000000017.00000004"</span>,</span><br><span class="line">            <span class="string">"target_object_locator"</span>: <span class="string">"@6"</span>,</span><br><span class="line">            <span class="string">"paused"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"used_replica"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"precalc_pgid"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"last_sent"</span>: <span class="string">"2016-01-22 23:11:28.788800"</span>,</span><br><span class="line">            <span class="string">"attempts"</span>: <span class="number">95</span>,</span><br><span class="line">            <span class="string">"snapid"</span>: <span class="string">"head"</span>,</span><br><span class="line">            <span class="string">"snap_context"</span>: <span class="string">"1=[]"</span>,</span><br><span class="line">            <span class="string">"mtime"</span>: <span class="string">"2016-01-21 23:41:18.001327"</span>,</span><br><span class="line">            <span class="string">"osd_ops"</span>: [</span><br><span class="line">                <span class="string">"write 0~4194304 [5@0]"</span></span><br><span class="line">            ]</span><br></pre></td></tr></table></figure></p>
<p>其中<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">"attempts"</span>: <span class="number">95</span>,</span><br></pre></td></tr></table></figure></p>
<p>这个可以说明请求了95次还没有回应，从这个分析，应该是问题提出者的环境中的某个osd出现了问题，阻塞了请求</p>
<h5 id="总结：-1">总结：</h5><p>在无法写入的时候，可以查看下客户端的sock去查看哪个请求被阻塞了，然后去排查对应的osd即可</p>
<h4 id="问题三：">问题三：</h4><p>CephFS（James Gallagher）</p>
<h5 id="问题原文">问题原文</h5><p>Hi,<br>I’m looking to implement the CephFS on my Firefly release (v0.80) with an XFS native file system, but so far I’m having some difficulties. After following the ceph/qsg and creating a storage cluster, I have the following topology<br>……</p>
<h5 id="分析：-2">分析：</h5><p>问题提出者在配置了 auth 后，在客户端进行cephfs 挂载的时候，报了文件系统的错误，这个原因是问题提出者没有弄清楚 auth 的格式，而用了主机名去替代了用户名称</p>
<p>这个地方是在server端去创建用户<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth get-or-create client.zp mon <span class="string">'allow *'</span> mds <span class="string">'allow *'</span> osd <span class="string">'allow *'</span></span><br></pre></td></tr></table></figure></p>
<p>找到认证的key<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth list</span><br></pre></td></tr></table></figure></p>
<p>然后在客户端挂载的格式为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t ceph <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>:/ /mnt -o name=zp,secret=&#123;secretkey&#125;</span><br></pre></td></tr></table></figure></p>
<p>这样就完成了认证模式下的挂载</p>
<h5 id="总结:">总结:</h5><p>在ceph的一些操作中，需要弄清楚里面的操作的格式问题，不然会出现一些很奇怪的问题</p>
<h4 id="问题四：">问题四：</h4><p>inkscope version 1.3.1 </p>
<h5 id="问题原文-1">问题原文</h5><p>Hi,<br> We are  glad to announce the  version 1.3.1 of inkscope.<br> you have a  description of this version at <a href="http://inkscope.blogspot.fr/" target="_blank" rel="external">http://inkscope.blogspot.fr/</a>,<br> the github projet: <a href="https://github.com/inkscope/inkscope-packaging" target="_blank" rel="external">https://github.com/inkscope/inkscope-packaging</a><br>  This version has been tested with Firefly, Hammer et Infernalis.<br>……</p>
<h5 id="分析：-3">分析：</h5><p>这个是inkscope的作者在发布了新版本后发送到了ceph的邮件列表，增加了一些新的功能，这个地方将单独用一篇文章介绍这个地方的安装<br>链接如下：</p>
<h5 id="总结">总结</h5><p>邮件列表不光是提问题的地方，还有一些优秀的资源的发布的作者也可以发布上去</p>
<h4 id="问题五：">问题五：</h4><p>Ceph monitors 100% full filesystem, refusing start</p>
<h5 id="问题原文-2">问题原文</h5><p>I have an issue with a (not in production!) Ceph cluster which I’m<br>trying to resolve.</p>
<h5 id="分析：-4">分析：</h5><p>这是作者在使用多个mon的时候，数据出现了磁盘满的情况，然后重启mon进行压缩的时候，发现这个到了mon的最小空间阀值无法启动，然后就无法压缩，这个问题，还是因为对硬件的不重视，对软件的要求不清楚造成的</p>
<h5 id="解决办法：">解决办法：</h5><p>mon的磁盘空间加大，这个在PB级别的集群中更需要重视这个问题，特别是在集群频繁的读写，或者pg变化比较多，osd变化比较多的情况下，这个数据量将是很大的，因为里面是用了leveldb的数据库，并且多个mon之间是需要同步的数据的，然后各自再做compact的操作，所以建议如下：<br>1、mon的数据分区需要是ssd的，加快数据的读写速度<br>2、mon的数据分区要100G以上，建议是150G，mon数据大概在80G左右后不会再大量的增长<br>3、在mon的参数中加入启动压缩的参数 mon_compact_on_start = false 和 mon_compact_on_bootstrap = false<br>4、尽量不要做在线的compact，这个是一个锁死的过程，此时mon会停止响应，可以采取重启的方式<br>5、mon的分区中dd 一个4G 左右的大文件，防止真的出现写满的情况下，再去重启进程的时候，好有空间可以释放<br>如果能按上面的几个操作去配置集群，关于mon的磁盘满的问题基本可以避免或者解决</p>
<h5 id="总结：-2">总结：</h5><p>关键的地方不要省配置，准备的越多，出问题的概率越小</p>
<hr>
<p>文档最后更新时间：<br>2016年01月28日 </p>
<p>欢迎打赏 and tell me</p>
<div style="text-align:center" markdown="1"><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/payforzp.png" alt="pay"><br></div>
]]></content>
    <summary type="html">
    <![CDATA[<p>关于归档方式本来开始准备采用邮件列表的vol和issue的方式，但是因为issue里面是以天为单位，而里面的问题可能是重复的，这个不好进行规整了，那么在我这里，准备以Thread的方式，每个Issue里面包含5个问题，每10个issue为一个vol，进行规整，以2016年开始为起点，前后去收集，会记录问题提问者和发起问题的时间</p>
<h2 id="前言：">前言：</h2><p>一个技术的邮件列表其实是个大宝藏，看你怎么去挖掘它了，里面有太多的灵光一现，太多的大牛的引导，而我等也只能在某些出现问题的时候，去搜索的时候，才发现这些问题是很多人遇到过的，最近看到一个技术人说的一句话深有同感,意思是说，很多时候我们花精力重复解决问题</p>
<ul>
<li>一流的公司在从整体上去解决一批的问题</li>
<li>二流的公司在一次又一次解决相同的问题</li>
</ul>
<p>与其等待别人去整理，还不如自己去做这个事情，没有压力，没有人催促，最后自己还能有所收获，何乐而不为，这个系列我会去将邮件列表的里面的内容进行整理，提出我个人的处理办法或者方式，或者总结一下里面别人的经验，这个系列将以现在的时间线往前和往后的方式进行整理，这个会是一个很长的过程，希望尽量的获取更多的东西，文档格式尽量保持一致</p>]]>
    
    </summary>
    
      <category term="momotan" scheme="http://www.zphj1987.com/tags/momotan/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于ceph的rbd属性设置研究]]></title>
    <link href="http://www.zphj1987.com/2016/01/20/%E5%85%B3%E4%BA%8Eceph%E7%9A%84rbd%E5%B1%9E%E6%80%A7%E8%AE%BE%E7%BD%AE%E7%A0%94%E7%A9%B6/"/>
    <id>http://www.zphj1987.com/2016/01/20/关于ceph的rbd属性设置研究/</id>
    <published>2016-01-20T14:57:53.000Z</published>
    <updated>2016-02-24T14:07:27.597Z</updated>
    <content type="html"><![CDATA[<p>一个rbd在存储到后台以后,在后台是对象存储的，对象</p>
<p>[root@lab8106 rbdre]# rados -p rbd  listomapvals  rbd_directory<br>id_399a456aa808<br>value (6 bytes) :<br>0000 : 02 00 00 00 7a 70                               : ….zp</p>
<p>name_zp<br>value (16 bytes) :<br>0000 : 0c 00 00 00 33 39 39 61 34 35 36 61 61 38 30 38 : ….399a456aa808</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一个rbd在存储到后台以后,在后台是对象存储的，对象</p>
<p>[root@lab8106 rbdre]# rados -p rbd  listomapvals  rbd_directory<br>id_399a456aa808<br>value (6 bytes) :]]>
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph单机多mon的实现]]></title>
    <link href="http://www.zphj1987.com/2016/01/14/ceph%E5%8D%95%E6%9C%BA%E5%A4%9Amon%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
    <id>http://www.zphj1987.com/2016/01/14/ceph单机多mon的实现/</id>
    <published>2016-01-14T09:03:56.000Z</published>
    <updated>2016-01-14T09:11:03.572Z</updated>
    <content type="html"><![CDATA[<p>ceph默认情况下是以主机名来作为mon的识别的，所以这个情况下用部署工具是无法创建多个mon的，这个地方使用手动的方式可以很方便的创建多个mon</p>
<h3 id="1、创建mon的数据存储目录">1、创建mon的数据存储目录</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /var/lib/ceph/mon/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="2、获取当前的monmap">2、获取当前的monmap</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mon getmap -o /tmp/monmap</span><br></pre></td></tr></table></figure>
<h3 id="3、根据当前的monmap生成mon的数据">3、根据当前的monmap生成mon的数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-mon -i <span class="number">1</span>  --mkfs --monmap /tmp/monmap</span><br></pre></td></tr></table></figure>
<h3 id="4、启动进程（后面指定端口）">4、启动进程（后面指定端口）</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-mon -i <span class="number">1</span>  --public-addr  <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6791</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>现在mon就加进去了</p>
<p>然后去写配置文件相关的信息即可，操作还是很便捷的，这个地方可以防止单mon的情况下的数据盘的损坏的情况，增加一点安全系数，当然最好是多主机的mon</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>ceph默认情况下是以主机名来作为mon的识别的，所以这个情况下用部署工具是无法创建多个mon的，这个地方使用手动的方式可以很方便的创建多个mon</p>
<h3 id="1、创建mon的数据存储目录">1、创建mon的数据存储目录</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /var/lib/ceph/mon/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="2、获取当前的monmap">2、获取当前的monmap</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mon getmap -o /tmp/monmap</span><br></pre></td></tr></table></figure>
<h3 id="3、根据当前的monmap生成mon的数据">3、根据当前的monmap生成mon的数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-mon -i <span class="number">1</span>  --mkfs --monmap /tmp/monmap</span><br></pre></td></tr></table></figure>
<h3 id="4、启动进程（后面指定端口）">4、启动进程（后面指定端口）</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-mon -i <span class="number">1</span>  --public-addr  <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6791</span></span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph使用memdisk做journal]]></title>
    <link href="http://www.zphj1987.com/2016/01/14/ceph%E4%BD%BF%E7%94%A8memdisk%E5%81%9Ajournal/"/>
    <id>http://www.zphj1987.com/2016/01/14/ceph使用memdisk做journal/</id>
    <published>2016-01-14T02:00:59.000Z</published>
    <updated>2016-01-22T16:34:27.000Z</updated>
    <content type="html"><![CDATA[<p>记得在很久很久以前，ceph当时的版本是有提供使用内存做journal的配置的，当时是使用的tmpfs，但是现在的版本在搜资料的时候，发现关于这个的没怎么找到资料，邮件列表里面有人有提到怎么做，看了下大致的原理，然后还是自己来实践一次</p>
<h3 id="预备知识：">预备知识：</h3><p>首先需要知道的是什么是内存盘，内存盘就是划分了一个内存空间来当磁盘使用来进行加速的，这个在某些操作系统里面会把/tmp/分区挂载到tmpfs下，来达到加速的目的，这样就是重启后，会清空/tmp的内容，centos7 默认的分区方式也使用了tmpfs来加速，df -h可以看下那个tmpfs就是内存盘了</p>
<p>本文使用的不是tmpfs，这个是因为tmpfs不是我们常见意义上的那种文件系统，它不能格式化，ceph 在进行日志创建的时候会去检查journal 所在分区的 uuid， 而tmpfs在检测的时候 会返回一个全0的字符串，这个在校验的时候显示的无效的，所以也就部署起来有问题，下面开始介绍我的做法，这个里面做法很多，步骤也可以自己去变化，这里只是提供了我的一种思路</p>
<p>我使用的是ramdisk，关于怎么做ramdisk这个也研究了一下，因为篇幅有点长并且属于预备步骤，请参考我的另外一篇文章：</p>
<p><a href="http://www.zphj1987.com/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/" title="centos7下做内存盘的方法" target="_blank" rel="external">centos7下做内存盘的方法</a></p>
<a id="more"></a>
<h3 id="测试环境：">测试环境：</h3><p>单机，四块SAS的OSD，日志为5G（内存盘大小为6G），副本 2， osd分组</p>
<p>说明：因为这里只去研究这个内存盘journal的实现，以及性能的差别，其他的组合方案需要自己去配置，所以单机的环境已经可以完成这个</p>
<h3 id="1、准备journal的内存盘">1、准备journal的内存盘</h3><h4 id="检查内存盘大小">检查内存盘大小</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># fdisk -l /dev/ram0</span></span><br><span class="line"></span><br><span class="line">Disk /dev/ram0: <span class="number">6797</span> MB, <span class="number">6797721600</span> bytes, <span class="number">13276800</span> sectors</span><br><span class="line">Units = sectors of <span class="number">1</span> * <span class="number">512</span> = <span class="number">512</span> bytes</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span> bytes / <span class="number">512</span> bytes</span><br><span class="line">I/O size (minimum/optimal): <span class="number">512</span> bytes / <span class="number">512</span> bytes</span><br></pre></td></tr></table></figure>
<p>我的大小为6G</p>
<h4 id="格式化内存盘，并且挂载">格式化内存盘，并且挂载</h4><h5 id="创建挂载目录（有多少osd建几个）">创建挂载目录（有多少osd建几个）</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mkdir -p /var/lib/ceph/mem/ceph-0</span></span><br></pre></td></tr></table></figure>
<h5 id="格式化memdisk(需要几个格式化几个)">格式化memdisk(需要几个格式化几个)</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mkfs.xfs /dev/ram0  -f</span></span><br></pre></td></tr></table></figure>
<h5 id="挂载内存盘">挂载内存盘</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mount /dev/ram0 /var/lib/ceph/mem/ceph-0/</span></span><br></pre></td></tr></table></figure>
<h5 id="挂载完了后的效果如下：">挂载完了后的效果如下：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda2        <span class="number">50</span>G  <span class="number">9.7</span>G   <span class="number">41</span>G  <span class="number">20</span>% /</span><br><span class="line">devtmpfs         <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs            <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /dev/shm</span><br><span class="line">tmpfs            <span class="number">24</span>G   <span class="number">17</span>M   <span class="number">24</span>G   <span class="number">1</span>% /run</span><br><span class="line">tmpfs            <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1       <span class="number">283</span>M   <span class="number">94</span>M  <span class="number">190</span>M  <span class="number">33</span>% /boot</span><br><span class="line">/dev/ram0       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">0</span></span><br><span class="line">/dev/ram1       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">1</span></span><br><span class="line">/dev/ram2       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">2</span></span><br><span class="line">/dev/ram3       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">3</span></span><br></pre></td></tr></table></figure>
<h3 id="2、准备ceph的环境">2、准备ceph的环境</h3><p>修改deploy的ceph.conf文件，在部署前修改好<br>单机环境添加下面的三个<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_crush_chooseleaf_<span class="built_in">type</span> = <span class="number">0</span></span><br><span class="line">osd_pool_default_size = <span class="number">2</span></span><br><span class="line">osd_journal = /var/lib/ceph/mem/<span class="variable">$cluster</span>-<span class="variable">$id</span>/journal</span><br></pre></td></tr></table></figure></p>
<p>意思就不在这里介绍了</p>
<h4 id="创建mon">创建mon</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy mon create lab8106</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy gatherkeys lab8106</span></span><br></pre></td></tr></table></figure>
<h4 id="创建osd">创建osd</h4><p>[root@lab8106 ceph]# ceph-deploy osd prepare lab8106:/dev/sdb1:/var/lib/ceph/mem/ceph-0/journal<br>[root@lab8106 ceph]# ceph-deploy osd activate lab8106:/dev/sdb1<br>部署完这个检查下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># df -h</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">……</span><br><span class="line">/dev/ram0       <span class="number">6.4</span>G  <span class="number">5.1</span>G  <span class="number">1.3</span>G  <span class="number">80</span>% /var/lib/ceph/mem/ceph-<span class="number">0</span></span><br><span class="line">/dev/ram1       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">1</span></span><br><span class="line">/dev/ram2       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">2</span></span><br><span class="line">/dev/ram3       <span class="number">6.4</span>G   <span class="number">33</span>M  <span class="number">6.3</span>G   <span class="number">1</span>% /var/lib/ceph/mem/ceph-<span class="number">3</span></span><br><span class="line">/dev/sdb1       <span class="number">280</span>G   <span class="number">34</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到内存盘分区内已经生成可一个5G的journal文件<br><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">[root<span class="annotation">@lab</span>8106 ceph]# ll <span class="regexp">/var/</span>lib<span class="regexp">/ceph/</span>osd/ceph-<span class="number">0</span></span><br><span class="line">total <span class="number">40</span></span><br><span class="line">……</span><br><span class="line">lrwxrwxrwx  <span class="number">1</span> root root   <span class="number">32</span> Jan <span class="number">14</span> <span class="number">10</span>:<span class="number">28</span> journal -&gt; <span class="regexp">/var/</span>lib<span class="regexp">/ceph/</span>mem<span class="regexp">/ceph-0/</span>journal</span><br></pre></td></tr></table></figure></p>
<p>可以看到osd分区的也是链接到了内存盘，环境没问题</p>
<h4 id="继续部署生效的三个osd">继续部署生效的三个osd</h4><p>部署完再次检查环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/ram0       <span class="number">6.4</span>G  <span class="number">5.1</span>G  <span class="number">1.3</span>G  <span class="number">80</span>% /var/lib/ceph/mem/ceph-<span class="number">0</span></span><br><span class="line">/dev/ram1       <span class="number">6.4</span>G  <span class="number">5.1</span>G  <span class="number">1.3</span>G  <span class="number">80</span>% /var/lib/ceph/mem/ceph-<span class="number">1</span></span><br><span class="line">/dev/ram2       <span class="number">6.4</span>G  <span class="number">5.1</span>G  <span class="number">1.3</span>G  <span class="number">80</span>% /var/lib/ceph/mem/ceph-<span class="number">2</span></span><br><span class="line">/dev/ram3       <span class="number">6.4</span>G  <span class="number">5.1</span>G  <span class="number">1.3</span>G  <span class="number">80</span>% /var/lib/ceph/mem/ceph-<span class="number">3</span></span><br><span class="line">/dev/sdb1       <span class="number">280</span>G   <span class="number">34</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line">/dev/sdc1       <span class="number">280</span>G   <span class="number">34</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">/dev/sdd1       <span class="number">280</span>G   <span class="number">34</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">2</span></span><br><span class="line">/dev/sde1       <span class="number">280</span>G   <span class="number">33</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>都挂载正确<br>检查集群的状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">68735617</span>-<span class="number">2</span>d30-<span class="number">4</span>a81-<span class="number">9865</span>-aeab3ea85e6e</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">2</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e21: <span class="number">4</span> osds: <span class="number">4</span> up, <span class="number">4</span> <span class="keyword">in</span></span><br><span class="line">      pgmap v35: <span class="number">192</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">136</span> MB used, <span class="number">1116</span> GB / <span class="number">1117</span> GB avail</span><br><span class="line">                 <span class="number">192</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>环境部署完毕</p>
<h3 id="开始测试">开始测试</h3><p>测试一：采用内存盘journal的方式<br>使用radosbench进行测试（采取默认的写，并且不删除的测试，尽量把内存写满，未进行任何调优）<br><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">[root<span class="annotation">@lab</span>8106 ceph]# rados bench -p rbd <span class="number">120</span> write --no-cleanup --run-name testmemdisk</span><br><span class="line">Total time <span class="string">run:</span>         <span class="number">120.568031</span></span><br><span class="line">Total writes <span class="string">made:</span>      <span class="number">5857</span></span><br><span class="line">Write <span class="string">size:</span>             <span class="number">4194304</span></span><br><span class="line">Bandwidth (MB/sec):     <span class="number">194.314</span> </span><br><span class="line"></span><br><span class="line">Stddev <span class="string">Bandwidth:</span>       <span class="number">144.18</span></span><br><span class="line">Max bandwidth (MB/sec): <span class="number">504</span></span><br><span class="line">Min bandwidth (MB/sec): <span class="number">0</span></span><br><span class="line">Average <span class="string">Latency:</span>        <span class="number">0.329322</span></span><br><span class="line">Stddev <span class="string">Latency:</span>         <span class="number">0.48777</span></span><br><span class="line">Max <span class="string">latency:</span>            <span class="number">3.01612</span></span><br><span class="line">Min <span class="string">latency:</span>            <span class="number">0.0377235</span></span><br></pre></td></tr></table></figure></p>
<p>测试二：采用默认的磁盘journal的方式，环境恢复要原始的情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados bench -p rbd 120 write --no-cleanup --run-name testmemdisk</span></span><br><span class="line">Total time run:         <span class="number">120.613851</span></span><br><span class="line">Total writes made:      <span class="number">3404</span></span><br><span class="line">Write size:             <span class="number">4194304</span></span><br><span class="line">Bandwidth (MB/sec):     <span class="number">112.889</span> </span><br><span class="line"></span><br><span class="line">Stddev Bandwidth:       <span class="number">26.3641</span></span><br><span class="line">Max bandwidth (MB/sec): <span class="number">160</span></span><br><span class="line">M<span class="keyword">in</span> bandwidth (MB/sec): <span class="number">0</span></span><br><span class="line">Average Latency:        <span class="number">0.566656</span></span><br><span class="line">Stddev Latency:         <span class="number">0.305038</span></span><br><span class="line">Max latency:            <span class="number">2.00623</span></span><br><span class="line">M<span class="keyword">in</span> latency:            <span class="number">0.105026</span></span><br></pre></td></tr></table></figure></p>
<p>测试的结果如上，上表格也许看的更直观，正好之前在找一个表格插件，现在用用</p>
<h3 id="内存盘journal与磁盘journal性能对比">内存盘journal与磁盘journal性能对比</h3><p></p><p><link rel="stylesheet" href="http://7xo9we.com1.z0.glb.clouddn.com/compareninja%2Fskin.css" type="text/css"></p><div id="tableWrapper" style="width: 100%; "><table id="vsTable"><tbody><tr><td class="cat title" style="width: 20%; "></td><td class="title" style="width: 40%; "><div class="">内存盘journal</div></td><td class="title" style="width: 40%; "><div class="">磁盘journal</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">测试时间(s)</div></td><td style="width: 40%; "><div class="">120.568031</div></td><td style="width: 40%; "><div class="">120.613851</div></td></tr><tr><td class="cat" style="width: 20%; "><div class="">写数据块数</div></td><td style="width: 40%; "><div class="">5857</div></td><td style="width: 40%; "><div class="">3404</div></td></tr><tr><td class="cat" style="width: 20%; "><div class="">总共写入数据(MB)</div></td><td style="width: 40%; "><div class="">23428</div></td><td style="width: 40%; "><div class="">13616</div></td></tr><tr><td class="cat" style="width: 20%; "><div class="">数据块大小</div></td><td style="width: 40%; "><div class="">4194304</div></td><td style="width: 40%; "><div class="">4194304</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">写带宽(MB/sec)</div></td><td style="width: 40%; "><div class="">194.314</div></td><td style="width: 40%; "><div class="">112.889</div></td></tr><tr><td class="cat" style="width: 20%; "><div class="">带宽标准偏差</div></td><td style="width: 40%; "><div class="">144.18</div></td><td style="width: 40%; "><div class="">26.3641</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">最大带宽(MB/sec)</div></td><td style="width: 40%; "><div class="">504</div></td><td style="width: 40%; "><div class="">160</div></td></tr><tr><td class="cat" style="width: 20%; "><div class="">平均延时</div></td><td style="width: 40%; "><div class="">0.32932</div></td><td style="width: 40%; "><div class="">0.566656</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">延时偏差</div></td><td style="width: 40%; "><div class="">0.48777</div></td><td style="width: 40%; "><div class="">0.305038</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">最大延时</div></td><td style="width: 40%; "><div class="">3.01612</div></td><td style="width: 40%; "><div class="">2.00623</div></td></tr><tr class="second"><td class="cat" style="width: 20%; "><div class="">最小延时</div></td><td style="width: 40%; "><div class="">0.0377235</div></td><td style="width: 40%; "><div class="">0.105026</div></td></tr></tbody></table></div><p></p>
<p>可以看到相关数据，光写带宽就提升了接近一倍，这个是因为，在磁盘journal情况下，写入journal的同时还有filestore的数据写入，相当于同时有两个写入在磁盘上，磁盘的性能自然只有一半了</p>
<p>以上就是关于journal的内存盘实现，这里面还会面临着其他的问题</p>
<ul>
<li>机器内存的占用问题</li>
<li>断电后的处理</li>
<li>同时断电是否会搞坏pg状态</li>
<li>搞坏的情况是否能恢复</li>
</ul>
<p>如果解决了这些问题，这个不失为一种性能提升的方案，毕竟内存的成本和速度是ssd的磁盘和单独磁盘journal不能比的，journal本身也是一种循环的写入的空间</p>
<hr>
<p>教程的方式暂时停止了，毕竟推广这块还是有问题<br>结尾还是来个支持的链接，如果你觉得文章写得好，欢迎打赏，如果有新文章我会第一时间推送给支持我的朋友</p>
<p><img src="http://7xo9we.com1.z0.glb.clouddn.com/zhifubao.png" alt=""></p>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- myweb -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-3305801963238415" data-ad-slot="9673687470" data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>]]></content>
    <summary type="html">
    <![CDATA[<p>记得在很久很久以前，ceph当时的版本是有提供使用内存做journal的配置的，当时是使用的tmpfs，但是现在的版本在搜资料的时候，发现关于这个的没怎么找到资料，邮件列表里面有人有提到怎么做，看了下大致的原理，然后还是自己来实践一次</p>
<h3 id="预备知识：">预备知识：</h3><p>首先需要知道的是什么是内存盘，内存盘就是划分了一个内存空间来当磁盘使用来进行加速的，这个在某些操作系统里面会把/tmp/分区挂载到tmpfs下，来达到加速的目的，这样就是重启后，会清空/tmp的内容，centos7 默认的分区方式也使用了tmpfs来加速，df -h可以看下那个tmpfs就是内存盘了</p>
<p>本文使用的不是tmpfs，这个是因为tmpfs不是我们常见意义上的那种文件系统，它不能格式化，ceph 在进行日志创建的时候会去检查journal 所在分区的 uuid， 而tmpfs在检测的时候 会返回一个全0的字符串，这个在校验的时候显示的无效的，所以也就部署起来有问题，下面开始介绍我的做法，这个里面做法很多，步骤也可以自己去变化，这里只是提供了我的一种思路</p>
<p>我使用的是ramdisk，关于怎么做ramdisk这个也研究了一下，因为篇幅有点长并且属于预备步骤，请参考我的另外一篇文章：</p>
<p><a href="http://www.zphj1987.com/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/" title="centos7下做内存盘的方法">centos7下做内存盘的方法</a></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[centos7下做内存盘的方法]]></title>
    <link href="http://www.zphj1987.com/2016/01/14/centos7%E4%B8%8B%E5%81%9A%E5%86%85%E5%AD%98%E7%9B%98%E7%9A%84%E6%96%B9%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/01/14/centos7下做内存盘的方法/</id>
    <published>2016-01-14T01:07:28.000Z</published>
    <updated>2016-01-14T01:58:56.316Z</updated>
    <content type="html"><![CDATA[<p>在找这个资料的时候，基本没几个能用的或者过时了的，或者是换了概念，做的不是需要的那种盘，只有少数文章有提到关键部分应该怎么去操作，现在还是自己总结一下</p>
<h3 id="内存盘tmpfs和ramdisk的区别">内存盘tmpfs和ramdisk的区别</h3><p>这个在网上的很多资料里面都有提到，很多文章去写怎么做ramdisk的时候，都是去做的tmpfs，两者虽然都是使用的内存来存储东西，但是是完全有区别的</p>
<ul>
<li>tmpfs这个只需要mount挂载就可以分配一个目录使用内存了，只是一个目录</li>
<li>ramdisk这个是真的分配一个空间，这个分区是可以格式化的（这个格式化是关键）</li>
<li>tmpfs卸载再挂载数据会消失，randisk卸载再挂载数据还在</li>
<li>二者共同点是，系统重启后，里面的东西会消失</li>
</ul>
<blockquote>
<p>本文章主要是讲怎么去做ramdisk</p>
</blockquote>
<a id="more"></a>
<p>ramdisk是依赖于内核模块brd的，首先可以查看下这个模块的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># modinfo brd</span></span><br><span class="line">filename:       /lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64/kernel/drivers/block/brd.ko</span><br><span class="line"><span class="built_in">alias</span>:          rd</span><br><span class="line"><span class="built_in">alias</span>:          block-major-<span class="number">1</span>-*</span><br><span class="line">license:        GPL</span><br><span class="line">rhelversion:    <span class="number">7.1</span></span><br><span class="line">srcversion:     F38BA5B60FC8B94786C7907</span><br><span class="line">depends:        </span><br><span class="line">intree:         Y</span><br><span class="line">vermagic:       <span class="number">3.10</span>.<span class="number">0</span> SMP mod_unload modversions </span><br><span class="line">parm:           rd_nr:Maximum number of brd devices (int)</span><br><span class="line">parm:           rd_size:Size of each RAM disk <span class="keyword">in</span> kbytes. (int)</span><br><span class="line">parm:           max_part:Maximum number of partitions per RAM disk (int)</span><br></pre></td></tr></table></figure></p>
<p>默认是不加载的，需要加载这个模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># modprobe brd</span></span><br></pre></td></tr></table></figure></p>
<p>加载模块后就会生成下面的的盘符路径，这个就是内存盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ll /dev/ram*</span></span><br><span class="line">brw-rw---- <span class="number">1</span> root disk <span class="number">1</span>, <span class="number">0</span> Jan <span class="number">14</span> <span class="number">00</span>:<span class="number">43</span> /dev/ram0</span><br><span class="line">brw-rw---- <span class="number">1</span> root disk <span class="number">1</span>, <span class="number">1</span> Jan <span class="number">14</span> <span class="number">00</span>:<span class="number">43</span> /dev/ram1</span><br><span class="line">brw-rw---- <span class="number">1</span> root disk <span class="number">1</span>, <span class="number">2</span> Jan <span class="number">14</span> <span class="number">00</span>:<span class="number">42</span> /dev/ram2</span><br><span class="line">brw-rw---- <span class="number">1</span> root disk <span class="number">1</span>, <span class="number">3</span> Jan <span class="number">14</span> <span class="number">00</span>:<span class="number">42</span> /dev/ram3</span><br></pre></td></tr></table></figure></p>
<p>这个的默认大小是16M，设备的数目是16个，这个显然是不符合我们的需求的</p>
<p>这个个数信息和大小信息是写在内核模块里面的,这个目前还找到办法在外面修改的地方，现在通过修改内核模块的方式来达到修改的目的</p>
<h3 id="1、获取内核源码">1、获取内核源码</h3><p> CentOS-7-x86_64-1503-01版本的内核是3.10.0-229.el7.x86_64，这个最好是使用的对应版本的内核代码，这样不会出现其他的问题，下载该distribution版本的内核源码，拷贝到根目录：<br><a href="http://vault.centos.org/7.1.1503/updates/Source/SPackages/kernel-3.10.0-229.1.2.el7.src.rpm" target="_blank" rel="external">http://vault.centos.org/7.1.1503/updates/Source/SPackages/kernel-3.10.0-229.1.2.el7.src.rpm</a></p>
<h4 id="安装该源码包">安装该源码包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rpm -i kernel-3.10.0-229.1.2.el7.src.rpm</span></span><br></pre></td></tr></table></figure>
<p>安装完了以后，这个rpm包里面的源码会被放在 ~/rpmbuild/SOURCES/ 这个目录内，源码文件是linux-3.10.0-229.1.2.el7.tar.xz </p>
<h3 id="2、编译内核源码">2、编译内核源码</h3><p>将linux-3.10.0-229.1.2.el7.tar.xz 文件拷贝到目录  /usr/src/zp 下<br>这个是你自己定义一个编译的目录</p>
<h3 id="3、解压内核源码">3、解压内核源码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># tar -xvf linux-3.10.0-229.1.2.el7.tar.xz</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># cd linux-3.10.0-229.1.2.el7/</span></span><br></pre></td></tr></table></figure>
<h3 id="4、清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作">4、清理编译环境的状态，如果你是下载的内核源码，而且是第一次编译，就没有必要执行这一步操作</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># make mrproper</span></span><br></pre></td></tr></table></figure>
<h4 id="将已存在的-/-config文件内容，作为新版本内核的默认值">将已存在的./.config文件内容，作为新版本内核的默认值</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># make oldconfig</span></span><br></pre></td></tr></table></figure>
<h4 id="配置内核的参数，修改ramdisk的相关属性">配置内核的参数，修改ramdisk的相关属性</h4><p>在内核配置菜单中配置ramdisk块驱动模块的个数和大小，并保存退出<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Device Drivers </span><br><span class="line">       |--&gt;Block devices </span><br><span class="line">                  |--&gt;  [M]RAM block device support </span><br><span class="line">                           (xx) Default number of RAM disks </span><br><span class="line">                           (xx) Default RAM disk size(kbytes)</span><br></pre></td></tr></table></figure></p>
<p>如果内存够大，可以修改大点，注意这个地方是每个内存盘的大小</p>
<h3 id="5、编译内核模块">5、编译内核模块</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># make modules -j8</span></span><br></pre></td></tr></table></figure>
<h4 id="编译后的Ramdisk模块的存放位置">编译后的Ramdisk模块的存放位置</h4><figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line"><span class="regexp">/usr/</span>src<span class="regexp">/zp/</span>linux-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229.1</span>.<span class="number">2</span>.el7<span class="regexp">/drivers/</span>block<span class="regexp">/brd.ko</span></span><br></pre></td></tr></table></figure>
<h3 id="6、安装新的brd-ko模块">6、安装新的brd.ko模块</h3><p>将旧的brd.ko模块从内核中移除。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rmmod brd</span></span><br></pre></td></tr></table></figure></p>
<p>将新的brd.ko模块拷贝到Centos7系统的 如下目录/lib/modules/3.10.0-229.el7.x86_64/kernel/drivers/block/，<br>覆盖原来的ramDisk模块brd.ko</p>
<h3 id="7、更新内核模块依赖">7、更新内核模块依赖</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># depmod -a</span></span><br></pre></td></tr></table></figure>
<h3 id="8、重新挂载内核模块。_如果加载的时候报错就强制加载_modprobe_-f_brd">8、重新挂载内核模块。 如果加载的时候报错就强制加载  <code>modprobe -f brd</code></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># modprobe brd</span></span><br></pre></td></tr></table></figure>
<h3 id="9、检查是否生成了">9、检查是否生成了</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># ls /dev/ram*</span></span><br></pre></td></tr></table></figure>
<p>然后就可以使用/dev/ram*这个设备了，当磁盘一样使用</p>
<p>我的为测试环境，内存不是那么大，就是5G内存盘，4个，做对比测试，ceph默认的5G的journal，这个内存就稍微给大那么一点点6G，防止单位换算的原因造成空间不够，需要重来</p>
<hr>
<p>这篇文章基本都是参考了：<br><a href="http://my.oschina.net/u/658505/blog/544547?fromerr=wWO13oYJ" target="_blank" rel="external">http://my.oschina.net/u/658505/blog/544547?fromerr=wWO13oYJ</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在找这个资料的时候，基本没几个能用的或者过时了的，或者是换了概念，做的不是需要的那种盘，只有少数文章有提到关键部分应该怎么去操作，现在还是自己总结一下</p>
<h3 id="内存盘tmpfs和ramdisk的区别">内存盘tmpfs和ramdisk的区别</h3><p>这个在网上的很多资料里面都有提到，很多文章去写怎么做ramdisk的时候，都是去做的tmpfs，两者虽然都是使用的内存来存储东西，但是是完全有区别的</p>
<ul>
<li>tmpfs这个只需要mount挂载就可以分配一个目录使用内存了，只是一个目录</li>
<li>ramdisk这个是真的分配一个空间，这个分区是可以格式化的（这个格式化是关键）</li>
<li>tmpfs卸载再挂载数据会消失，randisk卸载再挂载数据还在</li>
<li>二者共同点是，系统重启后，里面的东西会消失</li>
</ul>
<blockquote>
<p>本文章主要是讲怎么去做ramdisk</p>
</blockquote>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
</feed>
