<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[但行好事，莫问前程]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-11-04T04:06:32.362Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Ceph用户邮件列表Vol45-Issue1]]></title>
    <link href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/"/>
    <id>http://www.zphj1987.com/2016/11/04/Ceph用户邮件列表Vol45-Issue1/</id>
    <published>2016-11-04T03:57:25.000Z</published>
    <updated>2016-11-04T04:06:32.362Z</updated>
    <content type="html"><![CDATA[<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html" target="_blank" rel="external">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br><a id="more"></a><br>I tried to mark the PG as complete using ceph-objectstore-tool but it did not help as the PG is in fact complete but for some reason blocks.</p>
<p>I tried recreating an empty osd.87 and importing the PG exported from other replica but it did not help.</p>
<p>Can someone help me please? This is really important.</p>
</blockquote>
<p>这个问题是作者一个集群中(ceph 0.94.5)出现了一个磁盘损坏以后造成了一些对象的丢失，然后在做了一定的处理以后，集群状态已经正常了，但是还是新的请求会出现block的状态，这个情况下如何处理才能让集群正常，作者贴出了pg dump，ceph -s,ceph osd dump相关信息，当出现异常的时候，需要人协助的时候，应该提供这些信息方便其他人定位问题，最后这个问题作者自己给出了自己的解决办法，出现的时候影响是当时的流量只有正常情况下的10%了，影响还是很大的</p>
<h3 id="复现问题过程">复现问题过程</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd put testremove testremove</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd map rbd testremove</span></span><br><span class="line">osdmap e85 pool <span class="string">'rbd'</span> (<span class="number">0</span>) object <span class="string">'testremove'</span> -&gt; pg <span class="number">0</span>.eaf226a7 (<span class="number">0.27</span>) -&gt; up ([<span class="number">1</span>,<span class="number">0</span>], p1) acting</span><br></pre></td></tr></table></figure>
<p>写入文件,找到文件，然后去后台删除对象<br>然后停止掉其中一个OSD，这里选择停掉主OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">1</span></span><br><span class="line">ceph osd out <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看状态pg被锁住状态active+degrade，不会迁移完整,并且会检测到了有数据unfound了</p>
<p>然后向这个对象发起get请求<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados -p rbd get testremove testfile</span></span><br></pre></td></tr></table></figure></p>
<p>前端rados请求会卡住，后端出现 requests are blocked</p>
<p>看下如何处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph pg <span class="number">0.27</span> mark_unfound_lost delete</span><br></pre></td></tr></table></figure></p>
<p>邮件列表作者的环境，这个命令也无法执行，直接卡死，后来发现有个执行窗口，就是这个对象所在的PG的OSD在启动过程中还是可以接受命令的，就在这个执行窗口执行这个命令就可以解决了</p>
<p>执行了以后可以执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rados  -p rbd  get testremove  a</span></span><br><span class="line">error getting rbd/testremove: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>这个时候查询集群的状态可以看到，集群已经正常的恢复了，不会因为一个对象的丢失造成集群的PG状态卡在待迁移状态</p>
<p>可以看到请求是失败的但是不会像之前一样卡死的状态，卡死是比失败更严重的一种状态</p>
<p>如果不想看到老的 slow request ,那么就重启这个卡住的PG所在的osd，如果本来就正常了，那么这个异常状态就会消失</p>
<p>这个是一个需要人工干预的状态，实际上模拟的就是对象丢失的场景，什么情况下会对象丢失，一般来说，底层磁盘的故障，写下去的对象当时记录着有，正好写入完成又准备写副本的时候，磁盘坏了，这个就有比较高的概率出现，所以出现了坏盘要尽早更换</p>
<p>本系列是只会对列表的当天的非re进行一个汇总，这样保持了一个问题的追踪都在一篇里面，所以这一天只有这一个问题</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-04</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="ceph_Vol_45_Issue_1">ceph Vol 45 Issue 1</h2><h3 id="1-unfound_objects_blocking_cluster,_need_help!(原文)">1.unfound objects blocking cluster, need help!(<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg32804.html">原文</a>)</h3><blockquote>
<p>Hi,</p>
<p>I have a production cluster on which 1 OSD on a failing disk was slowing the whole cluster down. I removed the OSD (osd.87) like usual in such case but this time it resulted in 17 unfound objects. I no longer have the files from osd.87. I was able to call “ceph pg PGID mark_unfound_lost delete” on 10 of those objects.</p>
<p>On the remaining objects 7 the command blocks. When I try to do “ceph pg PGID query” on this PG it also blocks. I suspect this is same reason why mark_unfound blocks.</p>
<p>Other client IO to PGs that have unfound objects are also blocked. When trying to query the OSDs which has the PG with unfound objects, “ceph tell” blocks.<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph日历格子之2016年10月(Vol45)]]></title>
    <link href="http://www.zphj1987.com/2016/11/03/Ceph%E6%97%A5%E5%8E%86%E6%A0%BC%E5%AD%90%E4%B9%8B2016%E5%B9%B410%E6%9C%88-Vol45/"/>
    <id>http://www.zphj1987.com/2016/11/03/Ceph日历格子之2016年10月-Vol45/</id>
    <published>2016-11-03T09:51:33.000Z</published>
    <updated>2016-11-04T04:05:19.603Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br><a id="more"></a></p>
<h2 id="二、内容">二、内容</h2><p>Ceph邮件列表2016年10月Issue格子   </p>
<h3 id="Vol_45">Vol 45</h3><table>
<thead>
<tr>
<th style="text-align:center">一</th>
<th style="text-align:center">二</th>
<th style="text-align:center">三</th>
<th style="text-align:center">四</th>
<th style="text-align:center">五</th>
<th style="text-align:center">六</th>
<th style="text-align:center">日</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center"><a href="http://www.zphj1987.com/2016/11/04/Ceph%E7%94%A8%E6%88%B7%E9%82%AE%E4%BB%B6%E5%88%97%E8%A1%A8Vol45-Issue1/" target="_blank" rel="external">Issue 1</a></td>
<td style="text-align:center">Issue2</td>
<td style="text-align:center">Issue3</td>
<td style="text-align:center">Issue4</td>
<td style="text-align:center">Issue5</td>
<td style="text-align:center">Issue6</td>
</tr>
<tr>
<td style="text-align:center">Issue 7</td>
<td style="text-align:center">Issue 8</td>
<td style="text-align:center">Issue9</td>
<td style="text-align:center">Issue 10</td>
<td style="text-align:center">Issue 11</td>
<td style="text-align:center">Issue 12</td>
<td style="text-align:center">Issue 13</td>
</tr>
<tr>
<td style="text-align:center">Issue 14</td>
<td style="text-align:center">Issue 15</td>
<td style="text-align:center">Issue16</td>
<td style="text-align:center">Issue 17</td>
<td style="text-align:center">Issue 18</td>
<td style="text-align:center">Issue 19</td>
<td style="text-align:center">Issue 20</td>
</tr>
<tr>
<td style="text-align:center">Issue 21</td>
<td style="text-align:center">Issue 22</td>
<td style="text-align:center">Issue 23</td>
<td style="text-align:center">Issue 24</td>
<td style="text-align:center">Issue 25</td>
<td style="text-align:center">Issue 26</td>
<td style="text-align:center">Issue 27</td>
</tr>
<tr>
<td style="text-align:center">Issue 28</td>
<td style="text-align:center">Issue 29</td>
<td style="text-align:center">Issue 30</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h2 id="三、本月主要问题如下：">三、本月主要问题如下：</h2><p>如何处理对象丢失引起的PG状态不对的问题(Issue1)</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-03</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rili/rili.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>准备策划一个系列，之前也做了一次尝试，中断了，现在准备续起来，能做就尽量坚持做下去，准备根据日历的形式梳理出来，如同打怪一样，一个个去干掉这些问题，每个issue里面会有每天邮件列表里面提出的问题，一般为8到9个问题，如果完成一个Issue，就会在这里给出对应的Issue的链接，相当于一个目录和进度的功能，所以本篇会是一个持续更新的过程<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph部署的时候修改默认权重]]></title>
    <link href="http://www.zphj1987.com/2016/11/02/Ceph%E9%83%A8%E7%BD%B2%E7%9A%84%E6%97%B6%E5%80%99%E4%BF%AE%E6%94%B9%E9%BB%98%E8%AE%A4%E6%9D%83%E9%87%8D/"/>
    <id>http://www.zphj1987.com/2016/11/02/Ceph部署的时候修改默认权重/</id>
    <published>2016-11-02T07:46:07.000Z</published>
    <updated>2016-11-02T09:49:40.801Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br><a id="more"></a></p>
<h2 id="二、修改">二、修改</h2><p>如果统一的初始值，那么直接添加参数即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_crush_initial_weight</span><br></pre></td></tr></table></figure></p>
<p>如果想自己添加算法，那么就根据下面的去做就可以了</p>
<h3 id="2-1_centos+jewel">2.1 centos+jewel</h3><p>修改：<br>/usr/lib/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k <span class="variable">$data</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/107374182 ; r = sprintf("%.4f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改这个地方的值就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">2</span>`</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2_centos+hammer">2.2 centos+hammer</h3><p>修改 /etc/init.d/ceph<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(df -P -k $osd_data/. | tail -1 | awk '&#123; print sprintf("%.2f",$2/1073741824)</span> &#125;')"</span></span><br></pre></td></tr></table></figure></p>
<p>修改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=<span class="string">"<span class="variable">$(echo 5)</span>"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-3_ubuntu+hammer">2.3 ubuntu+hammer</h3><p>由于ubuntu用initctl控制服务，不是用的/etc/init.d/ceph/,所以要修改另外的一个路径<br>修改/usr/libexec/ceph/ceph-osd-prestart.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`df -P -k /var/lib/ceph/osd/<span class="variable">$&#123;cluster:-ceph&#125;</span>-<span class="variable">$id</span>/ | tail -<span class="number">1</span> | awk <span class="string">'&#123; d= $2/1073741824 ; r = sprintf("%.2f", d); print r &#125;'</span>`</span><br></pre></td></tr></table></figure></p>
<p>修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">defaultweight=`<span class="built_in">echo</span> <span class="number">8</span>`</span><br></pre></td></tr></table></figure></p>
<h2 id="三、总结">三、总结</h2><p>这个比较简单，通过修改取值就可以改变默认配置了,上面的可以根据自己的需求加入算法即可</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-11-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/weight.jpg" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>部署集群的时候权重是默认生成的，这个是根据磁盘大小分配的，我们有的时候需要去修改一下这个默认权重<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday深圳20161029总结]]></title>
    <link href="http://www.zphj1987.com/2016/10/31/Cephday%E6%B7%B1%E5%9C%B320161029%E6%80%BB%E7%BB%93/"/>
    <id>http://www.zphj1987.com/2016/10/31/Cephday深圳20161029总结/</id>
    <published>2016-10-31T04:34:15.000Z</published>
    <updated>2016-10-31T07:31:46.461Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br><a id="more"></a></p>
<h2 id="二、PPT内容">二、PPT内容</h2><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/01_%E4%BC%81%E4%B8%9A%E7%BA%A7Ceph%E4%B9%8B%E8%B7%AF%20-%20iSCSI%E5%AE%9E%E8%B7%B5%E4%B8%8E%E4%BC%98%E5%8C%96%20by_%E9%82%B1%E5%B0%9A%E9%AB%98.pdf" width="850" height="530"></center>


<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/02_YY%E4%BA%91%E5%B9%B3%E5%8F%B0Ceph%E5%AE%9E%E8%B7%B5%20by_%E6%88%9A%E6%98%B1.pdf" width="850" height="690"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/03_%E4%B8%AD%E5%85%B4%E4%BA%91%E5%AD%98%E5%82%A8%E4%B9%8B%E8%B7%AFby_%E9%AA%86%E7%A7%91%E5%AD%A6&%E4%BB%BB%E7%84%95%E6%96%87.pdf" width="850" height="540"></center>

<center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/04_Swift%20Using%20Ceph%20Backend%20by_%E4%BA%8E%E8%88%AA.pdf" width="850" height="530"></center>

<h2 id="三、总结">三、总结</h2><p>第一篇来自杉岩数据的分享，主要集中在ceph在iscsi方面的实现，从开源解决方案来说，有一些弊端，或者高级的属性无法支持，在这个基础上，杉岩数据给出了自己的企业级解决方案，增加了企业级的支持，从功能来说还是增加了不少功能，如果能开源当然就更好了</p>
<p>第二篇来自YY云平台的分享，讲述了ceph在YY平台中的实践，这里面比较有用的是超多osd的线程过载问题，以及ubuntu下的系统检查触发的坑，这个都是很有价值的分享，ubuntu下面有几个类似的检查服务我曾经使用的时候也踩过类似的坑</p>
<p>第三篇来自中兴的分享，中兴基于ceph有一套商业存储，这里面比较有用的分享是mon数据的恢复问题，这个功能一开始的出来的时候我就进行了测试，触发了一个bug，然后有个人进行更深入的测试，从这次分享来看，深入测试的那个人应该来自中兴的，这里面提出了多种方案，其中的基于map的重建是之前没见过的，其他两种都是实践过，没有问题，再一个分享就是文件系统的配额的，这个给出的两种方案也是可行的，一种基于内核态的，一种基于用户态的，这个都是比较好的分享</p>
<p>第四篇来自奥思数据分享，主要集中在swift对象存储的分享，这个接触不多，研究对象存储的同学可以看看有什么可以借鉴的没</p>
<p>从上面的几篇分享来看，都是很好的分享，分享者自己都提出问题给出了解决方案，或者指明了一些方向，在技术相对封闭了环境下也是不可多得资料，希望类似的活动越来越多，多进行交流，提高行业整体水平，在ceph布道过程中，ceph中国社区在背后默默做了很多的工作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-31</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdayshenzheng/shenzhen.gif" alt=""><br></center>

<h2 id="一、前言">一、前言</h2><p>本次的Cephday是在深圳举办的，由于台风的原因推迟了一周举办，本来计划好去深圳参加一下，本周正好有事不能参加了，只能期待下次的武汉站的活动了，每次活动一方面促进了同行业人员的沟通，一方面会带来一些技术的分享，那么就对这次分享的PPT做一个个人总结<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix实现自定义自动发现的流程]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix实现自定义自动发现的流程/</id>
    <published>2016-10-28T05:56:24.000Z</published>
    <updated>2016-11-01T03:56:27.977Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>这里我的宏就是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&#123;<span class="comment">#POOLNAME&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>然后要创建一个监控项原型：<br>也是一个名称和一个键值：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- 名称：<span class="built_in">test</span> on <span class="variable">$1</span></span><br><span class="line">- 键值：ceph.pools.used[&#123;<span class="comment">#POOLNAME&#125;]</span></span><br></pre></td></tr></table></figure>
<p>这个地方名称可以用参数形式，包含的就是下面的那个键值中的参数对应的位置的值</p>
<p>然后需要去写一个这样的键值的收集<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">UserParameter=ceph.pools, python /sbin/ceph-status.py pools</span><br><span class="line">UserParameter=ceph.pools.used[*], python /sbin/ceph-status.py pool_used <span class="variable">$1</span></span><br></pre></td></tr></table></figure></p>
<p>测试下效果：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools.used["rbd"]</span></span><br><span class="line"><span class="number">888</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方，上面的是去做的自动发现，下面的键值就是根据上面的自动发现返回的值去作为一个参数进行新的查询，后面的$1就是将参数传到收集的脚本里面去的，这样就能根据自动发现的不同的名称返回来不同的值，从而添加不同的监控项目，而不需要自己一个个添加了</p>
<h2 id="三、总结">三、总结</h2><p>自动发现实际上就是需要首先去获得需要监控的值，然后将这个值作为一个新的参数传递到另外一个收集数据的item里面去，这样就可以了，这里是做的最基本的单项数据的获取，后面应该会遇到多重变量的情况，就需要赋值多重变量，到时需要用到再记录下</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>本章介绍如何去自定义一个zabbix自动发现的整个流程</p>
<h2 id="二、过程">二、过程</h2><p>首先需要在模板当中创建一个自动发现的规则，这个地方只需要一个名称和一个键值，例如</p>
<ul>
<li>名称：Ceph Cluster Pool Discovery</li>
<li>键值：ceph.pools</li>
</ul>
<p>过滤器中间要添加你需要的用到的值宏<br>我的数据是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k ceph.pools</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"rbd"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"metedata"</span>&#125;,&#123;<span class="string">"&#123;#POOLNAME&#125;"</span>:<span class="string">"data"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[zabbix自动发现的python方式数据生成]]></title>
    <link href="http://www.zphj1987.com/2016/10/28/zabbix%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0%E7%9A%84python%E6%96%B9%E5%BC%8F%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/"/>
    <id>http://www.zphj1987.com/2016/10/28/zabbix自动发现的python方式数据生成/</id>
    <published>2016-10-27T17:11:08.000Z</published>
    <updated>2016-10-27T17:29:38.340Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br><a id="more"></a></p>
<p>我们同样的来看看ceph原生的命令的json接口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s -f json</span></span><br><span class="line"></span><br><span class="line">&#123;<span class="string">"health"</span>:&#123;<span class="string">"health"</span>:&#123;<span class="string">"health_services"</span>:[&#123;<span class="string">"mons"</span>:[&#123;<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"kb_total"</span>:<span class="number">52403200</span>,<span class="string">"kb_used"</span>:<span class="number">32905432</span>,<span class="string">"kb_avail"</span>:<span class="number">19497768</span>,<span class="string">"avail_percent"</span>:<span class="number">37</span>,<span class="string">"last_updated"</span>:<span class="string">"2016-10-28 01:15:29.431854"</span>,<span class="string">"store_stats"</span>&#123;<span class="string">"bytes_total"</span>:<span class="number">20206814</span>,<span class="string">"bytes_sst"</span>:<span class="number">16929998</span>,<span class="string">"bytes_log"</span>:<span class="number">3080192</span>,<span class="string">"bytes_misc"</span>:<span class="number">196624</span>,<span class="string">"last_updated"</span>:<span class="string">"0.000000"</span>&#125;,<span class="string">"health"</span>:<span class="string">"HEALTH_OK"</span>&#125;]&#125;]&#125;,<span class="string">"timechecks"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">4</span>,<span class="string">"round"</span>:<span class="number">0</span>,<span class="string">"round_status"</span>:<span class="string">"finished"</span>&#125;,<span class="string">"summary"</span>:[],<span class="string">"overall_status"</span>:<span class="string">"HEALTH_OK"</span>,<span class="string">"detail"</span>:[]&#125;,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"election_epoch"</span>:<span class="number">4</span>,<span class="string">"quorum"</span>:[<span class="number">0</span>],<span class="string">"quorum_names"</span>:[<span class="string">"lab8106"</span>],<span class="string">"monmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">1</span>,<span class="string">"fsid"</span>:<span class="string">"fae7a8db-c671-4b45-a784-ddb41e633905"</span>,<span class="string">"modified"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"created"</span>:<span class="string">"2016-10-19 22:26:28.879232"</span>,<span class="string">"mons"</span>:[&#123;<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"addr"</span>:<span class="string">"192.168.8.106:6789\/0"</span>&#125;]&#125;,<span class="string">"osdmap"</span>:&#123;<span class="string">"osdmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">63</span>,<span class="string">"num_osds"</span>:<span class="number">2</span>,<span class="string">"num_up_osds"</span>:<span class="number">2</span>,<span class="string">"num_in_osds"</span>:<span class="number">2</span>,<span class="string">"full"</span>:<span class="literal">false</span>,<span class="string">"nearfull"</span>:<span class="literal">false</span>,<span class="string">"num_remapped_pgs"</span>:<span class="number">0</span>&#125;&#125;,<span class="string">"pgmap"</span>:&#123;<span class="string">"pgs_by_state"</span>:[&#123;<span class="string">"state_name"</span>:<span class="string">"active+clean"</span>,<span class="string">"count"</span>:<span class="number">80</span>&#125;],<span class="string">"version"</span>:<span class="number">19174</span>,<span class="string">"num_pgs"</span>:<span class="number">80</span>,<span class="string">"data_bytes"</span>:<span class="number">45848191333</span>,<span class="string">"bytes_used"</span>:<span class="number">45966077952</span>,<span class="string">"bytes_avail"</span>:<span class="number">551592390656</span>,<span class="string">"bytes_total"</span>:<span class="number">597558468608</span>&#125;,<span class="string">"fsmap"</span>:&#123;<span class="string">"epoch"</span>:<span class="number">5</span>,<span class="string">"id"</span>:<span class="number">1</span>,<span class="string">"up"</span>:<span class="number">1</span>,<span class="string">"in"</span>:<span class="number">1</span>,<span class="string">"max"</span>:<span class="number">1</span>,<span class="string">"by_rank"</span>:[&#123;<span class="string">"filesystem_id"</span>:<span class="number">1</span>,<span class="string">"rank"</span>:<span class="number">0</span>,<span class="string">"name"</span>:<span class="string">"lab8106"</span>,<span class="string">"status"</span>:<span class="string">"up:active"</span>&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>同样也是这个类型的数据，好了，这里直接上代码：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">def get_cluster_pools():</span><br><span class="line">    try:</span><br><span class="line">        pool_list=[]</span><br><span class="line">        data_dic = &#123;&#125;</span><br><span class="line">        cluster_pools = commands.getoutput(<span class="string">'timeout 10 ceph osd pool ls -f json 2&gt;/dev/null'</span>)</span><br><span class="line">        json_str = json.loads(cluster_pools)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> json_str:</span><br><span class="line">            pool_dic = &#123;&#125;</span><br><span class="line">            pool_dic[<span class="string">'&#123;#POOLNAME&#125;'</span>] = str(item)</span><br><span class="line">            pool_list.append(pool_dic)</span><br><span class="line">        data_dic[<span class="string">'data'</span>] = pool_list</span><br><span class="line">        <span class="built_in">return</span> json.dumps(data_dic,separators=(<span class="string">','</span>, <span class="string">':'</span>))</span><br><span class="line">    except:</span><br><span class="line">        <span class="built_in">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>输出如下<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;&#34;data&#34;:[&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;rbd&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;metedata&#34;&#125;,&#123;&#34;&#123;#POOLNAME&#125;&#34;:&#34;data&#34;&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>跟上面的格式一样了，关键在对字典进行赋值的处理，然后进行一个空格处理就完成了</p>
<h2 id="三、总结">三、总结</h2><p>还是接触的太少，造成简单的处理都需要花费比较久的时间</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>zabbix里面有个功能是自动发现，比如文件系统和网卡的获取的时候，因为预先无法知道这个网卡的名称，所以就有了这个自动发现的功能，这里我是因为要用到存储池的自动发现，所以需要对数据进行生成</p>
<h2 id="二、实现">二、实现</h2><p>我们看下原生的接口的数据类型：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># zabbix_get -s 127.0.0.1 -k "net.if.discovery"</span></span><br><span class="line">&#123;<span class="string">"data"</span>:[&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp3s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0-nic"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"docker0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp4s0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"enp2s0f1"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"virbr0"</span>&#125;,&#123;<span class="string">"&#123;#IFNAME&#125;"</span>:<span class="string">"lo"</span>&#125;]&#125;</span><br></pre></td></tr></table></figure></p>
<p>数据为格式化好了的json数据，这个地方弄了好半天，因为网上很多人是用字符串拼接的方式，实际这个是字典嵌套了列表，列表又嵌套了字典，就是后面的地方开始没弄懂怎么有大括号的<br>]]>
    
    </summary>
    
      <category term="zabbix" scheme="http://www.zphj1987.com/tags/zabbix/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Docker与Ceph的分与合]]></title>
    <link href="http://www.zphj1987.com/2016/10/19/Docker%E4%B8%8ECeph%E7%9A%84%E5%88%86%E4%B8%8E%E5%90%88/"/>
    <id>http://www.zphj1987.com/2016/10/19/Docker与Ceph的分与合/</id>
    <published>2016-10-19T15:46:12.000Z</published>
    <updated>2016-10-19T15:48:24.553Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br><a id="more"></a></p>
<h2 id="二、实践">二、实践</h2><h3 id="2-1、配置docker的基础环境">2.1、配置docker的基础环境</h3><p>拉取基础镜像<br>这个是拉取的灵雀云的docker仓库的centos<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull index.alauda.cn/library/centos</span><br></pre></td></tr></table></figure></p>
<p>启动docker进程,并且设置自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure></p>
<p>查询当前机器上面的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">index.alauda.cn/library/centos   latest              <span class="number">904</span>d6c400333        <span class="number">4</span> months ago        <span class="number">196.7</span> MB</span><br></pre></td></tr></table></figure></p>
<p>我们先对我们的镜像做一些基本的设置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -itd --name=cephbase --net=host --pid=host index.alauda.cn/library/centos /bin/bash</span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker attach cephbase</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 /]<span class="comment"># df -h</span></span><br><span class="line">Filesystem                                                                                     Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/docker-<span class="number">8</span>:<span class="number">2</span>-<span class="number">83216</span>-dd340d1f6a68b6849b9500c4e6f9b7fb1901c3c0cb1ce0d7336f5104a1ef4a10   <span class="number">10</span>G  <span class="number">240</span>M  <span class="number">9.8</span>G   <span class="number">3</span>% /</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs                                                                                           <span class="number">24</span>G     <span class="number">0</span>   <span class="number">24</span>G   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda2                                                                                       <span class="number">50</span>G   <span class="number">31</span>G   <span class="number">20</span>G  <span class="number">62</span>% /etc/hosts</span><br><span class="line">shm</span><br></pre></td></tr></table></figure></p>
<p>可以看到我们已经进入了容器内部了，下面需要做的事情，就是将ceph运行需要的一些软件装上去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install wget --nogpgcheck</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># rm -rf /etc/yum.repos.d/*.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># sed -i 's/$releasever/7.2.1511/g' /etc/yum.repos.d/CentOS-Base.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># vi /etc/yum.repos.d/ceph.repo</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum makecache</span></span><br><span class="line">[root@lab8106 /]<span class="comment"># yum install ceph ceph-deploy</span></span><br></pre></td></tr></table></figure></p>
<p>检查软件版本装对了没<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 /]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">10.2</span>.<span class="number">3</span> (ecc23778eb545d8dd55e2e4735b53cc93f92e65b)</span><br><span class="line">[root@lab8106 /]<span class="comment"># ceph-deploy --version</span></span><br><span class="line"><span class="number">1.5</span>.<span class="number">36</span></span><br></pre></td></tr></table></figure></p>
<p>可以退出了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p>
<p>查看之前的容器的ID<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE                            COMMAND             CREATED             STATUS                      PORTS               NAMES</span><br><span class="line"><span class="number">48420</span>c9955b5        index.alauda.cn/library/centos   <span class="string">"/bin/bash"</span>         About an hour ago   Exited (<span class="number">0</span>) <span class="number">14</span> seconds ago                       cephbase</span><br></pre></td></tr></table></figure></p>
<p>将容器保存为一个新的镜像，cephbase<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker commit 48420c9955b5 cephbase</span></span><br><span class="line">sha256:ffe236ee2bb61d2809bf1f4c03596f83b9c0e8a6<span class="built_in">fc</span>2eb9013a81abb25be833e9</span><br></pre></td></tr></table></figure></p>
<p>查看当前的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">cephbase                         latest              ffe236ee2bb6        About a minute ago   <span class="number">1.39</span> GB</span><br></pre></td></tr></table></figure></p>
<p>基础镜像就完成，包括了ceph运行需要的软件</p>
<p>我们来创建mon的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=monnode --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph  -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>进入到容器当中去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>在容器当中执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 deploy]<span class="comment"># ceph-deploy mon create lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>我们来创建osd的容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --privileged  -itd  --name=osd0 --net=host  -v  /var/<span class="built_in">log</span>/ceph:/var/<span class="built_in">log</span>/ceph -v /var/run/ceph:/var/run/ceph -v /var/lib/ceph/:/var/lib/ceph/  -v /etc/ceph:/etc/ceph -v /var/lib/ceph/osd/ceph-<span class="number">0</span>:/var/lib/ceph/osd/ceph-<span class="number">0</span> -v /sys/fs/cgroup:/sys/fs/cgroup  ceph  /sbin/init</span><br></pre></td></tr></table></figure></p>
<p>我们将网络映射到主机上，也就是容器和主机公用网络和主机名，然后把本地的一个数据盘的目录映射进去用于osd的部署，这里都是使用-v进行映射</p>
<p>这个地方因为是centos7，所以systemctl内部是无法使用的，而ceph是需要这个来控制服务的，所以需要提权，并且把入口改为/sbin/init</p>
<h2 id="三、回顾流程">三、回顾流程</h2><ul>
<li>下载centos基础镜像 </li>
<li>修改镜像的内容并提交为新的镜像</li>
<li>基于新的镜像启动容器（采用host映射，目录映射，所有数据都是留在物理机）</li>
<li>进入容器进行ceph的部署 </li>
<li>进入容器启动相关进程</li>
</ul>
<p>这样ceph是运行到了docker中，即使把docker容器销毁掉，因为基于主机名和网络的配置跟宿主机是一致的，所以直接在宿主机上也是能马上启动起来的</p>
<h2 id="四、为何用容器">四、为何用容器</h2><p>基于容器的技术是最近几年开始火起来的，目前的云计算还处于火热期，openstack还是显得比较重型的，很多时候我们只需要的是一个能够运行我们web服务的环境，然后容器技术就应运而生了，直接启动一个容器，就能实现，这个对于宿主机来说方便的只是启动一个进程那么简单</p>
<p>对于庞大复杂的服务来说，如何做到环境一致也是一直很难做到的，一排物理机，因为各种各样的原因，升级，重装系统，很难保证整套系统基础环境的一致性，而基于docker的环境就能很方便的实现这个，相当于把整个运行环境打了一个包，所有的宿主机能够很方便的统一到相同的环境，即使重装了宿主机，也能方便的用一两条命令将环境部署到统一，比如上面所说的ceph，升级了基础镜像内的软件包，然后将所有的运行进程进行一次重启，就相当于运行了一个新的环境</p>
<p>容器还能够做的事情就是能够很便捷的把一个复杂环境运行起来，特别对于web类的服务，一台机器上可以跑一排的对外服务，即使出了问题，也能很快的再运行起来，这个对于传统的环境来说就是很难实现的，这里讲一下calamari，这个监控系统不是很复杂，但是因为依赖的软件的问题，造成很多人无法正常运行起来，这个后面我会出一个集成好calamari的docker环境，实现一键运行</p>
<p>在低版本的os上能够运行高版本的服务，比如在centos6上运行centos7的docker环境</p>
<h2 id="五、总结">五、总结</h2><p>本篇的文章的标题为docker与ceph的分与合，一套系统除了自身需要稳定性以外，系统自身最好不要受制于其他系统，需要在设计初期就能保证，各个模块都能轻松的剥离，否则很容易受制于另外一套系统，所以基于上面的方案来说，docker和ceph既是合在一起的，也是分开的,本篇只是讲了一个框架，实际部署ceph的过程当中还是有一些小问题需要具体处理的，不是很难，权限问题，目录问题</p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-19</td>
</tr>
</tbody>
</table>
<h2 id="附录：">附录：</h2><p>docker的常用操作<br>查询镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker images</span></span><br></pre></td></tr></table></figure></p>
<p>查询容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker ps</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br></pre></td></tr></table></figure></p>
<p>删除容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rm 64f617dfada5</span></span><br></pre></td></tr></table></figure></p>
<p>删除镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker rmi node</span></span><br></pre></td></tr></table></figure></p>
<p>进入容器内部<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec -it monnode /bin/bash</span></span><br></pre></td></tr></table></figure></p>
<p>让容器执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># docker exec monnode uptime</span></span><br></pre></td></tr></table></figure></p>
<p>退出容器,不停止容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ctrl+p然后ctrl+q</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/ceph/cephdocker.png" alt="dockerceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>docker是一个管理工具，在操作系统之上提供了一个新的独立轻环境，好处是本地提供了一个基础镜像，然后基于镜像再运行环境，也可以把环境重新打包为镜像，管理起来类似于git，感觉非常的方便，并且能够做到一处提交，处处可以取到相同的环境，大大的减少了因为环境偏差造成的系统不稳定</p>
<p>目前有不少生成环境已经把ceph和docker结合在一起运行了，这个有的是确实能够理解docker的好处，也能够有技术力量去进行维护，这个地方相当于两套系统了，并且关于技术的传递也增加了难度，特别是一套系统是docker+ceph的环境，并且又出现相关人员离职的情况，新来的人如果不是技术很熟，之前的技术文档没有记录很全的话，再去运维这一套系统还是比较有难度的</p>
<p>本篇目的是记录一下docker与ceph的结合的方式，关于ceph和docker的分与合，只有做到能剥离的系统，才不会因为技术原因受限<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中PG和PGP的区别]]></title>
    <link href="http://www.zphj1987.com/2016/10/19/Ceph%E4%B8%ADPG%E5%92%8CPGP%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://www.zphj1987.com/2016/10/19/Ceph中PG和PGP的区别/</id>
    <published>2016-10-19T07:26:28.000Z</published>
    <updated>2016-10-19T07:27:09.332Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/pgnew.png" alt="pg"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>首先来一段英文关于PG和PGP区别的解释：</p>
<blockquote>
<p>PG = Placement Group<br>PGP = Placement Group for Placement purpose </p>
<p>pg_num = number of placement groups mapped to an OSD</p>
<p>When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD. </p>
<p>Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role.<br>By Karan Singh </p>
</blockquote>
<p>以上是来自邮件列表的 <code>Karan Singh</code> 的PG和PGP的相关解释，他也是 <code>Learning Ceph</code> 和 <code>Ceph Cookbook</code> 的作者，以上的解释没有问题，我们来看下具体在集群里面具体作用<br><a id="more"></a></p>
<h2 id="二、实践">二、实践</h2><p>环境准备，因为是测试环境，我只准备了两台机器，每台机器4个OSD，所以做了一些参数的设置，让数据尽量散列<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">osd_crush_chooseleaf_type = 0</span><br></pre></td></tr></table></figure></p>
<p>以上为修改的参数，这个是让我的环境故障域为OSD分组的</p>
<p>创建测试需要的存储池<br>我们初始情况只创建一个名为testpool包含6个PG的存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool create testpool 6 6</span></span><br><span class="line">pool <span class="string">'testpool'</span> created</span><br></pre></td></tr></table></figure></p>
<p>我们看一下默认创建完了后的PG分布情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">0</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">0</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">0</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">0</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">0</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">0</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>我们写入一些对象，因为我们关心的不仅是pg的变动，同样关心PG内对象有没有移动,所以需要准备一些测试数据，这个调用原生rados接口写最方便<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p testpool bench <span class="number">20</span> write --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>我们再来查询一次<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">75</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">83</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">144</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">146</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到写入了一些数据，其中的第二列为这个PG当中的对象的数目，第三列为PG所在的OSD</p>
<h3 id="增加PG测试">增加PG测试</h3><p>我们来扩大PG再看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool set testpool pg_num 12</span></span><br><span class="line"><span class="built_in">set</span> pool <span class="number">1</span> pg_num to <span class="number">12</span></span><br></pre></td></tr></table></figure></p>
<p>再次查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1.1</span> <span class="number">37</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.9</span> <span class="number">38</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">41</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.8</span> <span class="number">42</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1</span>.b <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.7</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">48</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.6</span> <span class="number">49</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1</span>.a <span class="number">49</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到上面新加上的PG的分布还是基于老的分布组合，并没有出现新的OSD组合，因为我们当前的设置是pgp为6,那么三个OSD的组合的个数就是6个，因为当前为12个pg，分布只能从6种组合里面挑选，所以会有重复的组合</p>
<p>根据上面的分布情况，可以确定的是，增加PG操作会引起PG内部对象分裂，分裂的份数是根据新增PG组合重复情况来的，比如上面的情况</p>
<ul>
<li>1.1的对象分成了两份[3,6,0]</li>
<li>1.3的对象分成了三份[4,1,2]</li>
<li>1.4的对象没有拆分[3,0,4]</li>
</ul>
<p>结论：增加PG会引起PG内的对象分裂，也就是在OSD上创建了新的PG目录，然后进行部分对象的move的操作</p>
<h3 id="增加PGP测试">增加PGP测试</h3><p>我们将原来的PGP从6调整到12<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd pool set testpool pgp_num 12</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|grep ^1|awk '&#123;print $1,$2,$15&#125;'</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">1</span>.a <span class="number">49</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1</span>.b <span class="number">48</span> [<span class="number">1</span>,<span class="number">6</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.1</span> <span class="number">37</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.0</span> <span class="number">41</span> [<span class="number">7</span>,<span class="number">0</span>,<span class="number">6</span>]</span><br><span class="line"><span class="number">1.3</span> <span class="number">48</span> [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.2</span> <span class="number">48</span> [<span class="number">7</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line"><span class="number">1.5</span> <span class="number">86</span> [<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line"><span class="number">1.4</span> <span class="number">80</span> [<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br><span class="line"><span class="number">1.7</span> <span class="number">48</span> [<span class="number">1</span>,<span class="number">6</span>,<span class="number">0</span>]</span><br><span class="line"><span class="number">1.6</span> <span class="number">49</span> [<span class="number">3</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line"><span class="number">1.9</span> <span class="number">38</span> [<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>]</span><br><span class="line"><span class="number">1.8</span> <span class="number">42</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>可以看到PG里面的对象并没有发生变化，而PG所在的对应关系发生了变化<br>我们看下与调整PGP前的对比<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">*1.1 37 [3,6,0]          1.1 37 [3,6,0]*&#10;1.9 38 [3,6,0]          1.9 38 [1,4,2]&#10;*1.0 41 [7,0,6]          1.0 41 [7,0,6]*&#10;1.8 42 [7,0,6]          1.8 42 [1,2,3]&#10;*1.3 48 [4,1,2]          1.3 48 [4,1,2]*&#10;1.b 48 [4,1,2]          1.b 48 [1,6,2]&#10;1.7 48 [4,1,2]          1.7 48 [1,6,0]&#10;*1.2 48 [7,4,1]          1.2 48 [7,4,1]*&#10;1.6 49 [7,4,1]          1.6 49 [3,6,7]&#10;1.a 49 [7,4,1]          1.a 49 [1,2,6]&#10;*1.5 86 [4,6,3]          1.5 86 [4,6,3]*&#10;*1.4 80 [3,0,4]          1.4 80 [3,0,4]*</span><br></pre></td></tr></table></figure></p>
<p>可以看到其中最原始的6个PG的分布并没有变化（标注了*号），变化的是后增加的PG，也就是将重复的PG分布进行新分布，这里并不是随机完全打散，而是根据需要去进行重分布</p>
<p>结论：调整PGP不会引起PG内的对象的分裂，但是会引起PG的分布的变动</p>
<h2 id="三、总结">三、总结</h2><ul>
<li>PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数</li>
<li>PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG当中</li>
<li>PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动</li>
</ul>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/pgnew.png" alt="pg"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>首先来一段英文关于PG和PGP区别的解释：</p>
<blockquote>
<p>PG = Placement Group<br>PGP = Placement Group for Placement purpose </p>
<p>pg_num = number of placement groups mapped to an OSD</p>
<p>When pg_num is increased for any pool, every PG of this pool splits into half, but they all remain mapped to their parent OSD. </p>
<p>Until this time, Ceph does not start rebalancing. Now, when you increase the pgp_num value for the same pool, PGs start to migrate from the parent to some other OSD, and cluster rebalancing starts. This is how PGP plays an important role.<br>By Karan Singh </p>
</blockquote>
<p>以上是来自邮件列表的 <code>Karan Singh</code> 的PG和PGP的相关解释，他也是 <code>Learning Ceph</code> 和 <code>Ceph Cookbook</code> 的作者，以上的解释没有问题，我们来看下具体在集群里面具体作用<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[配置Ceph的IPV6集群]]></title>
    <link href="http://www.zphj1987.com/2016/10/17/%E9%85%8D%E7%BD%AECeph%E7%9A%84IPV6%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.zphj1987.com/2016/10/17/配置Ceph的IPV6集群/</id>
    <published>2016-10-17T08:19:22.000Z</published>
    <updated>2016-10-31T07:35:25.618Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/ipv6.jpeg" alt="ipv6"><br></center><br>一、前言<br>对于IPV6实在是非常的陌生，所以本篇开始会讲一下最基本的网络配置，首先让网络能通起来，最开始就是因为不熟悉IPV6,而直接使用了link local地址，造成了mon部署的时候进程无法绑定到IP，从而端口没有启动，这个是在ceph社区群友 <code>ceph-长沙-柠檬</code> 同学的帮助下才发现问题的</p>
<p>IPV6是会有个link local地址的，在一个接口可以配置很多IPv6地址，所以学习路由就有可能出现很多下一跳。所以出现Link Local地址唯一标识一个节点。在本地链路看到下一跳都是对端的Link Local地址。这个地址一般是以fe80开头的，子网掩码为64，这个地方需要给机器配置一个唯一的全局单播地址</p>
<blockquote>
<p>However, with IPv6, all (IPv6) interfaces will have a link local address. This address is intended to allow communications over the attached links and so is defined to be usable only on that link.</p>
</blockquote>
<a id="more"></a>
<h2 id="二、网络配置">二、网络配置</h2><p>linux下用默认的网卡配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ifconfig </span></span><br><span class="line">eno16777736: flags=<span class="number">4163</span>&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu <span class="number">1500</span></span><br><span class="line">        inet <span class="number">192.168</span>.<span class="number">0.46</span>  netmask <span class="number">255.255</span>.<span class="number">0.0</span>  broadcast <span class="number">192.168</span>.<span class="number">255.255</span></span><br><span class="line">        inet6 fe80::<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x20&lt;link&gt;</span><br><span class="line">        ether <span class="number">00</span>:<span class="number">0</span>c:<span class="number">29</span>:c5:<span class="number">5</span>a:<span class="number">4</span>b  txqueuelen <span class="number">1000</span>  (Ethernet)</span><br><span class="line">        RX packets <span class="number">18422</span>  bytes <span class="number">1254119</span> (<span class="number">1.1</span> MiB)</span><br><span class="line">        RX errors <span class="number">0</span>  dropped <span class="number">6</span>  overruns <span class="number">0</span>  frame <span class="number">0</span></span><br><span class="line">        TX packets <span class="number">1938</span>  bytes <span class="number">890164</span> (<span class="number">869.3</span> KiB)</span><br><span class="line">        TX errors <span class="number">0</span>  dropped <span class="number">0</span> overruns <span class="number">0</span>  carrier <span class="number">0</span>  collisions <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<h3 id="取消NetworkManager管理">取消NetworkManager管理</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl <span class="built_in">disable</span> NetworkManager</span><br><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure>
<p>以免NetworkManager的干扰</p>
<p>这个地方我没有做自定义的IPV6的设置，让其默认的生成的地方，可以看到上面的node1的link local地址地址为 fe80::20c:29ff:fec5:5a4b<br>我的另外一台的地址为 fe80::20c:29ff:feda:6849</p>
<blockquote>
<p>node1 fe80::20c:29ff:fec5:5a4b  prefixlen 64<br>node2 fe80::20c:29ff:feda:6849 prefixlen 64</p>
</blockquote>
<p>这个地方都是没有单播地址的，需要配置一个</p>
<p>配置的时候关闭掉ipv4的IP，防止影响，确认配置的就是ipv6环境，去掉IPv4的配置即可，我的网卡配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">TYPE=<span class="string">"Ethernet"</span></span><br><span class="line">BOOTPROTO=<span class="string">"static"</span></span><br><span class="line">DEFROUTE=<span class="string">"yes"</span></span><br><span class="line">PEERDNS=<span class="string">"yes"</span></span><br><span class="line">PEERROUTES=<span class="string">"yes"</span></span><br><span class="line">NM_CONTROLLED=no</span><br><span class="line">IPV4_FAILURE_FATAL=<span class="string">"no"</span></span><br><span class="line">IPV6INIT=<span class="string">"yes"</span></span><br><span class="line">IPV6ADDR=<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b/<span class="number">64</span></span><br><span class="line">IPV6_AUTOCONF=no</span><br><span class="line">IPV6_DEFROUTE=<span class="string">"yes"</span></span><br><span class="line">IPV6_PEERDNS=<span class="string">"yes"</span></span><br><span class="line">IPV6_PEERROUTES=<span class="string">"yes"</span></span><br><span class="line">IPV6_FAILURE_FATAL=<span class="string">"no"</span></span><br><span class="line">NAME=<span class="string">"eno16777736"</span></span><br><span class="line">UUID=<span class="string">"0146f40c-6f4d-4c63-a9cd-7f89264613f3"</span></span><br><span class="line">DEVICE=<span class="string">"eno16777736"</span></span><br><span class="line">ONBOOT=<span class="string">"yes"</span></span><br></pre></td></tr></table></figure></p>
<p>检查配置情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ifconfig </span></span><br><span class="line">eno16777736: flags=<span class="number">4163</span>&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu <span class="number">1500</span></span><br><span class="line">        inet6 fe80::<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x20&lt;link&gt;</span><br><span class="line">        inet6 <span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b  prefixlen <span class="number">64</span>  scopeid <span class="number">0</span>x0&lt;global&gt;</span><br><span class="line">        ether <span class="number">00</span>:<span class="number">0</span>c:<span class="number">29</span>:c5:<span class="number">5</span>a:<span class="number">4</span>b  txqueuelen <span class="number">1000</span>  (Ethernet)</span><br><span class="line">        RX packets <span class="number">9133</span>  bytes <span class="number">597664</span> (<span class="number">583.6</span> KiB)</span><br><span class="line">        RX errors <span class="number">0</span>  dropped <span class="number">1</span>  overruns <span class="number">0</span>  frame <span class="number">0</span></span><br><span class="line">        TX packets <span class="number">466</span>  bytes <span class="number">137983</span> (<span class="number">134.7</span> KiB)</span><br><span class="line">        TX errors <span class="number">0</span>  dropped <span class="number">0</span> overruns <span class="number">0</span>  carrier <span class="number">0</span>  collisions <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到有两个inet6这样就是对的了</p>
<blockquote>
<p>windows远程ssh连接的方式:ssh 2008:20c:20c:20c:20c:29ff:fec5:5a4b</p>
</blockquote>
<h3 id="配置hosts">配置hosts</h3><p>在配置文件/etc/hosts中添加如下内容<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2008:20c:20c:20c:20c:29ff:fec5:5a4b node1&#10;2008:20c:20c:20c:20c:29ff:feda:6849 node2</span><br></pre></td></tr></table></figure></p>
<p>检测是否连通<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># ping6 -I eno16777736 2008:20c:20c:20c:20c:29ff:feda:6849</span></span><br></pre></td></tr></table></figure></p>
<p>ping主机名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]<span class="comment"># ping6 -I eno16777736 node2</span></span><br></pre></td></tr></table></figure></p>
<p>注意ping6需要加上网卡名称</p>
<p>同样的操作在node2上也配置好，网络到这里就配置好了</p>
<h2 id="三、集群配置">三、集群配置</h2><h3 id="创建初始配置文件">创建初始配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph-deploy new node1</span></span><br><span class="line">[root@node1 ceph]<span class="comment"># cat ceph.conf </span></span><br><span class="line">[global]</span><br><span class="line">fsid = f0bf4130<span class="operator">-f</span>4f0-<span class="number">4214</span>-<span class="number">8</span>b98-<span class="number">67103</span>ad55d65</span><br><span class="line">ms_<span class="built_in">bind</span>_ipv6 = <span class="literal">true</span></span><br><span class="line">mon_initial_members = node1</span><br><span class="line">mon_host = [<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b]</span><br><span class="line">auth_cluster_required =cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure>
<h3 id="创建mon">创建mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph-deploy mon create node1</span></span><br></pre></td></tr></table></figure>
<h3 id="检查状态">检查状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster d2882f75-<span class="number">1209</span>-<span class="number">4667</span>-bef8-<span class="number">3051</span>c84cb83c</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            no osds</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;node1=[<span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">29</span>ff:fec5:<span class="number">5</span>a4b]:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> node1</span><br><span class="line">     osdmap e8: <span class="number">0</span> osds: <span class="number">0</span> up, <span class="number">0</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v2664: <span class="number">0</span> pgs, <span class="number">0</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">0</span> kB used, <span class="number">0</span> kB / <span class="number">0</span> kB avail</span><br></pre></td></tr></table></figure>
<h3 id="检查端口">检查端口</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node1 ceph]<span class="comment"># netstat -tunlp|grep tcp6</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">22</span>                   :::*                    LISTEN      <span class="number">1155</span>/sshd           </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> ::<span class="number">1</span>:<span class="number">25</span>                  :::*                    LISTEN      <span class="number">1294</span>/master         </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> <span class="number">2008</span>:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">20</span>c:<span class="number">2</span>:<span class="number">6789</span> :::*                    LISTEN      <span class="number">8997</span>/ceph-mon</span><br></pre></td></tr></table></figure>
<p>可以看到集群已经正确的监听在了ipv6上了，后续的操作跟普通的IPV4集群一样的</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph/ipv6.jpeg" alt="ipv6"><br></center><br>一、前言<br>对于IPV6实在是非常的陌生，所以本篇开始会讲一下最基本的网络配置，首先让网络能通起来，最开始就是因为不熟悉IPV6,而直接使用了link local地址，造成了mon部署的时候进程无法绑定到IP，从而端口没有启动，这个是在ceph社区群友 <code>ceph-长沙-柠檬</code> 同学的帮助下才发现问题的</p>
<p>IPV6是会有个link local地址的，在一个接口可以配置很多IPv6地址，所以学习路由就有可能出现很多下一跳。所以出现Link Local地址唯一标识一个节点。在本地链路看到下一跳都是对端的Link Local地址。这个地址一般是以fe80开头的，子网掩码为64，这个地方需要给机器配置一个唯一的全局单播地址</p>
<blockquote>
<p>However, with IPv6, all (IPv6) interfaces will have a link local address. This address is intended to allow communications over the attached links and so is defined to be usable only on that link.</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph技能树]]></title>
    <link href="http://www.zphj1987.com/2016/10/17/Ceph%E6%8A%80%E8%83%BD%E6%A0%91/"/>
    <id>http://www.zphj1987.com/2016/10/17/Ceph技能树/</id>
    <published>2016-10-17T06:32:50.000Z</published>
    <updated>2016-10-31T06:51:57.542Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>
<a id="more"></a>
<h2 id="二、技能图">二、技能图</h2><p><div class="video-container"><embed src="https://coggle.it/diagram/WAR1HK8sAvMFM_kd/a0b529bd1fc87c52d12a2b18a5b602929218d60384ca9664cc243e1cbcf72649" allowfullscreen="true"><br></div><br>本图支持下载为pdf,PNG，或者mm文件，Chrome支持滚轮缩放，下载PDF效果还不错</p>
<h2 id="三、变更记录">三、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中国社区QQ群数据分析]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph中国社区QQ群数据分析/</id>
    <published>2016-10-13T15:59:58.000Z</published>
    <updated>2016-10-13T16:09:33.153Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>
<a id="more"></a>
<h2 id="三、数据展示">三、数据展示</h2><p>第一张图包含信息如下</p>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephchina/ceph%E7%A4%BE%E5%8C%BA.png" alt=""><br></center>
</li>
<li><p>男性居多</p>
</li>
<li>80后居多</li>
</ul>
<p>图示的好处就是告诉你，多，大概多成一个什么程度</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph的参数mon_osd_down_out_subtree_limit细解]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E7%9A%84%E5%8F%82%E6%95%B0mon-osd-down-out-subtree-limit%E7%BB%86%E8%A7%A3/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph的参数mon-osd-down-out-subtree-limit细解/</id>
    <published>2016-10-13T03:34:29.000Z</published>
    <updated>2016-10-13T03:53:59.910Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br><a id="more"></a><br>参数，那么这个参数做什么用的，我们来看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph --show-config|grep mon_osd_down_out_subtree_limit</span></span><br><span class="line">mon_osd_down_out_subtree_<span class="built_in">limit</span> = rack</span><br></pre></td></tr></table></figure></p>
<p>首先解释下这个参数是做什么的，这个是控制标记为out的最小子树(bucket)，默认的这个为rack，这个可能我们平时感知不到这个有什么作用，大部分情况下，我们一般都为主机分组或者做了故障域，也很少做到测试去触发它，本篇文章将告诉你这个参数在什么情况下生效，对我们又有什么作用</p>
<p>准备两个物理节点，每个节点上3个osd，一共六个osd，上面的down out的时间已经修改为20s，那么会在20s后出现out的情况</p>
<h2 id="三、测试过程">三、测试过程</h2><h3 id="3-1_测试默认参数停止一台主机单个OSD">3.1 测试默认参数停止一台主机单个OSD</h3><p>首先用默认的<code>mon_osd_down_out_subtree_limit = rack</code>去做测试<br>开启几个监控终端方便观察<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -w</span><br><span class="line">watch ceph osd tree</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/mutiscreen1.png" alt="screen"></p>
<p>在其中的一台上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:15:39.673898 mon.0 [INF] osd.5 out (down for 20.253201)&#10;2016-10-13 10:15:39.757399 mon.0 [INF] osdmap e60: 6 osds: 5 up, 5 in</span><br></pre></td></tr></table></figure></p>
<p>停止一个后正常out</p>
<h3 id="3-2_测试默认参数停止掉一台主机所有osd">3.2 测试默认参数停止掉一台主机所有osd</h3><p>我们再来停止一台主机所有osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:17:09.699129 mon.0 [INF] osd.3 out (down for 23.966959)&#10;2016-10-13 10:17:09.699178 mon.0 [INF] osd.4 out (down for 23.966958)&#10;2016-10-13 10:17:09.699222 mon.0 [INF] osd.5 out (down for 23.966958)</span><br></pre></td></tr></table></figure></p>
<p>可以看到这台主机上的节点全部都正常out了</p>
<h3 id="3-3_测试修改参数后停止一台主机单个OSD">3.3 测试修改参数后停止一台主机单个OSD</h3><p>我们再调整下参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = rack</span><br></pre></td></tr></table></figure></p>
<p>将这个参数设置为host<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = host</span><br></pre></td></tr></table></figure></p>
<p>重启所有的进程，让配置生效，我们测试下只断一个osd的时候能不能out<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>停止掉osd.5<br>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:48:45.612206 mon.0 [INF] osd.5 out (down for 21.966238)</span><br></pre></td></tr></table></figure></p>
<p>可以看到可以osd.5可以正常的out</p>
<h3 id="3-4_测试修改参数后停止一台主机所有OSD">3.4 测试修改参数后停止一台主机所有OSD</h3><p>我们再来停止lab8107的所有的osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>停止掉 lab8107 所有的osd,可以看到没有out了,这个是因为把故障out设置为host级别了，这个地方出现host级别故障的时候，就不进行迁移了</p>
<h2 id="四、总结">四、总结</h2><p>关键的地方在于总结了，首先我们要想一想，ceph机器的迁移开不开（noout），关于这个问题，一定有两个答案</p>
<ul>
<li>开，不开的话，盘再坏怎么办，就会丢数据了</li>
<li>不开，人工触发，默认的情况下迁移数据会影响前端业务</li>
</ul>
<p>这里这个参数其实就是将我们的问题更加细腻的控制了，我们现在根据这个参数就能做到，迁移可以开，坏掉一个盘的时候我让它迁移，一个盘的数据恢复影响和时间是可以接受的，主机损坏我不让他迁移，为什么？主机损坏你去让他迁移，首先会生成一份数据，等主机好了，数据又要删除一份数据，这个对于磁盘都是消耗，主机级别的故障一定是可修复的，这个地方主机down机，主机电源损坏，这部分数据都是在的，那么这个地方就是需要人工去做这个修复的工作的，对于前端的服务是透明的，默认的控制是down rack才不去标记out，这个当然你也可以控制为这个，比如有个rack掉电，就不做恢复，如果down了两台主机，让他去做恢复，当然个人不建议这么做，这个控制就是自己去判断这个地方需要做不</p>
<p>ceph里面还是提供了一些细微粒度的控制，值得去与实际的应用场景结合，当然默认的参数已经能应付大部分的场景，控制的更细只是让其变得更好</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[创建一个自定义名称的Ceph集群]]></title>
    <link href="http://www.zphj1987.com/2016/10/12/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E7%9A%84Ceph%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.zphj1987.com/2016/10/12/创建一个自定义名称的Ceph集群/</id>
    <published>2016-10-12T02:44:17.000Z</published>
    <updated>2016-10-12T02:45:39.610Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>
<a id="more"></a>
<h2 id="三、简单的创建过程">三、简单的创建过程</h2><p>创建mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph mon create lab8107</span><br></pre></td></tr></table></figure></p>
<p>获取部署密钥<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph gatherkeys lab8107</span><br></pre></td></tr></table></figure></p>
<p>部署osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph osd prepare lab8107:/dev/sdb</span><br><span class="line">ceph-deploy  --cluster myceph osd activate lab8107:/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>查询集群状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph --cluster myceph <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<h2 id="四、总结">四、总结</h2><p>最简单的修改名称主要步骤就在这里了，关键部分就是修改那几个文件里面的集群的名称，这个里面是用一个变量写成了ceph，根据自己的需要进行修改即可</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[编译一个支持多线程的php安装包]]></title>
    <link href="http://www.zphj1987.com/2016/10/10/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84php%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    <id>http://www.zphj1987.com/2016/10/10/编译一个支持多线程的php安装包/</id>
    <published>2016-10-10T04:27:18.000Z</published>
    <updated>2016-10-10T04:44:29.648Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br><a id="more"></a></p>
<h2 id="二、获取源码">二、获取源码</h2><p>地址：<br><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="symbol">https:</span>/<span class="regexp">/uk.repo.webtatic.com/yum</span><span class="regexp">/el7/</span><span class="constant">SRPMS/RPMS/</span></span><br></pre></td></tr></table></figure></p>
<p>这个是webtatic发行的php版本，做了一些修改和优化</p>
<p>选择需要的版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># wget https://uk.repo.webtatic.com/yum/el7/SRPMS/RPMS/php56w-5.6.26-1.w7.src.rpm</span></span><br></pre></td></tr></table></figure></p>
<p>解压安装包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpm2cpio php56w-5.6.26-1.w7.src.rpm |cpio -div</span></span><br></pre></td></tr></table></figure></p>
<p>解压完成了后，当前目录下面会有很多文件<br>修改当前目录下面的php56.spec<br>在编译相关的configure后面增加</p>
<blockquote>
<p>—enable-maintainer-zts</p>
</blockquote>
<p>拷贝解压和修改的文件到源码编译目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># cp -ra * /root/rpmbuild/SOURCES/</span></span><br></pre></td></tr></table></figure></p>
<h2 id="三、编译rpm包">三、编译rpm包</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpmbuild -bb php56.spec</span></span><br></pre></td></tr></table></figure>
<p>如果提示缺依赖，就把相关的依赖包安装好就可以了，编译环境最好跟最终使用环境是一样的环境，执行完成了以后，会生成rpm安装包</p>
<h2 id="四、增加多线程支持">四、增加多线程支持</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pecl install pthreads-<span class="number">2.0</span>.<span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>这个会下载源码，然后自动编译成可用的内核模块，将这个内核模块的配置文件和模块文件拷贝到最终使用环境即可</p>
<p>检查是否安装成功<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># php -m|grep pth</span></span><br><span class="line">pthreads</span><br></pre></td></tr></table></figure></p>
<p>可用看到已经支持了</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[fio测试数据的可视化]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/fio%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://www.zphj1987.com/2016/09/28/fio测试数据的可视化/</id>
    <published>2016-09-28T10:13:04.000Z</published>
    <updated>2016-09-28T10:17:48.586Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br><a id="more"></a></p>
<h2 id="图例">图例</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fio/fiokeshuhua.png" alt=""><br></center>

<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python生成csv中文乱码解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/Python%E7%94%9F%E6%88%90csv%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/28/Python生成csv中文乱码的解决办法/</id>
    <published>2016-09-28T06:36:43.000Z</published>
    <updated>2016-09-28T06:39:34.698Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br><a id="more"></a></p>
<h2 id="二、处理过程">二、处理过程</h2><h3 id="原始代码">原始代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line"><span class="comment">#import codecs</span></span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line"><span class="comment">#    csvfile.write(codecs.BOM_UTF8)</span></span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>运行以后：<br>Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/2cve2nr8jyy4chs7kvur5wwt/image_1atnnp5i41b7lf7tumgj6175k9.png" alt="image_1atnnp5i41b7lf7tumgj6175k9.png-4.3kB"></p>
<h3 id="修改代码">修改代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line">import codecs</span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line">    csvfile.write(codecs.BOM_UTF8)</span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>跟上面的代码相比，引入了两行代码</p>
<blockquote>
<p>import codecs<br>csvfile.write(codecs.BOM_UTF8)</p>
</blockquote>
<p>我们再来看效果Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/k9m15wfa83wbrhuc6b0xyftg/image_1atnnsp1713931d1h1e641l4f13kim.png" alt="image_1atnnsp1713931d1h1e641l4f13kim.png-3.5kB"><br>问题解决</p>
<h2 id="三、总结">三、总结</h2><p>网上找了一些资料，这个方式比较快而简单，就先用这个方式解决，方法有很多</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[不小心清空了Ceph的OSD的分区表如何恢复]]></title>
    <link href="http://www.zphj1987.com/2016/09/24/%E4%B8%8D%E5%B0%8F%E5%BF%83%E6%B8%85%E7%A9%BA%E4%BA%86Ceph%E7%9A%84OSD%E7%9A%84%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/09/24/不小心清空了Ceph的OSD的分区表如何恢复/</id>
    <published>2016-09-23T16:56:27.000Z</published>
    <updated>2016-09-23T17:07:47.286Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br><a id="more"></a></p>
<h2 id="二、实践过程">二、实践过程</h2><p>我们现在有一个正常的集群，我们假设这些分区都是一致的，用的是默认的分区的方式，我们先来看看默认的分区方式是怎样的</p>
<h3 id="2-1_破坏环境">2.1 破坏环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list</span></span><br><span class="line">···</span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 ceph data, active, cluster ceph, osd.<span class="number">0</span>, journal /dev/sdb2</span><br><span class="line"> /dev/sdb2 ceph journal, <span class="keyword">for</span> /dev/sdb1</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>查看分区情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>来一个破坏，这里是破坏 <code>osd.0</code>，对应盘符 <code>/dev/sdb</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy disk zap lab8106:/dev/sdb</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">34</span>): /usr/bin/ceph-deploy disk zap lab8106:/dev/sdb</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] Warning: The kernel is still using the old partition table.</span><br><span class="line">[lab8106][DEBUG ] The new table will be used at the next reboot.</span><br><span class="line">[lab8106][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or</span><br><span class="line">[lab8106][DEBUG ] other utilities.</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>即使这个 osd 被使用在，还是被破坏了，这里假设上面的就是一个误操作，我们看下带来了哪些变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">58</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">02</span> /var/lib/ceph/osd/ceph-<span class="number">0</span>/journal -&gt; /dev/disk/by-partuuid/bd81471d-<span class="number">13</span>ff-<span class="number">44</span>ce-<span class="number">8</span>a33-<span class="number">92</span>a8df9e8eee</span><br></pre></td></tr></table></figure></p>
<p>如果你用命令行看，就可以看到上面的链接已经变红了，分区没有了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list </span></span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line"> /dev/sdb2 other</span><br></pre></td></tr></table></figure></p>
<p>已经跟上面有变化了，没有ceph的相关信息了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br></pre></td></tr></table></figure></p>
<p>分区表完全没有信息了，到这我们可以确定分区表完全没了，如果现在重启将会发生什么？重启以后这个磁盘就是一个裸盘，没有分区的裸盘</p>
<h4 id="2-2_处理办法">2.2 处理办法</h4><p>首先一个办法就是当这个OSD坏了，然后直接按照删除节点，添加节点就可以了，这个应该是最主流，最通用的处理办法，但是这个在生产环境环境当中造成的数据迁移还是非常大的，我们尝试做恢复，这就是本篇主要讲的东西</p>
<h5 id="关闭迁移">关闭迁移</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd set noout</span></span><br></pre></td></tr></table></figure>
<h5 id="停止OSD">停止OSD</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>现在的OSD还是有进程的，所以需要停止掉再做处理<br>通过其他节点查看分区的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdc  unit s print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdc: <span class="number">585937500</span>s</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start     End         Size        File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">2048</span>s     <span class="number">2097152</span>s    <span class="number">2095105</span>s                 ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">2099200</span>s  <span class="number">585937466</span>s  <span class="number">583838267</span>s  xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>我们现在进行分区表的恢复，记住上面的数值，我print的时候是加了unit s这个是要精确的值的,下面的创建会用到的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2099200s 585937466s</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2048s 2097152s</span></span><br></pre></td></tr></table></figure>
<p>我们再来检查下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name     Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               primary</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          primary</span><br></pre></td></tr></table></figure></p>
<p>分区表已经回来了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># umount /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># partprobe</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新挂载看看，没有问题，还要做下其他的处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-0/journal</span></span><br></pre></td></tr></table></figure></p>
<p>我们先删除掉journal的链接文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-osd -i 0 --osd-journal=/dev/sdb2 --mkjournal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">09</span>-<span class="number">24</span> <span class="number">00</span>:<span class="number">36</span>:<span class="number">06.595992</span> <span class="number">7</span>f9d0afbc880 -<span class="number">1</span> created new journal /dev/sdb2 <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ln -s /dev/sdb2 /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">9</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">37</span> journal -&gt; /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>上面操作就是创建journal相关的,注意下我上面的操作—osd-journal=/dev/sdb2这个地方，我是便于识别，这个地方要写上dev/sdb2的uuid的路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /dev/disk/by-partuuid/03fc6039-ad80-4b8d-86ec-aeee14fb3bb6 </span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">33</span> /dev/disk/by-partuuid/<span class="number">03</span><span class="built_in">fc</span>6039-ad80-<span class="number">4</span>b8d-<span class="number">86</span>ec-aeee14fb3bb6 -&gt; ../../sdb2</span><br></pre></td></tr></table></figure></p>
<p>也就是这个链接的这一串，这个防止盘符串了情况下journal无法找到的问题</p>
<h4 id="启动osd">启动osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># systemctl start ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>检查下，到这osd就正常的恢复了</p>
<h2 id="三、为什么有这篇">三、为什么有这篇</h2><p>一直都知道分区表是可以恢复的，也一直知道会有误操作，但是一直没有去把ceph中完整流程走下来，前两天一个哥们环境副本一，然后自己给搞错了，出现不得不恢复的情况，正好自己一直想把这个问题的处理办法给记录下来，所以就有了这篇，万一哪天有人碰到了，就把这篇发给他</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-24</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph的Mon数据重新构建工具]]></title>
    <link href="http://www.zphj1987.com/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/"/>
    <id>http://www.zphj1987.com/2016/09/20/Ceph的Mon数据重新构建工具/</id>
    <published>2016-09-20T08:09:53.000Z</published>
    <updated>2016-10-12T09:11:02.645Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心 Mon 万一三个同时都挂掉了怎么办，那么集群所有的数据是不是都丢了，关于后台真实数据恢复，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，Ceph 的中国（Redhat）的一位开发者 <a href="https://github.com/tchaikov" target="_blank" rel="external">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到 Ceph 的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292" target="_blank" rel="external">issue</a></p>
<a id="more"></a>
<h2 id="打包一个合进新代码的master版本的ceph包">打包一个合进新代码的master版本的ceph包</h2><h3 id="从github上面获取代码">从github上面获取代码</h3><p>默认的分支就是master的直接去clone就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># git clone https://github.com/ceph/ceph.git</span></span><br></pre></td></tr></table></figure></p>
<h3 id="检查是否是master分支">检查是否是master分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># git branch</span></span><br><span class="line">* master</span><br></pre></td></tr></table></figure>
<h3 id="检查代码是否是合进需要的代码了">检查代码是否是合进需要的代码了</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat ceph/doc/rados/troubleshooting/troubleshooting-mon.rst |grep rebuild</span></span><br><span class="line">  <span class="comment"># rebuild the monitor store from the collected map, if the cluster does not</span></span><br><span class="line">  <span class="comment"># i.e. use "ceph-monstore-tool /tmp/mon-store rebuild" instead</span></span><br><span class="line">  ceph-monstore-tool /tmp/mon-store rebuild -- --keyring /path/to/admin.keyring</span><br><span class="line"><span class="comment">#. then rebuild the store</span></span><br></pre></td></tr></table></figure>
<p>因为这个代码是最近才合进去的 ，所以一定要检查代码的正确性</p>
<h3 id="创建一个源码包">创建一个源码包</h3><p>进入到代码的根目录，修改make-dist文件里面的一个地方(第46行)，否则打出来的包可能没有版本号，因为打包的时候检查了有没有git目录<br>修改下面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tar cvf $outfile.version.tar $outfile/src/.git_version $outfile/src/ceph_ver.h $outfile/ceph.spec</span></span><br><span class="line">tar cvf <span class="variable">$outfile</span>.version.tar <span class="variable">$outfile</span>/src/.git_version <span class="variable">$outfile</span>/src/ceph_ver.h <span class="variable">$outfile</span>/ceph.spec <span class="variable">$outfile</span>/.git</span><br></pre></td></tr></table></figure></p>
<h4 id="如果不改，就可能出现">如果不改，就可能出现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version HEAD-HASH-NOTFOUND (GITDIR-NOTFOUND)</span><br></pre></td></tr></table></figure>
<h4 id="创建源码包">创建源码包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment">#cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#./make-dist</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp ceph-11.0.0-2460-g22053d0.tar.bz2 /root/rpmbuild/SOURCES/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp -f ceph.spec /root/rpmbuild/SPECS/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># rpmbuild -bb /root/rpmbuild/SPECS/ceph.spec</span></span><br></pre></td></tr></table></figure>
<p>执行完了以后就去这个路径取包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /root/rpmbuild/RPMS/x86_64/</span></span><br><span class="line">total <span class="number">1643964</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">1972</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">42259096</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-base-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">320843080</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">35</span> ceph-common-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">58138088</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">36</span> ceph-mds-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<h3 id="准备测试环境">准备测试环境</h3><p>使用打好的包进行集群的配置，创建一个正常的集群，这里就不讲述怎么配置集群了</p>
<h4 id="模拟mon损坏">模拟mon损坏</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-mon@lab8106</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mv /var/lib/ceph/mon/ceph-lab8106/  /var/lib/ceph/mon/ceph-lab8106bk</span></span><br></pre></td></tr></table></figure>
<p>按上面的操作以后，mon的数据相当于全部丢失了，本测试环境是单mon的，多mon原理一样</p>
<h4 id="重构数据">重构数据</h4><p>创建一个临时目录,停止掉所有的osd，这个地方因为mon已经完全挂掉了,所以停止所有osd也没什么大的影响了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mkdir /tmp/mon-store</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br></pre></td></tr></table></figure></p>
<p>注意如果有多台OSD机器，那么在一台台的OSD主机进行上面的操作，这个目录的数据要保持递增的，也就是一直对着这个目录弄，假如换了一台机器那么先把这个数据传递到另外一台机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8106 ~]<span class="comment"># rsync -avz /tmp/mon-store 192.168.8.107:/tmp/mon-store</span></span><br><span class="line">sending incremental file list</span><br><span class="line">created directory /tmp/mon-store</span><br><span class="line">mon-store/</span><br><span class="line">mon-store/kv_backend</span><br><span class="line">mon-store/store.db/</span><br><span class="line">mon-store/store.db/<span class="number">000005</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000008</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000009</span>.log</span><br><span class="line">mon-store/store.db/CURRENT</span><br><span class="line">mon-store/store.db/LOCK</span><br><span class="line">mon-store/store.db/MANIFEST-<span class="number">000007</span></span><br><span class="line"></span><br><span class="line">sent <span class="number">11490</span> bytes  received <span class="number">153</span> bytes  <span class="number">7762.00</span> bytes/sec</span><br><span class="line">total size is <span class="number">74900</span>  speedup is <span class="number">6.43</span></span><br></pre></td></tr></table></figure></p>
<p>等192.168.8.106的机器全部做完了，然后这个/tmp/mon-store传递到了192.168.8.107的机器上，然后再开始做192.168.8.107这台机器的，等全部做外了，把这个/tmp/mon-store弄到需要恢复mon的机器上</p>
<h3 id="根据获得的数据进行重构">根据获得的数据进行重构</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-monstore-tool /tmp/mon-store rebuild</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cp -ra /tmp/mon-store/* /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/done</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/systemd</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph -R /var/lib/ceph/mon/</span></span><br></pre></td></tr></table></figure>
<h3 id="启动mon">启动mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-mon@lab8106</span></span><br></pre></td></tr></table></figure>
<p>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到可以好了，在实践过程中，发现如果对修复的数据，马上进行破坏，再次进行修复的时候，就无法恢复了，应该是个bug，已经提交给作者 Issue:<a href="https://github.com/ceph/ceph/pull/11126" target="_blank" rel="external">11226</a></p>
<h3 id="无法恢复的数据">无法恢复的数据</h3><ul>
<li>pg settings: the full ratio and nearfull ratio 设置会丢失，这个无关紧要，再设置一次就可以了</li>
<li>MDS Maps: the MDS maps are lost.</li>
</ul>
<h2 id="总结">总结</h2><p>因为工具才出来，可能难免有些bug，这个是为未来提供一种恢复数据的方式，使得 Ceph 变得更加的健壮</p>
<h2 id="附加知识">附加知识</h2><p>如果指定ceph版本进行编译<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph.git</span><br><span class="line">git checkout -b myceph v10.<span class="number">2.3</span></span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure></p>
<p>v10.2.3为发行版本的tag，也就是release的版本号码，这个操作是切换到指定的tag，并且下载依赖的一些模块</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-20</td>
</tr>
<tr>
<td style="text-align:center">增加git版本选择</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-12</td>
</tr>
</tbody>
</table>
<h2 id="For_me">For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心 Mon 万一三个同时都挂掉了怎么办，那么集群所有的数据是不是都丢了，关于后台真实数据恢复，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，Ceph 的中国（Redhat）的一位开发者 <a href="https://github.com/tchaikov">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到 Ceph 的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292">issue</a></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[替换OSD操作的优化与分析]]></title>
    <link href="http://www.zphj1987.com/2016/09/19/%E6%9B%BF%E6%8D%A2OSD%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BC%98%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/09/19/替换OSD操作的优化与分析/</id>
    <published>2016-09-19T02:56:54.000Z</published>
    <updated>2016-09-19T03:07:09.282Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/" target="_blank" rel="external">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br><a id="more"></a></p>
<h2 id="二、测试三种方法">二、测试三种方法</h2><h3 id="方法一：首先_out_一个_OSD，然后剔除_OSD，然后增加_OSD">方法一：首先 out 一个 OSD，然后剔除 OSD，然后增加 OSD</h3><ol>
<li>停止指定 OSD 进程</li>
<li>out 指定 OSD</li>
<li>crush remove 指定 OSD</li>
<li>增加一个新的 OSD</li>
</ol>
<p>一般生产环境会设置为 <code>noout</code>，当然不设置也可以，那就交给程序去控制节点的 out，默认是在进程停止后的五分钟，总之这个地方如果有 out 触发，不管是人为触发，还是自动触发数据流是一定的，我们这里为了便于测试，使用的是人为触发，上面提到的预制环境就是设置的 <code>noout</code></p>
<p>开始测试前获取最原始的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布,保存到文件pg1.txt，这个 PG 分布记录是 PG 所在的 OSD，记录下来，方便后面进行比较，从而得出需要迁移的数据 </p>
<h4 id="停止指定的_OSD_进程">停止指定的 OSD 进程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@15</span></span><br></pre></td></tr></table></figure>
<p>停止进程并不会触发迁移，只会引起 PG 状态的变化，比如原来主 PG 在停止的 OSD 上，那么停止掉 OSD 以后，原来的副本的那个 PG 就会角色升级为主 PG 了</p>
<h4 id="out_掉一个_OSD">out 掉一个 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd out 15</span></span><br></pre></td></tr></table></figure>
<p>在触发 out 以前，当前的 PG 状态应该有 <code>active+undersized+degraded</code>,触发 out 以后，所有的 PG 的状态应该会慢慢变成 <code>active+clean</code>,等待集群正常后，再次查询当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg2.txt</span></span><br></pre></td></tr></table></figure></p>
<p>保存当前的 PG 分布为pg2.txt<br>比较 out 前后的 PG 的变化情况，下面是比较具体的变化情况，只列出变化的部分<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们关心的是变动的数目，只统计变动的 PG 的数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">102</span></span><br></pre></td></tr></table></figure></p>
<p>第一次 out 以后有102个 PG 的变动,这个数字记住，后面的统计会用到</p>
<h4 id="从_crush_里面删除_OSD">从 crush 里面删除 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<p>crush 删除以后同样会触发迁移，等待 PG 的均衡，也就是全部变成 <code>active+clean</code> 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布的状态<br>现在来比较 crush remove 前后的 PG 变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg2.txt pg3.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">137</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新加上新的 OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure></p>
<p>加完以后统计当前的新的 PG 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg4.txt</span></span><br></pre></td></tr></table></figure></p>
<p>比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg3.txt pg4.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">167</span></span><br></pre></td></tr></table></figure></p>
<p>整个替换流程完毕，统计上面的 PG 总的变动</p>
<blockquote>
<p>102 +137 +167 = 406</p>
</blockquote>
<p>也就是按这个方法的变动为406个 PG，因为是只有双主机，里面可能存在某些放大问题，这里不做深入的讨论，因为我的三组测试环境都是一样的情况，只做横向比较，原理相通，这里是用数据来分析出差别</p>
<h3 id="方法二：先crush_reweight_0_，然后out，然后再增加osd">方法二：先crush reweight 0 ，然后out，然后再增加osd</h3><p>首先恢复环境为测试前的环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>记录最原始的 PG 分布情况</p>
<h4 id="crush_reweight_指定OSD">crush reweight 指定OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.16 0</span></span><br><span class="line">reweighted item id <span class="number">16</span> name <span class="string">'osd.16'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>等待平衡了以后记录当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg2.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<p>比较前后的变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg1.txt 2pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">166</span></span><br></pre></td></tr></table></figure></p>
<h4 id="crush_remove_指定_OSD">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.16</span></span><br><span class="line">removed item id <span class="number">16</span> name <span class="string">'osd.16'</span> from crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为上面 crush 已经是0了所以删除也不会引起 PG 变动<br>然后直接 <code>ceph osd rm osd.16</code> 同样没有 PG 变动</p>
<h4 id="增加新的_OSD">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>等待平衡以后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>来比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg2.txt 2pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">159</span></span><br></pre></td></tr></table></figure></p>
<p>总的 PG 变动为</p>
<blockquote>
<p>166+159=325</p>
</blockquote>
<h3 id="方法3：开始做norebalance，然后做crush_remove，然后做add">方法3：开始做norebalance，然后做crush remove，然后做add</h3><p>恢复环境为初始环境，然后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg1.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<h4 id="给集群做多种标记，防止迁移">给集群做多种标记，防止迁移</h4><p>设置为 norebalance，nobackfill，norecover,后面是有地方会解除这些设置的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norebalance</span></span><br><span class="line"><span class="built_in">set</span> norebalance</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set nobackfill</span></span><br><span class="line"><span class="built_in">set</span> nobackfill</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norecover</span></span><br><span class="line"><span class="built_in">set</span> norecover</span><br></pre></td></tr></table></figure></p>
<h4 id="crush_reweight_指定_OSD">crush reweight 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.15 0</span></span><br><span class="line">reweighted item id <span class="number">15</span> name <span class="string">'osd.15'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为已经做了上面的标记，所以只会出现状态变化，而没有真正的迁移，我们也先统计一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg2.txt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg2.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">158</span></span><br></pre></td></tr></table></figure></p>
<p>注意这里只是计算了，并没有真正的数据变动，可以通过监控两台的主机的网络流量来判断,所以这里的变动并不用计算到需要迁移的 PG 数目当中</p>
<h4 id="crush_remove_指定_OSD-1">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<h4 id="删除指定的_OSD">删除指定的 OSD</h4><p>删除以后同样是没有 PG 的变动的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd rm osd.<span class="number">15</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方有个小地方需要注意一下，不做 ceph auth del osd.15 把15的编号留着，这样好判断前后的 PG 的变化，不然相同的编号，就无法判断是不是做了迁移了</p>
<h4 id="增加新的_OSD-1">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>我的环境下，新增的 OSD 的编号为16了</p>
<h4 id="解除各种标记">解除各种标记</h4><p>我们放开上面的设置，看下数据的变动情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norebalance</span></span><br><span class="line"><span class="built_in">unset</span> norebalance</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset nobackfill</span></span><br><span class="line"><span class="built_in">unset</span> nobackfill</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norecover</span></span><br><span class="line"><span class="built_in">unset</span> norecover</span><br></pre></td></tr></table></figure></p>
<p>设置完了后数据才真正开始变动了，可以通过观察网卡流量看到，来看下最终pg变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg3.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">195</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们只需要跟最开始的 PG 分布状况进行比较就可以了，因为中间的状态实际上都没有做数据的迁移，所以不需要统计进去，可以看到这个地方动了195个 PG<br>总共的 PG 迁移量为</p>
<blockquote>
<p>195</p>
</blockquote>
<h2 id="三、数据汇总">三、数据汇总</h2><p>现在通过表格来对比下三种方法的迁移量的比较(括号内为迁移 PG 数目)</p>
<table>
<thead>
<tr>
<th style="text-align:center">　</th>
<th style="text-align:left">方法一</th>
<th style="text-align:left">方法二</th>
<th style="text-align:left">方法三</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">所做操作</td>
<td style="text-align:left">stop osd (0)<br>out osd(102)<br>crush remove osd (137)<br> add osd(167)</td>
<td style="text-align:left">crush reweight osd(166)<br>out osd(0)<br>crush remove osd (0)<br>add osd(159)</td>
<td style="text-align:left">set 标记(0)<br>crush reweight osd(0)<br>crush remove osd (0)<br>add osd(195)</td>
</tr>
<tr>
<td style="text-align:center">PG迁移数量</td>
<td style="text-align:left">406</td>
<td style="text-align:left">325</td>
<td style="text-align:left">195</td>
</tr>
</tbody>
</table>
<p>可以很清楚的看到三种不同的方法，最终的触发的迁移量是不同的，处理的好的话，能节约差不多一半的迁移的数据量，这个对于生产环境来说还是很好的，关于这个建议先在测试环境上进行测试，然后再操作，上面的操作只要不对磁盘进行格式化，操作都是可逆的，也就是可以比较放心的做，记住所做的操作，每一步都做完都去检查 PG 的状态是否是正常的</p>
<h2 id="四、总结">四、总结</h2><p>从我自己的操作经验来看，最开始是用的第一种方法，后面就用第二种方法减少了一部分迁移量，最近看到资料写做剔除OSD的时候可以关闭迁移防止无效的过多的迁移，然后就测试了一下，确实能够减少不少的迁移量，这个减少在某些场景下还是很好的，当然如果不太熟悉，用哪一种都可以，最终能达到的目的是一样的</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-19</td>
</tr>
</tbody>
</table>
<h2 id="六、For_me">六、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos7下Jewel版本radosgw服务启动]]></title>
    <link href="http://www.zphj1987.com/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/09/12/Centos7下Jewel版本radosgw服务启动/</id>
    <published>2016-09-12T05:47:47.000Z</published>
    <updated>2016-09-12T06:00:17.703Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>
<a id="more"></a>
<h2 id="二、预备环境">二、预备环境</h2><h3 id="一个完整的集群">一个完整的集群</h3><p>拥有一个正常的集群是需要提前准备好的，ceph -s检查正确的输出</p>
<h3 id="关闭各种auth">关闭各种auth</h3><p>这个地方也可以不关闭，注意配置好用户认证就可以了，这里关闭了，配置起来方便，我是从来不开的,也避免了新手不会配置用户造成认证的各种异常<br>关闭认证就是在ceph.conf里面添加下面字段<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br></pre></td></tr></table></figure></p>
<h3 id="安装ceph-radosgw的包">安装ceph-radosgw的包</h3><p>这个因为默认不会安装，所以要安装好<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<h2 id="三、默认启动过程">三、默认启动过程</h2><p>我们先什么都不配置，看下一般的会怎么处理</p>
<h3 id="3-1_启动服务">3.1 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-radosgw.target</span><br></pre></td></tr></table></figure>
<h3 id="3-2_检查服务的状态">3.2 检查服务的状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw.target </span></span><br><span class="line">● ceph-radosgw.target - ceph target allowing to start/stop all ceph-radosgw@.service instances at once</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw.target; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> CST; <span class="number">51</span>s ago</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Stopping ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Starting ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">51</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br></pre></td></tr></table></figure>
<p>可以看到进程是启动的，没有任何异常</p>
<h3 id="3-3_检查端口是否启动">3.3 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br></pre></td></tr></table></figure>
<p>但是并没有生成任何端口，这个是因为还没有配置实例,这个地方就是新手经常卡住的地方</p>
<h2 id="四、下面开始配置默认单实例">四、下面开始配置默认单实例</h2><h3 id="4-1_写配置文件">4.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写一个最简配置文件<br>注意下面的client.radosgw1这个包起来的，这个是固定写法，在 <code>systemctl</code> 启动服务的时候 <code>@</code> 取后面的radosgw1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4-2_启动服务">4.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3_检查服务状态">4.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> CST; <span class="number">17</span>s ago</span><br><span class="line"> Main PID: <span class="number">19996</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">19996</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="4-4_检查端口是否启动">4.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7480</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">19996</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到默认的端口是7480</p>
<h2 id="五、配置多个自定义端口实例">五、配置多个自定义端口实例</h2><h3 id="5-1_写配置文件">5.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写下配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7481</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[client.radosgw2]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7482</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方配置两个实例，用了不同的名称，用了不同的端口</p>
<h3 id="5-2_启动服务">5.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw2</span></span><br></pre></td></tr></table></figure>
<h3 id="5-3_检查服务状态">5.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> CST; <span class="number">1</span>min <span class="number">4</span>s ago</span><br><span class="line"> Main PID: <span class="number">20509</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">20509</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw2</span></span><br><span class="line">● ceph-radosgw@radosgw2.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> CST; <span class="number">1</span>min <span class="number">3</span>s ago</span><br><span class="line"> Main PID: <span class="number">20696</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw2.service</span><br><span class="line">           └─<span class="number">20696</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw2 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="5-4_检查端口是否启动">5.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7481</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20509</span>/radosgw       </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7482</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20696</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到服务和端口都能正常的启动了</p>
<p>好了，关于centos7下jewel版本的radosgw配置的启动已经介绍完了，这里不涉及更多深入的东西，其他的东西可以参照其他文档配置即可，这个地方只是对启动服务这里专门的介绍一下</p>
<h2 id="六、总结">六、总结</h2><p>从上面的过程可以看出大致的流程如下</p>
<ul>
<li>安装软件</li>
<li>启动服务</li>
<li>检查服务状态</li>
<li>检查服务端口</li>
</ul>
<p>这些很多都是基础的做法，在centos7下面虽然比6做了一些改变，但是掌握了一些通用的排查方法后，是很容易举一反三的，因为看到有新手不熟悉启动，所以写下这篇文章，自己因为也没经常用，所以也写下当个笔记了</p>
<h2 id="七、For_me">七、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
