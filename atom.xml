<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[但行好事，莫问前程]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2017-03-23T15:08:10.520Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[编译的Ceph二进制文件过大问题]]></title>
    <link href="http://www.zphj1987.com/2017/03/23/compile-ceph-binary-big/"/>
    <id>http://www.zphj1987.com/2017/03/23/compile-ceph-binary-big/</id>
    <published>2017-03-23T15:01:23.000Z</published>
    <updated>2017-03-23T15:08:10.520Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/codebug.jpg" alt="binary"><br></center>

<h2 id="前言">前言</h2><p>在ceph的研发群里看到一个cepher提出一个问题，编译的ceph的二进制文件过大，因为我一直用的打包好的rpm包，没有关注这个问题，重新编译了一遍发现确实有这个问题</p>
<p>本篇就是记录如何解决这个问题的<br><a id="more"></a></p>
<h2 id="打rpm包的方式">打rpm包的方式</h2><p>用我自己的环境编译的时候发现一个问题，编译出来的rpm包还是很大，开始怀疑是机器的原因，换了一台发现二进制包就很小了，然后查询了很多资料以后，找到了问题所在</p>
<p>在打rpm包的时候可以通过宏变量去控制是否打出一个的debug的包，这个包的作用就是把二进制文件当中包含的debug的相关的全部抽离出来形成一个新的rpm包，而我的环境不知道什么时候在/root/.rpmmacros添加进去了一个<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">d%ebug_package      %&#123;nil&#125;</span><br></pre></td></tr></table></figure></p>
<p>搜寻资料后确定就是这个的问题,这个变量添加了以后，在打包的时候就不会进行debug相关包的剥离，然后打出的包就是巨大的，可以这样检查自己的rpmbuild的宏变量信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment">#  rpmbuild --showrc|grep debug_package</span></span><br><span class="line">    %&#123;!?__debug_package:</span><br><span class="line">    %&#123;?__debug_package:%&#123;__debug_install_post&#125;&#125;</span><br><span class="line">-<span class="number">14</span>: _<span class="built_in">enable</span>_debug_packages	<span class="number">1</span></span><br><span class="line">-<span class="number">14</span>: debug_package	</span><br><span class="line">%global __debug_package <span class="number">1</span></span><br><span class="line">-<span class="number">14</span>: install	%&#123;?_<span class="built_in">enable</span>_debug_packages:%&#123;?buildsubdir:%&#123;debug_package&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果开启了debug包抽离（默认就是开启的），那么rpmbuild在打包的过程中会有个调用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/rpm/find-debuginfo.sh --strict-build-id -m --run-dwz --dwz-low-mem-die-limit <span class="number">10000000</span> --dwz-max-die-limit <span class="number">110000000</span> /root/rpmbuild/BUILD/ceph-<span class="number">10.2</span>.<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>这个就是rpmbuild过程中，进行抽离debug信息的操作，也就是缩小二进制的过程，这个并不能直接执行命令，需要用rpmbuild -bb ceph.spec 打包的时候内部自动进行调用的</p>
<p>上面是rpm打包过程中进行的二进制缩小，那么如果我们是源码编译安装时候，如何缩小这个二进制，答案当然是可以的</p>
<h2 id="源码编译安装的方式">源码编译安装的方式</h2><p>./configure 后make生成的二进制文件就在./src下面了<br>我们以ceph-mon为例进行抽离</p>
<p>这个-O3并没有影响到太多的生成的二进制的大小，—with-debug会有一定的影响，关键还是strip的这个操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./configure --with-debug  CXXFLAGS=-O3 CFLAGS=-O3 CCASFLAGS=-O3</span><br></pre></td></tr></table></figure></p>
<p>所以默认的就行</p>
<p>如果整体进行安装就使用make install-strip安装即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># make install-strip</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll /usr/local/bin/ceph-osd</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14266576</span> Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">57</span> /usr/<span class="built_in">local</span>/bin/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll /usr/local/bin/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">57</span> /usr/<span class="built_in">local</span>/bin/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">248</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">54</span> src/ceph-osd</span><br></pre></td></tr></table></figure></p>
<h2 id="关键的strip的用法">关键的strip的用法</h2><p>gcc编译的时候带上-g参数,就是添加了debug的信息</p>
<blockquote>
<p>gcc -g -o</p>
</blockquote>
<h3 id="分离debug_information">分离debug information</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment">#objcopy --only-keep-debug src/ceph-osd src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">248</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">54</span> src/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd.debug -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">235</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">08</span> src/ceph-osd.debug</span><br></pre></td></tr></table></figure>
<p>另外一种方法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># cp src/ceph-osd src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># strip --only-keep-debug src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd.debug -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">235</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">10</span> src/ceph-osd.debug</span><br></pre></td></tr></table></figure></p>
<h3 id="从原始文件去掉_debug_information">从原始文件去掉 debug information</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># objcopy --strip-debug src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">18</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">11</span> src/ceph-osd</span><br><span class="line">objcopy --strip-debug main</span><br></pre></td></tr></table></figure>
<p>另外一种方法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># strip --strip-debug --strip-unneeded src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">12</span> src/ceph-osd</span><br></pre></td></tr></table></figure></p>
<h3 id="启用debuglink模式">启用debuglink模式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># objcopy --add-gnu-debuglink  src/ceph-osd.debug src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># gdb src/ceph-osd</span></span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># gdb -s src/ceph-osd.debug -e src/ceph-osd</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>二进制包里面包含了debug的一些相关信息，可以通过strip的方式将内部的debug内容清理掉，这样就可以得到比较小的二进制包了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-23</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/codebug.jpg" alt="binary"><br></center>

<h2 id="前言">前言</h2><p>在ceph的研发群里看到一个cepher提出一个问题，编译的ceph的二进制文件过大，因为我一直用的打包好的rpm包，没有关注这个问题，重新编译了一遍发现确实有这个问题</p>
<p>本篇就是记录如何解决这个问题的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph实现数据的'不拆分']]></title>
    <link href="http://www.zphj1987.com/2017/03/22/ceph-no-distribute-all/"/>
    <id>http://www.zphj1987.com/2017/03/22/ceph-no-distribute-all/</id>
    <published>2017-03-22T07:49:16.000Z</published>
    <updated>2017-03-22T09:02:35.391Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octopus.png" alt="oct"><br></center>

<h2 id="前言">前言</h2><p>之前看过一个朋友一篇文章，讲述的是Vsan为什么使用的是两副本，而ceph则大多数情况下需要三副本，当时个人观点是这个并不是关键点，但是在仔细考虑了问题的出发点以后，这个也可以说是其中的一个点<br><a id="more"></a><br>一个集群数据丢失可以从多方面去看</p>
<ul>
<li>发生丢失数据的事件，这个来说，出现这个事件的概率是一致的，同等硬件情况下没有谁的系统能够说在两副本情况下把这个出现坏盘概率做的比其他系统更低</li>
<li>发生坏盘事件以后，数据丢失波及的范围，这个就是那个朋友提出的一个观点，对于Vsan来说因为文件的不拆分，也就是在丢了的情况下，只是局部数据的丢失，而ceph的数据因为拆分到整个集群，基本上说就是全军覆没了，这一点没有什么争议</li>
</ul>
<p>一般来说，ceph都是配置的分布式文件系统，也就是数据以PG为组合，以对象为最小单元的形式分布到整个集群当中去，通过控制crush能够增加一定的可用概率，但是有没有办法实现真的丢盘的情况下，数据波及没有那么广，答案当然是有的，只是需要做一些更细微的控制，前端的使用的接口也需要做一定的改动，本篇将讲述这个如何去实现，以及前端可能需要的变动</p>
<h2 id="方案实现">方案实现</h2><p>首先来一张示意图，来介绍大致的实现方式，下面再给出操作步骤</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/doeql98mri7dues48uf8hk5y/osd%E4%B8%8D%E6%8B%86%E5%88%86.png" alt="osd不拆分.png-15.7kB"><br></center>

<p>主要包括三步</p>
<ul>
<li>横向划条带 </li>
<li>创建对应规则 </li>
<li>根据规则创建相关存储池</li>
</ul>
<h3 id="横向划条带">横向划条带</h3><p>创建虚拟根<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush add-bucket default<span class="operator">-a</span> root</span><br><span class="line">ceph osd crush add-bucket default-b root</span><br><span class="line">ceph osd crush add-bucket default-c root</span><br><span class="line">ceph osd crush add-bucket default<span class="operator">-d</span> root</span><br></pre></td></tr></table></figure></p>
<p>创建虚拟主机<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph  osd crush add-bucket host1<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host2<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host3<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host1-b host</span><br><span class="line">ceph  osd crush add-bucket host2-b host</span><br><span class="line">ceph  osd crush add-bucket host3-b host</span><br><span class="line">ceph  osd crush add-bucket host1-c host</span><br><span class="line">ceph  osd crush add-bucket host2-c host</span><br><span class="line">ceph  osd crush add-bucket host3-c host</span><br><span class="line">ceph  osd crush add-bucket host1<span class="operator">-d</span> host</span><br><span class="line">ceph  osd crush add-bucket host2<span class="operator">-d</span> host</span><br><span class="line">ceph  osd crush add-bucket host3<span class="operator">-d</span> host</span><br></pre></td></tr></table></figure></p>
<p>将虚拟主机挪到虚拟根里面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush move host1<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host2<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host3<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host1-b root=default-b</span><br><span class="line">ceph osd crush move host2-b root=default-b</span><br><span class="line">ceph osd crush move host3-b root=default-b</span><br><span class="line">ceph osd crush move host1-c root=default-c</span><br><span class="line">ceph osd crush move host2-c root=default-c</span><br><span class="line">ceph osd crush move host3-c root=default-c</span><br><span class="line">ceph osd crush move host1<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush move host2<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush move host3<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>将osd塞入到指定的bucker内<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd  crush create-or-move  osd.<span class="number">0</span> <span class="number">1.83</span>  host=host1<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">4</span> <span class="number">1.83</span>  host=host2<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">8</span> <span class="number">1.83</span>  host=host3<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">1</span> <span class="number">1.83</span>  host=host1-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">5</span> <span class="number">1.83</span>  host=host2-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">9</span> <span class="number">1.83</span>  host=host3-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">2</span> <span class="number">1.83</span>  host=host1-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">6</span> <span class="number">1.83</span>  host=host2-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">10</span> <span class="number">1.83</span>  host=host3-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">3</span> <span class="number">1.83</span>  host=host1<span class="operator">-d</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">7</span> <span class="number">1.83</span>  host=host2<span class="operator">-d</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">11</span> <span class="number">1.83</span>  host=host3<span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>查看现在的树<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID  WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line"> -<span class="number">8</span> <span class="number">5.44080</span> root default<span class="operator">-d</span>                                     </span><br><span class="line">-<span class="number">18</span> <span class="number">1.81360</span>     host host1<span class="operator">-d</span>                                   </span><br><span class="line">  <span class="number">3</span> <span class="number">1.81360</span>         osd.<span class="number">3</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">19</span> <span class="number">1.81360</span>     host host2<span class="operator">-d</span>                                   </span><br><span class="line">  <span class="number">7</span> <span class="number">1.81360</span>         osd.<span class="number">7</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">20</span> <span class="number">1.81360</span>     host host3<span class="operator">-d</span>                                   </span><br><span class="line"> <span class="number">11</span> <span class="number">1.81360</span>         osd.<span class="number">11</span>        up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">7</span> <span class="number">5.44080</span> root default-c                                     </span><br><span class="line">-<span class="number">15</span> <span class="number">1.81360</span>     host host1-c                                   </span><br><span class="line">  <span class="number">2</span> <span class="number">1.81360</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">16</span> <span class="number">1.81360</span>     host host2-c                                   </span><br><span class="line">  <span class="number">6</span> <span class="number">1.81360</span>         osd.<span class="number">6</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">17</span> <span class="number">1.81360</span>     host host3-c                                   </span><br><span class="line"> <span class="number">10</span> <span class="number">1.81360</span>         osd.<span class="number">10</span>        up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">6</span> <span class="number">5.44080</span> root default-b                                     </span><br><span class="line">-<span class="number">12</span> <span class="number">1.81360</span>     host host1-b                                   </span><br><span class="line">  <span class="number">1</span> <span class="number">1.81360</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">13</span> <span class="number">1.81360</span>     host host2-b                                   </span><br><span class="line">  <span class="number">5</span> <span class="number">1.81360</span>         osd.<span class="number">5</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">14</span> <span class="number">1.81360</span>     host host3-b                                   </span><br><span class="line">  <span class="number">9</span> <span class="number">1.81360</span>         osd.<span class="number">9</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">5</span> <span class="number">5.44080</span> root default<span class="operator">-a</span>                                     </span><br><span class="line"> -<span class="number">9</span> <span class="number">1.81360</span>     host host1<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">0</span> <span class="number">1.81360</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">10</span> <span class="number">1.81360</span>     host host2<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">4</span> <span class="number">1.81360</span>         osd.<span class="number">4</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">11</span> <span class="number">1.81360</span>     host host3<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">8</span> <span class="number">1.81360</span>         osd.<span class="number">8</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">1</span>       <span class="number">0</span> root default                                       </span><br><span class="line"> -<span class="number">2</span>       <span class="number">0</span>     host host1                                     </span><br><span class="line"> -<span class="number">3</span>       <span class="number">0</span>     host host2                                     </span><br><span class="line"> -<span class="number">4</span>       <span class="number">0</span>     host host3</span><br></pre></td></tr></table></figure></p>
<p>下面老的一些bucket可以清理掉<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool delete rbd rbd  --yes-i-really-really-mean-it</span><br><span class="line">ceph osd crush rule rm replicated_ruleset</span><br><span class="line">ceph osd crush remove host1</span><br><span class="line">ceph osd crush remove host2</span><br><span class="line">ceph osd crush remove host3</span><br><span class="line">ceph osd crush remove default</span><br></pre></td></tr></table></figure></p>
<h3 id="创建对应规则">创建对应规则</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush rule create-simple rule048  default<span class="operator">-a</span> host</span><br><span class="line">ceph osd crush rule create-simple rule159  default-b host</span><br><span class="line">ceph osd crush rule create-simple rule2610  default-c host</span><br><span class="line">ceph osd crush rule create-simple rule3711  default<span class="operator">-d</span> host</span><br></pre></td></tr></table></figure>
<p>检查下规则<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd crush rule dump|grep "rule_name\|item_name"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule048"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-a"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule159"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-b"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule2610"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-c"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule3711"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-d"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="根据规则创建相关存储池">根据规则创建相关存储池</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poola048 64 64 replicated rule048</span></span><br><span class="line">pool <span class="string">'poola048'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poolb159 64 64 replicated rule159</span></span><br><span class="line">pool <span class="string">'poolb159'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poolc2610 64 64 replicated rule2610</span></span><br><span class="line">pool <span class="string">'poolc2610'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poold3711 64 64 replicated rule3711</span></span><br><span class="line">pool <span class="string">'poold3711'</span> created</span><br></pre></td></tr></table></figure>
<p>检查存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">1</span> <span class="string">'poola048'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">145</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">2</span> <span class="string">'poolb159'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">1</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">147</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">3</span> <span class="string">'poolc2610'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">2</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">149</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">4</span> <span class="string">'poold3711'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">3</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">151</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>到这里基本的环境就配置好了，采用的是副本2，但是虚拟组里面留了三个osd，这个后面会解释</p>
<h2 id="如何使用">如何使用</h2><p>假设现在前端需要8个image用来使用了，那么我们创建的时候，就将这个8个平均分布到上面的四个存储里面去，这里是因为是划成了四个条带，在实际环境当中，可以根据需要进行划分，在选择用哪个存储的时候可以去用轮询的算法，进行轮询，也可以自定义去选择在哪个存储池创建，这个都是可以控制的</p>
<h3 id="创建image">创建image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd -p poola048 create image1 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poola048 create image2 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolb159 create image3 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolb159 create image4 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolc2610 create image6 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolc2610 create image7 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poold3711 create image8 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poold3711 create image9 --size <span class="number">1</span>G</span><br></pre></td></tr></table></figure>
<h3 id="如何跟virsh对接">如何跟virsh对接</h3><p>如果你熟悉virsh配置文件的话，可以看到rbd相关的配置文件是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">source</span> protocol=<span class="string">'rbd'</span> name=<span class="string">'volumes/volume-f20fd994-e600-41da-a6d8-6e216044dbb1'</span>&gt;</span><br><span class="line">        &lt;host name=<span class="string">'192.168.10.4'</span> port=<span class="string">'6789'</span>/&gt;</span><br><span class="line">&lt;/<span class="built_in">source</span>&gt;</span><br></pre></td></tr></table></figure></p>
<p>在cinder的相关配置当中虽然我们指定了volume这个存储池值是一个定值，在这个配置文件当中也就读取了这个值，那么需要改造的接口就是在创建云盘的时候，不去将cinder的存储池固定死，volumes/volume-f20fd994-e600-41da-a6d8-6e216044dbb1这样的值可以是上面的poola048/image1,也可以是poolc2610/image6,这个地方就是需要改动的地方，将整个值包含存储池的值作为一个变量，这个改动应该属于可改的</p>
<h2 id="分析">分析</h2><p>按上面的进行处理以后，那么再出现同时坏了两个盘的情况下，数据丢失的波及范围跟Vsan已经是一致了，因为数据打散也只是在这个三个里面打散了，真的出现磁盘损坏波及的也是局部的数据了</p>
<p>问题：<br>1、分布范围小了性能怎么样<br>比完全分布来说性能肯定降低了一些，但是如果说对于负载比较高的情况，每个盘都在跑的情况下，这个性能是一定的，底层的磁盘提供的带宽是一定的，这个跟VSAN一样的</p>
<p>并且这个上面所示的是极端的情况下的，缩小到3个OSD一组条带，也可以自行放宽到6个一个条带，这个只是提供了一种方法，缩小了波及范围</p>
<p>2、副本2为什么留3个osd一个条带<br>比副本数多1的话，这样在坏了一个盘也可以迁移，所以一般来说，至少比副本数多1的故障域</p>
<p>3、如何扩容<br>扩容就增加条带即可，并且可以把老的存储池规则指定到新的磁盘的条带上面</p>
<p>4、这个方法还可以用故障域增加可用性么<br>可以的，可以从每个故障域里面抽出OSD即可，只要保证底层的数据不重叠，实际是两个不同的需求</p>
<h2 id="总结">总结</h2><p>本篇是提供了一种可能性，在实际运行环境当中，可以根据自己的环境进行设计，设计的方法就是，假设一个数据的全部副本都丢了的情况，允许的数据波及范围是多少，如果拆分两份就是波及二分之一，我的测试环境是分成了四个条带，也就是只影响四分之一的数据</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-22</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octopus.png" alt="oct"><br></center>

<h2 id="前言">前言</h2><p>之前看过一个朋友一篇文章，讲述的是Vsan为什么使用的是两副本，而ceph则大多数情况下需要三副本，当时个人观点是这个并不是关键点，但是在仔细考虑了问题的出发点以后，这个也可以说是其中的一个点<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于Docker UI 配置ceph集群]]></title>
    <link href="http://www.zphj1987.com/2017/03/16/base-on-docker-ui-deploy-ceph/"/>
    <id>http://www.zphj1987.com/2017/03/16/base-on-docker-ui-deploy-ceph/</id>
    <published>2017-03-16T10:08:16.000Z</published>
    <updated>2017-03-16T10:18:33.594Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker2.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>前一篇介绍了docker在命令行下面进行的ceph部署，本篇用docker的UI进行ceph的部署，目前来说市面上还没有一款能够比较简单就能直接在OS上面去部署Ceph的管理平台，这是因为OS的环境差异化太大，并且包的版本，以及各种软件的适配都可能造成失败，而docker比较固化环境，因此即使一个通用的UI也能很方便的部署出一个Cpeh集群</p>
<p>本篇就是对Docker UI部署集群做一个实践，对ceph了解，对docker了解，对dokcer的UI操作进行一定的了解的情况下，再做实践会比较好，总体上还是比较简单的<br><a id="more"></a></p>
<h2 id="安装并运行portainer">安装并运行portainer</h2><h3 id="安装软件">安装软件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">wget https://github.com/portainer/portainer/releases/download/<span class="number">1.12</span>.<span class="number">1</span>/portainer-<span class="number">1.12</span>.<span class="number">1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xvpfz portainer-<span class="number">1.12</span>.<span class="number">1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> portainer</span><br></pre></td></tr></table></figure>
<h3 id="运行软件">运行软件</h3><figure class="highlight coffeescript"><table><tr><td class="code"><pre><span class="line">.<span class="regexp">/portainer -H unix:/</span><span class="regexp">//</span><span class="reserved">var</span>/run/docker.sock  -p <span class="string">":9999"</span></span><br></pre></td></tr></table></figure>
<p>注意下这里-H是指定的docker的连接，也就是要控制哪个docker，这个支持本地的sock的方式，也支持远程的tcp的方式，这个进入ui界面后还可以添加更多的<br>-p是指定的访问的接口</p>
<h3 id="扩展知识">扩展知识</h3><p>如何在centos7下面启用 remote api<br>打开文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure></p>
<p>在 <code>$INSECURE_REGISTRY</code> 后面添加 <code>-H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ExecStart=/usr/bin/dockerd-current \</span><br><span class="line">          --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \</span><br><span class="line">          --default-runtime=docker-runc \</span><br><span class="line">          --exec-opt native.cgroupdriver=systemd \</span><br><span class="line">          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \</span><br><span class="line">          <span class="variable">$OPTIONS</span> \</span><br><span class="line">          <span class="variable">$DOCKER_STORAGE_OPTIONS</span> \</span><br><span class="line">          <span class="variable">$DOCKER_NETWORK_OPTIONS</span> \</span><br><span class="line">          <span class="variable">$ADD_REGISTRY</span> \</span><br><span class="line">          <span class="variable">$BLOCK_REGISTRY</span> \</span><br><span class="line">          <span class="variable">$INSECURE_REGISTRY</span>  -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2376</span> -H unix:///var/run/docker.sock</span><br></pre></td></tr></table></figure></p>
<p>修改好了后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#systemctl daemon-reload</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#systemctl restart docker</span></span><br></pre></td></tr></table></figure></p>
<p>检查端口和asok<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep 2376</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">2376</span>                 :::*                    LISTEN      <span class="number">24484</span>/dockerd-curre </span><br><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/run/docker.sock</span></span><br><span class="line">srw-rw---- <span class="number">1</span> root root <span class="number">0</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">39</span> /var/run/docker.sock</span><br></pre></td></tr></table></figure></p>
<p>生成了配置没有问题</p>
<h4 id="portainer的自身数据">portainer的自身数据</h4><p>默认情况下portainer的数据是存储在/data目录下面的，如果想重新配置密码或者内容的话，删除这个目录里面的数据就行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /data/</span></span><br><span class="line">total <span class="number">24</span></span><br><span class="line">-rw------- <span class="number">1</span> root root <span class="number">32768</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">32</span> portainer.db</span><br><span class="line">drwx------ <span class="number">2</span> root root     <span class="number">6</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">32</span> tls</span><br></pre></td></tr></table></figure></p>
<h2 id="UI界面登陆">UI界面登陆</h2><p>直接访问宿主机的<code>http://ip:9999</code><br><img src="http://static.zybuluo.com/zphj1987/0qlepujxmrt4wqadoa01zfg1/image_1bbb4ogmqu1ir8049n1okfq4j9.png" alt="login"><br>输入一个8位数的密码<br>输入好了以后，登陆即可</p>
<p><img src="http://static.zybuluo.com/zphj1987/6grz7nooae5smur1bcb54lb6/image_1bbb4r1eb1qnj0pcjmsbf1ucgm.png" alt="endponit"></p>
<p>检查endpoint，可以看到就是我刚才命令行当中加入的sock</p>
<h2 id="获取image">获取image</h2><p><img src="http://static.zybuluo.com/zphj1987/zvhkf4ujoblfwfvzx6860fxt/image_1bbb4vs5h1ri522q8avkrb1ko716.png" alt="get ceph"></p>
<p>在上面填写<code>ceph/daemon</code> 然后点击pull</p>
<p>有可能会超时，如果多次失败，就去后台命令行执行，这个地方等同于后台的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull ceph/daemon</span><br></pre></td></tr></table></figure></p>
<p>也可以直接在后台执行这个命令<br>可以用dstat -n观察下载的速度</p>
<p>下载好了去页面上看下是否好了<br><img src="http://static.zybuluo.com/zphj1987/pxml52emnm8c3gkzzt90rh27/image_1bbb6c50tip1iud1gfv9m4uku1j.png" alt="download"></p>
<h2 id="配置CEPH集群">配置CEPH集群</h2><p>配置集群可以都在页面做了，因为之前有篇命令行部署docker的ceph，建议先回顾一下，再看这个比较好</p>
<h3 id="创建MON">创建MON</h3><p>点击增加容器<br><img src="http://static.zybuluo.com/zphj1987/myuenisz3pj34mawmdupfifp/image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p>
<p>注意创建好两个目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/ceph</span><br><span class="line">mkdir -p /var/lib/ceph/</span><br></pre></td></tr></table></figure></p>
<p>这两个目录里面不要有任何东西,保持空目录状态</p>
<p><img src="http://static.zybuluo.com/zphj1987/ul8yqkh5dlzx0t9jrkzc84jz/image_1bbb6pbf811pesikkrmemt9du2d.png" alt="ceph mon"></p>
<ul>
<li>填写名称为mon，这个是容器名称，可以自定义</li>
<li>填写Image，这个填写下载好的ceph/daemon</li>
<li>填写command,这个填写mon，为固定值</li>
<li>填写Entry Ponit ,这个填写/entrypoint.sh，为固定值</li>
<li>填写Environment variable，这个填写两个变量<ul>
<li>MON_IP  192.168.8.106</li>
<li>CEPH_PUBLIC_NETWORK 192.168.0.0/16</li>
</ul>
</li>
</ul>
<p>填写完了切换第二个标签页Volumes<br><img src="http://static.zybuluo.com/zphj1987/euivfncy3gw2h25wjyc42ien/image_1bbb6rsb01etg1ebt1hr317lo1met2q.png" alt="volume"></p>
<ul>
<li>填写Volume<ul>
<li>/etc/ceph /etc/ceph</li>
<li>/var/lib/ceph/ /var/lib/ceph/</li>
</ul>
</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/2goxtvodd8fbd2aap5n7uc62/image_1bbb6tgov1kvr1rcc1keg1e0a1i4537.png" alt="network"></p>
<ul>
<li>填写Network为host</li>
<li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li>
</ul>
<p>没出异常的话，就可以进入console进行查询了<br><img src="http://static.zybuluo.com/zphj1987/pqn0zofgbx078kul5wzvn3mw/image_1bbb726491l5it2d1kf31at614lb3k.png" alt="console"><br>点击connect<br><img src="http://static.zybuluo.com/zphj1987/wgvt131186es0erb9kys6mqc/image_1bbb73gjif91s70a6f8pg1vg141.png" alt="image_1bbb73gjif91s70a6f8pg1vg141.png-79.5kB"><br>没有问题</p>
<h3 id="创建OSD">创建OSD</h3><p>点击增加容器<br><img src="http://static.zybuluo.com/zphj1987/myuenisz3pj34mawmdupfifp/image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p>
<p><img src="http://static.zybuluo.com/zphj1987/v5kvkfunnbb0y3ueo95gptdr/image_1bbb7a1dm1gv1n4j1odoo3k1n2u4e.png" alt="osd0"></p>
<ul>
<li>填写Name，这个为容器名称，可以自定义</li>
<li>填写Image,这个为ceph/daemon,固定的值</li>
<li>填写command,这个为osd_ceph_disk，为定值</li>
<li>填写Entry Ponit ,这个填写/entrypoint.sh，为固定值</li>
<li>填写Environment variable，这个填写一个OSD磁盘变量<ul>
<li>OSD_DEVICE /dev/sdb</li>
</ul>
</li>
</ul>
<p>切换到第二个Volume标签页</p>
<ul>
<li>填写Volume<ul>
<li>/etc/ceph /etc/ceph</li>
<li>/var/lib/ceph/ /var/lib/ceph/</li>
<li>/dev/ /dev/</li>
</ul>
</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/3r350azqxged9dix7yaesia6/image_1bbb7aqg21jso1ku51mdgajtr0p4r.png" alt="osd0 add"></p>
<p>切换到Network标签页</p>
<ul>
<li>填写Network为host</li>
<li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/7l50q9mdjffu2qmo0pwsl7yl/image_1bbb7c5d17b21o1uoc1i7h1cr458.png" alt="osdsd add"><br>切换到Security/Host标签页<br>勾选上 <code>privileged</code>,一定要选上，不然没有权限去格式化磁盘</p>
<p><img src="http://static.zybuluo.com/zphj1987/8thiyj24pflih4urdohkn1rt/image_1bbb7okcj8mj1c301tdb16mtecn5l.png" alt="osd addd "><br>上面都填写完了后就点击create<br>没出异常的话，就可以进入console进行查询了<br><img src="http://static.zybuluo.com/zphj1987/hwganai6phk6q2x5xrowfozc/image_1bbb7ufgk12nj1unpoq5taa1iah9.png" alt="good"></p>
<p>基本上一个简单的集群就配置好了，跨主机的情况，就提前把配置文件拷贝到另外一台主机，还有bootstrap keyring也拷贝过去，就可以了，这里就不做过多的赘述</p>
<h2 id="总结">总结</h2><p>本篇基于portainer以及一个现有的ceph容器做的部署实践，从整个操作来说，UI的部署，环境的搭建都非常的简单，这个得益于UI环境的简单，还有docker的封装，更多的玩法可以自己去探索，也可以运用这个UI做更多其他的容器操作</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-16</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker2.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>前一篇介绍了docker在命令行下面进行的ceph部署，本篇用docker的UI进行ceph的部署，目前来说市面上还没有一款能够比较简单就能直接在OS上面去部署Ceph的管理平台，这是因为OS的环境差异化太大，并且包的版本，以及各种软件的适配都可能造成失败，而docker比较固化环境，因此即使一个通用的UI也能很方便的部署出一个Cpeh集群</p>
<p>本篇就是对Docker UI部署集群做一个实践，对ceph了解，对docker了解，对dokcer的UI操作进行一定的了解的情况下，再做实践会比较好，总体上还是比较简单的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于docker部署ceph以及修改docker image]]></title>
    <link href="http://www.zphj1987.com/2017/03/15/base-on-docker-deploy-ceph/"/>
    <id>http://www.zphj1987.com/2017/03/15/base-on-docker-deploy-ceph/</id>
    <published>2017-03-15T04:05:35.000Z</published>
    <updated>2017-03-16T03:42:01.873Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker.png" alt="docker"><br></center>

<h2 id="前言">前言</h2><p>容器和ceph的结合已经在一些生产环境当中做了尝试，容器的好处就是对运行环境的一个封装，传统的方式是集成为ISO，这个需要一定的维护量，而容器的相关操作会简单很多，也就有了一些尝试，个人觉得如果玩的转容器可以考虑，当然得懂ceph，不然两套系统在一起，问题都不知道是哪个的，就比较麻烦了</p>
<p>本篇是基于之前我的填坑群里面的牛鹏举的一个问题，他的环境出现了创建osd的时候权限问题，我这边没遇到，现在实践了一遍，感觉应该是之前目录提前创建了的问题<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><h3 id="安装docker">安装docker</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install docker</span><br></pre></td></tr></table></figure>
<h3 id="下载ceph镜像">下载ceph镜像</h3><p>这个镜像是sebastien维护的，他是redhat的ceph工程师，ceph-ansible的负责人,很多一线的资料都是来自他的分享，这个是一个集成好的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull ceph/daemon</span><br></pre></td></tr></table></figure></p>
<p>准备好一些目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/ceph</span><br><span class="line">mkdir -p /var/lib/ceph/</span><br></pre></td></tr></table></figure></p>
<p>注意只需要做这个两个目录，不要创建子目录，docker内部有相关的操作</p>
<h3 id="创建一个mon">创建一个mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --net=host  --name=mon \</span><br><span class="line">-v /etc/ceph:/etc/ceph \</span><br><span class="line">-v /var/lib/ceph/:/var/lib/ceph \</span><br><span class="line"><span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">8.106</span> \</span><br><span class="line"><span class="operator">-e</span> CEPH_PUBLIC_NETWORK=<span class="number">192.168</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">ceph/daemon mon</span><br></pre></td></tr></table></figure>
<p>MON_IP就是宿主机的IP地址</p>
<p>执行完了后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment">#  docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED              STATUS              PORTS               NAMES</span><br><span class="line"><span class="number">86</span>ed05173432        ceph/daemon         <span class="string">"/entrypoint.sh mon"</span>   About a minute ago   Up <span class="number">59</span> seconds                           mon</span><br></pre></td></tr></table></figure></p>
<p>可以看到退出了，我们来docker logs -f mon看下日志的输出<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker logs -f mon</span></span><br><span class="line">/sbin/ip</span><br><span class="line">creating /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">creating /etc/ceph/ceph.mon.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-osd/ceph.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-mds/ceph.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-rgw/ceph.keyring</span><br><span class="line">monmaptool: monmap file /etc/ceph/monmap-ceph</span><br><span class="line">monmaptool: <span class="built_in">set</span> fsid to cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">monmaptool: writing epoch <span class="number">0</span> to /etc/ceph/monmap-ceph (<span class="number">1</span> monitors)</span><br><span class="line">creating /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /etc/ceph/ceph.mon.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">ceph-mon: <span class="built_in">set</span> fsid to cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">ceph-mon: created monfs at /var/lib/ceph/mon/ceph-lab8106 <span class="keyword">for</span> mon.lab81</span><br></pre></td></tr></table></figure></p>
<p>提示成功了</p>
<p>我们看下生成的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /etc/ceph</span></span><br><span class="line">total <span class="number">16</span></span><br><span class="line">-rw------- <span class="number">1</span> root  root  <span class="number">137</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.client.admin.keyring</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root  root  <span class="number">285</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.conf</span><br><span class="line">-rw------- <span class="number">1</span> <span class="number">64045</span> <span class="number">64045</span>  <span class="number">77</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.mon.keyring</span><br><span class="line">-rw-r--r-- <span class="number">1</span> <span class="number">64045</span> <span class="number">64045</span> <span class="number">187</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> monmap-ceph</span><br></pre></td></tr></table></figure></p>
<p>从这里可以看到内部的cpeh的用户的id是64045，所以在docker宿主机不要随便去给ceph权限，可能id不匹配，容器内部还是无法操作</p>
<h3 id="创建一个osd">创建一个osd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --net=host --name=myosd1 \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">-v /etc/ceph:/etc/ceph \</span><br><span class="line">-v /var/lib/ceph/:/var/lib/ceph \</span><br><span class="line">-v /dev/:/dev/ \</span><br><span class="line"><span class="operator">-e</span> OSD_DEVICE=/dev/sdb \</span><br><span class="line">ceph/daemon osd_ceph_disk</span><br></pre></td></tr></table></figure>
<p>如果查询日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker logs <span class="operator">-f</span> myosd1</span><br></pre></td></tr></table></figure></p>
<p>如果执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon ceph <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<p>如果想进入容器内部<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon  /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>修改集群的副本数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon  ceph osd pool <span class="built_in">set</span> rbd size <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看集群状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker exec -it mon  ceph -s</span></span><br><span class="line">    cluster cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr no daemons active </span><br><span class="line">     osdmap e7: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds,require_kraken_osds</span><br><span class="line">      pgmap v15: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">34288</span> kB used, <span class="number">279</span> GB / <span class="number">279</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>上面的操作都很顺利，但是某些情况可能出现异常情况，或者镜像内部本身就有问题需要自己修改，这个怎么处理</p>
<h2 id="碰上问题想修改image">碰上问题想修改image</h2><p>我们看下我们运行的docker<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker ps </span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line"><span class="number">874</span>d78ccae55        ceph/daemon         <span class="string">"/entrypoint.sh osd_c"</span>   <span class="number">14</span> hours ago        Up <span class="number">14</span> hours                             myosd1</span><br><span class="line"><span class="number">86</span>ed05173432        ceph/daemon         <span class="string">"/entrypoint.sh mon"</span>     <span class="number">15</span> hours ago        Up <span class="number">15</span> hours                             mon</span><br></pre></td></tr></table></figure></p>
<p>COMMAND这里有个/entrypoint.sh</p>
<p>如果存在ENTRYPOINT和CMD，那么CMD就是ENTRYPOINT的参数，如果没有ENTRYPOINT，则CMD就是默认执行指令<br>也就是容器启动的时候默认是会去执行/entrypoint.sh 这个了</p>
<p>我们不需要他执行这个，就需要加参数了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker run -i -t --entrypoint /bin/bash ceph/daemon</span></span><br></pre></td></tr></table></figure></p>
<p>比如我上次做的一个操作，把ceph用户绑定到root的id<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">64045</span>:<span class="number">64045</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># sed -i 's/64045/0/g' /etc/passwd</span></span><br><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">0</span>:<span class="number">0</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>退出容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># exit</span></span><br></pre></td></tr></table></figure></p>
<p>查询我们最后运行的容器，修改回entrypoint我们再把容器修改提交到基础image<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES</span><br><span class="line"><span class="number">9</span>b269bf751f9        ceph/daemon         <span class="string">"/bin/bash"</span>         <span class="number">2</span> minutes ago       Exited (<span class="number">0</span>) <span class="number">15</span> seconds ago                       angry_hawking</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#  docker commit 9b269bf751f9 ceph/daemon</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker run -i -t --entrypoint /entrypoint.sh ceph/daemon</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES</span><br><span class="line">c2ea602c18ac        ceph/daemon         <span class="string">"/entrypoint.sh"</span>    <span class="number">10</span> seconds ago      Exited (<span class="number">1</span>) <span class="number">7</span> seconds ago                       ecstatic_bartik</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># docker commit c2ea602c18ac ceph/daemon</span></span><br></pre></td></tr></table></figure></p>
<p>再次启动容器,并且检查内容，可以看到已经修改好了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker run -i -t --entrypoint /bin/bash ceph/daemon</span></span><br><span class="line">root@<span class="number">65</span>b538fdc61e:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">0</span>:<span class="number">0</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>如果需要做其他的改动，这样改下就行</p>
<h2 id="总结">总结</h2><p>本篇主要是根据sebastien的镜像做的部署，并且给出一些常用的命令，以及如何进入固化的容器的内部进行修改，方便自己调试环境</p>
<h2 id="相关资料">相关资料</h2><p><a href="http://www.sebastien-han.fr/blog/2015/06/23/bootstrap-your-ceph-cluster-in-docker/" target="_blank" rel="external">bootstrap-your-ceph-cluster-in-docker/</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-15</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker.png" alt="docker"><br></center>

<h2 id="前言">前言</h2><p>容器和ceph的结合已经在一些生产环境当中做了尝试，容器的好处就是对运行环境的一个封装，传统的方式是集成为ISO，这个需要一定的维护量，而容器的相关操作会简单很多，也就有了一些尝试，个人觉得如果玩的转容器可以考虑，当然得懂ceph，不然两套系统在一起，问题都不知道是哪个的，就比较麻烦了</p>
<p>本篇是基于之前我的填坑群里面的牛鹏举的一个问题，他的环境出现了创建osd的时候权限问题，我这边没遇到，现在实践了一遍，感觉应该是之前目录提前创建了的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在线动态修改ulimit]]></title>
    <link href="http://www.zphj1987.com/2017/03/06/online-change-ulimit/"/>
    <id>http://www.zphj1987.com/2017/03/06/online-change-ulimit/</id>
    <published>2017-03-06T10:20:26.000Z</published>
    <updated>2017-03-06T10:28:35.755Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/limit.jpg" alt="limit"><br></center>

<h2 id="前言">前言</h2><p>系统中有些地方会进行资源的限制，其中的一个就是open file的限制，操作系统默认限制的是1024,这个值可以通过各种方式修改，本篇主要讲的是如何在线修改，生产上是不可能随便重启进程的<br><a id="more"></a></p>
<h2 id="实践">实践</h2><h3 id="查看系统默认的限制">查看系统默认的限制</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ulimit -a|grep open</span></span><br><span class="line">open files                      (-n) <span class="number">1024</span></span><br></pre></td></tr></table></figure>
<p>默认的打开文件是1024<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ps -ef|grep ceph-osd</span></span><br><span class="line">ceph     <span class="number">28176</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">18</span>:<span class="number">08</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> /usr/bin/ceph-osd <span class="operator">-f</span> --cluster ceph --id <span class="number">0</span> --setuser ceph --setgroup ceph</span><br><span class="line">root     <span class="number">28619</span> <span class="number">26901</span>  <span class="number">0</span> <span class="number">18</span>:<span class="number">10</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep --color=auto ceph-osd</span><br><span class="line">[root@lab8106 ~]<span class="comment"># cat /proc/28176/limits |grep open</span></span><br><span class="line">Max open files            <span class="number">1048576</span>              <span class="number">1048576</span>              files</span><br></pre></td></tr></table></figure></p>
<p>ceph osd的进程的这个参数是1048576 </p>
<h3 id="通过配置文件修改">通过配置文件修改</h3><p>这个参数控制是放在：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat  /usr/lib/systemd/system/ceph-osd@.service |grep LimitNOFILE -B 1</span></span><br><span class="line">[Service]</span><br><span class="line">LimitNOFILE=<span class="number">1048576</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方设置的，如果我们有需要修改，那么可以修改这里，这不是本篇的重点，对于运行中的进程如何修改呢</p>
<h3 id="在线修改进程的limit">在线修改进程的limit</h3><p>这里调用的是prlimit进行的在线修改<br>查询指定进程的限制<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176</span></span><br><span class="line">RESOURCE   DESCRIPTION                             SOFT      HARD UNITS</span><br><span class="line">AS         address space <span class="built_in">limit</span>                unlimited unlimited bytes</span><br><span class="line">CORE       max core file size                         <span class="number">0</span> unlimited blocks</span><br><span class="line">CPU        CPU time                           unlimited unlimited seconds</span><br><span class="line">DATA       max data size                      unlimited unlimited bytes</span><br><span class="line">FSIZE      max file size                      unlimited unlimited blocks</span><br><span class="line">LOCKS      max number of file locks held      unlimited unlimited </span><br><span class="line">MEMLOCK    max locked-in-memory address space     <span class="number">65536</span>     <span class="number">65536</span> bytes</span><br><span class="line">MSGQUEUE   max bytes <span class="keyword">in</span> POSIX mqueues            <span class="number">819200</span>    <span class="number">819200</span> bytes</span><br><span class="line">NICE       max nice prio allowed to raise             <span class="number">0</span>         <span class="number">0</span> </span><br><span class="line">NOFILE     max number of open files             <span class="number">1048576</span>   <span class="number">1048576</span> </span><br><span class="line">NPROC      max number of processes              <span class="number">1048576</span>   <span class="number">1048576</span> </span><br><span class="line">RSS        max resident <span class="built_in">set</span> size              unlimited unlimited pages</span><br><span class="line">RTPRIO     max real-time priority                     <span class="number">0</span>         <span class="number">0</span> </span><br><span class="line">RTTIME     timeout <span class="keyword">for</span> real-time tasks        unlimited unlimited microsecs</span><br><span class="line">SIGPENDING max number of pending signals         <span class="number">192853</span>    <span class="number">192853</span> </span><br><span class="line">STACK      max stack size                       <span class="number">8388608</span> unlimited bytes</span><br></pre></td></tr></table></figure></p>
<p>修改指定运行进程的限制<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176 --nofile=104857</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176 |grep NOFILE</span></span><br><span class="line">NOFILE     max number of open files              <span class="number">104857</span>    <span class="number">104857</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到修改成功了</p>
<h2 id="总结">总结</h2><p>一般来说ulimit这个限制都是在终端上修改对下次生效，本篇用来记录如何在线修改，如果碰到了，可以这样处理</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/limit.jpg" alt="limit"><br></center>

<h2 id="前言">前言</h2><p>系统中有些地方会进行资源的限制，其中的一个就是open file的限制，操作系统默认限制的是1024,这个值可以通过各种方式修改，本篇主要讲的是如何在线修改，生产上是不可能随便重启进程的<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[python执行rados命令例子]]></title>
    <link href="http://www.zphj1987.com/2017/02/28/python-command-rados-sample/"/>
    <id>http://www.zphj1987.com/2017/02/28/python-command-rados-sample/</id>
    <published>2017-02-28T15:48:15.000Z</published>
    <updated>2017-02-28T16:07:00.766Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/we-love-python-800-600.png" alt="python"><br></center>

<h2 id="前言">前言</h2><p>我们以前的管理平台在python平台下面做的，内部做的一些操作采用的是命令执行，然后解析的方式去做的，ceph自身有python的rados接口，可以直接调用原生接口，然后直接解析json的方式，这样更靠近底层<br><a id="more"></a><br>在看ceph-dash内部的实现的时候，发现里面的获取集群信息的代码可以留存备用</p>
<h2 id="代码实例">代码实例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">from rados import Rados</span><br><span class="line">from rados import Error as RadosError</span><br><span class="line"></span><br><span class="line">class CephClusterCommand(dict):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span><br><span class="line">    Issue a ceph command on the given cluster and provide the returned json</span><br><span class="line">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, cluster, **kwargs):</span><br><span class="line">        dict.__init__(self)</span><br><span class="line">        ret, buf, err = cluster.mon_<span class="built_in">command</span>(json.dumps(kwargs), <span class="string">''</span>, timeout=<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">if</span> ret != <span class="number">0</span>:</span><br><span class="line">            self[<span class="string">'err'</span>] = err</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.update(json.loads(buf))</span><br><span class="line"></span><br><span class="line">config=&#123;<span class="string">'conffile'</span>: <span class="string">'/etc/ceph/ceph.conf'</span>, <span class="string">'conf'</span>: &#123;&#125;&#125;</span><br><span class="line">with Rados(**config) as cluster:</span><br><span class="line">    cluster_status = CephClusterCommand(cluster, prefix=<span class="string">'status'</span>, format=<span class="string">'json'</span>)</span><br><span class="line">    <span class="built_in">print</span> cluster_status</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>调用原生接口的好处在于,只需要很少的库就可以取得监控系统所需要的值</p>
<p>最近在研究系统的时候发现一个问题</p>
<blockquote>
<p>跟着错误的文档实践只会掉进同一个坑</p>
</blockquote>
<p>在遇到一个小的错误的时候，翻到了一个github的Issue，然后看到一个人把自己的配置过程和配置文件详详细细的都写在Issue下面，然后就跟着他的过程走了一遍，发现不论怎么弄都是同样的错误</p>
<p>而返回去根据另一个正确的文档又走一遍的时候，发现终于跑通了，回顾了一遍，发现是那个错误的过程里面的配置文件里面是有配置项目，不兼容的，而软件也没有抛出相关的错误，然后在同一个地方找了两天</p>
<p>所以如果有碰到无法解决的操作步骤文档的时候，就尽量不要去根据那个文档操作了，除非自己对细节弄的很清楚了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-02-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/we-love-python-800-600.png" alt="python"><br></center>

<h2 id="前言">前言</h2><p>我们以前的管理平台在python平台下面做的，内部做的一些操作采用的是命令执行，然后解析的方式去做的，ceph自身有python的rados接口，可以直接调用原生接口，然后直接解析json的方式，这样更靠近底层<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[记最近一次ceph故障修复]]></title>
    <link href="http://www.zphj1987.com/2017/02/24/remember-a-ceph-recover/"/>
    <id>http://www.zphj1987.com/2017/02/24/remember-a-ceph-recover/</id>
    <published>2017-02-24T13:56:43.000Z</published>
    <updated>2017-02-25T06:09:08.837Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/zhengli.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>所谓吃一堑长一智，每次面对问题才是最好的学习机会，在面对问题的时候，尽量是能够自己去解决，或者去尝试能够最接近答案，确实无法解决再去寻求他人帮助，这样成长的会更快一些，在学校读书做题的时候，老师也是经常告诉我们要忍住，不要去直接翻答案，在当今的互联网飞速的发展下，在google的帮助下，基本上90%的问题都能找到正确的答案，而我们其实真正需要锻炼的是实践能力和甄别的能力<br><a id="more"></a><br>去年一年给不少的生产环境解决过问题，在相互交流几次以后，解决问题的过程，基本也熟悉了，一般解决问题的大致流程都是：</p>
<ul>
<li>告之我环境的当前状况，需要实现的情况</li>
<li>准备好远程的环境</li>
<li>告之对方可能出现的情况，是否可操作，然后解决问题</li>
<li>交流问题的出现原因以及解决的办法</li>
</ul>
<p>目前来看，基本都解决了，对于我来说是一次处理故障经验的累积，对对方来说是环境的恢复，以及下次在出现相同故障的时候，自己能够处理好类似问题</p>
<p>本次恢复对于我来说也是刷新了的认识，进展只到了解决问题的地方，就结束了，那么我就记录下这次解决问题当中的收获</p>
<h2 id="处理过程">处理过程</h2><p>故障的发生应该是在一次掉电后触发的,整个集群在重新启动以后，出现了多块磁盘故障的问题，也有主机无法启动的情况，整个集群的PG状态处于一个混乱的状态，stale和incomplete以及peering状态的都很多</p>
<p>告之对方，需要把相关的osd节点全部都启动起来，然后再看是否有恢复的可能，常规来说，如果三台机器同时出现磁盘损坏，那么这个集群的数据必然会丢失，并且丢失的数据基本将是覆盖所有数据</p>
<p>在将近一周的时间以后，集群环境磁盘都能挂载，环境可以进行处理了</p>
<h3 id="出现pg状态一直是peering状态的情况">出现pg状态一直是peering状态的情况</h3><p>用ceph -s 检查集群的状态，集群的状态所有的osd都是正常的up in状态，但是pg状态就是peering状态无法恢复，然后查看都是来自其中的某一个osd，登陆上机器后查看osd的日志，显示无法获取心跳，但是网络明明是好的，并且还能登陆到其他机器上，这就奇怪了，这里先讲下这个地方对方环境埋下的一个坑</p>
<p>hosts文件里面是这种组合</p>
<blockquote>
<p>10.10.10.101  node1<br>192.168.10.1  node1<br>10.10.10.102  node2<br>192.168.10.2  node2<br>10.10.10.103  node3<br>192.168.10.3  node3</p>
</blockquote>
<p>也就是一个主机名映射了两个IP，这个对方说没问题，我也就不多说了，只是我的环境是不会允许这么配置，正是因为这个配置，也就间接隐藏了一个错误的配置，这个错误就是居然在环境当中配置两台主机相同的IP，这也就是为什么出现相同的IP我还能登陆机器</p>
<p>环境配置成了 </p>
<blockquote>
<p>10.10.10.102  node3<br>192.168.10.3  node3</p>
</blockquote>
<p>也就是node3和node2的集群IP冲突了，所以我在ssh node3的时候能正确登陆node3 ssh node2也能正确登陆node2，只是集群用的IP冲突了，而两台机器之间网络又可以通过其他的网段通信，集群的osd状态是正常，只是pg异常了</p>
<p>IP冲突在生产环境中是大忌，可能会毁掉整个集群的状态，这个有多大影响？你可以试下配置好一个集群，然后把两个节点的IP配置成一样，然后检查集群的状态和你的上面运行的存储的状态，这个环境因为是在不提供服务状态下，所以带来的影响没有那么大</p>
<p>在排查到这个错误的时候，已经是晚上快11点了，对方也要回家了，作为运维比较苦逼的就是很多时候，需要待在公司到晚上很晚才能离开，所以问了下是否能留远程给我，得到了许可，可以继续操作，因为这个环境状态来看我觉得还在我的可控范围内，所以想继续尝试，对方也是问过几次，这个环境是否可恢复，我给出的回答也是尽量，IP冲突的问题解决后，重新启动OSD，集群基本快正常了，还是有一些异常的PG需要处理</p>
<h3 id="出现osd无法启动">出现osd无法启动</h3><blockquote>
<p>verify_reply couldn’t decrypt with error: error decoding block for decryption</p>
</blockquote>
<p>这个错误之前有处理经验，时间偏移过大引起认证不通过，登陆上osd对应的机器，检查发现时间偏移了几个小时，引起错误，检查发现ntp配置文件使用的是默认配置文件，至少这台没配置ntp，调整好时间，重启osd解决无法启动问题</p>
<h3 id="出现PG_incomplete的状态">出现PG incomplete的状态</h3><p>这个状态一般是环境出现过特别的异常状况，PG无法完成状态的更新，这个只要没有OSD被踢出去或者损坏掉，就好办，这个环境的多个PG的数据是一致的，只是状态不对，这个就把PG对应的OSD全部停掉，然后用ceph-object-tool 进行mark complete,然后重启osd，一般来说这个都是能解决了，没出意外，这个环境用工具进行了修复，当然8个这样的PG操作起来还是要比较长的时间，一个个的确认状态，还好都解决了，这个工具是jewel上面集成的，所以在老版本出现这个问题，可以通过升级后进行处理，之前有个别人的生产环境这么处理过这个问题</p>
<h3 id="出现PG_inconsistent状态的">出现PG inconsistent状态的</h3><p>这个是环境中有数据不一致了，这个算比较小的问题，直接对pg进行了repair的修复，然后pg状态就正常了，不得不说现在的ceph比很久前的版本要好很多，Jewel版本的修复工具已经非常完善了，基本能覆盖很多常规的故障</p>
<h3 id="出现PG_处于activaing状态">出现PG 处于activaing状态</h3><p>上面的各种问题都处理过来了，到了最后一个，有一个PG处于activating状态，对于ceph来说真是一个都不能少，这个影响的是这个PG所在的存储池当中的数据，影响的范围也是存储池级别的，所以还是希望能够修复好，在反复重启这个pg的所在的osd后，发现这个pg总是无法正常，并且这个机器所在的OSD还会down掉，开始以为是操作没完成，需要很多数据要处理，所以增加了osd_op_thread_suicide_timeout的超时值，发现增大到180s以后还是会挂掉，然后报一堆东西，这个时候想起来还没去检查下这个PG是不是数据之前掉了，检查后就发现了问题，主PG里面的目录居然是空的，而另外两个副本里面的数据都是完整的并且一样的，应该是数据出了问题，造成PG无法正常</p>
<p>停止掉PG所在的三个OSD，使用ceph-object-tool进行pg数据备份，然后用ceph-object-tool在主PG上删除那个空的pg，这里要注意不要手动删除数据，用工具删除会去清理相关的元数据，而手动去删除可能会残留元数据而引起异常，然后用ceph-object-tool进行数据的导入，然后重启节点，还是无法正常，然后开日志看，发现是对象权限问题，用工具导入的时候，pg内的对象是root权限的，而ceph 启动的权限无法读取，手动给这个pg目录进行给予ceph的权限，重启osd，整个集群正常了</p>
<p>然后通知对方，环境修复好了，对方回复，有空检查下虚拟机情况，然后就没有然后了···</p>
<h2 id="这个环境暴露的问题">这个环境暴露的问题</h2><p>1、主机名hosts内单主机名对应多IP<br>这个对于对主机名敏感的应用会受影响<br>2、环境出现IP冲突<br>这个属于细节问题，当然也是最不好排查的一种故障<br>3、环境内没配置内网ntp<br>操作其中的某台机器的时候报了，不清楚整个环境没配置还是只是一台没配置<br>4、有一个mon挂掉了，没有先处理<br>这个对于进行故障处理的时候经常会请求到故障的mon上，造成卡操作，因为不清楚mon状态，所以没启动，直接注释配置文件进行操作<br>5、主机环境的内存分配设置有问题<br>这个因为没太多交流也就不多说了<br>6、ceph版本为10.2.2<br>这个版本有什么问题吗？用的不是好好的吗？这个问题我一直认为公司一定需要有人会选择版本，这里说下怎么选,你也可以自检下自己的版本，当然你们研发觉得没问题，我也就不做过多评论，每个人有自己的想法，别乱来就好</p>
<ul>
<li>软件的开发一般会是隔一个版本会是一个长期支持版本，所以尽量不要选择<br>INFERNALIS这个版本，Jewel刚出来，那么你应该选择harmer最后一个版本，而这个时候就会有人选择了Jewel版本，也就是上面的10.2.2或者更低10.0.x</li>
<li>长期支持版本出来了，那么尽量等版本出到4或者5再去用，也就是现在的Jewel的10.2.5，这个版本不会出现大的bug，K版本就不要随便上生产，等等后面的 Luminous的稳定版本</li>
<li>内部做升级需求的时候，在选择下个版本的时候同样是选择下一个长期支持版本的稳定版本，这样能保证软件的稳定性，以及版本的生命周期尽量长</li>
<li>正常运行的时候都没事，碰上异常经常一搜就是已经解决了bug，等到这个时候再升级，就是故障中升级，拉长了故障恢复时间</li>
<li>所以现在的版本能够用Jewel的最后一个版本和hammer的最后一个版本，一些能backports的功能,也会合成到hammer的最后一个版本</li>
<li>小版本中间也可能有大变化，0.94.4和0.94.7这两个都是节点版本，节点版本就是你从更低的版本往更高的版本升级的时候，需要先来到节点版本，然后才能往上走，也就是迭代升级，如果不了解清楚，直接把集群升级到起不来也是会出现的事情</li>
</ul>
<h2 id="总结">总结</h2><p>本次处理的故障属于组合型的,本篇是博客当中贴命令最少的一篇，当然内容不少，相信你看了处理过程也会有所收获，不管你是搞云计算还是云存储，一定要重视数据的可恢复问题</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-02-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/zhengli.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>所谓吃一堑长一智，每次面对问题才是最好的学习机会，在面对问题的时候，尽量是能够自己去解决，或者去尝试能够最接近答案，确实无法解决再去寻求他人帮助，这样成长的会更快一些，在学校读书做题的时候，老师也是经常告诉我们要忍住，不要去直接翻答案，在当今的互联网飞速的发展下，在google的帮助下，基本上90%的问题都能找到正确的答案，而我们其实真正需要锻炼的是实践能力和甄别的能力<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[根据一段话判断情绪]]></title>
    <link href="http://www.zphj1987.com/2017/02/10/word-to-motion/"/>
    <id>http://www.zphj1987.com/2017/02/10/word-to-motion/</id>
    <published>2017-02-10T06:15:22.000Z</published>
    <updated>2017-02-10T08:31:11.309Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/gre.jpg" alt="emotion"><br></center>

<h2 id="引言">引言</h2><p>看到一个好玩的项目，<a href="https://www.anotherhome.net/2920" target="_blank" rel="external">女朋友的微博情绪监控</a>,这个是根据一段话来判断情绪的，记得之前有在哪里看到过，未来的一切都是API，也就是很多东西会被封装好，你只需要去用就可以了，这个就是一个很好的例子，你可以不懂语意分析，不懂分词，这些都不要紧，只要你给出你的素材，后面就交给api去处理<br><a id="more"></a></p>
<h2 id="代码">代码</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">def main():</span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        <span class="built_in">help</span>()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        printpromotion(sys.argv[<span class="number">1</span>])</span><br><span class="line">def <span class="built_in">help</span>():</span><br><span class="line">    <span class="built_in">print</span> <span class="string">""</span><span class="string">"Usage : qingxu.py [-h] [word]</span><br><span class="line">        情绪鉴定 - 判断一段话的情绪</span><br><span class="line">        OPTIONS</span><br><span class="line">        ========</span><br><span class="line">        sample:</span><br><span class="line">        [root@host ~]# python  qingxu.py 开心</span><br><span class="line">        说的话: word</span><br><span class="line">        正面情绪: 98.3%</span><br><span class="line">        负面情绪: 1.7%</span><br><span class="line">        ========</span><br><span class="line">        "</span><span class="string">""</span></span><br><span class="line">def printpromotion(word):</span><br><span class="line">    weburl=<span class="string">'https://api.prprpr.me/emotion/wenzhi?password=DIYgod&amp;text='</span>+word</span><br><span class="line">    r = requests.get(<span class="string">'%s'</span> %weburl)</span><br><span class="line">    json_str = json.loads(r.text)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"说的话:"</span>,<span class="string">"%s"</span> %word</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"正面情绪:"</span>,(format(json_str[<span class="string">"positive"</span>],<span class="string">'0.1%'</span>))</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"负面情绪:"</span>,(format(json_str[<span class="string">"negative"</span>],<span class="string">'0.1%'</span>))</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="运行效果">运行效果</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># python qingxu.py 很高兴</span></span><br><span class="line">说的话: 很高兴</span><br><span class="line">正面情绪: <span class="number">92.4</span>%</span><br><span class="line">负面情绪: <span class="number">7.6</span>%</span><br><span class="line">[root@lab8106 ~]<span class="comment"># python qingxu.py 被坑了</span></span><br><span class="line">说的话: 被坑了</span><br><span class="line">正面情绪: <span class="number">5.7</span>%</span><br><span class="line">负面情绪: <span class="number">94.3</span>%</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>内部的语义分析的准确度有多少还不清楚，但是也是一个很好玩的东西，程序员的想法还是挺多的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-02-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/gre.jpg" alt="emotion"><br></center>

<h2 id="引言">引言</h2><p>看到一个好玩的项目，<a href="https://www.anotherhome.net/2920">女朋友的微博情绪监控</a>,这个是根据一段话来判断情绪的，记得之前有在哪里看到过，未来的一切都是API，也就是很多东西会被封装好，你只需要去用就可以了，这个就是一个很好的例子，你可以不懂语意分析，不懂分词，这些都不要紧，只要你给出你的素材，后面就交给api去处理<br>]]>
    
    </summary>
    
      <category term="杂七杂八" scheme="http://www.zphj1987.com/tags/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[预估ceph的迁移数据量]]></title>
    <link href="http://www.zphj1987.com/2017/02/08/%E9%A2%84%E4%BC%B0ceph%E7%9A%84%E8%BF%81%E7%A7%BB%E6%95%B0%E6%8D%AE%E9%87%8F/"/>
    <id>http://www.zphj1987.com/2017/02/08/预估ceph的迁移数据量/</id>
    <published>2017-02-08T15:52:02.000Z</published>
    <updated>2017-02-08T15:54:20.970Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/calculator.png" alt="cal"><br></center>

<h2 id="引言">引言</h2><p>我们在进行 ceph 的 osd 的增加和减少的维护的时候，会碰到迁移数据，但是我们平时会怎么去回答关于迁移数据量的问题，一般来说，都是说很多，或者说根据环境来看，有没有精确的一个说法，到底要迁移多少数据?这个我以前也有思考过这个问题，当时想是对比前后的pg的分布，然后进行计算，正好在翻一些资料的时候，看到有alram写的一篇博客，alram是Inktank的程序员，也就是sage所在的公司，程序是一个python脚本，本篇会分析下这个对比的思路，以及运行效果</p>
<p>计算迁移量只需要一个修改后的crushmap就可以了，这个是离线计算的，所以不会对集群有什么影响<br><a id="more"></a></p>
<h2 id="运行效果">运行效果</h2><h3 id="准备修改后的crushmap">准备修改后的crushmap</h3><p>获取当前crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd getcrushmap -o crushmap</span><br></pre></td></tr></table></figure></p>
<p>解码crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crushtool <span class="operator">-d</span> crushmap -o crushmap.txt</span><br></pre></td></tr></table></figure></p>
<p>修改crushmap.txt<br>这个根据自己需要，修改成自己想修改成的crushmap即可，可以是增加，也可以是删除</p>
<h3 id="减少节点的计算">减少节点的计算</h3><p>假如删除一个osd.5 我们需要迁移多少数据<br>将crushmap里面的osd.5的weight改成0<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crushtool -c crushmap.txt -o crushmapnew</span><br></pre></td></tr></table></figure></p>
<p>运行计算脚本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># python jisuan.py --crushmap-file crushmapnew</span></span><br><span class="line">POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   </span><br><span class="line">rbd                  <span class="number">59</span>                   <span class="number">6157238296</span>           <span class="number">1469</span>                </span><br><span class="line">data                 <span class="number">54</span>                   <span class="number">5918162968</span>           <span class="number">1412</span>                </span><br><span class="line">metadata             <span class="number">53</span>                   <span class="number">5825888280</span>           <span class="number">1390</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到迁移的数据量<br>REMAPPED OSDs 下面就是有多少份的PG数据需要迁移，这里面计算的方式是比较前后的分布</p>
<blockquote>
<p>[1,2] - &gt; [1,2] 迁移0个<br>[1,2] - &gt; [4,2] 迁移1个<br>[1,2] - &gt; [4,3] 迁移2个</p>
</blockquote>
<p>上面的统计的是这样的个数，所以不太好说是PG或者是OSD，可以理解为PG内数据的份数，因为单个PG可能需要迁移一份，也有可能迁移两份，或者多份</p>
<h3 id="增加节点的计算">增加节点的计算</h3><p>如果增加一个osd.6 我们需要迁移多少数据<br>直接运行脚本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># python jisuan.py --crushmap-file crushmapnew</span></span><br><span class="line">POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   </span><br><span class="line">rbd                  <span class="number">0</span>                    <span class="number">0</span>                    <span class="number">0</span>                   </span><br><span class="line">data                 <span class="number">0</span>                    <span class="number">0</span>                    <span class="number">0</span>                   </span><br><span class="line">metadata             <span class="number">0</span>                    <span class="number">0</span>                    <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到没有输出，这个是因为计算的脚本里面有个地方报错了，ceph内部有个限制，在将crushmap import进osdmap的时候，ceph会验证osdmap里面的osd个数和crushmap里面的osd个数是不是相同<br>所以这个地方需要多做一步，将osd的个数设置成跟预估的一致，这个是唯一对现有集群做的修改操作，<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd setmaxosd 7</span></span><br><span class="line"><span class="built_in">set</span> new max_osd = <span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<p>然后再次运行就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># python jisuan.py --crushmap-file crushmapnew</span></span><br><span class="line">POOL                 REMAPPED OSDs        BYTES REBALANCE      OBJECTS REBALANCE   </span><br><span class="line">rbd                  <span class="number">31</span>                   <span class="number">3590324224</span>           <span class="number">856</span>                 </span><br><span class="line">data                 <span class="number">34</span>                   <span class="number">3372220416</span>           <span class="number">804</span>                 </span><br><span class="line">metadata             <span class="number">41</span>                   <span class="number">4492099584</span>           <span class="number">1071</span></span><br></pre></td></tr></table></figure></p>
<p>上面就是运行的效果，下面我们对内部的逻辑进行分析</p>
<h2 id="代码和代码分析">代码和代码分析</h2><h3 id="代码">代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line">import ast</span><br><span class="line">import json</span><br><span class="line">import os</span><br><span class="line">import subprocess</span><br><span class="line">import argparse</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">FNULL = open(os.devnull, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># assume the osdmap test output</span></span><br><span class="line"><span class="comment"># is the same lenght and order...</span></span><br><span class="line"><span class="comment"># if add support for PG increase</span></span><br><span class="line"><span class="comment"># that's gonna break</span></span><br><span class="line">def diff_output(original, new, pools):</span><br><span class="line">    number_of_osd_remap = <span class="number">0</span></span><br><span class="line">    osd_data_movement = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    results = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    pg_data, pg_objects = get_pg_info()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(original)):</span><br><span class="line">        orig_i = original[i]</span><br><span class="line">        new_i = new[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> orig_i[<span class="number">0</span>].isdigit():</span><br><span class="line">            pg_id = orig_i.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            pool_id = pg_id[<span class="number">0</span>]</span><br><span class="line">            pool_name = pools[pool_id][<span class="string">'pool_name'</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> not pool_name <span class="keyword">in</span> results:</span><br><span class="line">                results[pool_name] = &#123;&#125;</span><br><span class="line">                results[pool_name][<span class="string">'osd_remap_counter'</span>] = <span class="number">0</span></span><br><span class="line">                results[pool_name][<span class="string">'osd_bytes_movement'</span>] = <span class="number">0</span></span><br><span class="line">                results[pool_name][<span class="string">'osd_objects_movement'</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            original_mappings = ast.literal_<span class="built_in">eval</span>(orig_i.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            new_mappings = ast.literal_<span class="built_in">eval</span>(new_i.split(<span class="string">'\t'</span>)[<span class="number">1</span>])</span><br><span class="line">            intersection = list(<span class="built_in">set</span>(original_mappings).intersection(<span class="built_in">set</span>(new_mappings)))</span><br><span class="line"></span><br><span class="line">            osd_movement_<span class="keyword">for</span>_this_pg = int(pools[pool_id][<span class="string">'pool_size'</span>]) - len(intersection)</span><br><span class="line">            osd_data_movement_<span class="keyword">for</span>_this_pg = int(osd_movement_<span class="keyword">for</span>_this_pg) * int(pg_data[pg_id])</span><br><span class="line">            osd_object_movement_<span class="keyword">for</span>_this_pg = int(osd_movement_<span class="keyword">for</span>_this_pg) * int(pg_objects[pg_id])</span><br><span class="line"></span><br><span class="line">            results[pool_name][<span class="string">'osd_remap_counter'</span>] += osd_movement_<span class="keyword">for</span>_this_pg</span><br><span class="line">            results[pool_name][<span class="string">'osd_bytes_movement'</span>] += int(osd_data_movement_<span class="keyword">for</span>_this_pg)</span><br><span class="line">            results[pool_name][<span class="string">'osd_objects_movement'</span>] += int(osd_object_movement_<span class="keyword">for</span>_this_pg)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> orig_i.startswith(<span class="string">'#osd'</span>):</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> results</span><br><span class="line"></span><br><span class="line">def get_pools_info(osdmap_path):</span><br><span class="line">    pools = &#123;&#125;</span><br><span class="line">    args = [<span class="string">'osdmaptool'</span>, <span class="string">'--print'</span>, osdmap_path]</span><br><span class="line">    osdmap_out = subprocess.check_output(args, stderr=FNULL).split(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> osdmap_out:</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">'pool'</span>):</span><br><span class="line">            pool_id = line.split()[<span class="number">1</span>]</span><br><span class="line">            pool_size = line.split()[<span class="number">5</span>]</span><br><span class="line">            pool_name = line.split()[<span class="number">2</span>].replace(<span class="string">"'"</span>,<span class="string">""</span>)</span><br><span class="line">            pools[pool_id] = &#123;&#125;</span><br><span class="line">            pools[pool_id][<span class="string">'pool_size'</span>] = pool_size</span><br><span class="line">            pools[pool_id][<span class="string">'pool_name'</span>] = pool_name</span><br><span class="line">        <span class="keyword">elif</span> line.startswith(<span class="string">'max_osd'</span>):</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> pools</span><br><span class="line"></span><br><span class="line">def get_osd_map(osdmap_path):</span><br><span class="line">    args = [<span class="string">'sudo'</span>, <span class="string">'ceph'</span>, <span class="string">'osd'</span>, <span class="string">'getmap'</span>, <span class="string">'-o'</span>, osdmap_path]</span><br><span class="line">    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)</span><br><span class="line"></span><br><span class="line">def get_pg_info():</span><br><span class="line">    pg_data = &#123;&#125;</span><br><span class="line">    pg_objects = &#123;&#125;</span><br><span class="line">    args = [<span class="string">'sudo'</span>, <span class="string">'ceph'</span>, <span class="string">'pg'</span>, <span class="string">'dump'</span>]</span><br><span class="line">    pgmap = subprocess.check_output(args, stderr=FNULL).split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> pgmap:</span><br><span class="line">        <span class="keyword">if</span> line[<span class="number">0</span>].isdigit():</span><br><span class="line">            pg_id = line.split(<span class="string">'\t'</span>)[<span class="number">0</span>]</span><br><span class="line">            pg_bytes = line.split(<span class="string">'\t'</span>)[<span class="number">6</span>]</span><br><span class="line">            pg_obj = line.split(<span class="string">'\t'</span>)[<span class="number">1</span>]</span><br><span class="line">            pg_data[pg_id] = pg_bytes</span><br><span class="line">            pg_objects[pg_id] = pg_obj</span><br><span class="line">        <span class="keyword">elif</span> line.startswith(<span class="string">'pool'</span>):</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> pg_data, pg_objects</span><br><span class="line"></span><br><span class="line">def osdmaptool_<span class="built_in">test</span>_map_pgs_dump(original_osdmap_path, crushmap):</span><br><span class="line">    new_osdmap_path = original_osdmap_path + <span class="string">'.new'</span></span><br><span class="line">    get_osd_map(original_osdmap_path)</span><br><span class="line">    args = [<span class="string">'osdmaptool'</span>, <span class="string">'--test-map-pgs-dump'</span>, original_osdmap_path]</span><br><span class="line">    original_osdmaptool_output = subprocess.check_output(args, stderr=FNULL).split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    args = [<span class="string">'cp'</span>, original_osdmap_path, new_osdmap_path]</span><br><span class="line">    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)</span><br><span class="line">    args = [<span class="string">'osdmaptool'</span>, <span class="string">'--import-crush'</span>, crushmap, new_osdmap_path]</span><br><span class="line">    subprocess.call(args, stdout=FNULL, stderr=subprocess.STDOUT)</span><br><span class="line">    args = [<span class="string">'osdmaptool'</span>, <span class="string">'--test-map-pgs-dump'</span>, new_osdmap_path]</span><br><span class="line">    new_osdmaptool_output = subprocess.check_output(args, stderr=FNULL).split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    pools = get_pools_info(original_osdmap_path)</span><br><span class="line">    results = diff_output(original_osdmaptool_output, new_osdmaptool_output, pools)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dump_plain_output(results):</span><br><span class="line">    sys.stdout.write(<span class="string">"%-20s %-20s %-20s %-20s\n"</span> % (<span class="string">"POOL"</span>, <span class="string">"REMAPPED OSDs"</span>, <span class="string">"BYTES REBALANCE"</span>, <span class="string">"OBJECTS REBALANCE"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> pool <span class="keyword">in</span> results:</span><br><span class="line">        sys.stdout.write(<span class="string">"%-20s %-20s %-20s %-20s\n"</span> % (</span><br><span class="line">            pool,</span><br><span class="line">            results[pool][<span class="string">'osd_remap_counter'</span>],</span><br><span class="line">            results[pool][<span class="string">'osd_bytes_movement'</span>],</span><br><span class="line">            results[pool][<span class="string">'osd_objects_movement'</span>]</span><br><span class="line">            ))</span><br><span class="line"></span><br><span class="line">def cleanup(osdmap):</span><br><span class="line">    FNULL.close()</span><br><span class="line">    new_osdmap = osdmap + <span class="string">'.new'</span></span><br><span class="line">    os.remove(new_osdmap)</span><br><span class="line"></span><br><span class="line">def parse_args():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Ceph CRUSH change data movement calculator.'</span>)</span><br><span class="line"></span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">'--osdmap-file'</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">"Where to save the original osdmap. Temp one will be &lt;location&gt;.new. Default: /tmp/osdmap"</span>,</span><br><span class="line">        default=<span class="string">"/tmp/osdmap"</span>,</span><br><span class="line">        dest=<span class="string">"osdmap_path"</span></span><br><span class="line">        )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">'--crushmap-file'</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">"CRUSHmap to run the movement test against."</span>,</span><br><span class="line">        required=True,</span><br><span class="line">        dest=<span class="string">"new_crushmap"</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">'--format'</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">"Output format. Default: plain"</span>,</span><br><span class="line">        choices=[<span class="string">'json'</span>, <span class="string">'plain'</span>],</span><br><span class="line">        dest=<span class="string">"format"</span>,</span><br><span class="line">        default=<span class="string">"plain"</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="built_in">return</span> args</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ctx = parse_args()</span><br><span class="line"></span><br><span class="line">    results = osdmaptool_<span class="built_in">test</span>_map_pgs_dump(ctx.osdmap_path, ctx.new_crushmap)</span><br><span class="line">    cleanup(ctx.osdmap_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ctx.format == <span class="string">'json'</span>:</span><br><span class="line">        <span class="built_in">print</span> json.dumps(results)</span><br><span class="line">    <span class="keyword">elif</span> ctx.format == <span class="string">'plain'</span>:</span><br><span class="line">        dump_plain_output(results)</span><br></pre></td></tr></table></figure>
<p>直接放在这里方便拷贝，也可以去原作者的gist里面去获取</p>
<h3 id="主要代码分析">主要代码分析</h3><p>首先获取osdmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd getmap -o /tmp/osdmap</span><br></pre></td></tr></table></figure></p>
<p>获取原始pg分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">使用osdmaptool  --test-map-pgs-dump /tmp/osdmap</span><br></pre></td></tr></table></figure></p>
<p>获取新的crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">这个是自己编辑成需要的crushmap</span><br></pre></td></tr></table></figure></p>
<p>将新的crushmap注入到osdmap里面得到新的osdmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmaptool --import-crush  crushmap  /tmp/new_osdmap_path</span><br></pre></td></tr></table></figure></p>
<p>根据新的osdmap进行计算新的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmaptool  --test-map-pgs-dump /tmp/new_osdmap_path</span><br></pre></td></tr></table></figure></p>
<p>然后比较两个输入进行对比得到结果</p>
<h2 id="相关链接">相关链接</h2><p><a href="http://blog-fromsomedude.rhcloud.com/2017/01/19/Calculate-data-migration-when-changing-the-CRUSHmap/" target="_blank" rel="external">Calculate data migration when changing the CRUSHmap</a><br><a href="https://gist.github.com/alram/c6b1129a4c9100ab5184197d1455a6bd" target="_blank" rel="external">alram/crush_data_movement_calculator.py</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-02-08</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/calculator.png" alt="cal"><br></center>

<h2 id="引言">引言</h2><p>我们在进行 ceph 的 osd 的增加和减少的维护的时候，会碰到迁移数据，但是我们平时会怎么去回答关于迁移数据量的问题，一般来说，都是说很多，或者说根据环境来看，有没有精确的一个说法，到底要迁移多少数据?这个我以前也有思考过这个问题，当时想是对比前后的pg的分布，然后进行计算，正好在翻一些资料的时候，看到有alram写的一篇博客，alram是Inktank的程序员，也就是sage所在的公司，程序是一个python脚本，本篇会分析下这个对比的思路，以及运行效果</p>
<p>计算迁移量只需要一个修改后的crushmap就可以了，这个是离线计算的，所以不会对集群有什么影响<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux 升级内核开启 TCP BBR 有多大好处]]></title>
    <link href="http://www.zphj1987.com/2017/01/24/Linux-kernel-TCP-BBR-better/"/>
    <id>http://www.zphj1987.com/2017/01/24/Linux-kernel-TCP-BBR-better/</id>
    <published>2017-01-24T02:54:44.000Z</published>
    <updated>2017-02-03T05:08:19.889Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/tcp_ip_protocol.gif" alt=""><br></center>

<p>如果你有订阅一些科技新闻，应该会有看过内核在4.9当中加入了一个新的算法，来解决在有一定的丢包率的情况下的带宽稳定的问题，这个是谷歌为我们带来的干货，新的 TCP 拥塞控制算法 BBR (Bottleneck Bandwidth and RTT)，谷歌一向的做法是，先上生产，然后发论文，然后有可能开源，所以这个已经合并到了内核4.9分支当中，算法带来的改变在出的测试报告当中有很详细的数据展示，这个看多了可能反而不知道到底会有什么明显改变，特别是对于我们自己的场景</p>
<p>那么本篇就是来做一个实践的，开看看在通用的一些场景下，这个改变有多大，先说下结果，是真的非常大<br><a id="more"></a></p>
<h2 id="实践">实践</h2><p>还是我的两台机器lab8106和lab8107,lab8106做一个webserver，lab8107模拟客户端，用简单的wget来进行测试，环境为同一个交换机上的万兆网卡服务器</p>
<p>我们本次测试只测试一种丢包率的情况就是1%，有兴趣的情况下，可以自己去做些其他丢包率的测试，大多数写在丢包率20%以上的时候，效果可能没那么好，这个高丢包率不是我们探讨的情况，毕竟不是常用的场景</p>
<h3 id="安装新内核">安装新内核</h3><p>内核可以自己选择4.9或者以上的进行安装，也可以用yum安装,这里只是测试，就yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum --enablerepo=elrepo-kernel install kernel-ml</span><br></pre></td></tr></table></figure></p>
<p>修改启动项<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-editenv list</span><br><span class="line">grub2-set-default <span class="string">'CentOS Linux (4.9.5-1.el7.elrepo.x86_64) 7 (Core)'</span></span><br><span class="line">grub2-editenv list</span><br></pre></td></tr></table></figure></p>
<h3 id="准备下载数据">准备下载数据</h3><p>准备一个web服务器然后把一个iso丢到根目录下，用于客户端的wget</p>
<h3 id="设置丢包率">设置丢包率</h3><p>这里用tc进行控制的，也就是一条命令就可以了,这个还可以做其他很多控制，可以自行研究<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tc qdisc add dev enp2s0f0 root netem loss <span class="number">1</span>%</span><br></pre></td></tr></table></figure></p>
<p>如果需要取消限制<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tc qdisc del root dev enp2s0f0</span><br></pre></td></tr></table></figure></p>
<h3 id="设置新的算法">设置新的算法</h3><p>讲下面的两个配置文件添加到/etc/sysctl.conf<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_congestion_control=bbr</span><br><span class="line">net.core.default_qdisc=fq</span><br></pre></td></tr></table></figure></p>
<p>然后执行sysctl -p让它生效</p>
<p>检查是参数是否生效<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 rpmbuild]<span class="comment"># sysctl net.ipv4.tcp_available_congestion_control</span></span><br><span class="line">net.ipv4.tcp_available_congestion_control = bbr cubic reno</span><br></pre></td></tr></table></figure></p>
<p>检查模块是否开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 rpmbuild]<span class="comment"># lsmod | grep bbr</span></span><br><span class="line">tcp_bbr                <span class="number">16384</span>  <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>如果需要恢复成默认的就修改成下面这个值，然后执行sysct -p恢复默认<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_congestion_control = cubic</span><br><span class="line">net.core.default_qdisc = pfifo_fast</span><br></pre></td></tr></table></figure></p>
<h3 id="开始测试">开始测试</h3><p>为了避免磁盘本身的写入速度的影响，我们直接将数据wget到内存当中去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># cd /dev/shm</span></span><br></pre></td></tr></table></figure></p>
<p>写入到这个目录当中的数据就是直接写入内存的<br>我们先来对比下没有丢包的时候的速度</p>
<h4 id="默认算法，无丢包率">默认算法，无丢包率</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> wget http://<span class="number">192.168</span>.<span class="number">8.106</span>/FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso</span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">24</span> <span class="number">12</span>:<span class="number">34</span>:<span class="number">01</span> (<span class="number">909</span> MB/s) - ‘FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso’ saved</span><br></pre></td></tr></table></figure>
<h4 id="BBR算法，无丢包率">BBR算法，无丢包率</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://<span class="number">192.168</span>.<span class="number">8.106</span>/FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso</span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">24</span> <span class="number">12</span>:<span class="number">36</span>:<span class="number">21</span> (<span class="number">913</span> MB/s) - ‘FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso’ saved</span><br></pre></td></tr></table></figure>
<p>上面的两组数据基本一样，没有什么差别<br>下面的测试将丢包率控制到1%，然后继续测试</p>
<h4 id="默认算法，1%丢包率">默认算法，1%丢包率</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://<span class="number">192.168</span>.<span class="number">8.106</span>/FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso</span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">24</span> <span class="number">12</span>:<span class="number">38</span>:<span class="number">47</span> (<span class="number">142</span> MB/s) - ‘FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso’ saved</span><br></pre></td></tr></table></figure>
<p>可以看到在1%丢包率下，速度已经降为正常的1/6左右了，是一个很大的衰减</p>
<h4 id="BBR算法，1%丢包率">BBR算法，1%丢包率</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://<span class="number">192.168</span>.<span class="number">8.106</span>/FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso</span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">24</span> <span class="number">12</span>:<span class="number">40</span>:<span class="number">25</span> (<span class="number">896</span> MB/s) - ‘FreeBSD-<span class="number">10.2</span>-RELEASE-amd64-dvd1.iso’</span><br></pre></td></tr></table></figure>
<p>可以看到在1%丢包率下，还能维持接近900MB/s的下载速度，相对于默认算法，相差了真是非常非常的大，google在很多情况下技术甩了其他公司真的是几条街了</p>
<h2 id="总结">总结</h2><p>上面的测试通过一个简单的场景来验证了bbr算法对于丢包情况下的带宽的优化，这个对于一些提供下载服务，并且有一定的丢包率的场景的情况下，能够有很大的改善，所以算法对于技术的改变还是非常大的，很多时候就是这种异常情况下的差别，才是真正的差别</p>
<p>顺便提一句微博的技术经理@来去之间说的一句话：</p>
<blockquote>
<p>曾经有同事问我，为啥有些新业务给老员工做，交学费，而不是市场上招人更有效率。。。俺说渣浪业务起起伏伏，如果所有战线都用雇佣兵，顺的时候势如破竹，逆的时候兵败山倒了。。公司和员工都是相互扶持的，有些新业务，员工有能力做，只是经验不足，公司多付出一些，就当给未来不顺的时候上一份保险了</p>
</blockquote>
<p>所以作为管理者，是不是多考虑多留住一些老员工，少期待一些雇佣兵</p>
<h2 id="相关链接">相关链接</h2><p><a href="https://www.zhihu.com/question/53559433" target="_blank" rel="external">Linux Kernel 4.9 中的 BBR 算法与之前的 TCP 拥塞控制相比有什么优势？</a><br><a href="https://www.mf8.biz/linux-kernel-with-tcp-bbr/" target="_blank" rel="external">Linux 升级内核开启 TCP BBR 实现高效单边加速</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-01-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/tcp_ip_protocol.gif" alt=""><br></center>

<p>如果你有订阅一些科技新闻，应该会有看过内核在4.9当中加入了一个新的算法，来解决在有一定的丢包率的情况下的带宽稳定的问题，这个是谷歌为我们带来的干货，新的 TCP 拥塞控制算法 BBR (Bottleneck Bandwidth and RTT)，谷歌一向的做法是，先上生产，然后发论文，然后有可能开源，所以这个已经合并到了内核4.9分支当中，算法带来的改变在出的测试报告当中有很详细的数据展示，这个看多了可能反而不知道到底会有什么明显改变，特别是对于我们自己的场景</p>
<p>那么本篇就是来做一个实践的，开看看在通用的一些场景下，这个改变有多大，先说下结果，是真的非常大<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd-mirror配置指南-单向备份]]></title>
    <link href="http://www.zphj1987.com/2017/01/22/rbd-mirror-configure-onesidebackup/"/>
    <id>http://www.zphj1987.com/2017/01/22/rbd-mirror-configure-onesidebackup/</id>
    <published>2017-01-22T08:35:53.000Z</published>
    <updated>2017-02-03T05:10:41.622Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/mirrorpng.png" alt=""><br></center>


<p>RBD 的 mirroring 功能将在Jewel中实现的，这个Jewel版本已经发布了很久了,这个功能已经在这个发布的版本中实现了，本来之前写过一篇文章，但是有几个朋友根据文档配置后，发现还是有问题，自己在进行再次配置的时候也发现有些地方没讲清楚，容易造成误解，这里对文档进行再一次的梳理</p>
<p>一、基本原理<br>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p>
<p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p>
<ul>
<li>灾难恢复</li>
<li>全球块设备分布（跨地理位置）</li>
</ul>
<p>二、内部的实现</p>
<p><img src="http://static.zybuluo.com/zphj1987/owvwxjz172f2aokh699fd008/%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p>
<p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p>
<a id="more"></a>
<p>二、一个新的进程<br>一个新的守护程序：rbd-mirror 将会负责将一个镜像从一个集群同步到另一个，rbd-mirror需要在两个集群上都配置，它会同时连接本地和远程的集群。在jewel版本中还是一对一的方式，在以后的版本中会实现一对多的，所以在以后的版本可以配置一对多的备份</p>
<p>作为起点，这个功能讲使用配置文件连接集群，使用用户和密钥。使用admin用户就可以了，使用的验证方式就是默认的cephx的方式</p>
<p>为了相互识别，两个集群都需要相互注册使用 <code>rbd mirror pool peer add</code>命令， 这个在下面会实践</p>
<p>二、镜像<br><img src="http://static.zybuluo.com/zphj1987/60ehzzmtrvrf8rhdl9yelyqw/ceph-rbd-mirror-inside.png" alt="ceph-rbd-mirror-inside.png-80.8kB"><br>The RBD mirroring 依赖两个新的rbd的属性</p>
<ul>
<li>journaling: 启动后会记录image的事件</li>
<li>mirroring: 明确告诉rbd-mirror需要复制这个镜像</li>
</ul>
<p>也有命令可以禁用单独的某个镜像。journaling可以看做是另一个rbd的image（一些rados对象），一般情况下，先写日志，然后返回客户端，然后被写入底层的rbd的image，出于性能考虑，这个journal可以跟它的镜像不在一个存储池当中，目前是一个image一个journal，最近应该会沿用这个策略，直到ceph引入一致性组。关于一致性组的概念就是一组卷，然后用的是一个RBD image。可以在所有的组中执行快照操作，有了一致性的保证，所有的卷就都在一致的状态。当一致性组实现的时候，我们就可以用一个journal来管理所有的RBD的镜像</p>
<p>可以给一个已经存在image开启journal么，可以的，ceph将会将你的镜像做一个快照，然后对快照做一个复制，然后开启journal，这都是后台执行的一个任务</p>
<p>可以启用和关闭单个镜像或者存储池的mirror功能，如果启用了journal功能，那么每个镜像将会被复制</p>
<p>可以使用 rbd mirror pool enable启用它</p>
<p>三、灾难恢复<br>交叉同步复制是可以的，默认的就是这个方式，这意味着<strong>两个地方的存储池名称需要相同的</strong>这个会带来两个问题</p>
<ul>
<li>使用相同的存储做备份做使用会影响性能的</li>
<li>相同的池名称在进行恢复的时候也更容易。openstack里面只需要记录卷ID即可</li>
</ul>
<p>每个image都有 mirroring_directory 记录当前active的地方。在本地镜像提示为 primary的时候，是可写的并且远程的站点上就会有锁，这个image就是不可写的。只有在primary镜像降级，备份的点升级就可以了，demoted 和 promoted来控制这里，这就是为什么引入了等级制度，一旦备份的地方升级了，那么主的就自动降级了，这就意味着同步的方向就会发生变化了</p>
<p>如果出现脑裂的情况，那么rbd-mirror将会停止同步，你自己需要判断哪个是最新的image，然后手动强制去同步<code>rbd mirror image resync</code></p>
<p>上面基本参照的是sebastien翻译的，原文只是做了简短的说明，下面是我的实践部分</p>
<hr>
<h1 id="配置实践部分">配置实践部分</h1><h2 id="先介绍下一些简单的概念">先介绍下一些简单的概念</h2><h3 id="rbd-mirror_进程">rbd-mirror 进程</h3><p>rbd-mirror进程负责将镜像从一个Ceph集群同步到另一个集群</p>
<p>根据复制的类型，rbd-mirror可以在单个集群上或者是镜像的两个集群上都运行</p>
<ul>
<li>单向备份<ul>
<li>当数据从主集群备份到备用的集群的时候，rbd-mirror仅在备份群集上运行。</li>
</ul>
</li>
<li>双向备份<ul>
<li>如果两个集群互为备份的时候，rbd-mirror需要在两个集群上都运行</li>
</ul>
</li>
</ul>
<p>为了更清晰的理解这个配置，我们本次实践只进行单向备份的配置，也就是只备份一个集群的镜像到另外一个集群</p>
<blockquote>
<p>rbd-mirror的每个实例必须能够同时连接到两个Ceph集群,因为需要同两个集群都进行数据通信<br>每个Ceph集群只运行一个rbd-mirror进程</p>
</blockquote>
<h3 id="Mirroring_模式">Mirroring 模式</h3><p>mirroring是基于存储池进行的peer，ceph支持两种模式的镜像，根据镜像来划分有：</p>
<ul>
<li><p>存储池模式</p>
<ul>
<li>一个存储池内的所有镜像都会进行备份</li>
</ul>
</li>
<li><p>镜像模式</p>
<ul>
<li>只有指定的镜像才会进行备份</li>
</ul>
</li>
</ul>
<p>本次配置选择的模式是镜像的模式，也就是指定的镜像才会进行备份</p>
<h3 id="Image_状态">Image 状态</h3><p>做了mirroring的Image的状态有:<br>primary (可以修改)<br>non-primary (不能修改).<br>当第一次对image进行开启mirroring的时候 .Images 自动 promoted 为 primary</p>
<h2 id="开始配置">开始配置</h2><p>首先配置两个集群，配置的集群都没有更改名称，都是ceph，我们通过配置文件来控制集群的识别，我的环境是单主机集群，lab8106和lab8107两台机器<br>lab8106为local集群，lab8107为remote集群，准备把lab8106的image备份到lab8107的集群上<br>在ceph.conf当中添加：</p>
<blockquote>
<p>rbd default features = 125</p>
</blockquote>
<p>需要exclusive-lock和journaling属性<br>开启这两个个属性可以在创建的时候指定<br>语法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd create &lt;image-name&gt; --size &lt;megabytes&gt; --pool &lt;pool-name&gt; --imagefeature &lt;feature&gt;</span><br></pre></td></tr></table></figure></p>
<p>例子：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd create image-<span class="number">1</span> --size <span class="number">1024</span> --pool rbd --image-feature exclusive-lock,journaling</span><br></pre></td></tr></table></figure></p>
<p>这个是在lab8106上执行，因为我们需要对lab8106进行备份<br>也可以在创建以后开启属性：<br>语法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd feature <span class="built_in">enable</span> &lt;pool-name&gt;/&lt;image-name&gt; &lt;feature-name&gt;</span><br></pre></td></tr></table></figure></p>
<p>例子：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd feature <span class="built_in">enable</span> rbd/image-<span class="number">1</span> exclusive-lock</span><br><span class="line">rbd feature <span class="built_in">enable</span> rbd/image-<span class="number">1</span> journaling</span><br></pre></td></tr></table></figure></p>
<p>上面有三种方法开启属性，选择习惯或者需要的一种就可以</p>
<h3 id="开启存储池的mirror的模式">开启存储池的mirror的模式</h3><p>我们准备开启集群镜像备份模式<br>语法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool <span class="built_in">enable</span> &lt;pool-name&gt; &lt;mode&gt;</span><br></pre></td></tr></table></figure></p>
<p>在lab8106主机上执行:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool <span class="built_in">enable</span> rbd image</span><br></pre></td></tr></table></figure></p>
<p>在lab8107主机上执行：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool <span class="built_in">enable</span> rbd image</span><br></pre></td></tr></table></figure></p>
<p>上面的操作是对rbd存储池启动image模式的mirror配置<br>如果需要关闭：<br>语法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool <span class="built_in">disable</span> &lt;pool-name&gt; &lt;mode&gt;</span><br></pre></td></tr></table></figure></p>
<p>执行:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool <span class="built_in">disable</span> rbd image</span><br></pre></td></tr></table></figure></p>
<h3 id="处理配置文件和kerring">处理配置文件和kerring</h3><p>在lab8106上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># scp /etc/ceph/ceph.conf lab8107:/etc/ceph/local.conf</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># scp /etc/ceph/ceph.client.admin.keyring lab8107:/etc/ceph/local.client.admin.keyring</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#cp /etc/ceph/ceph.conf /etc/ceph/local.conf</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#cp /etc/ceph/ceph.client.admin.keyring /etc/ceph/local.client.admin.keyring</span></span><br></pre></td></tr></table></figure></p>
<p>在lab8107上执行：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph]<span class="comment"># scp /etc/ceph/ceph.conf lab8106:/etc/ceph/remote.conf</span></span><br><span class="line">[root@lab8107 ceph]<span class="comment"># scp /etc/ceph/ceph.client.admin.keyring lab8106:/etc/ceph/remote.client.admin.keyring</span></span><br><span class="line">[root@lab8107 ceph]<span class="comment">#cp /etc/ceph/ceph.conf /etc/ceph/remote.conf</span></span><br><span class="line">[root@lab8107 ceph]<span class="comment">#cp /etc/ceph/ceph.client.admin.keyring /etc/ceph/remote.client.admin.keyring</span></span><br></pre></td></tr></table></figure></p>
<p>执行完了后在两台机器上给予权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># chown ceph:ceph -R /etc/ceph</span></span><br><span class="line">[root@lab8107 ceph]<span class="comment"># chown ceph:ceph -R /etc/ceph</span></span><br></pre></td></tr></table></figure></p>
<p>检验上面设置是否完成<br>在lab8106执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph --cluster local mon stat</span></span><br><span class="line">e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;, election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph --cluster remote mon stat</span></span><br><span class="line">e1: <span class="number">1</span> mons at &#123;lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;, election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8107</span><br></pre></td></tr></table></figure></p>
<p>在lab8107执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph<span class="comment"># ceph --cluster local mon stat</span></span><br><span class="line">e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;, election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">root@lab8107:~/ceph<span class="comment"># ceph --cluster remote mon stat</span></span><br><span class="line">e1: <span class="number">1</span> mons at &#123;lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;, election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8107</span><br></pre></td></tr></table></figure></p>
<p>到这里就是两个集群可以通过local和remote进行通信了</p>
<h3 id="增加peer">增加peer</h3><p>我们这里是做单个集群的备份，为了方便我们这里都用admin的keyring<br>语法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool peer add &lt;pool-name&gt; &lt;client-name&gt;@&lt;cluster-name&gt;</span><br></pre></td></tr></table></figure></p>
<p>这个是为了让rbd-mirror进程找到它peer的集群的存储池<br>在lab8106上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rbd --cluster local mirror pool peer add rbd client.admin@remote</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># rbd --cluster remote mirror pool peer add rbd client.admin@local</span></span><br></pre></td></tr></table></figure></p>
<p>查询peer状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rbd mirror pool info rbd --cluster local</span></span><br><span class="line">Mode: image</span><br><span class="line">Peers: </span><br><span class="line">  UUID                                 NAME   CLIENT       </span><br><span class="line">  a050a0f5-<span class="number">9448</span>-<span class="number">43</span>f2-<span class="number">872</span>f-<span class="number">87</span>c394083871 remote client.admin</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># rbd mirror pool info rbd --cluster remote</span></span><br><span class="line">Mode: image</span><br><span class="line">Peers: </span><br><span class="line">  UUID                                 NAME  CLIENT       </span><br><span class="line">  <span class="number">8</span>d7b3fa4-be44-<span class="number">4</span>e25-b0b7-cf4bdb62bf10 <span class="built_in">local</span> client.admin</span><br></pre></td></tr></table></figure></p>
<p>如果需要删除peer<br>语法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror pool peer remove &lt;pool-name&gt; &lt;peer-uuid&gt;</span><br></pre></td></tr></table></figure></p>
<p>查询存储池状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rbd mirror pool status rbd</span></span><br><span class="line">health: OK</span><br><span class="line">images: <span class="number">0</span> total</span><br></pre></td></tr></table></figure></p>
<h3 id="开启image的mirror">开启image的mirror</h3><p>在lab8106执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd mirror image <span class="built_in">enable</span> rbd/image-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查询镜像的状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rbd info rbd/image-1</span></span><br><span class="line">rbd image <span class="string">'image-1'</span>:</span><br><span class="line">	size <span class="number">1024</span> MB <span class="keyword">in</span> <span class="number">256</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">102</span>c2ae8944a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: exclusive-lock, journaling</span><br><span class="line">	flags: </span><br><span class="line">	journal: <span class="number">102</span>c2ae8944a</span><br><span class="line">	mirroring state: enabled</span><br><span class="line">	mirroring global id: dabdbbed-<span class="number">7</span>c06-<span class="number">4</span>e1d-b860-<span class="number">8</span>dd104509565</span><br><span class="line">	mirroring primary: <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="开启rbd-mirror的同步进程">开启rbd-mirror的同步进程</h3><p>先用调试模式启动进程看看情况<br>在lab8107的机器上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph]<span class="comment"># rbd-mirror -d --setuser ceph --setgroup ceph --cluster remote -i admin</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">43</span>:<span class="number">53.688820</span> <span class="number">7</span><span class="built_in">fc</span>926dc6c40  <span class="number">0</span> <span class="built_in">set</span> uid:gid to <span class="number">167</span>:<span class="number">167</span> (ceph:ceph)</span><br><span class="line"><span class="number">2017</span>-<span class="number">01</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">43</span>:<span class="number">53.688840</span> <span class="number">7</span><span class="built_in">fc</span>926dc6c40  <span class="number">0</span> ceph version <span class="number">10.2</span>.<span class="number">5</span> (c461ee19ecbc0c5c330aca20f7392c9a00730367), process rbd-mirror, pid <span class="number">32080</span></span><br></pre></td></tr></table></figure></p>
<p>如果确认没问题就用服务来控制启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/ceph-rbd-mirror@.service</span><br></pre></td></tr></table></figure></p>
<p>修改</p>
<blockquote>
<p>Environment=CLUSTER=remote</p>
</blockquote>
<p>然后启动<br>语法为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph]<span class="comment">#systemctl start ceph-rbd-mirror@&lt;client-id&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>在lab8107上启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph]<span class="comment"># systemctl start  ceph-rbd-mirror@admin</span></span><br><span class="line">[root@lab8107 ceph]<span class="comment"># ps -ef|grep rbd</span></span><br><span class="line">ceph      <span class="number">4325</span>     <span class="number">1</span>  <span class="number">1</span> <span class="number">17</span>:<span class="number">59</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> /usr/bin/rbd-mirror <span class="operator">-f</span> --cluster remote --id admin --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure></p>
<p>查询镜像的同步的状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd mirror image status rbd/image-1 --cluster remote</span></span><br><span class="line">image-<span class="number">1</span>:</span><br><span class="line">  global_id:   dabdbbed-<span class="number">7</span>c06-<span class="number">4</span>e1d-b860-<span class="number">8</span>dd104509565</span><br><span class="line">  state:       up+replaying</span><br><span class="line">  description: replaying, master_position=[object_number=<span class="number">2</span>, tag_tid=<span class="number">2</span>, entry_tid=<span class="number">3974</span>], mirror_position=[object_number=<span class="number">3</span>, tag_tid=<span class="number">2</span>, entry_tid=<span class="number">2583</span>], entries_behind_master=<span class="number">1391</span></span><br><span class="line">  last_update: <span class="number">2017</span>-<span class="number">01</span>-<span class="number">22</span> <span class="number">17</span>:<span class="number">54</span>:<span class="number">22</span></span><br></pre></td></tr></table></figure></p>
<p>检查数据是否同步<br>在lab8107执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph]<span class="comment"># rbd info rbd/image-1</span></span><br><span class="line">rbd image <span class="string">'image-1'</span>:</span><br><span class="line">	size <span class="number">1024</span> MB <span class="keyword">in</span> <span class="number">256</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">127</span>b515f007c</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: exclusive-lock, journaling</span><br><span class="line">	flags: </span><br><span class="line">	journal: <span class="number">127</span>b515f007c</span><br><span class="line">	mirroring state: enabled</span><br><span class="line">	mirroring global id: fb976ffb<span class="operator">-a</span>71e-<span class="number">4714</span>-<span class="number">8464</span>-<span class="number">06381643</span>f984</span><br><span class="line">	mirroring primary: <span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到数据已经同步过来了</p>
<h2 id="总结">总结</h2><p>通过配置文件控制，可以实现集群名称不修改<br>rbd-mirror进程是在备份的集群上面启动的，并且是要能跟主集群和备份集群都能通信的，也就是peer都需要做，并且用户权限要控制好</p>
<p>根据上面的操作流程操作下来，应该是能够配置好rbd-mirror的</p>
<h2 id="相关链接">相关链接</h2><p><a href="http://www.sebastien-han.fr/blog/2016/03/28/ceph-jewel-preview-ceph-rbd-mirroring/" target="_blank" rel="external">Ceph Jewel Preview: Ceph RBD mirroring</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-01-22</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/mirrorpng.png" alt=""><br></center>


<p>RBD 的 mirroring 功能将在Jewel中实现的，这个Jewel版本已经发布了很久了,这个功能已经在这个发布的版本中实现了，本来之前写过一篇文章，但是有几个朋友根据文档配置后，发现还是有问题，自己在进行再次配置的时候也发现有些地方没讲清楚，容易造成误解，这里对文档进行再一次的梳理</p>
<p>一、基本原理<br>我们试图解决的或者至少需要克服的问题是，ceph在内部是强一致性的，这个对于跨区域的情况数据同步是无法接受的，一个请求需要异地返回再确认完成，这个在性能上肯定是无法接受的，这就是为什么基本上无法部署跨区域的ceph集群</p>
<p>因此我们需要有一种机制能够让我们在不同区域的集群之间复制块设备。这个能够帮助我们实现两个功能：</p>
<ul>
<li>灾难恢复</li>
<li>全球块设备分布（跨地理位置）</li>
</ul>
<p>二、内部的实现</p>
<p><img src="http://static.zybuluo.com/zphj1987/owvwxjz172f2aokh699fd008/%E7%94%BB%E5%9B%BE.png" alt="画图.png-34.8kB"></p>
<p>从上图所示是进行的主备模式的备份，其实这个只是看怎么应用了，在里面是自动实现的主主的模式，双向同步的，只是在应用中需要注意不要去同时操作同一个image，这个功能是作为主备去使用的，以备真正有问题的时候去实现故障恢复，这个同步是异步的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的rbd备份软件ceph-backup]]></title>
    <link href="http://www.zphj1987.com/2017/01/19/ceph-rbd-ceph-backup/"/>
    <id>http://www.zphj1987.com/2017/01/19/ceph-rbd-ceph-backup/</id>
    <published>2017-01-19T14:49:33.000Z</published>
    <updated>2017-03-02T07:10:36.787Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/backupmp.png" alt=""><br></center>

<p>teralytics是一家国外的大数据公司，这个是他们开源的ceph的备份的工具，在twitter上搜索相关信息的时候看到，觉得不错就拿来试用一番</p>
<h2 id="这是个什么软件">这是个什么软件</h2><p>一个用来备份 ceph 的 rbd 的image的开源软件，提供了两种模式<br>增量：在给定备份时间窗口内基于 rbd 快照的增量备份<br>完全：完整镜像导出时不包含快照</p>
<blockquote>
<p>注意一致性：此工具可以生成 rbd 镜像的快照，而不会感知到它们的文件系统的状态，注意下 rbd 快照的一致性限制（<a href="http://docs.ceph.com/docs/hammer/rbd/rbd-snapshot/" target="_blank" rel="external">官网文档</a>） 由于“完全”模式不使用快照，“完全”模式下的实时映像备份不一致（“增量”模式始终使用快照）</p>
</blockquote>
<p>超过时间窗口以后，会进行一次全量备份，并且把之前的快照删除掉，重新进行一次全量备份，并且基于这个时间窗口计算是否需要删除备份的文件</p>
<p>软件包含以下功能：</p>
<ul>
<li>支持存储池和多image的指定</li>
<li>支持自定义备份目标路径</li>
<li>配置文件支持</li>
<li>支持备份窗口设置</li>
<li>支持压缩选项</li>
<li>支持增量和全量备份的配置<a id="more"></a>
<h2 id="编译安装">编译安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#git clone https://github.com/teralytics/ceph-backup.git</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cd ceph-backup</span></span><br><span class="line">[root@lab8106 ceph-backup]<span class="comment"># python setup.py install</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>安装过程中会下载一些东西，注意要有网络，需要等待一会</p>
<h2 id="准备配置文件">准备配置文件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-backup]<span class="comment"># mkdir /etc/cephbackup/</span></span><br><span class="line">[root@lab8106 ceph-backup]<span class="comment"># cp ceph-backup.cfg /etc/cephbackup/cephbackup.conf</span></span><br></pre></td></tr></table></figure>
<p>我的配置文件如下，备份 rbd 存储的 zp 的镜像，支持多 image，images后面用逗号隔开就可以<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat /etc/cephbackup/cephbackup.conf </span></span><br><span class="line">[rbd]</span><br><span class="line">window size = <span class="number">7</span></span><br><span class="line">window unit = days</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = yes</span><br><span class="line">ceph config = /etc/ceph/ceph.conf</span><br><span class="line">backup mode = full</span><br><span class="line">check mode = no</span><br></pre></td></tr></table></figure></p>
<h2 id="开始备份">开始备份</h2><h3 id="全量备份配置">全量备份配置</h3><p>上面的配置文件已经写好了，直接执行备份命令就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cephbackup</span></span><br><span class="line">Starting backup <span class="keyword">for</span> pool rbd</span><br><span class="line">Full ceph backup</span><br><span class="line">Images to backup:</span><br><span class="line">	rbd/zp</span><br><span class="line">Backup folder: /tmp/</span><br><span class="line">Compression: True</span><br><span class="line">Check mode: False</span><br><span class="line">Taking full backup of images: zp</span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40960</span> MB <span class="keyword">in</span> <span class="number">10240</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">25496</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags: </span><br><span class="line">Exporting image zp to /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">Compress mode activated</span><br><span class="line"><span class="comment"># rbd export rbd/zp /tmp/rbd/zp/zp_UTC20170119T092933.full</span></span><br><span class="line">Exporting image: <span class="number">100</span>% complete...done.</span><br><span class="line"><span class="comment"># tar Scvfz /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz /tmp/rbd/zp/zp_UTC20170119T092933.full</span></span><br><span class="line">tar: Removing leading `/<span class="string">' from member names</span></span><br></pre></td></tr></table></figure></p>
<p>压缩的如果开了，正好文件也是稀疏文件的话，需要等很久，压缩的效果很好，dd 生成的文件可以压缩到很小</p>
<p>检查备份生成的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /tmp/rbd/zp/zp_UTC20170119T092933.full*</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">42949672960</span> Jan <span class="number">19</span> <span class="number">17</span>:<span class="number">29</span> /tmp/rbd/zp/zp_UTC20170119T092933.full</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root           <span class="number">0</span> Jan <span class="number">19</span> <span class="number">17</span>:<span class="number">29</span> /tmp/rbd/zp/zp_UTC20170119T092933.full.tar.gz</span><br></pre></td></tr></table></figure></p>
<h3 id="全量备份的还原">全量备份的还原</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd import /tmp/rbd/zp/zp_UTC20170119T092933.full zpbk</span><br></pre></td></tr></table></figure>
<p>检查数据，没有问题</p>
<h3 id="增量备份配置">增量备份配置</h3><p>写下增量配置的文件，修改下备份模式的选项<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[rbd]</span><br><span class="line">window size = <span class="number">7</span></span><br><span class="line">window unit = day</span><br><span class="line">destination directory = /tmp/</span><br><span class="line">images = zp</span><br><span class="line">compress = yes</span><br><span class="line">ceph config = /etc/ceph/ceph.conf</span><br><span class="line">backup mode = incremental</span><br><span class="line">check mode = no</span><br></pre></td></tr></table></figure></p>
<p>执行多次进行增量备份以后是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> [root@lab8106 ~]<span class="comment">#ll  /tmp/rbd/zpbk/</span></span><br><span class="line">total <span class="number">146452</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">42949672960</span> Jan <span class="number">19</span> <span class="number">18</span>:<span class="number">04</span> zpbk@UTC20170119T100339.full</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root       <span class="number">66150</span> Jan <span class="number">19</span> <span class="number">18</span>:<span class="number">05</span> zpbk@UTC20170119T100546.diff_from_UTC20170119T100339</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root          <span class="number">68</span> Jan <span class="number">19</span> <span class="number">18</span>:<span class="number">05</span> zpbk@UTC20170119T100550.diff_from_UTC20170119T100546</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root          <span class="number">68</span> Jan <span class="number">19</span> <span class="number">18</span>:<span class="number">06</span> zpbk@UTC20170119T100606.diff_from_UTC20170119T100550</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root          <span class="number">68</span> Jan <span class="number">19</span> <span class="number">18</span>:<span class="number">06</span> zpbk@UTC20170119T100638.diff_from_UTC20170119T100606</span><br></pre></td></tr></table></figure></p>
<h3 id="增量备份的还原">增量备份的还原</h3><p>分成多个步骤进行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>、进行全量的恢复</span><br><span class="line"><span class="comment"># rbd import config@UTC20161130T170848.full dest_image</span></span><br><span class="line"><span class="number">2</span>、重新创建基础快照</span><br><span class="line"><span class="comment"># rbd snap create dest_image@UTC20161130T170848</span></span><br><span class="line"><span class="number">3</span>、还原增量的快照(多次执行)</span><br><span class="line"><span class="comment"># rbd import-diff config@UTC20161130T170929.diff_from_UTC20161130T170848 dest_image</span></span><br></pre></td></tr></table></figure></p>
<p>本测试用例还原步骤就是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd  import zpbk@UTC20170119T100339.full zpnew</span><br><span class="line">rbd snap create zpnew@UTC20170119T100339</span><br><span class="line">rbd import-diff zpbk@UTC20170119T100546.diff_from_UTC20170119T100339  zpnew</span><br><span class="line">rbd import-diff zpbk@UTC20170119T100550.diff_from_UTC20170119T100546  zpnew</span><br><span class="line">rbd import-diff zpbk@UTC20170119T100606.diff_from_UTC20170119T100550  zpnew</span><br><span class="line">rbd import-diff zpbk@UTC20170119T100638.diff_from_UTC20170119T100606  zpnew</span><br></pre></td></tr></table></figure></p>
<p>检查数据，没有问题</p>
<h2 id="总结">总结</h2><p>这个软件基于python的实现，可以说作者的实现逻辑是很清晰的，并且提供了配置文件的方式，基本上是各个细节都考虑的比较到位，很容易上手，可以直接拿来使用，或者集成到自己的平台中去，是一个很好的软件</p>
<h2 id="补充">补充</h2><p>集群有个bug，在rbd import名称的时候如果带了@符号，那么导入的时候就会有问题，具体如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd import /tmp/ls@<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>那么无法 rbd info ls@1，无法 rbd rm ls@1操作了，这个地方需要代码进行修改进行屏蔽，一般正常操作也没问题，但是万一出现了，怎么解决呢？</p>
<p>下面举个例子来讲述解决过程：<br>假设我的操作是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rbd import /tmp/ls@1</span></span><br></pre></td></tr></table></figure></p>
<p>首先查询下image的id<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rados -p rbd get rbd_id.ls@1 rbd_id.ls@1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#cat rbd_id.ls@1</span></span><br><span class="line"><span class="number">304</span>b76b8b4567</span><br></pre></td></tr></table></figure></p>
<p>得到id是这个<br>删除header（后缀是上面获取的id）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rados -p rbd rm rbd_header.304b76b8b4567</span></span><br></pre></td></tr></table></figure></p>
<p>删除data<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rados -p rbd rm rbd_data.304b76b8b4567.0000000000000000</span></span><br></pre></td></tr></table></figure></p>
<p>删除id文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rados -p rbd rm rbd_id.ls@1</span></span><br></pre></td></tr></table></figure></p>
<p>查询元数据信息进行删除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#rados -p rbd listomapvals rbd_directory</span></span><br></pre></td></tr></table></figure></p>
<p>删除的一个是上面的获取的id，一个是名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd rmomapkey rbd_directory id_304b76b8b4567</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd rmomapkey rbd_directory name_ls@1</span></span><br></pre></td></tr></table></figure></p>
<p>再次检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd listomapvals rbd_directory</span><br></pre></td></tr></table></figure></p>
<p>再次rbd ls检查，已经好了</p>
<h2 id="相关链接">相关链接</h2><p><a href="http://www.zphj1987.com/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/" target="_blank" rel="external">rbd的增量备份和恢复</a><br><a href="https://github.com/teralytics/ceph-backup" target="_blank" rel="external">ceph-backup的github</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-01-19</td>
</tr>
<tr>
<td style="text-align:center">误导入的恢复</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/backupmp.png" alt=""><br></center>

<p>teralytics是一家国外的大数据公司，这个是他们开源的ceph的备份的工具，在twitter上搜索相关信息的时候看到，觉得不错就拿来试用一番</p>
<h2 id="这是个什么软件">这是个什么软件</h2><p>一个用来备份 ceph 的 rbd 的image的开源软件，提供了两种模式<br>增量：在给定备份时间窗口内基于 rbd 快照的增量备份<br>完全：完整镜像导出时不包含快照</p>
<blockquote>
<p>注意一致性：此工具可以生成 rbd 镜像的快照，而不会感知到它们的文件系统的状态，注意下 rbd 快照的一致性限制（<a href="http://docs.ceph.com/docs/hammer/rbd/rbd-snapshot/">官网文档</a>） 由于“完全”模式不使用快照，“完全”模式下的实时映像备份不一致（“增量”模式始终使用快照）</p>
</blockquote>
<p>超过时间窗口以后，会进行一次全量备份，并且把之前的快照删除掉，重新进行一次全量备份，并且基于这个时间窗口计算是否需要删除备份的文件</p>
<p>软件包含以下功能：</p>
<ul>
<li>支持存储池和多image的指定</li>
<li>支持自定义备份目标路径</li>
<li>配置文件支持</li>
<li>支持备份窗口设置</li>
<li>支持压缩选项</li>
<li>支持增量和全量备份的配置]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA['sortbitwise'是什么意思]]></title>
    <link href="http://www.zphj1987.com/2017/01/12/sortbitwise-mean/"/>
    <id>http://www.zphj1987.com/2017/01/12/sortbitwise-mean/</id>
    <published>2017-01-12T04:39:24.000Z</published>
    <updated>2017-02-03T05:17:24.304Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/techie.jpg" alt=""><br></center>

<h2 id="问题">问题</h2><p>flag sortbitwise 在ceph中是什么意思,在Jewel版本下可以看到多了这个flags</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 current]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster ffe7a8db-c671-<span class="number">4</span>b45<span class="operator">-a</span>784-ddb41e633905</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">      fsmap e4: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab8106=up:active&#125;</span><br><span class="line">     osdmap e132: <span class="number">8</span> osds: <span class="number">8</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v206294: <span class="number">201</span> pgs, <span class="number">5</span> pools, <span class="number">4684</span> MB data, <span class="number">1214</span> objects</span><br><span class="line">            <span class="number">9669</span> MB used, <span class="number">2216</span> GB / <span class="number">2226</span> GB avail</span><br><span class="line">                 <span class="number">201</span> active+clean</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="找到的相关资料">找到的相关资料</h2><blockquote>
<p>After upgrading, users should set the ‘sortbitwise’ flag to enable the new internal object sort order: ceph osd set sortbitwise<br>This flag is important for the new object enumeration API and for new backends like BlueStore.</p>
</blockquote>
<p>From <a href="http://docs.ceph.com/docs/master/release-notes/#upgrading-from-infernalis-or-hammer" target="_blank" rel="external">Ceph release notes</a></p>
<blockquote>
<p>commit 383185bfbae74797cdb44f50b4bf651422800ff1<br>Author: Sage Weil <a href="&#109;&#x61;&#105;&#x6c;&#116;&#x6f;&#x3a;&#115;&#x61;&#103;&#x65;&#x40;&#x72;&#101;&#100;&#104;&#x61;&#116;&#46;&#x63;&#x6f;&#109;">&#115;&#x61;&#103;&#x65;&#x40;&#x72;&#101;&#100;&#104;&#x61;&#116;&#46;&#x63;&#x6f;&#109;</a><br>Date: Fri Aug 7 16:14:09 2015 -0400<br>mon/OSDMonitor: osd set/unset sortbitwise<br> Add monitor command to flip the switch on the OSD hobject_t sort<br> order.</p>
</blockquote>
<p>From git</p>
<p>第一次在源码中出现:</p>
<blockquote>
<p>commit 138f58493715e386929f152424b70df37843541b<br>Author: John Spray <a href="&#x6d;&#97;&#105;&#108;&#x74;&#111;&#58;&#106;&#x6f;&#x68;&#110;&#x2e;&#x73;&#112;&#114;&#97;&#x79;&#x40;&#114;&#x65;&#x64;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;">&#106;&#x6f;&#x68;&#110;&#x2e;&#x73;&#112;&#114;&#97;&#x79;&#x40;&#114;&#x65;&#x64;&#104;&#x61;&#x74;&#x2e;&#x63;&#111;&#x6d;</a><br>Date: Mon Aug 17 14:40:46 2015 -0400<br>osdc/Objecter: new-style pgls<br> Signed-off-by: John Spray <a href="&#109;&#97;&#105;&#x6c;&#x74;&#111;&#x3a;&#x6a;&#111;&#104;&#110;&#x2e;&#115;&#112;&#114;&#x61;&#121;&#64;&#114;&#101;&#x64;&#104;&#97;&#x74;&#46;&#x63;&#x6f;&#x6d;">&#x6a;&#111;&#104;&#110;&#x2e;&#115;&#112;&#114;&#x61;&#121;&#64;&#114;&#101;&#x64;&#104;&#97;&#x74;&#46;&#x63;&#x6f;&#x6d;</a><br> Signed-off-by: Sage Weil <a href="&#x6d;&#97;&#x69;&#108;&#x74;&#x6f;&#58;&#x73;&#97;&#103;&#101;&#64;&#114;&#x65;&#x64;&#104;&#97;&#x74;&#46;&#x63;&#111;&#x6d;">&#x73;&#97;&#103;&#101;&#64;&#114;&#x65;&#x64;&#104;&#97;&#x74;&#46;&#x63;&#111;&#x6d;</a></p>
</blockquote>
<p>From git</p>
<p>Related github issue: <a href="https://github.com/ceph/ceph/pull/4919/commits" target="_blank" rel="external">https://github.com/ceph/ceph/pull/4919/commits</a></p>
<p>初步结论: sortbitwise 内部排序算法的一个变化.之所以暴露出来是因为要兼容一些pre-jewel版本.在新的版本中应该保持开启状态.</p>
<p>以上转载自博客：<a href="https://medium.com/@george.shuklin/what-is-sortbitwise-flag-means-in-ceph-b4176748da42#.dfaw54rpf" target="_blank" rel="external">What ‘sortbitwise’ flag means in Ceph?</a></p>
<h2 id="红帽的官方回答">红帽的官方回答</h2><ul>
<li>如果你使用dev版本Infernalis或仍在开发中的LTS版本的Ceph的Jewel版本，你会看到这个标志在ceph状态输出默认启用</li>
<li>这个标志sortbitwise在Infernalis版本中引入</li>
<li>这个标志是在这个版本提交的upstream commit 968261b11ac30622c0606d1e2ddf422009e7d330</li>
</ul>
<p>下载ceph的源码，进入源码目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 all]<span class="comment"># git show 968261b11ac30622c0606d1e2ddf422009e7d330</span></span><br><span class="line">commit <span class="number">968261</span>b11ac30622c0606d1e2ddf422009e7d330</span><br><span class="line">Author: Sage Weil &lt;sage@redhat.com&gt;</span><br><span class="line">Date:   Fri Aug <span class="number">7</span> <span class="number">16</span>:<span class="number">01</span>:<span class="number">12</span> <span class="number">2015</span> -<span class="number">0400</span></span><br><span class="line"></span><br><span class="line">    osd/OSDMap: add a SORTBITWISE OSDMap flag</span><br><span class="line"></span><br><span class="line">    This flag will indicate that hobject_t<span class="string">'s shall hence-forth be</span><br><span class="line">    sorted in a bitwise fashion.</span><br><span class="line"></span><br><span class="line">    Signed-off-by: Sage Weil &lt;sage@redhat.com&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>正如我们在上面给定的提交的描述中所说，该标志将表明hobject_t的将以 bitwise fashion方式排序。<br>现在意味着现在的对象将在OSDs中以按位方式排序，并且此标志默认在Infernalis和Jewel发布版本中启用。</p>
<h2 id="总结">总结</h2><p>目前来看这个是底层的一个排序的算法的变动，对上层目前还不清楚是有什么可以可见的变化，总之，这个让它默认开启就行，不要去修改它就可以了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-01-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/techie.jpg" alt=""><br></center>

<h2 id="问题">问题</h2><p>flag sortbitwise 在ceph中是什么意思,在Jewel版本下可以看到多了这个flags</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 current]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster ffe7a8db-c671-<span class="number">4</span>b45<span class="operator">-a</span>784-ddb41e633905</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">      fsmap e4: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab8106=up:active&#125;</span><br><span class="line">     osdmap e132: <span class="number">8</span> osds: <span class="number">8</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v206294: <span class="number">201</span> pgs, <span class="number">5</span> pools, <span class="number">4684</span> MB data, <span class="number">1214</span> objects</span><br><span class="line">            <span class="number">9669</span> MB used, <span class="number">2216</span> GB / <span class="number">2226</span> GB avail</span><br><span class="line">                 <span class="number">201</span> active+clean</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[解决calamari无法获取节点信息的bug]]></title>
    <link href="http://www.zphj1987.com/2017/01/09/calamari-node-info/"/>
    <id>http://www.zphj1987.com/2017/01/09/calamari-node-info/</id>
    <published>2017-01-09T07:06:51.000Z</published>
    <updated>2017-02-03T05:18:13.739Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/saltstack.png" alt="salt-stack"><br></center>

<h2 id="前言">前言</h2><p>一直在做calamari的相关的一些打包和安装的工作，都是业余弄的东西，所以并没有仔细的进行功能点的验证测试，正好ceph社区群里面有人问了个问题</p>
<blockquote>
<p>calamari上是不是能看到ceph的version?</p>
</blockquote>
<p>对于这个问题，好像确实没有见到过，而之前正好有个页面看到是空的，当时还不清楚这个是什么用的</p>
<p><img src="http://static.zybuluo.com/zphj1987/mjyvzkd7q9x6eplb538o7ltc/image_1b611hfic17lj15ej1jp1pb31m979.png" alt="origin"></p>
<p>而另外一位群友贴出了这个地方的是有值的，这个地方是有BUG的，在咨询了相关的问题描述以后，我们来看下，可以如何解决这个问题<br><a id="more"></a></p>
<h2 id="问题解决过程">问题解决过程</h2><p>salt的软件版本：</p>
<ul>
<li>salt-master-2015.8.1-1.el7.noarch</li>
<li>salt-2015.8.1-1.el7.noarch</li>
<li>salt-minion-2015.8.1-1.el7.noarch</li>
</ul>
<h3 id="问题描述">问题描述</h3><p>calamari的salt-master节点在读取</p>
<blockquote>
<p>/var/cache/salt/master/minions/{minion-hostname}/data.p</p>
</blockquote>
<p>的时候有权限问题，在修改权限以后，可以读取到了，但是在重启了salt-minion以后，这个文件会被更新，然后权限又变成无法读取的</p>
<h3 id="相关知识补充">相关知识补充</h3><p>Grains - salt-minion 自身的一些静态信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">salt <span class="string">'*'</span> grains.ls       <span class="comment"># 查看 grains 分类</span></span><br><span class="line">salt <span class="string">'*'</span> grains.items    <span class="comment"># 查看 grains 所有信息</span></span><br><span class="line">salt <span class="string">'*'</span> grains.item os  <span class="comment"># 查看 grains 某个信息</span></span><br><span class="line">salt <span class="string">'*'</span> grains.get os</span><br></pre></td></tr></table></figure></p>
<p>上面的是salt-minion的静态信息的查询的相关的命令，salt-minion在进行重启的时候会将一些静态的信息推送到salt-master上面去，而这个生成的信息正好就是我们上面提出有权限问题的data.p这个存储的文件的，那么解决问题就是修改这个地方的权限的问题了</p>
<h3 id="修改salt-master代码">修改salt-master代码</h3><p>这个问题通过修改salt-master的master.py代码可以解决</p>
<p>写入这个grains信息的代码在/usr/lib/python2.7/site-packages/salt/master.py这个文件当中，代码段如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">  def _pillar(self, load):</span><br><span class="line">···</span><br><span class="line">        <span class="keyword">if</span> self.opts.get(<span class="string">'minion_data_cache'</span>, False):</span><br><span class="line">            cdir = os.path.join(self.opts[<span class="string">'cachedir'</span>], <span class="string">'minions'</span>, load[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">if</span> not os.path.isdir(cdir):</span><br><span class="line">                os.makedirs(cdir)</span><br><span class="line">            datap = os.path.join(cdir, <span class="string">'data.p'</span>)</span><br><span class="line">            tmpfh, tmpfname = tempfile.mkstemp(dir=cdir)</span><br><span class="line">            os.close(tmpfh)</span><br><span class="line">            with salt.utils.fopen(tmpfname, <span class="string">'w+b'</span>) as fp_:</span><br><span class="line">                fp_.write(</span><br><span class="line">                    self.serial.dumps(</span><br><span class="line">                        &#123;<span class="string">'grains'</span>: load[<span class="string">'grains'</span>],</span><br><span class="line">                         <span class="string">'pillar'</span>: data&#125;)</span><br><span class="line">                    )</span><br><span class="line">            <span class="comment"># On Windows, os.rename will fail if the destination file exists.</span></span><br><span class="line">            salt.utils.atomicfile.atomic_rename(tmpfname, datap)</span><br><span class="line">        <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure></p>
<p>就是这个函数就是负责这个文件写入的，我们只在这个代码里面增加一个文件的权限的控制，在<code>salt.utils.atomicfile.atomic_rename(tmpfname, datap)</code>这行之上增加一行代码<code>os.chmod(tmpfname, 0o644)</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> def _pillar(self, load):</span><br><span class="line">···</span><br><span class="line">        <span class="keyword">if</span> self.opts.get(<span class="string">'minion_data_cache'</span>, False):</span><br><span class="line">            cdir = os.path.join(self.opts[<span class="string">'cachedir'</span>], <span class="string">'minions'</span>, load[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">if</span> not os.path.isdir(cdir):</span><br><span class="line">                os.makedirs(cdir)</span><br><span class="line">            datap = os.path.join(cdir, <span class="string">'data.p'</span>)</span><br><span class="line">            tmpfh, tmpfname = tempfile.mkstemp(dir=cdir)</span><br><span class="line">            os.close(tmpfh)</span><br><span class="line">            with salt.utils.fopen(tmpfname, <span class="string">'w+b'</span>) as fp_:</span><br><span class="line">                fp_.write(</span><br><span class="line">                    self.serial.dumps(</span><br><span class="line">                        &#123;<span class="string">'grains'</span>: load[<span class="string">'grains'</span>],</span><br><span class="line">                         <span class="string">'pillar'</span>: data&#125;)</span><br><span class="line">                    )</span><br><span class="line">            <span class="comment"># On Windows, os.rename will fail if the destination file exists.</span></span><br><span class="line">            os.chmod(tmpfname, <span class="number">0</span>o644)</span><br><span class="line">            salt.utils.atomicfile.atomic_rename(tmpfname, datap)</span><br><span class="line">        <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure>
<p>修改好了以后，重启下salt-master，然后重启下salt-minion<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart salt-minion</span><br></pre></td></tr></table></figure></p>
<p>检查权限，已经看到权限变成了644了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/cache/salt/master/minions/lab8106/data.p </span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">5331</span> Jan  <span class="number">9</span> <span class="number">15</span>:<span class="number">33</span> /var/cache/salt/master/minions/lab8106/data.p</span><br></pre></td></tr></table></figure></p>
<p>现在再看下前台页面效果:</p>
<p><img src="http://static.zybuluo.com/zphj1987/4qkz1xo0kretsrvn8nalmt17/image_1b612ekp81ec0uk140t1b5tdjfm.png" alt="changge"></p>
<p>问题解决</p>
<h2 id="总结">总结</h2><p>calamari有一些各种各样的小问题，总体上还是一款非常简洁漂亮的管理界面，在没有监控系统的情况下，还是一个不错的选择</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-01-09</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/saltstack.png" alt="salt-stack"><br></center>

<h2 id="前言">前言</h2><p>一直在做calamari的相关的一些打包和安装的工作，都是业余弄的东西，所以并没有仔细的进行功能点的验证测试，正好ceph社区群里面有人问了个问题</p>
<blockquote>
<p>calamari上是不是能看到ceph的version?</p>
</blockquote>
<p>对于这个问题，好像确实没有见到过，而之前正好有个页面看到是空的，当时还不清楚这个是什么用的</p>
<p><img src="http://static.zybuluo.com/zphj1987/mjyvzkd7q9x6eplb538o7ltc/image_1b611hfic17lj15ej1jp1pb31m979.png" alt="origin"></p>
<p>而另外一位群友贴出了这个地方的是有值的，这个地方是有BUG的，在咨询了相关的问题描述以后，我们来看下，可以如何解决这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph编译加速的小技巧]]></title>
    <link href="http://www.zphj1987.com/2017/01/05/Ceph-compile-speedup/"/>
    <id>http://www.zphj1987.com/2017/01/05/Ceph-compile-speedup/</id>
    <published>2017-01-05T09:46:54.000Z</published>
    <updated>2017-02-03T05:19:18.556Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octoalien.jpg" alt="此处输入图片的描述"><br></center>

<p>总结了几个小技巧，用于在ceph编译过程中，能够更快一点<br><a id="more"></a></p>
<h2 id="修改clone的地址">修改clone的地址</h2><blockquote>
<p>git clone  <a href="https://github.com/ceph/ceph.git" target="_blank" rel="external">https://github.com/ceph/ceph.git</a></p>
</blockquote>
<p>可以修改成</p>
<blockquote>
<p>git clone  git://github.com/ceph/ceph.git</p>
</blockquote>
<p>某些时候可能可以加快一些<br><img src="http://static.zybuluo.com/zphj1987/1nhwqu5rigkblr4ql9decmv5/1.png" alt="1.png-5.9kB"></p>
<p><img src="http://static.zybuluo.com/zphj1987/tmcr90ufu6abgmffmikep9sl/2.png" alt="1.png-5."></p>
<h2 id="根据需要下载分支">根据需要下载分支</h2><p>假如现在想看10.2.5版本的代码</p>
<h3 id="常规做法">常规做法</h3><p>先下载整个库<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/ceph/ceph.git all</span><br></pre></td></tr></table></figure></p>
<p>总共的下载对象数目为46万</p>
<blockquote>
<p>Counting objects: 460384</p>
</blockquote>
<p>这个是包含所有的分支和分支内的文件的所有版本的<br>我们切换到分支<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mytest]<span class="comment">#cd all</span></span><br><span class="line">[root@lab8106 all]<span class="comment"># git branch</span></span><br><span class="line">* master</span><br><span class="line">[root@lab8106 all]<span class="comment"># git checkout -b all10.2.5  v10.2.5</span></span><br><span class="line">Switched to a new branch <span class="string">'all10.2.5'</span></span><br><span class="line">[root@lab8106 all]<span class="comment"># git branch</span></span><br><span class="line">* all10.<span class="number">2.5</span></span><br><span class="line">  master</span><br><span class="line">[root@lab8106 all]<span class="comment"># ls -R|wc -l</span></span><br><span class="line"><span class="number">4392</span></span><br><span class="line">可以看到有这么多的文件</span><br></pre></td></tr></table></figure></p>
<h3 id="现在只复制一个分支的">现在只复制一个分支的</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mytest]<span class="comment"># git clone -b v10.2.5 --single-branch   git://github.com/ceph/ceph.git single</span></span><br></pre></td></tr></table></figure>
<p>总共下载的对象数目为34万</p>
<blockquote>
<p>Counting objects: 344026<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mytest]<span class="comment"># cd single/</span></span><br><span class="line">[root@lab8106 single]<span class="comment"># git checkout -b single10.2.5</span></span><br><span class="line">Switched to a new branch <span class="string">'single10.2.5'</span></span><br><span class="line">[root@lab8106 single]<span class="comment"># git branch</span></span><br><span class="line">* single10.<span class="number">2.5</span></span><br><span class="line">[root@lab8106 single]<span class="comment"># ls -R |wc -l</span></span><br><span class="line"><span class="number">4392</span></span><br></pre></td></tr></table></figure></p>
</blockquote>
<h3 id="现在只复制一个分支的最后一个版本的代码">现在只复制一个分支的最后一个版本的代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mytest]<span class="comment"># git clone -b v10.2.5 --single-branch --depth 1  git://github.com/ceph/ceph.git singledep1</span></span><br></pre></td></tr></table></figure>
<p>总共下载的对象数目为3682</p>
<blockquote>
<p>Counting objects: 3682<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mytest]<span class="comment">#  cd singledep1/</span></span><br><span class="line"> [root@lab8106 singledep1]<span class="comment"># git checkout -b singledep110.2.5</span></span><br><span class="line">Switched to a new branch <span class="string">'singledep110.2.5'</span></span><br><span class="line">[root@lab8106 singledep1]<span class="comment"># git branch</span></span><br><span class="line">* singledep110.<span class="number">2.5</span></span><br><span class="line">[root@lab8106 singledep1]<span class="comment"># ls -R |wc -l</span></span><br><span class="line"><span class="number">4392</span></span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>从上面的可以看到三个版本的代码是一致的，那么区别在哪里</p>
<ul>
<li>clone：包含所有分支和分支的所有文件版本</li>
<li>clone single-branch：包含指定分支和指定分支的所有文件的版本</li>
<li>clone single-branch depth 1 ：包含指定分支和指定分支的最后一个版本的文件</li>
</ul>
<h2 id="准备编译前的install-deps慢">准备编译前的install-deps慢</h2><p>提前准备好epel<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install http://mirrors.aliyun.com/epel/<span class="number">7</span>/x86_64/e/epel-release-<span class="number">7</span>-<span class="number">8</span>.noarch.rpm</span><br><span class="line">rm -rf /etc/yum.repos.d/epel*</span><br></pre></td></tr></table></figure></p>
<p>装完了删除，这个是为了绕过包验证<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-<span class="number">7</span>.repo</span><br></pre></td></tr></table></figure></p>
<p>删除慢速的 aliyuncs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'/aliyuncs/d'</span> /etc/yum.repos.d/epel.repo</span><br></pre></td></tr></table></figure></p>
<p>install-deps.sh第72行的需要修改</p>
<blockquote>
<p>yum-config-manager —add-repo <a href="https://dl.fedoraproject.org/pub/epel/$MAJOR_VERSION/x86_64/" target="_blank" rel="external">https://dl.fedoraproject.org/pub/epel/$MAJOR_VERSION/x86_64/</a><br>执行下面的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed -i <span class="string">'s/https:\/\/dl.fedoraproject.org\/pub\//http:\/\/mirrors.aliyun.com\//g'</span> install-deps.sh</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>然后执行install-deps.sh，这样会快很多的</p>
<h2 id="总结">总结</h2><p>目前就这么多，后续有更多的影响速度的地方会增加上去</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octoalien.jpg" alt="此处输入图片的描述"><br></center>

<p>总结了几个小技巧，用于在ceph编译过程中，能够更快一点<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph 的crush算法 straw]]></title>
    <link href="http://www.zphj1987.com/2017/01/05/ceph-crush-straw/"/>
    <id>http://www.zphj1987.com/2017/01/05/ceph-crush-straw/</id>
    <published>2017-01-05T04:45:23.000Z</published>
    <updated>2017-02-03T05:20:55.938Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/crushmode.jpg" alt=""><br></center>

<p>很多年以前，Sage 在写CRUSH的原始算法的时候，写了不同的Bucket类型，可以选择不同的伪随机选择算法，大部分的模型是基于RJ Honicky写的RUSH algorithms 这个算法，这个在网上可以找到资料，这里面有一个新的特性是sage很引以为豪的，straw算法，也就是我们现在常用的一些算法，这个算法有下面的特性：</p>
<ul>
<li>items 可以有任意的weight</li>
<li>选择一个项目的算法复杂度是O(n)</li>
<li>如果一个item的weight调高或者调低，只会在调整了的item直接变动，而没有调整的item是不会变动的<a id="more"></a>
</li>
</ul>
<blockquote>
<p>O(n)找到一个数组里面最大的一个数，你要把n个变量都扫描一遍，操作次数为n，那么算法复杂度是O(n)<br>冒泡法的算法复杂度是O(n²)</p>
</blockquote>
<p>这个过程的算法基本动机看起来像画画的颜料吸管，最长的一个将会获胜，每个item 基于weight有自己的随机straw长度</p>
<p>这些看上去都很好，但是第三个属性实际上是不成立的，这个straw 长度是基于bucket中的其他的weights来进行的一个复杂的算法的，虽然iteam的PG的计算方法是很独立的，但是一个iteam的权重变化实际上影响了其他的iteam的比例因子，这意味着一个iteam的变化可能会影响其他的iteam</p>
<p>这个看起来是显而易见的，但是事实上证明，8年都没有人去仔细研究底层的代码或者算法，这个影响就是用户做了一个很小的权重变化，但是看到了一个很大的数据变动过程，sage 在做的时候写过一个很好的测试，来验证了第三个属性是真的，但是当时的测试只用了几个比较少的组合，如果大量测试是会发现这个问题的</p>
<p>sage注意到这个问题也是很多人抱怨在迁移的数据超过了预期的数据，但是这个很难量化和验证，所以被忽视了很久</p>
<p>无论如何，这是个坏消息</p>
<p>好消息是，sage找到了如何解决分布算法来的实现这三个属性，新的算法被称为 ‘straw2’,下面是不同的算法<br>straw的算法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">max_x = -<span class="number">1</span></span><br><span class="line">max_item = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> each item:</span><br><span class="line">    x = random value from <span class="number">0</span>..<span class="number">65535</span></span><br><span class="line">    x *= scaling factor</span><br><span class="line">    <span class="keyword">if</span> x &gt; max_x:</span><br><span class="line">       max_x = x</span><br><span class="line">       max_item = item</span><br><span class="line"><span class="built_in">return</span> item</span><br></pre></td></tr></table></figure></p>
<p>这个就有问题了scaling factor(比例因子) 是其他iteam的权重所有的，这个就意味着改变A的权重，可能会影响到B和C的权重了</p>
<p>新的straw2的算法是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">max_x = -<span class="number">1</span></span><br><span class="line">max_item = -<span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> each item:</span><br><span class="line">   x = random value from <span class="number">0</span>..<span class="number">65535</span></span><br><span class="line">   x = ln(x / <span class="number">65536</span>) / weight</span><br><span class="line">   <span class="keyword">if</span> x &gt; max_x:</span><br><span class="line">      max_x = x</span><br><span class="line">      max_item = item</span><br><span class="line"><span class="built_in">return</span> item</span><br></pre></td></tr></table></figure></p>
<p>可以看到这个是一个weight的简单的函数，这个意味着改变一个item的权重不会影响到其他的项目</p>
<p>sage发现问题的一半，然后 sam根据<a href="https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables" target="_blank" rel="external">这个算法</a>解决了问题</p>
<p>计算ln()函数有点讨厌，因为这个是一个浮点功能，CRUSH是定点运算（整数型），当前的实施方法是128KB的查找表，在做一个小的单元测试的时候比straw慢了25%，单这个可能跟一些缓存和输入也有关系</p>
<p>以上是2014年sage在开发者邮件列表里面提出来的，相信到现在为止straw2的算法已经改进了很多，目前默认的还是straw算法，内核在kernel4.1以后才支持的这个属性的</p>
<p>那么我们在0.9x中来看下这个属性,来从实际环境中看下具体有什么区别</p>
<hr>
<h1 id="实践过程">实践过程</h1><p><img src="http://static.zybuluo.com/zphj1987/g64ch504j19wcfcfu74fdg8f/%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83.png" alt="基础环境.png-8.6kB"></p>
<p>基础的环境为这个，我的机器为8个osd的单机节点，通过修改crush模拟成如上图所示的环境，设置的pg数目为800，保证每个osd上的pg为100左右，这个增加pg的数目，来扩大测试的样本</p>
<p>straw2和straw的区别在于，straw算法改变一个bucket的权重的时候，因为内部算法的问题，造成了其他机器的item的计算因子也会变化，就会出现其他没修改权重的bucket也会出现pg的相互间的流动，这个跟设计之初的想法是不一致的，造成的后果就是，在增加或者减少存储节点的时候，如果集群比较大，数据比较多，就会造成很大的无关数据的迁移，这个就是上面提到的问题</p>
<p>为了解决这个问题就新加入了算法straw2，这个算法保证在bucket的crush权重发生变化的时候，只会在变化的bucket有数据流入或者流出，不会出现其他bucket间的数据流动，减少数据的迁移量，下面的测试将会直观的看到这种变化</p>
<h2 id="环境配置">环境配置</h2><p>调整tunables 为 hammer，这个里面才支持crush v4(straw2)属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush tunables hammer</span></span><br><span class="line">adjusted tunables profile to hammer</span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush set-tunable straw_calc_version 1</span></span><br><span class="line">adjusted tunable straw_calc_version to <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>设置完了检查这两个个属性，如果是straw_calc_version 0的时候profile会显示unknow<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush dump|egrep "allowed_bucket_algs|profile"</span></span><br><span class="line">        <span class="string">"allowed_bucket_algs"</span>: <span class="number">54</span>,</span><br><span class="line">        <span class="string">"profile"</span>: <span class="string">"hammer"</span>,</span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush dump|grep alg</span></span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw"</span>,</span><br></pre></td></tr></table></figure></p>
<p>设置完了后并不能马上生效的，这个是为了防止集群大的变动,可以用这个触发，或者等待下次crush发生变动的时候会自动触发<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush reweight-all</span><br></pre></td></tr></table></figure></p>
<h2 id="先来测试straw">先来测试straw</h2><p>开始第一步测试，将osd.7从集群中crush改为0，那么变动的就是host4的crush，那么我们来看下数据的变化<br>首先需要记录原始的pg分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~ ceph pg dump pgs|awk <span class="string">'&#123;print $1,$15&#125;'</span> &gt; oringin</span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush reweight osd.7 0</span></span><br><span class="line">reweighted item id <span class="number">7</span> name <span class="string">'osd.7'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br><span class="line">root@lab8107:~ceph pg dump pgs|awk <span class="string">'&#123;print $1,$15&#125;'</span> &gt; rewei70</span><br></pre></td></tr></table></figure></p>
<p>现在比较oringin 和rewei70 的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diff oringin rewei70 -y -W <span class="number">30</span> --suppress-common-lines</span><br></pre></td></tr></table></figure></p>
<p>查看非调整节点的数据流动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">0.3</span>d [<span class="number">2</span>]      | <span class="number">0.3</span>d [<span class="number">5</span>]</span><br><span class="line"><span class="number">0.316</span> [<span class="number">2</span>]     | <span class="number">0.316</span> [<span class="number">5</span>]</span><br><span class="line"><span class="number">0.26</span>c [<span class="number">5</span>]     | <span class="number">0.26</span>c [<span class="number">1</span>]</span><br><span class="line"><span class="number">0.241</span> [<span class="number">2</span>]     | <span class="number">0.241</span> [<span class="number">0</span>]</span><br><span class="line"><span class="number">0.235</span> [<span class="number">5</span>]     | <span class="number">0.235</span> [<span class="number">2</span>]</span><br><span class="line"><span class="number">0.128</span> [<span class="number">0</span>]     | <span class="number">0.128</span> [<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>再来一次将osd.6的crush weight弄成0<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush reweight osd.<span class="number">6</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>再次查看变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">0</span>.cb [<span class="number">4</span>]      | <span class="number">0</span>.cb [<span class="number">2</span>]</span><br><span class="line"><span class="number">0.30</span>b [<span class="number">4</span>]     | <span class="number">0.30</span>b [<span class="number">2</span>]</span><br><span class="line"><span class="number">0.2</span>e9 [<span class="number">1</span>]     | <span class="number">0.2</span>e9 [<span class="number">4</span>]</span><br><span class="line"><span class="number">0.2</span>d8 [<span class="number">3</span>]     | <span class="number">0.2</span>d8 [<span class="number">1</span>]</span><br><span class="line"><span class="number">0.28</span>e [<span class="number">3</span>]     | <span class="number">0.28</span>e [<span class="number">4</span>]</span><br><span class="line"><span class="number">0.286</span> [<span class="number">1</span>]     | <span class="number">0.286</span> [<span class="number">4</span>]</span><br><span class="line"><span class="number">0.1</span>f7 [<span class="number">3</span>]     | <span class="number">0.1</span>f7 [<span class="number">1</span>]</span><br><span class="line"><span class="number">0.1</span>b6 [<span class="number">1</span>]     | <span class="number">0.1</span>b6 [<span class="number">4</span>]</span><br><span class="line"><span class="number">0.163</span> [<span class="number">0</span>]     | <span class="number">0.163</span> [<span class="number">3</span>]</span><br><span class="line"><span class="number">0.14</span>f [<span class="number">2</span>]     | <span class="number">0.14</span>f [<span class="number">4</span>]</span><br><span class="line"><span class="number">0.10</span>a [<span class="number">0</span>]     | <span class="number">0.10</span>a [<span class="number">3</span>]</span><br></pre></td></tr></table></figure></p>
<p>上面的两组就是在一个bucket的里面的出现单点和整个bucket的crush weight减少的时候触发的其他节点的数据变动</p>
<h2 id="现在把环境恢复后再来测试straw2">现在把环境恢复后再来测试straw2</h2><p>修改crush map 里面的bucket的alg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd getcrushmap -o crushmap.txt</span></span><br><span class="line">got crush map from osdmap epoch <span class="number">390</span></span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># crushtool -d crushmap.txt -o crushmap-decompile</span></span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># vim crushmap-decompile</span></span><br><span class="line">将文件里面的所有straw修改成straw2</span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># crushtool -c crushmap-decompile  -o crushmap-compile</span></span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd setcrushmap -i crushmap-compile</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>如果出现报错就把crushmap里面的straw2_calc_version改成straw_calc_version</p>
</blockquote>
<p>并且设置算法(最关键的一步，否则即使设置straw2也不生效)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush <span class="built_in">set</span>-tunable straw_calc_version <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>查询当前的crush算法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush dump|grep alg</span></span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">            <span class="string">"alg"</span>: <span class="string">"straw2"</span>,</span><br><span class="line">        <span class="string">"allowed_bucket_algs"</span>: <span class="number">54</span>,</span><br></pre></td></tr></table></figure></p>
<p>做一次重新内部算法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush reweight-all</span><br></pre></td></tr></table></figure></p>
<p>可以重复上面的测试了</p>
<p>获取当前的pg分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgf]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;' &gt; oringin</span></span><br><span class="line">root@lab8107:~/ceph/crush<span class="comment"># ceph osd crush reweight osd.7 0</span></span><br><span class="line">[root@lab8106 pgf]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;' &gt; rewei70</span></span><br></pre></td></tr></table></figure></p>
<p>比较调整前后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diff oringin rewei70  -y -W <span class="number">30</span> --suppress-common-lines|less</span><br></pre></td></tr></table></figure></p>
<p>再次调整osd.6<br><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="title">ceph</span> osd crush reweight osd.<span class="number">6</span> <span class="number">0</span></span><br><span class="line">ceph pg dump pgs|awk <span class="string">'&#123;print <span class="variable">$1</span>,<span class="variable">$15</span>&#125;'</span> &gt; rewei60</span><br></pre></td></tr></table></figure></p>
<p>已经没有非调整bucket的pg在节点间的变化了</p>
<h2 id="简短的做个总结就是">简短的做个总结就是</h2><p>straw算法里面添加节点或者减少节点，其他服务器上的osd之间会有pg的流动<br>straw2算法里面添加节点或者减少节点，只会pg从变化的节点移出或者从其他点移入，其他节点间没有数据流动</p>
<h4 id="设置方法">设置方法</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush tunables hammer</span><br><span class="line">ceph osd crush <span class="built_in">set</span>-tunable straw_calc_version <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>开始设置好了 新创建的默认就是会straw2就会省去修改crushmap的操作</p>
<p>注意librados是服务端支持，客户端就支持，涉及到内核客户端的，就需要内核版本的支持，内核从4.1开始支持，也就是cephfs和rbd的块设备方式需要内核4.1及以上支持，openstack对接的是librados可以默认支持，其他的也都默认可以支持的</p>
<h2 id="相关链接">相关链接</h2><p><a href="https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables" target="_blank" rel="external">https://en.wikipedia.org/wiki/Exponential_distribution#Distribution_of_the_minimum_of_exponential_random_variables</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/crushmode.jpg" alt=""><br></center>

<p>很多年以前，Sage 在写CRUSH的原始算法的时候，写了不同的Bucket类型，可以选择不同的伪随机选择算法，大部分的模型是基于RJ Honicky写的RUSH algorithms 这个算法，这个在网上可以找到资料，这里面有一个新的特性是sage很引以为豪的，straw算法，也就是我们现在常用的一些算法，这个算法有下面的特性：</p>
<ul>
<li>items 可以有任意的weight</li>
<li>选择一个项目的算法复杂度是O(n)</li>
<li>如果一个item的weight调高或者调低，只会在调整了的item直接变动，而没有调整的item是不会变动的]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd的image对象数与能写入文件数的关系]]></title>
    <link href="http://www.zphj1987.com/2017/01/03/rbd-image-write-objects/"/>
    <id>http://www.zphj1987.com/2017/01/03/rbd-image-write-objects/</id>
    <published>2017-01-03T06:35:53.000Z</published>
    <updated>2017-02-03T05:22:08.983Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/2017.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>收到一个问题如下：</p>
<blockquote>
<p>一个300TB 的RBD，只有7800万的objects，如果存储小文件的话，感觉不够用</p>
</blockquote>
<p>对于这个问题，我原来的理解是：对象默认设置的大小是4M一个，存储下去的数据，如果小于4M，就会占用一个小于4M的对象，如果超过4M，那么存储的数据就会进行拆分成多个4M，这个地方其实是不严谨的</p>
<p>对于rados接口来说，数据是多大对象put进去就是多大的对象，并没有进行拆分，进行拆分的是再上一层的应用，比如rbd，比如cephfs</p>
<p>那么对于rbd的image显示的对象数目和文件数目有什么关系呢？本篇将来看看这个问题，到底会不会出现上面的问题<br><a id="more"></a></p>
<h2 id="实践过程">实践过程</h2><p>创建一个image<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd create --image zpsize --size 100M</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd info zpsize</span></span><br><span class="line">rbd image <span class="string">'zpsize'</span>:</span><br><span class="line">	size <span class="number">102400</span> kB <span class="keyword">in</span> <span class="number">25</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">85</span>c66b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到，这个image从集群中分配到了25个对象，每个对象的大小为4M，假如我们写入1000个小文件看下会是什么情况</p>
<p>映射到本地并且格式化xfs文件系统<br><figure class="highlight nix"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map zpsize</span></span><br><span class="line">/dev/rbd0</span><br><span class="line">[root@lab8106 ~]<span class="comment"># mkfs.xfs -f /dev/rbd0 </span></span><br><span class="line"><span class="variable">meta-data=</span>/dev/rbd0              <span class="variable">isize=</span><span class="number">256</span>    <span class="variable">agcount=</span><span class="number">4</span>, <span class="variable">agsize=</span><span class="number">6144</span> <span class="variable">blks</span><br><span class="line">         =</span>                       <span class="variable">sectsz=</span><span class="number">512</span>   <span class="variable">attr=</span><span class="number">2</span>, <span class="variable">projid32bit=</span><span class="number">1</span></span><br><span class="line">         =                       <span class="variable">crc=</span><span class="number">0</span>        <span class="variable">finobt=</span><span class="number">0</span></span><br><span class="line"><span class="variable">data     =</span>                       <span class="variable">bsize=</span><span class="number">4096</span>   <span class="variable">blocks=</span><span class="number">24576</span>, <span class="variable">imaxpct=</span><span class="number">25</span></span><br><span class="line">         =                       <span class="variable">sunit=</span><span class="number">1024</span>   <span class="variable">swidth=</span><span class="number">1024</span> blks</span><br><span class="line"><span class="variable">naming   =</span>version <span class="number">2</span>              <span class="variable">bsize=</span><span class="number">4096</span>   <span class="variable">ascii-ci=</span><span class="number">0</span> <span class="variable">ftype=</span><span class="number">0</span></span><br><span class="line"><span class="variable">log      =</span>internal log           <span class="variable">bsize=</span><span class="number">4096</span>   <span class="variable">blocks=</span><span class="number">624</span>, <span class="variable">version=</span><span class="number">2</span></span><br><span class="line">         =                       <span class="variable">sectsz=</span><span class="number">512</span>   <span class="variable">sunit=</span><span class="number">8</span> blks, <span class="variable">lazy-count=</span><span class="number">1</span></span><br><span class="line"><span class="variable">realtime =</span>none                   <span class="variable">extsz=</span><span class="number">4096</span>   <span class="variable">blocks=</span><span class="number">0</span>, <span class="variable">rtextents=</span><span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>挂载到本地<br>[root@lab8106 ~]# mount /dev/rbd0 /mnt</p>
<p>写入1000个1K小文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># seq 1000|xargs -i dd if=/dev/zero of=/mnt/a&#123;&#125; bs=1K count=1</span></span><br></pre></td></tr></table></figure></p>
<p>没有报错提示，正常写入了，我们看下写入了多少对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd ls|grep rbd_data.85c66b8b4567</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000018</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000000</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000006</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000001</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000017</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">000000000000000</span>c</span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000012</span></span><br><span class="line">rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000002</span></span><br></pre></td></tr></table></figure></p>
<p>只写入了少量的对象，我们尝试下载下来看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll -hl rbd_data.85c66b8b4567.0000000000000018</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">4.0</span>M Jan  <span class="number">3</span> <span class="number">14</span>:<span class="number">27</span> rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000018</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd get rbd_data.85c66b8b4567.0000000000000000 rbd_data.85c66b8b4567.0000000000000000</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ll -hl rbd_data.85c66b8b4567.0000000000000000</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">4.0</span>M Jan  <span class="number">3</span> <span class="number">14</span>:<span class="number">27</span> rbd_data.<span class="number">85</span>c66b8b4567.<span class="number">0000000000000000</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到还是4M的对象，实际上写入的小文件已经进行了合并了，在底层已经是一个4M的对象文件了</p>
<h2 id="总结">总结</h2><p>本篇的结论就是，rbd层之上的写入的文件的个数与底层的对象数目是没有关系的，对象数目和对象大小是底层处理的，再上一层就是文件系统去处理的了，总空间占用上是一致的</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/2017.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>收到一个问题如下：</p>
<blockquote>
<p>一个300TB 的RBD，只有7800万的objects，如果存储小文件的话，感觉不够用</p>
</blockquote>
<p>对于这个问题，我原来的理解是：对象默认设置的大小是4M一个，存储下去的数据，如果小于4M，就会占用一个小于4M的对象，如果超过4M，那么存储的数据就会进行拆分成多个4M，这个地方其实是不严谨的</p>
<p>对于rados接口来说，数据是多大对象put进去就是多大的对象，并没有进行拆分，进行拆分的是再上一层的应用，比如rbd，比如cephfs</p>
<p>那么对于rbd的image显示的对象数目和文件数目有什么关系呢？本篇将来看看这个问题，到底会不会出现上面的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[处理Ceph osd的journal的uuid问题]]></title>
    <link href="http://www.zphj1987.com/2016/12/26/manage-ceph-osd-journal-uuid/"/>
    <id>http://www.zphj1987.com/2016/12/26/manage-ceph-osd-journal-uuid/</id>
    <published>2016-12-26T05:41:14.000Z</published>
    <updated>2017-02-03T05:25:59.462Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/writefor.jpg" alt="write"><br></center>

<h2 id="前言">前言</h2><p>之前有一篇文章介绍的是，在centos7的jewel下面如果自己做的分区如何处理自动挂载的问题，当时的环境对journal的地方采取的是文件的形式处理的，这样就没有了重启后journal的磁盘偏移的问题<br><a id="more"></a><br>如果采用的是ceph自带的deploy去做分区的处理的时候，是调用的sgdisk去对磁盘做了一些处理的，然后deploy能够识别一些特殊的标记，然后去做了一些其他的工作，而自己分区的时候，是没有做这些标记的这样就可能会有其他的问题</p>
<p>我们看下如何在部署的时候就处理好journal的uuid的问题</p>
<h2 id="实践">实践</h2><h3 id="按常规流程部署OSD">按常规流程部署OSD</h3><p>准备测试的自分区磁盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/dev/sde bs=<span class="number">4</span>M count=<span class="number">100</span>;</span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/dev/sdf bs=<span class="number">4</span>M count=<span class="number">100</span>; parted /dev/sde mklabel gpt;</span><br><span class="line">parted /dev/sdf mklabel gpt;</span><br><span class="line">parted /dev/sde mkpart primary <span class="number">1</span> <span class="number">100</span>%;</span><br><span class="line">parted /dev/sdf mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>使用的sde1作为数据盘，使用sdf1作为ssd的独立分区的journal磁盘</p>
<p>我们线按照常规的步骤去部署下</p>
<h5 id="做osd的prepare操作">做osd的prepare操作</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy osd prepare lab8106:/dev/sde1:/dev/sdf1</span></span><br><span class="line">···</span><br><span class="line">[lab8106][WARNIN] adjust_symlink: Creating symlink /var/lib/ceph/tmp/mnt.<span class="number">7</span>HuS8k/journal -&gt; /dev/sdf1</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<h5 id="做osd的activate操作">做osd的activate操作</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf</span></span><br><span class="line">···</span><br><span class="line">[lab8106][WARNIN] ceph_disk.main.Error: Error: [<span class="string">'ceph-osd'</span>, <span class="string">'--cluster'</span>, <span class="string">'ceph'</span>, <span class="string">'--mkfs'</span>, <span class="string">'--mkkey'</span>, <span class="string">'-i'</span>, <span class="string">'7'</span>, <span class="string">'--monmap'</span>, <span class="string">'/var/lib/ceph/tmp/mnt.yOP4gv/activate.monmap'</span>, <span class="string">'--osd-data'</span>, <span class="string">'/var/lib/ceph/tmp/mnt.yOP4gv'</span>, <span class="string">'--osd-journal'</span>, <span class="string">'/var/lib/ceph/tmp/mnt.yOP4gv/journal'</span>, <span class="string">'--osd-uuid'</span>, <span class="string">'5c59284b-8d82-4cc6-b566-8b102dc25568'</span>, <span class="string">'--keyring'</span>, <span class="string">'/var/lib/ceph/tmp/mnt.yOP4gv/keyring'</span>, <span class="string">'--setuser'</span>, <span class="string">'ceph'</span>, <span class="string">'--setgroup'</span>, <span class="string">'ceph'</span>] failed : <span class="number">2016</span>-<span class="number">12</span>-<span class="number">26</span> <span class="number">13</span>:<span class="number">11</span>:<span class="number">54.211543</span> <span class="number">7</span>f585e926800 -<span class="number">1</span> filestore(/var/lib/ceph/tmp/mnt.yOP4gv) mkjournal error creating journal on /var/lib/ceph/tmp/mnt.yOP4gv/journal: (<span class="number">13</span>) Permission denied</span><br><span class="line">[lab8106][WARNIN] <span class="number">2016</span>-<span class="number">12</span>-<span class="number">26</span> <span class="number">13</span>:<span class="number">11</span>:<span class="number">54.211564</span> <span class="number">7</span>f585e926800 -<span class="number">1</span> OSD::mkfs: ObjectStore::mkfs failed with error -<span class="number">13</span></span><br><span class="line">[lab8106][WARNIN] <span class="number">2016</span>-<span class="number">12</span>-<span class="number">26</span> <span class="number">13</span>:<span class="number">11</span>:<span class="number">54.211616</span> <span class="number">7</span>f585e926800 -<span class="number">1</span>  ** ERROR: error creating empty object store <span class="keyword">in</span> /var/lib/ceph/tmp/mnt.yOP4gv: (<span class="number">13</span>) Permission denied</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>可以看到提示的是权限不足，我们检查下权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mount /dev/sde1 /mnt</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ll /mnt/</span></span><br><span class="line">total <span class="number">32</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">193</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> activate.monmap</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph  <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> ceph_fsid</span><br><span class="line">drwxr-xr-x <span class="number">3</span> ceph ceph  <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> current</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph  <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> fsid</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph   <span class="number">9</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> journal -&gt; /dev/sdf1</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph  <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> journal_uuid</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph  <span class="number">21</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> magic</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph   <span class="number">4</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> store_version</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph  <span class="number">53</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> superblock</span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph   <span class="number">2</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> whoami</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ll /dev/sdf1</span></span><br><span class="line">brw-rw---- <span class="number">1</span> root disk <span class="number">8</span>, <span class="number">81</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">03</span> /dev/sdf1</span><br></pre></td></tr></table></figure></p>
<p>创建sdf1的journal的时候权限有问题，我们给下磁盘权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># chown ceph:ceph /dev/sdf1 </span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到成功了</p>
<h5 id="检查下osd的目录：">检查下osd的目录：</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-7</span></span><br><span class="line">total <span class="number">56</span></span><br><span class="line">-rw-r--r--   <span class="number">1</span> root root  <span class="number">193</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">15</span> activate.monmap</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph    <span class="number">3</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">15</span> active</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph   <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> ceph_fsid</span><br><span class="line">drwxr-xr-x <span class="number">166</span> ceph ceph <span class="number">4096</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">16</span> current</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph   <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> fsid</span><br><span class="line">lrwxrwxrwx   <span class="number">1</span> ceph ceph    <span class="number">9</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">11</span> journal -&gt; /dev/sdf1</span><br></pre></td></tr></table></figure>
<p>可以看到journal链接到了/dev/sdf1，这次的部署是成功了，但是这里就有个问题，如果下次重启的时候，sdf1不是sdf1盘符变了，那么问题就会产生了，osd可能就无法启动了</p>
<h3 id="优化下部署流程">优化下部署流程</h3><p>这里是优化后的流程，解决上面的问题的<br>准备测试的自分区磁盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/dev/sde bs=<span class="number">4</span>M count=<span class="number">100</span>;</span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/dev/sdf bs=<span class="number">4</span>M count=<span class="number">100</span>; </span><br><span class="line">parted /dev/sde mklabel gpt;</span><br><span class="line">parted /dev/sdf mklabel gpt;</span><br><span class="line">parted /dev/sde mkpart primary <span class="number">1</span> <span class="number">100</span>%;</span><br><span class="line">parted /dev/sdf mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>给jounral盘做一个标记(特殊标记，下面的字符串不要变动固定写法)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/sbin/sgdisk  --change-name=<span class="number">1</span>:<span class="string">'ceph journal'</span> --typecode=<span class="number">1</span>:<span class="number">45</span>b0969e-<span class="number">9</span>b03-<span class="number">4</span>f30-b4c6-b4b80ceff106  -- /dev/sdf</span><br></pre></td></tr></table></figure></p>
<p>给数据盘做一个标记(特殊标记，下面的字符串不要变动固定写法)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/sbin/sgdisk  --change-name=<span class="number">1</span>:<span class="string">'ceph data'</span> --typecode=<span class="number">1</span>:<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff05d -- /dev/sde</span><br></pre></td></tr></table></figure></p>
<p>检查下当前的分区标记情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-disk list</span></span><br><span class="line">/dev/sde :</span><br><span class="line"> /dev/sde1 ceph data, unprepared</span><br><span class="line">/dev/sdf :</span><br><span class="line"> /dev/sdf1 ceph journal</span><br></pre></td></tr></table></figure></p>
<h5 id="做osd的prepare操作-1">做osd的prepare操作</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd prepare lab8106:/dev/sde1:/dev/sdf1</span><br><span class="line">ceph-deploy osd activate lab8106:/dev/sde1:/dev/sdf1</span><br></pre></td></tr></table></figure>
<p>再次检查下当前的分区标记情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk list</span></span><br><span class="line">···</span><br><span class="line">/dev/sde :</span><br><span class="line"> /dev/sde1 ceph data, active, cluster ceph, osd.<span class="number">8</span>, journal /dev/sdf1</span><br><span class="line">/dev/sdf :</span><br><span class="line"> /dev/sdf1 ceph journal, <span class="keyword">for</span> /dev/sde1</span><br></pre></td></tr></table></figure></p>
<h5 id="查看jounral的数据">查看jounral的数据</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-8</span></span><br><span class="line">total <span class="number">56</span></span><br><span class="line">-rw-r--r--   <span class="number">1</span> root root  <span class="number">193</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">26</span> activate.monmap</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph    <span class="number">3</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">26</span> active</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph   <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">25</span> ceph_fsid</span><br><span class="line">drwxr-xr-x <span class="number">164</span> ceph ceph <span class="number">4096</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">26</span> current</span><br><span class="line">-rw-r--r--   <span class="number">1</span> ceph ceph   <span class="number">37</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">25</span> fsid</span><br><span class="line">lrwxrwxrwx   <span class="number">1</span> ceph ceph   <span class="number">58</span> Dec <span class="number">26</span> <span class="number">13</span>:<span class="number">25</span> journal -&gt; /dev/disk/by-partuuid/<span class="built_in">cd</span>72d6e8-<span class="number">07</span>d0-<span class="number">4</span><span class="built_in">cd</span>3-<span class="number">8</span>c6b<span class="operator">-a</span>33d624cae36</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>可以看到已经正确的链接了,并且部署过程中也没有了上面的需要进行权限的处理，这个是deploy工具在中间帮做了</p>
<h2 id="总结">总结</h2><p>处理的核心在于做的那两个标记，其他的就交给deploy工具自己处理就行了，如果有兴趣可以深入研究，没兴趣的话，就安装上面说的方法进行处理就行</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/writefor.jpg" alt="write"><br></center>

<h2 id="前言">前言</h2><p>之前有一篇文章介绍的是，在centos7的jewel下面如果自己做的分区如何处理自动挂载的问题，当时的环境对journal的地方采取的是文件的形式处理的，这样就没有了重启后journal的磁盘偏移的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何避免Cephfs被完全毁掉]]></title>
    <link href="http://www.zphj1987.com/2016/12/23/%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8DCephfs%E8%A2%AB%E5%AE%8C%E5%85%A8%E6%AF%81%E6%8E%89/"/>
    <id>http://www.zphj1987.com/2016/12/23/如何避免Cephfs被完全毁掉/</id>
    <published>2016-12-23T14:46:35.000Z</published>
    <updated>2017-01-12T05:47:32.986Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/kulesza-215.jpg" alt="rollback"><br></center>

<h2 id="前提">前提</h2><p>一套系统的最低要求是可恢复，也就是数据不丢失，但是在各种各样的原因下，整套系统都有被毁掉的可能，一直以来有个观点就是存储是需要两套的，一般情况下很难实现，但是如何把故障发生的概率降低到最低，这个是我们需要考虑的问题<br><a id="more"></a><br>最近在社区群里面又听闻一个案例，一套系统的文件系统被重置掉了，也就是fs被重建了，实际上这属于一个不应该有的操作，但是已经发生的事情，就看怎么样能在下次避免或者把损失降到最低，对于hammer版本来说，重建cephfs只是把目录树给冲掉了，实际的目录还是能创建起来，但是这其实是一个BUG，并且在最新的Jewel下已经解决掉这个问题，这就造成无法重建目录树，在Jewel下，在不修改代码的情况下，文件都可以扫描回来，但是全部塞到了一个目录下，对于某些场景来说，这个已经是最大限度的恢复了，至少文件还在，如果文件类型可知，也可以一个个去人工识别的，虽然工作量异常的大，但至少文件回来了，这种情况，如果有保留文件名和文件md5值的强制要求的话，文件是可以完全找回来的，当然，这都是一些防范措施，看有没有重视，或者提前做好了预备</p>
<p>本篇就是对于情况下，如何基于快照做一个防范措施，以防误操作引起的数据无法挽回的措施</p>
<h2 id="实践">实践</h2><p>对于元数据存储池来说，元数据的大小并不大，百万文件的元数据也才几百兆，所以我们有没有什么办法去形成一种保护措施，答案是有的</p>
<p>我们知道，ceph的存储池是有快照的，对于rbd场景来说，快照可以交给存储池去做快照管理，也可以交给Image自己做快照管理，二者差别在于，是大批量的快照还是只需要部分的快照，对于存储池快照来说，给存储池做一个快照，实际上就是对这个存储池中的所有的对象做了一个快照</p>
<p>我们先来看看，这个地方是如何基于快照去做文件的目录树恢复的</p>
<h3 id="准备测试数据">准备测试数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># df -TH|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/ ceph      <span class="number">897</span>G  <span class="number">110</span>M  <span class="number">897</span>G   <span class="number">1</span>% /mnt</span><br><span class="line">[root@lab8106 mnt]<span class="comment"># cp -ra /usr/share/doc/ce* /mnt</span></span><br><span class="line">[root@lab8106 mnt]<span class="comment"># ll /mnt</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Dec <span class="number">30</span>  <span class="number">2015</span> celt051-<span class="number">0.5</span>.<span class="number">1.3</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Mar  <span class="number">7</span>  <span class="number">2016</span> centos-logos-<span class="number">70.0</span>.<span class="number">6</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Mar  <span class="number">7</span>  <span class="number">2016</span> centos-release</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Dec <span class="number">21</span> <span class="number">15</span>:<span class="number">04</span> ceph</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Sep  <span class="number">9</span> <span class="number">17</span>:<span class="number">21</span> ceph-deploy-<span class="number">1.5</span>.<span class="number">34</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">0</span> Mar  <span class="number">7</span>  <span class="number">2016</span> certmonger-<span class="number">0.78</span>.<span class="number">4</span></span><br></pre></td></tr></table></figure>
<h3 id="准备快照和需要的相关数据">准备快照和需要的相关数据</h3><p>对元数据池做一个快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd pool mksnap metadata snap1</span></span><br><span class="line">created pool metadata snap snap1</span><br></pre></td></tr></table></figure></p>
<p>记录下元数据池的对象名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `rados -p metadata ls`;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$a</span> &gt;&gt; metalist;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure></p>
<p>一个简单的循环就可以拿到列表，注意，这里并不需要把数据get下来，我们只需要记录一次列表就行，这个过程，即使很多对象的情况，这个操作也是很快的</p>
<h3 id="毁掉我们的文件系统">毁掉我们的文件系统</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># umount /mnt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-mds@lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph mds fail 0</span></span><br><span class="line">failed mds gid <span class="number">4140</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph fs rm ceph --yes-i-really-mean-it</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster ffe7a8db-c671-<span class="number">4</span>b45<span class="operator">-a</span>784-ddb41e633905</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e24: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v111: <span class="number">192</span> pgs, <span class="number">3</span> pools, <span class="number">397</span> kB data, <span class="number">52</span> objects</span><br><span class="line">            <span class="number">105</span> MB used, <span class="number">834</span> GB / <span class="number">834</span> GB avail</span><br><span class="line">                 <span class="number">192</span> active+clean</span><br></pre></td></tr></table></figure>
<p>可以看到上面的操作已经把文件系统给推掉了</p>
<h3 id="新创建一个文件系统">新创建一个文件系统</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph fs new ceph metadata data</span></span><br><span class="line">new fs with metadata pool <span class="number">1</span> and data pool <span class="number">2</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl start ceph-mds@lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># mount -t ceph 192.168.8.106:/ /mnt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ll /mnt</span></span><br><span class="line">total <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看到上面的操作以后，我们的目录树已经空空如也了，到这里如果没有做上面的快照相关操作，需要恢复的话，基本需要去对源码进行修改，并且需要对代码非常的熟悉才能做，一般是没有办法了，我们来看下我们基于快照的情况下，是如何恢复的<br>先umount掉挂载点<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">umount /mnt</span><br></pre></td></tr></table></figure></p>
<p>还记得上面的快照名称和对象列表吧，我们现在对数据进行回滚：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mds]<span class="comment"># systemctl stop ceph-mds@lab8106</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># for a in `cat metalist`;do rados  -p metadata rollback $a snap1;done;</span></span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>重启一下mds<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mds]<span class="comment"># systemctl restart ceph-mds@lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>检查下目录树，没问题，都恢复了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mds]<span class="comment"># mount -t ceph 192.168.8.106:/ /mnt</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># ll /mnt</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root   <span class="number">3577</span> Dec <span class="number">30</span>  <span class="number">2015</span> celt051-<span class="number">0.5</span>.<span class="number">1.3</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root   <span class="number">1787</span> Mar  <span class="number">7</span>  <span class="number">2016</span> centos-logos-<span class="number">70.0</span>.<span class="number">6</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root  <span class="number">20192</span> Mar  <span class="number">7</span>  <span class="number">2016</span> centos-release</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root  <span class="number">19768</span> Dec <span class="number">21</span> <span class="number">15</span>:<span class="number">04</span> ceph</span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root  <span class="number">13572</span> Sep  <span class="number">9</span> <span class="number">17</span>:<span class="number">21</span> ceph-deploy-<span class="number">1.5</span>.<span class="number">34</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">147227</span> Mar  <span class="number">7</span>  <span class="number">2016</span> certmonger-<span class="number">0.78</span>.<span class="number">4</span></span><br></pre></td></tr></table></figure></p>
<h3 id="如果数据被不小心清空了">如果数据被不小心清空了</h3><p>上面是基于重建fs情况下的恢复，下面来个更极端的，元数据池的对象全部被删除了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mds]<span class="comment"># for a in `rados -p metadata ls`;do rados -p metadata rm $a ;done;</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># rados  -p metadata ls</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># systemctl restart ceph-mds@lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>这个时候查看ceph -s状态，mds都无法启动，我们来做下恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mds]<span class="comment"># systemctl stop ceph-mds@lab8106</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># ceph mds fail 0</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># ceph fs rm ceph --yes-i-really-mean-it</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># ceph fs new ceph metadata data</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># for a in `cat metalist`;do rados  -p metadata rollback $a snap1;done;</span></span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">rolled back pool metadata to snapshot snap1</span><br><span class="line">···</span><br><span class="line">[root@lab8106 mds]<span class="comment"># rados  -p metadata ls|wc -l</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line">[root@lab8106 mds]<span class="comment"># systemctl start ceph-mds@lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>这个时候需要多等下mds恢复正常，有可能记录了原来的客户端信息，需要做重连，如果一直没恢复就重启下mds<br>挂载以后，可以看到，对象数据都回来了</p>
<h2 id="总结">总结</h2><p>这个能算一个防患于未然的办法，如果对于纯数据存储的情况，存储池的快照也是能够在某些场景下发挥很大的作用的，当然什么时机做快照，保留什么多少版本，什么时候删除快照，这个都是有学问的，需要根据实际的场景和压力去做</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/kulesza-215.jpg" alt="rollback"><br></center>

<h2 id="前提">前提</h2><p>一套系统的最低要求是可恢复，也就是数据不丢失，但是在各种各样的原因下，整套系统都有被毁掉的可能，一直以来有个观点就是存储是需要两套的，一般情况下很难实现，但是如何把故障发生的概率降低到最低，这个是我们需要考虑的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph数据盘怎样实现自动挂载]]></title>
    <link href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/"/>
    <id>http://www.zphj1987.com/2016/12/22/Ceph数据盘怎样实现自动挂载/</id>
    <published>2016-12-22T15:16:23.000Z</published>
    <updated>2017-01-12T05:47:40.578Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mountain.png" alt="mount"><br></center>

<h2 id="前言">前言</h2><p>在Centos7 下，现在采用了 systemctl来控制服务，这个刚开始用起来可能不太习惯，不过这个服务比之前的服务控制要强大的多，可以做更多的控制，本节将来介绍下关于 Ceph的 osd 磁盘挂载的问题</p>
<p>很多人部署以后，发现在Centos7下用Jewel的时候没有去写fstab也没有写配置文件，重启服务器一样能够挂载起来了，关于这个有另外一篇文章：「<a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/" target="_blank" rel="external">ceph在centos7下一个不容易发现的改变</a>」<br><a id="more"></a><br>还有一些人发现自己的却启动不起来，需要写配置文件或者fstab</p>
<p>本篇就是来解决这个疑惑的，以及在不改变原配置方法的情况下如何加入这种自启动</p>
<h2 id="实践过程">实践过程</h2><h3 id="首先来第一种部署的方法">首先来第一种部署的方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd prepare lab8106:/dev/sde</span><br><span class="line">ceph-deploy osd activate lab8106:/dev/sde1</span><br></pre></td></tr></table></figure>
<p>这个方法会把/dev/sde自动分成两个分区，一个分区给journal使用，一个分区给osd的数据使用，这种方法部署以后，是可以自动起来的，启动的挂载过程就是这个服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start ceph-disk@/dev/sde1</span><br></pre></td></tr></table></figure></p>
<h3 id="再来看第二种方法">再来看第二种方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdf mklabel gpt</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdf mkpart primary 1 100%</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdf print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdf: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name     Flags</span><br><span class="line"> <span class="number">1</span>      <span class="number">1049</span>kB  <span class="number">300</span>GB  <span class="number">300</span>GB               primary</span><br></pre></td></tr></table></figure>
<p>提前做好了分区的工作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd prepare lab8106:/dev/sdf1</span><br><span class="line">ceph-deploy osd activate lab8106:/dev/sdf1</span><br></pre></td></tr></table></figure></p>
<p>可以看到prepare的时候是对着分区去做的<br>这种方法journal是以文件的方式在数据目录生成的,可以看到两个目录的 df 看到的就是不一样的，多的那个是 journal 文件的大小<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/dev/sde1       <span class="number">279</span>G   <span class="number">34</span>M  <span class="number">279</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">6</span></span><br><span class="line">/dev/sdf1       <span class="number">280</span>G  <span class="number">1.1</span>G  <span class="number">279</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<p>重启服务器<br>可以看到上面的sde1挂载了而自己分区的sdf1没有挂载</p>
<p>我们去手动执行下:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  systemctl start ceph-disk@/dev/sdf1</span></span><br><span class="line">Job <span class="keyword">for</span> ceph-disk@-dev-sdf1.service failed because the control process exited with error code. See <span class="string">"systemctl status ceph-disk@-dev-sdf1.service"</span> and <span class="string">"journalctl -xe"</span> <span class="keyword">for</span> details.</span><br></pre></td></tr></table></figure></p>
<p>看下报错<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-disk@-dev-sdf1.service</span></span><br><span class="line">● ceph-disk@-dev-sdf1.service - Ceph disk activation: /dev/sdf1</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-disk@.service; static; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: <span class="built_in">exit</span>-code) since Thu <span class="number">2016</span>-<span class="number">12</span>-<span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> CST; <span class="number">46</span>s ago</span><br><span class="line">  Process: <span class="number">16334</span> ExecStart=/bin/sh -c flock /var/lock/ceph-disk /usr/sbin/ceph-disk --verbose --log-stdout trigger --sync %f (code=exited, status=<span class="number">1</span>/FAILURE)</span><br><span class="line"> Main PID: <span class="number">16334</span> (code=exited, status=<span class="number">1</span>/FAILURE)</span><br><span class="line"></span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: main(sys.argv[<span class="number">1</span>:])</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: File <span class="string">"/usr/lib/python2.7/site-packages/ceph_disk/main.py"</span>, line <span class="number">4962</span>, <span class="keyword">in</span> main</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: args.func(args)</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: File <span class="string">"/usr/lib/python2.7/site-packages/ceph_disk/main.py"</span>, line <span class="number">4394</span>, <span class="keyword">in</span> main_trigger</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: raise Error(<span class="string">'unrecognized partition type %s'</span> % parttype)</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 sh[<span class="number">16334</span>]: ceph_disk.main.Error: Error: unrecognized partition <span class="built_in">type</span> <span class="number">0</span><span class="built_in">fc</span>63daf-<span class="number">8483</span>-<span class="number">4772</span>-<span class="number">8</span>e79-<span class="number">3</span>d69d8477de4</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 systemd[<span class="number">1</span>]: ceph-disk@-dev-sdf1.service: main process exited, code=exited, status=<span class="number">1</span>/FAILURE</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 systemd[<span class="number">1</span>]: Failed to start Ceph disk activation: /dev/sdf1.</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 systemd[<span class="number">1</span>]: Unit ceph-disk@-dev-sdf1.service entered failed state.</span><br><span class="line">Dec <span class="number">22</span> <span class="number">10</span>:<span class="number">15</span>:<span class="number">52</span> lab8106 systemd[<span class="number">1</span>]: ceph-disk@-dev-sdf1.service failed.</span><br></pre></td></tr></table></figure></p>
<p>关键在这句</p>
<blockquote>
<p>raise Error(‘unrecognized partition type %s’ % parttype)</p>
</blockquote>
<p>检查分区情况，可以看到确实跟另外一种方法部署的OSD情况不同<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk list</span></span><br><span class="line">···</span><br><span class="line">/dev/sde2 ceph journal, <span class="keyword">for</span> /dev/sde1</span><br><span class="line"> /dev/sde1 ceph data, active, cluster ceph, osd.<span class="number">6</span>, journal /dev/sde2</span><br><span class="line">dev/sdf :</span><br><span class="line"> /dev/sdf1 other, xfs, mounted on /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<p>这里要如何处理,才能实现自动挂载，方法是有的</p>
<p>这个地方需要做一步这个操作（注意下面的1：后面是写死的字符串固定的值）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/sbin/sgdisk --typecode=<span class="number">1</span>:<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff05d -- /dev/sdi</span><br><span class="line"></span><br><span class="line">/dev/sdi :</span><br><span class="line"> /dev/sdi1 ceph data, active, cluster ceph, osd.<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<p>我们来验证一下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-osd@7</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># umount /dev/sdi1 </span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl start ceph-disk@/dev/sdi1</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># df -h|grep sdi</span></span><br><span class="line">/dev/sdi1       <span class="number">280</span>G  <span class="number">1.1</span>G  <span class="number">279</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<p>可以用服务挂载了<br>这个是代码里面写死的判断值，来判断osd是ready的了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/python2.<span class="number">7</span>/site-packages/ceph_disk/main.py</span><br><span class="line"></span><br><span class="line"><span class="string">'osd'</span>: &#123;</span><br><span class="line">            <span class="string">'ready'</span>: <span class="string">'4fbd7e29-9d25-41b8-afd0-062c0ceff05d'</span>,</span><br><span class="line">            <span class="string">'tobe'</span>: <span class="string">'89c57f98-2fe5-4dc0-89c1-f3ad0ceff2be'</span>,</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>通过本篇的介绍，应该能够清楚什么情况下不自动挂载，什么情况下自动挂载，怎么去实现自动挂载，虽然上面只用了一调命令就实现了，不过我找了很久才定位到这个命令的，当然自己也掌握了这个知识点，公众号已经可以留言了，欢迎留言</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mountain.png" alt="mount"><br></center>

<h2 id="前言">前言</h2><p>在Centos7 下，现在采用了 systemctl来控制服务，这个刚开始用起来可能不太习惯，不过这个服务比之前的服务控制要强大的多，可以做更多的控制，本节将来介绍下关于 Ceph的 osd 磁盘挂载的问题</p>
<p>很多人部署以后，发现在Centos7下用Jewel的时候没有去写fstab也没有写配置文件，重启服务器一样能够挂载起来了，关于这个有另外一篇文章：「<a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/">ceph在centos7下一个不容易发现的改变</a>」<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
