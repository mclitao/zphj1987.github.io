<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[但行好事，莫问前程]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2017-05-24T03:00:24.481Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Freebsd10.2安装包升级pkg引起环境破坏的解决]]></title>
    <link href="http://www.zphj1987.com/2017/05/24/Freebsd-pkg-destroy/"/>
    <id>http://www.zphj1987.com/2017/05/24/Freebsd-pkg-destroy/</id>
    <published>2017-05-24T02:40:34.000Z</published>
    <updated>2017-05-24T03:00:24.481Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/brock.png?imageMogr2/thumbnail/!75p" alt=""><br></center>


<h2 id="前言">前言</h2><p>freebsd10.2环境在安装一个新软件包的时候提示升级pkg到1.10.1，然后点击了升级，然后整个pkg环境就无法使用了</p>
<h2 id="记录">记录</h2><p>升级完了软件包以后第一个错误提示</p>
<blockquote>
<p>FreeBSD: /usr/local/lib/libpkg.so.3: Undefined symbol “utimensat” </p>
</blockquote>
<p>这个是因为这个库是在freebsd的10.3当中才有的库，而我的环境是10.2的环境</p>
<h3 id="网上有一个解决办法">网上有一个解决办法</h3><p>更新源<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/local/etc/pkg/repos/FreeBSD.conf</span></span><br><span class="line">FreeBSD: &#123;</span><br><span class="line">  url: <span class="string">"pkg+http://pkg.FreeBSD.org/<span class="variable">$&#123;ABI&#125;</span>/release_2"</span>,</span><br><span class="line">  enabled: yes</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>检查当前版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg --version</span></span><br><span class="line"><span class="number">1.10</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>更新缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg update</span></span><br></pre></td></tr></table></figure></p>
<p>卸载<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg delete -f pkg</span></span><br></pre></td></tr></table></figure></p>
<p>重新安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg install -y pkg</span></span><br><span class="line"><span class="comment"># pkg2ng</span></span><br></pre></td></tr></table></figure></p>
<p>检查版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg --version</span></span><br><span class="line"><span class="number">1.5</span>.<span class="number">4</span></span><br></pre></td></tr></table></figure></p>
<p>这个在我的环境下没有生效</p>
<h3 id="还有一个办法">还有一个办法</h3><p>有个pkg-static命令可以使用，，然后/var/cache/pkg里边缓存的包。执行命令：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg-static install -f /var/cache/pkg/pkg-1.5.4.txz</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">这个在我的环境下报错</span><br><span class="line">```bash</span><br><span class="line">root@mkiso:/usr/ports/ports-mgmt/pkg <span class="comment"># pkg info sqlite3</span></span><br><span class="line">pkg: warning: database version <span class="number">34</span> is newer than libpkg(<span class="number">3</span>) version <span class="number">33</span>, but still compatible</span><br><span class="line">pkg: sqlite error <span class="keyword">while</span> executing INSERT OR ROLLBACK INTO pkg_search(id, name, origin) VALUES (?<span class="number">1</span>, ?<span class="number">2</span> || <span class="string">'-'</span> || ?<span class="number">3</span>, ?<span class="number">4</span>); <span class="keyword">in</span> file pkgdb.c:<span class="number">1544</span>: no such table: pkg_search</span><br></pre></td></tr></table></figure></p>
<p>这个在网上看到有很多人出现了</p>
<h3 id="最终解决的办法">最终解决的办法</h3><p>在邮件列表里面看到一个解决办法，我是用的这个办法解决了的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#pkg shell</span></span><br></pre></td></tr></table></figure></p>
<p>进入交互模式,执行下面的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE VIRTUAL TABLE pkg_search USING fts4(id, name, origin);</span><br><span class="line">pragma user_version=<span class="number">33</span>;</span><br></pre></td></tr></table></figure></p>
<p>执行完了以后pkg 环境可用了</p>
<h2 id="避免这个问题">避免这个问题</h2><p>锁定本机的pkg版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pkg lock pkg</span><br></pre></td></tr></table></figure></p>
<p>如果需要手动找包就是这个路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://pkg.freebsd.org/FreeBSD:<span class="number">10</span>:amd64/</span><br></pre></td></tr></table></figure></p>
<p>我的机器最终版本是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#pkg -v</span></span><br><span class="line"><span class="number">1.8</span>.<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考资料">参考资料</h2><p><a href="http://www.07net01.com/2017/02/1816847.html" target="_blank" rel="external">freebsd pkg升级问题报错</a><br><a href="http://glasz.org/sheeplog/2017/02/freebsd-usrlocalliblibpkgso3-undefined-symbol-utimensat.html" target="_blank" rel="external">FreeBSD: /usr/local/lib/libpkg.so.3: Undefined symbol “utimensat” </a><br><a href="http://bbs.chinaunix.net/thread-4260263-1-1.html" target="_blank" rel="external">升级pkg失败, 安装低版本pkg失败</a><br><a href="https://lists.freebsd.org/pipermail/freebsd-ports/2017-January/106799.html" target="_blank" rel="external">pkg database issue: database version 34 is newer than libpkg(3) version 33 ?</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/brock.png?imageMogr2/thumbnail/!75p" alt=""><br></center>


<h2 id="前言">前言</h2><]]>
    </summary>
    
      <category term="freebsd" scheme="http://www.zphj1987.com/tags/freebsd/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph OSD从filestore 转换到 bluestore的方法]]></title>
    <link href="http://www.zphj1987.com/2017/05/03/Ceph-filestore-to-bluestore/"/>
    <id>http://www.zphj1987.com/2017/05/03/Ceph-filestore-to-bluestore/</id>
    <published>2017-05-03T09:57:27.000Z</published>
    <updated>2017-05-03T10:13:00.113Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/blueprint.png" alt="myceph"><br></center>

<h2 id="前言">前言</h2><p>前段时间看到<a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&amp;mid=2651256389&amp;idx=1&amp;sn=e11edcce5722853f442b9a7b8211787e&amp;chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&amp;mpshare=1&amp;scene=23&amp;srcid=0502SazrSPsWnszP3xfdEId4#rd" target="_blank" rel="external">豪迈的公众号</a>上提到了这个离线转换工具，最近看到群里有人问，找了下没什么相关文档，就自己写了一个，供参考<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><h3 id="获取代码并安装">获取代码并安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph.git</span><br><span class="line"><span class="built_in">cd</span> ceph</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line">./make-dist</span><br><span class="line">rpm -bb ceph.spec</span><br></pre></td></tr></table></figure>
<p>生成rpm安装包后进行安装,这个过程就不讲太多，根据各种文档安装上最新的版本即可，这个代码合进去时间并不久，大概是上个月才合进去的</p>
<h3 id="配置集群">配置集群</h3><p>首先配置一个filestore的集群，这个也是很简单的，我的环境配置一个单主机三个OSD的集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">3</span>daaf51a-eeba-<span class="number">43</span>a6-<span class="number">9</span>f58-c26c5796f928</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr active: lab8106 </span><br><span class="line">     osdmap e16: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">      pgmap v34: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">323</span> MB used, <span class="number">822</span> GB / <span class="number">822</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">0.80338</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.80338</span>     host lab8106                                   </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26779</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure></p>
<h3 id="写入少量数据">写入少量数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd bench 10 write --no-cleanup</span></span><br></pre></td></tr></table></figure>
<h3 id="设置noout">设置noout</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set noout</span></span><br><span class="line">noout is <span class="built_in">set</span></span><br></pre></td></tr></table></figure>
<h3 id="停止OSD-0">停止OSD.0</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd down 0</span></span><br><span class="line">osd.<span class="number">0</span> is already down.</span><br></pre></td></tr></table></figure>
<p>将数据换个目录挂载，换个新盘挂载到原路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /var/lib/ceph/osd/ceph-0.old/</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># umount /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0.old/</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># mount /dev/sde1 /var/lib/ceph/osd/ceph-0/</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 ~]<span class="comment"># df -h|grep osd</span></span><br><span class="line">/dev/sdc1       <span class="number">275</span>G  <span class="number">833</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">/dev/sdd1       <span class="number">275</span>G  <span class="number">833</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">2</span></span><br><span class="line">/dev/sdb1       <span class="number">275</span>G  <span class="number">759</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span>.old</span><br><span class="line">/dev/sde1       <span class="number">280</span>G   <span class="number">33</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>在配置文件/etc/ceph/ceph.conf中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">enable</span>_experimental_unrecoverable_data_corrupting_features = bluestore</span><br></pre></td></tr></table></figure></p>
<p>如果需要指定osd的block的路径需要写配置文件<br>在做<code>ceph-objectstore-tool --type bluestore --data-path  --op mkfs</code>这个操作之前，在配置文件的全局里面添加上</p>
<blockquote>
<p>bluestore_block_path = /dev/sde2</p>
</blockquote>
<p>然后再创建的时候就可以是链接到设备了，这个地方写全局变量，然后创建完了后就删除掉这项配置文件，写单独的配置文件的时候发现没读取成功,生成后应该是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">total <span class="number">20</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root  <span class="number">9</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> block -&gt; /dev/sde2</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">2</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> bluefs</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">37</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> fsid</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">8</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> kv_backend</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">4</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> mkfs_<span class="keyword">done</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">10</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> <span class="built_in">type</span></span><br></pre></td></tr></table></figure></p>
<p>如果不增加这个就是以文件形式的存在</p>
<h3 id="获取osd-0的fsid">获取osd.0的fsid</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat /var/lib/ceph/osd/ceph-0.old/fsid </span></span><br><span class="line">b2f73450-<span class="number">5</span>c4a-<span class="number">45</span>fb-<span class="number">9</span>c24-<span class="number">8218</span>a5803434</span><br></pre></td></tr></table></figure>
<h3 id="创建一个bluestore的osd-0">创建一个bluestore的osd.0</h3><figure class="highlight brainfuck"><table><tr><td class="code"><pre><span class="line"><span class="title">[</span><span class="comment">root@lab8106</span> <span class="comment">~</span><span class="title">]</span><span class="comment">#</span> <span class="comment">ceph</span><span class="literal">-</span><span class="comment">objectstore</span><span class="literal">-</span><span class="comment">tool</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">type</span> <span class="comment">bluestore</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">data</span><span class="literal">-</span><span class="comment">path</span> <span class="comment">/var/lib/ceph/osd/ceph</span><span class="literal">-</span><span class="comment">0</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">fsid</span> <span class="comment">b2f73450</span><span class="literal">-</span><span class="comment">5c4a</span><span class="literal">-</span><span class="comment">45fb</span><span class="literal">-</span><span class="comment">9c24</span><span class="literal">-</span><span class="comment">8218a5803434</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">op</span> <span class="comment">mkfs</span></span><br></pre></td></tr></table></figure>
<h3 id="转移数据">转移数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0.old --target-data-path /var/lib/ceph/osd/ceph-0 --op dup</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown -R ceph:ceph /var/lib/ceph/osd/ceph-0</span></span><br></pre></td></tr></table></figure>
<p>这个操作是将之前的filestore的数据转移到新的bluestore上了</p>
<h3 id="启动OSD-0">启动OSD.0</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 osd]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 osd]<span class="comment"># ceph -s</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">05</span>-<span class="number">03</span> <span class="number">17</span>:<span class="number">05</span>:<span class="number">13.119492</span> <span class="number">7</span>f20a501b700 -<span class="number">1</span> WARNING: the following dangerous and experimental features are enabled: bluestore</span><br><span class="line"><span class="number">2017</span>-<span class="number">05</span>-<span class="number">03</span> <span class="number">17</span>:<span class="number">05</span>:<span class="number">13.150181</span> <span class="number">7</span>f20a501b700 -<span class="number">1</span> WARNING: the following dangerous and experimental features are enabled: bluestore</span><br><span class="line">    cluster <span class="number">3</span>daaf51a-eeba-<span class="number">43</span>a6-<span class="number">9</span>f58-c26c5796f928</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            noout flag(s) <span class="built_in">set</span></span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr active: lab8106 </span><br><span class="line">     osdmap e25: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags noout</span><br><span class="line">      pgmap v80: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">724</span> MB data, <span class="number">182</span> objects</span><br><span class="line">            <span class="number">3431</span> MB used, <span class="number">555</span> GB / <span class="number">558</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>成功转移</p>
<h3 id="不同的block方式">不同的block方式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/ -al|grep block</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> ceph ceph <span class="number">10737418240</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">32</span> block</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-4/ -al|grep block</span></span><br><span class="line">lrwxrwxrwx  <span class="number">1</span> ceph ceph  <span class="number">58</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">16</span> block -&gt; /dev/disk/by-partuuid/<span class="number">846</span>e93a2-<span class="number">0</span>f6d-<span class="number">47</span>d4-<span class="number">8</span>a90-<span class="number">85</span>ab3cf4ec4e</span><br><span class="line">-rw-r--r--  <span class="number">1</span> ceph ceph  <span class="number">37</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">16</span> block_uuid</span><br></pre></td></tr></table></figure>
<p>可以看到直接创建的时候的block是以链接的方式链接到一个分区的，而不改配置文件的转移的方式里面是一个文件的形式，根据需要进行选择</p>
<h2 id="总结">总结</h2><p>转移工具的出现方便了以后从filestore到bluestore的转移，可以采取一个个osd的转移方式将整个集群进行转移，而免去了剔除osd，再添加的方式，减少了迁移量，可以一个个的离线进行操作</p>
<p>ceph的工具集越来越完整了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-03</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/blueprint.png" alt="myceph"><br></center>

<h2 id="前言">前言</h2><p>前段时间看到<a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&amp;mid=2651256389&amp;idx=1&amp;sn=e11edcce5722853f442b9a7b8211787e&amp;chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&amp;mpshare=1&amp;scene=23&amp;srcid=0502SazrSPsWnszP3xfdEId4#rd">豪迈的公众号</a>上提到了这个离线转换工具，最近看到群里有人问，找了下没什么相关文档，就自己写了一个，供参考<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[多MDS变成单MDS的方法]]></title>
    <link href="http://www.zphj1987.com/2017/05/03/mutimds-to-single-mds/"/>
    <id>http://www.zphj1987.com/2017/05/03/mutimds-to-single-mds/</id>
    <published>2017-05-03T07:53:10.000Z</published>
    <updated>2017-05-03T07:54:24.241Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>之前有个cepher的环境上是双活MDS的，需要变成MDS，目前最新版本是支持这个操作的</p>
<a id="more"></a>
<h2 id="方法">方法</h2><h3 id="设置最大mds">设置最大mds</h3><p>多活的mds的max_mds会超过1，这里需要先将max_mds设置为1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mds <span class="built_in">set</span> max_mds <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h3 id="deactive_mds">deactive mds</h3><p>看下需要停掉的mds是rank 0 还是rank1,然后执行下面的命令即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server8 ~]<span class="comment"># zbkc -s|grep mdsmap</span></span><br><span class="line">     mdsmap e13: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab8106=up:clientreplay&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个输出的lab8106前面的0，就是这个mds的rank，根据需要停止对应的rank<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mds deactivate <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>不建议用多活mds</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-03</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>之前有个cepher的环境上是双活MDS的，需要变成MDS，目前最新版本是支持这个操作的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph根据Crush位置读取数据]]></title>
    <link href="http://www.zphj1987.com/2017/04/27/Ceph-depend-Crush-read-data/"/>
    <id>http://www.zphj1987.com/2017/04/27/Ceph-depend-Crush-read-data/</id>
    <published>2017-04-27T08:47:04.000Z</published>
    <updated>2017-04-27T09:10:58.547Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/read.gif?imageMogr2/thumbnail/!75p" alt=""><br></center>

<h2 id="前言">前言</h2><p>在ceph研发群里面看到一个cepher在问关于怎么读取ceph的副本的问题，这个功能应该在2012年的时候,我们公司的研发就修改了代码去实现这个功能，只是当时的硬件条件所限，以及本身的稳定性问题，后来没有在生产当中使用<br><a id="more"></a><br>我们都知道ceph在写数据的时候，是先写主本，然后去写副本，而读取的时候，实际上只有主本能够提供服务，这对于磁盘的整体带宽来说，并没有充分的发挥其性能，所以能够读取副本当然是会有很大好处的，特别是对于读场景比较多的情况</p>
<p>那么在ceph当中是不是有这个功能呢？其实是有的，这个地方ceph更往上走了一层，是基于crush定义的地址去进行文件的读取，这样在读取的客户端眼里，就没有什么主副之分，他会按自己想要的区域去尽量读取，当然这个区域没有的时候就按正常读取就可以了<br><!--more--></p>
<h2 id="实践">实践</h2><p>如果你看过关于ceph hadoop的相关配置文档，应该会看到这么一个配置</p>
<blockquote>
<p>ceph.localize.reads<br>Allow reading from file replica objects<br>Default value: true</p>
</blockquote>
<p>显示的是可以从非主本去读取对象，这个对于hadoop场景肯定是越近越好的，可以在ceph的代码里面搜索下 localize-reads<br><a href="https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (std::vector&lt;const char*&gt;::iterator i = args.begin(); i != args.end(); ) &#123;</span><br><span class="line">  <span class="keyword">if</span> (ceph_argparse_double_dash(args, i)) &#123;</span><br><span class="line">    <span class="built_in">break</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ceph_argparse_flag(args, i, <span class="string">"--localize-reads"</span>, (char*)NULL)) &#123;</span><br><span class="line">    cerr &lt;&lt; <span class="string">"setting CEPH_OSD_FLAG_LOCALIZE_READS"</span> &lt;&lt; std::endl;</span><br><span class="line">    filer_flags |= CEPH_OSD_FLAG_LOCALIZE_READS;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ceph_argparse_flag(args, i, <span class="string">"-h"</span>, <span class="string">"--help"</span>, (char*)NULL)) &#123;</span><br><span class="line">    usage();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    ++i;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到在ceph-fuse的情况下，是有这个隐藏的一个参数的，本篇就是用这个隐藏的参数来进行实践</p>
<h3 id="配置一个两节点集群">配置一个两节点集群</h3><p>配置完成了以后ceph的目录树如下,mon部署在lab8106上面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">1.07336</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.53778</span>     host lab8106                                   </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26999</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">3</span> <span class="number">0.53558</span>     host lab8107                                   </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">3</span> <span class="number">0.26779</span>         osd.<span class="number">3</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br><span class="line">[root@lab8107 ~]<span class="comment"># ceph -s|grep mon</span></span><br><span class="line">monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="在lab8107上挂载客户端">在lab8107上挂载客户端</h3><p>在/etc/ceph/ceph.conf中增加一个配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">crush_location = <span class="string">"host=lab8107 root=default"</span></span><br></pre></td></tr></table></figure></p>
<p>这个配置的作用是告诉这个客户端尽量去读取lab8107上面的对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph-fuse -m lab8106:6789 /mnt  --localize-reads</span></span><br></pre></td></tr></table></figure></p>
<p>写入一个大文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># dd if=/dev/zero of=a bs=4M count=4000</span></span><br></pre></td></tr></table></figure></p>
<p>在lab8106和lab8107上监控磁盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># iostat -dm 1</span></span><br></pre></td></tr></table></figure></p>
<p>读取数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># dd if=a of=/dev/null</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到只有lab8107上有磁盘的读取，也就是读取的数据里面肯定也有副本，都是从lab8107上面读取了</p>
<p>如果需要多次测试，需要清除下缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sync; <span class="built_in">echo</span> <span class="number">3</span> &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure></p>
<p>并且重新挂载客户端，这个读取crush的位置的操作是在mount的时候读取的</p>
<h2 id="使用场景">使用场景</h2><p>上面的配置是可以指定多个平级的位置的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">crush_location = <span class="string">"host=lab8106 host=lab8107 root=default"</span></span><br></pre></td></tr></table></figure></p>
<p>这样，在一些读请求很多的场景下，可以把整个后端按逻辑上划分为一个个的区域，然后前面的客户端就可以平级分配到这些区域当中，这样就可以比较大的限度去把副本的读取也调动起来的</p>
<p>目前在ceph-fuse上已经实现，rbd里面也有类似的一些处理，这个是一个很不错的功能</p>
<h2 id="总结">总结</h2><p>ceph里面有很多可配置的东西，怎么用好它，最大限度的去适配使用场景，还是有很大的可调的空间的，所谓学无止境，我也在学习python coding了，有很多想法等着去实现</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/read.gif?imageMogr2/thumbnail/!75p" alt=""><br></center>

<h2 id="前言">前言</h2><p>在ceph研发群里面看到一个cepher在问关于怎么读取ceph的副本的问题，这个功能应该在2012年的时候,我们公司的研发就修改了代码去实现这个功能，只是当时的硬件条件所限，以及本身的稳定性问题，后来没有在生产当中使用<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于backfill参数建议]]></title>
    <link href="http://www.zphj1987.com/2017/04/27/about-backfill-conf/"/>
    <id>http://www.zphj1987.com/2017/04/27/about-backfill-conf/</id>
    <published>2017-04-27T01:36:17.000Z</published>
    <updated>2017-04-27T02:31:18.735Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fill.gif" alt=""><br></center>

<h2 id="前言">前言</h2><p>在做一个比较满的集群的扩容的时候，遇到了一些问题，在这里做下总结，一般来说很难遇到，扩容要趁早，不然出的问题都是稀奇古怪的一些问题<br><a id="more"></a></p>
<h2 id="建议">建议</h2><p>环境一般来说在70%左右就需要考虑扩容了，这个时候的扩容数据迁移的少，遇到的问题自然会少很多，所谓的参数设置并不是一个单纯的参数的设置，所以一般来说在调优参数的时候，个人觉得只有适配硬件进行调优，所以本篇的参数同样是一个组合形式的</p>
<p>首先罗列出本篇涉及的所有参数</p>
<blockquote>
<p>mon_osd_full_ratio = 0.95<br>osd_backfill_full_ratio = 0.85<br>osd_max_backfills = 1</p>
</blockquote>
<p>最少的OSD的PG数目</p>
<blockquote>
<p>min_pg=`ceph osd df|awk ‘{print $9}’|awk ‘NF’|grep -v PGS|sort|head -n 1`</p>
</blockquote>
<p>那么最好满足</p>
<blockquote>
<p>(osd_max_backfills/min_pg)+osd_backfill_full_ratio &lt; mon_osd_full_ratio</p>
</blockquote>
<p>这个在老版本里面进行backfill full的检测的时候，只在启动backfill的时候做了检测，如果设置的backfill足够大，而迁移的又足够多的时候，就会一下涌过去，直径把OSD给弄full然后挂掉了，新版本还没验证是否做了实时控制，但是如果遵循了上面的设置，即使没控制一样不会出问题</p>
<h2 id="总结">总结</h2><p>有的参数不光对速度有控制，对量上面同样可能有影响，所以在设置的时候，需要尽量综合考虑</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fill.gif" alt=""><br></center>

<h2 id="前言">前言</h2><p>在做一个比较满的集群的扩容的时候，遇到了一些问题，在这里做下总结，一般来说很难遇到，扩容要趁早，不然出的问题都是稀奇古怪的一些问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mds/journal.cc: 2929: FAILED assert解决]]></title>
    <link href="http://www.zphj1987.com/2017/04/27/mds-journal-cc-2929-FAILED-assert/"/>
    <id>http://www.zphj1987.com/2017/04/27/mds-journal-cc-2929-FAILED-assert/</id>
    <published>2017-04-27T01:24:39.000Z</published>
    <updated>2017-04-27T01:31:00.360Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/session.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在处理一个其他双活MDS无法启动环境的时候，查看mds的日志看到了这个错误mds/journal.cc: 2929: FAILED assert(mds-&gt;sessionmap.get_version() == cmapv)，在查询资料以后，暂时得到了解决,在生产环境下还是不建议使用双活MDS<br><a id="more"></a></p>
<h2 id="处理步骤">处理步骤</h2><p>这个是双MDS多活情况下出现的一个问题，在什么情况下出现还无法判断，目前只看到是有这个问题，并且有其他人也出现了 <a href="http://tracker.ceph.com/issues/17113" target="_blank" rel="external">issue17113</a><br>按照<a href="http://docs.ceph.com/docs/master/cephfs/disaster-recovery/" target="_blank" rel="external">disaster-recovery</a>建议的步骤做了如下处理：</p>
<h3 id="备份下journal">备份下journal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephfs-journal-tool journal <span class="built_in">export</span> backup.bin</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cephfs-journal-tool journal reset</span><br><span class="line">cephfs-table-tool all reset session</span><br></pre></td></tr></table></figure>
<p>做了上两步后环境并没有恢复,还有个下面的操作没有做，这个操作会引起数据的丢失， MDS ranks other than 0 will be ignored: as a result it is possible for this to result in data loss，所以暂缓操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph fs reset &lt;fs name&gt; --yes-i-really-mean-it</span><br></pre></td></tr></table></figure></p>
<p>再次启动后还是，看到日志提示的是sessionmap的问题，正常情况下这个地方重置了session应该是可以好的</p>
<p>Yan, Zheng 2014年的时候在<a href="https://www.mail-archive.com/ceph-devel@vger.kernel.org/msg18629.html" target="_blank" rel="external">邮件列表</a>里面提过一个配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds wipe_sessions = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>当时解决一个replay的问题，尝试加入这个参数，然后启动mds</p>
<p>环境恢复了变成了双active，提示还有damage，但是数据属于可访问了</p>
<h3 id="后续操作">后续操作</h3><p>建议是导出数据，重新配置为主备MDS集群，然后倒入数据</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/session.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在处理一个其他双活MDS无法启动环境的时候，查看mds的日志看到了这个错误mds/journal.cc: 2929: FAILED assert(mds-&gt;sessionmap.get_version() == cmapv)，在查询资料以后，暂时得到了解决,在生产环境下还是不建议使用双活MDS<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rados put striper功能的调试]]></title>
    <link href="http://www.zphj1987.com/2017/04/26/rados-put-strip-debug/"/>
    <id>http://www.zphj1987.com/2017/04/26/rados-put-strip-debug/</id>
    <published>2017-04-26T08:00:40.000Z</published>
    <updated>2017-04-26T08:01:51.060Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/strip.jpg" alt="strip"><br></center>

<h2 id="前言">前言</h2><p>之前对于striper这个地方的功能并没研究太多，只是知道这个里面可以以条带方式并行的去写对象，从而加大并发性来提高性能，而默认的条带数目为1，也就是以对象大小去写，并没有条带，所以不是很好感觉到差别，今天就尝试下用rados命令来看下这个条带是怎么回事<br><a id="more"></a></p>
<h2 id="实践过程">实践过程</h2><p>最开始我的集群是用rpm包进行安装的，这个可以做一些常规的测试，如果需要改动一些代码的话，就比较麻烦了，本文后面会讲述怎么改动一点点代码，然后进行测试</p>
<p>我们一般来说用rados put操作就是一个完整的文件，并不会进行拆分，我们尝试下看下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/zero of=16M bs=4M count=4</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd put 16M 16M</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd stat 16M</span></span><br><span class="line">rbd/<span class="number">16</span>M mtime <span class="number">2017</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">15</span>:<span class="number">08</span>:<span class="number">14.000000</span>, size <span class="number">16777216</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到我们put 16M的文件，在后台就是一个16M的对象</p>
<p>这个rados命令还有个参数是striper<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados  --help|grep stri</span></span><br><span class="line">   --striper</span><br><span class="line">        Use radostriper interface rather than pure rados</span><br></pre></td></tr></table></figure></p>
<p>我们来用这个命令试一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/zero of=strip16M bs=4M count=4</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd put strip16M strip16M --striper</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd ls |grep strip</span></span><br><span class="line">strip16M.<span class="number">0000000000000002</span></span><br><span class="line">strip16M.<span class="number">0000000000000003</span></span><br><span class="line">strip16M.<span class="number">0000000000000000</span></span><br><span class="line">strip16M.<span class="number">0000000000000001</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd  --striper ls |grep strip</span></span><br><span class="line">strip16M</span><br><span class="line">[root@lab8106 ~]<span class="comment">#  rados  -p rbd stat strip16M.0000000000000002</span></span><br><span class="line">rbd/strip16M.<span class="number">0000000000000002</span> mtime <span class="number">2017</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">15</span>:<span class="number">11</span>:<span class="number">06.000000</span>, size <span class="number">4194304</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到这个16M的文件是被拆分成了4M一个的对象，存储到了后台的,我们开启下日志后看下有没有什么详细的信息，因为在rados参数当中确实没有找到可配置的选项<br>在/etc/ceph/ceph.conf当中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">debug_rados=<span class="number">20</span></span><br><span class="line">debug_striper=<span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<p>再次测试<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/zero of=strip116M bs=4M count=4</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd put strip116M strip116M --striper</span></span><br><span class="line">···</span><br><span class="line">sc is one, reset su to os</span><br><span class="line">su <span class="number">4194304</span> sc <span class="number">1</span> os <span class="number">4194304</span> stripes_per_object <span class="number">1</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>这个地方解释下意思</p>
<blockquote>
<p>strip count is 1,重置strip unit为object size ，也就是4M<br>strip unit 4194304 ，strip count 1，object size 4194304,每个对象的条带为1</p>
</blockquote>
<p>这个代码里面写了<br><a href="https://github.com/ceph/ceph/blob/master/src/tools/rados/rados.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/tools/rados/rados.cc</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--striper</span><br><span class="line">       Use radostriper interface rather than pure rados</span><br></pre></td></tr></table></figure></p>
<p>也就是这个rados在加了参数之后是调用了radostriper interface这个接口的，所以猜测这个条带的相关参数应该是在接口里面写死了的<br><a href="https://github.com/ceph/ceph/blob/master/src/libradosstriper/RadosStriperImpl.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/libradosstriper/RadosStriperImpl.cc</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/// default object layout</span><br><span class="line">struct ceph_file_layout default_file_layout = &#123;</span><br><span class="line"> fl_stripe_unit: init_le32(<span class="number">1</span>&lt;&lt;<span class="number">22</span>),</span><br><span class="line"> fl_stripe_count: init_le32(<span class="number">1</span>),</span><br><span class="line"> fl_object_size: init_le32(<span class="number">1</span>&lt;&lt;<span class="number">22</span>),</span><br><span class="line"> fl_cas_<span class="built_in">hash</span>: init_le32(<span class="number">0</span>),</span><br><span class="line"> fl_object_stripe_unit: init_le32(<span class="number">0</span>),</span><br><span class="line"> fl_unused: init_le32(-<span class="number">1</span>),</span><br><span class="line"> fl_pg_pool : init_le32(-<span class="number">1</span>),</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>下面开始看下调试模式下改下这几个数值</p>
<h3 id="下载代码">下载代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph.git</span><br><span class="line">git checkout -b myceph2 v10.<span class="number">2.3</span></span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>
<p>切换到10.2.3版本,用的make模式，没用cmake<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ceph</span><br><span class="line">./install-deps.sh</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make -j <span class="number">12</span></span><br></pre></td></tr></table></figure></p>
<p>启动开发模式服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> src</span><br><span class="line">./vstart.sh --mon_num <span class="number">1</span> --osd_num <span class="number">3</span> --mds_num <span class="number">1</span>  --short -n <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>这样，dev cluster就起来了。修改部分源码重新make之后，需要关闭cluster，重启让代码生效，当然最好的是，你修改哪个模块，就重启那个模块就行，这里使用重启集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./stop.sh all</span><br><span class="line">./vstart.sh --mon_num <span class="number">1</span> --osd_num <span class="number">3</span> --mds_num <span class="number">1</span> --short  <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>查看状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -s -c ./ceph.conf</span></span><br></pre></td></tr></table></figure></p>
<p>我们修改下代码<br>vim libradosstriper/RadosStriperImpl.cc<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/// default object layout</span><br><span class="line">struct ceph_file_layout default_file_layout = &#123;</span><br><span class="line"> fl_stripe_unit: init_le32(<span class="number">1</span>&lt;&lt;<span class="number">21</span>),</span><br><span class="line"> fl_stripe_count: init_le32(<span class="number">2</span>),</span><br><span class="line"> fl_object_size: init_le32(<span class="number">1</span>&lt;&lt;<span class="number">22</span>),</span><br><span class="line"> fl_cas_<span class="built_in">hash</span>: init_le32(<span class="number">0</span>),</span><br><span class="line"> fl_object_stripe_unit: init_le32(<span class="number">0</span>),</span><br><span class="line"> fl_unused: init_le32(-<span class="number">1</span>),</span><br><span class="line"> fl_pg_pool : init_le32(-<span class="number">1</span>),</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>修改的是stripe_unit为2M，stripe_count为2，object_size为4M，也就是条带为2<br>修改完了后重新make<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./stop.sh all</span><br><span class="line">./vstart.sh --mon_num <span class="number">1</span> --osd_num <span class="number">3</span> --mds_num <span class="number">1</span> --short  <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>初始化集群，修改下配置文件增加调试信息<br>vim ./ceph.conf<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">debug_rados=<span class="number">20</span></span><br><span class="line">debug_striper=<span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<p>创建文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/zero of=debugstrip16M bs=4M count=4</span></span><br><span class="line">[root@lab8106 src]<span class="comment"># ./rados -c ./ceph.conf --striper  -p rbd  put  debugstrip16M debugstrip16M</span></span><br><span class="line">[root@lab8106 src]<span class="comment">#./rados -c ./ceph.conf  -p rbd  stat debugstrip16M.0000000000000001</span></span><br><span class="line">rbd/debugstrip16M.<span class="number">0000000000000001</span> mtime <span class="number">2017</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">15</span>:<span class="number">38</span>:<span class="number">41.483464</span> </span><br><span class="line"><span class="number">2017</span>-<span class="number">04</span>-<span class="number">26</span> <span class="number">15</span>:<span class="number">37</span>:<span class="number">27.000000</span>, size <span class="number">4194304</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到对象还是4M<br>我们截取下日志分析<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">su <span class="number">2097152</span> sc <span class="number">2</span> os <span class="number">4194304</span> stripes_per_object <span class="number">2</span></span><br><span class="line">off <span class="number">0</span> blockno <span class="number">0</span> stripeno <span class="number">0</span> stripepos <span class="number">0</span> objectsetno <span class="number">0</span> objectno <span class="number">0</span> block_start <span class="number">0</span></span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000000</span> (<span class="number">0</span>) </span><br><span class="line">off <span class="number">2097152</span> blockno <span class="number">1</span> stripeno <span class="number">0</span> stripepos <span class="number">1</span> objectsetno <span class="number">0</span> objectno <span class="number">1</span> block_start <span class="number">0</span> </span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000001</span> (<span class="number">1</span>) </span><br><span class="line">off <span class="number">4194304</span> blockno <span class="number">2</span> stripeno <span class="number">1</span> stripepos <span class="number">0</span> objectsetno <span class="number">0</span> objectno <span class="number">0</span> block_start <span class="number">2097152</span></span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000000</span> (<span class="number">0</span>)   </span><br><span class="line">off <span class="number">6291456</span> blockno <span class="number">3</span> stripeno <span class="number">1</span> stripepos <span class="number">1</span> objectsetno <span class="number">0</span> objectno <span class="number">1</span> block_start <span class="number">2097152</span></span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000001</span> (<span class="number">1</span>)</span><br><span class="line">off <span class="number">8388608</span> blockno <span class="number">4</span> stripeno <span class="number">2</span> stripepos <span class="number">0</span> objectsetno <span class="number">1</span> objectno <span class="number">2</span> block_start <span class="number">0</span></span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000002</span> (<span class="number">2</span>) </span><br><span class="line">off <span class="number">10485760</span> blockno <span class="number">5</span> stripeno <span class="number">2</span> stripepos <span class="number">1</span> objectsetno <span class="number">1</span> objectno <span class="number">3</span> block_start <span class="number">0</span></span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000003</span> (<span class="number">3</span>) </span><br><span class="line">off <span class="number">12582912</span> blockno <span class="number">6</span> stripeno <span class="number">3</span> stripepos <span class="number">0</span> objectsetno <span class="number">1</span> objectno <span class="number">2</span> block_start <span class="number">2097152</span> </span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000002</span> (<span class="number">2</span>)</span><br><span class="line">off <span class="number">14680064</span> blockno <span class="number">7</span> stripeno <span class="number">3</span> stripepos <span class="number">1</span> objectsetno <span class="number">1</span> objectno <span class="number">3</span> block_start <span class="number">2097152</span> </span><br><span class="line">added new extent(debugstrip16M.<span class="number">0000000000000003</span> (<span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
<p>从上面可以看到先在debugstrip16M.0000000000000000写了2M，在debugstrip16M.0000000000000001写了2M，<br>然后在debugstrip16M.0000000000000000追加写了2M，并且是从block_start 2097152开始的，每个对象是写了两次的并且每次写的就是条带的大小的2M，跟修改上面的条带大小和对象大小是一致的，并且可以很清楚的看到写对象的过程</p>
<h2 id="总结">总结</h2><p>本篇尝试了用rados来测试strip功能，并且顺带讲了下怎么在开发模式下修改代码并测试，如果自己写客户端的话，利用librados的时候，可以考虑使用libradosstriper条带来增加一定的性能</p>
<h2 id="参考文档">参考文档</h2><p><a href="http://ivanjobs.github.io/2016/05/11/prepare-ceph-dev-env/" target="_blank" rel="external">准备Ceph开发环境</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-26</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/strip.jpg" alt="strip"><br></center>

<h2 id="前言">前言</h2><p>之前对于striper这个地方的功能并没研究太多，只是知道这个里面可以以条带方式并行的去写对象，从而加大并发性来提高性能，而默认的条带数目为1，也就是以对象大小去写，并没有条带，所以不是很好感觉到差别，今天就尝试下用rados命令来看下这个条带是怎么回事<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephfs的文件存到哪里了]]></title>
    <link href="http://www.zphj1987.com/2017/04/20/where-is-cephfs-data-store/"/>
    <id>http://www.zphj1987.com/2017/04/20/where-is-cephfs-data-store/</id>
    <published>2017-04-20T02:22:17.000Z</published>
    <updated>2017-04-20T02:35:07.836Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/file.png" alt="file"><br></center>

<h2 id="前言">前言</h2><p>在ceph里面使用rbd接口的时候，存储的数据在后台是以固定的prifix的对象存在的，这样就能根据相同的前缀对象去对image文件进行拼接或者修复</p>
<p>在文件系统里面这一块就要复杂一些，本篇就写的关于这个，文件和对象的对应关系是怎样的，用系统命令怎么定位，又是怎么得到这个路径的<br><a id="more"></a></p>
<h2 id="实践">实践</h2><h3 id="根据系统命令进行文件的定位">根据系统命令进行文件的定位</h3><p>写入测试文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/mnt/testfile bs=<span class="number">4</span>M count=<span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>查看文件的映射<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># cephfs /mnt/testfile  map</span></span><br><span class="line">WARNING: This tool is deprecated.  Use the layout.* xattrs to query and modify layouts.</span><br><span class="line">    FILE OFFSET                    OBJECT        OFFSET        LENGTH  OSD</span><br><span class="line">              <span class="number">0</span>      <span class="number">10000001188.00000000</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">1</span></span><br><span class="line">        <span class="number">4194304</span>      <span class="number">10000001188.00000001</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">0</span></span><br><span class="line">        <span class="number">8388608</span>      <span class="number">10000001188.00000002</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">1</span></span><br><span class="line">       <span class="number">12582912</span>      <span class="number">10000001188.00000003</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">0</span></span><br><span class="line">       <span class="number">16777216</span>      <span class="number">10000001188.00000004</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">1</span></span><br><span class="line">       <span class="number">20971520</span>      <span class="number">10000001188.00000005</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">0</span></span><br><span class="line">       <span class="number">25165824</span>      <span class="number">10000001188.00000006</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">0</span></span><br><span class="line">       <span class="number">29360128</span>      <span class="number">10000001188.00000007</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">1</span></span><br><span class="line">       <span class="number">33554432</span>      <span class="number">10000001188.00000008</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">1</span></span><br><span class="line">       <span class="number">37748736</span>      <span class="number">10000001188.00000009</span>             <span class="number">0</span>       <span class="number">4194304</span>  <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>查找文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 mnt]<span class="comment"># ceph osd map data 10000001188.00000000</span></span><br><span class="line">osdmap e109 pool <span class="string">'data'</span> (<span class="number">2</span>) object <span class="string">'10000001188.00000000'</span> -&gt; pg <span class="number">2.9865</span>f84d (<span class="number">2</span>.d) -&gt; up ([<span class="number">1</span>], p1) acting ([<span class="number">1</span>], p1)</span><br><span class="line">[root@lab8106 mnt]<span class="comment"># ll /var/lib/ceph/osd/ceph-1/current/2.d_head/10000001188.00000000__head_9865F84D__2 </span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph <span class="number">4194304</span> Apr <span class="number">20</span> <span class="number">09</span>:<span class="number">35</span> /var/lib/ceph/osd/ceph-<span class="number">1</span>/current/<span class="number">2</span>.d_head/<span class="number">10000001188.00000000</span>__head_9865F84D__2</span><br></pre></td></tr></table></figure></p>
<p>根据上面的命令已经把文件和对象的关系找到了，我们要看下这个关系是根据什么计算出来的</p>
<h3 id="根据算法进行文件定位">根据算法进行文件定位</h3><p>写入测试文件(故意用bs=3M模拟后台不为整的情况)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/zero of=/mnt/myfile bs=3M count=10</span></span><br></pre></td></tr></table></figure></p>
<p>获取文件的inode信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># fileinode=`stat  -c %i  "/mnt/myfile"`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo $fileinode</span></span><br></pre></td></tr></table></figure></p>
<p>获取文件的大小和对象个数信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># filesize=`stat  -c %s  "/mnt/myfile"`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo $filesize</span></span><br><span class="line"><span class="number">31457280</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># objectnumori=`echo "scale = 1; $filesize/$objectsize"|bc`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo $objectnumori</span></span><br><span class="line"><span class="number">7.5</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># objectnum=`echo $(($&#123;objectnumori//.*/+1&#125;))`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo $objectnum</span></span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure></p>
<p>获取对象名称前缀<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># declare -l $objectname</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># objectname=`echo "obase=16;$fileinode"|bc`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo $objectname</span></span><br><span class="line"><span class="number">1000000118</span>b</span><br></pre></td></tr></table></figure></p>
<p>上面的declare -l操作后，对象名称的变量才能自动赋值为小写的，否则的话就是大写的，会出现对应不上的问题<br>对象的后缀(后面的0即为编号)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#objectbackname=`printf "%.8x\n" 0`</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#echo $objectbackname</span></span><br></pre></td></tr></table></figure></p>
<p>真正的对象名称为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#realobjectback=$objectname.$objectbackname</span></span><br></pre></td></tr></table></figure></p>
<p>打印出所有对象名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># for num in `seq  0 $objectnum` ;do backname=`printf "%.8x\n" $num`;echo $objectname.$backname;done;</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000000</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000001</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000002</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000003</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000004</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000005</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000006</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000007</span></span><br><span class="line"><span class="number">1000000118</span>b.<span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到用算法进行定位的时候，整个过程都没有跟集群ceph进行查询交互，只用到了获取文件的stat的信息，所以根据算法就可以完全定位到具体的对象名称了</p>
<h2 id="总结">总结</h2><p>本篇是介绍了cephfs中文件跟后台具体对象对应的关系，这个对于系统的可恢复性上面还是有很大的作用的，在cephfs当中只要对象还在，数据就还在，哪怕所有的服务全挂掉，这个在之前的某个别人的生产环境当中已经实践过一次，当然那个是rbd的相对来说要简单一些，当然文件系统的恢复也可以用OSD重构集群的方式进行恢复，本篇的对于元数据丢失的情况下文件恢复会有一定的指导作用</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-20</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/file.png" alt="file"><br></center>

<h2 id="前言">前言</h2><p>在ceph里面使用rbd接口的时候，存储的数据在后台是以固定的prifix的对象存在的，这样就能根据相同的前缀对象去对image文件进行拼接或者修复</p>
<p>在文件系统里面这一块就要复杂一些，本篇就写的关于这个，文件和对象的对应关系是怎样的，用系统命令怎么定位，又是怎么得到这个路径的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[为什么删除的Ceph对象还能get]]></title>
    <link href="http://www.zphj1987.com/2017/04/19/why-rm-object-can-get/"/>
    <id>http://www.zphj1987.com/2017/04/19/why-rm-object-can-get/</id>
    <published>2017-04-19T07:12:42.000Z</published>
    <updated>2017-04-19T07:19:25.755Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rm.jpg" alt="rm"><br></center>

<h2 id="前言">前言</h2><p>在很久以前在研究一套文件系统的时候，当时发现一个比较奇怪的现象，没有文件存在，磁盘容量还在增加，在研究了一段时间后，发现这里面有一种比较奇特的处理逻辑</p>
<p>这套文件系统在处理一个文件的时候放入的是一个临时目录，最开始在发送第一个写请求后，在操作系统层面马上进行了一个delete操作，而写还在继续，并且需要处理这个数据的进程一直占着的，一旦使用完这个文件，不需要做处理，这个文件就会自动释放掉，而无需担心临时文件占用空间的问题</p>
<p>在Ceph集群当中，有人碰到了去后台的OSD直接rm一个对象后，在前端通过rados还能get到这个删除的对象，而不能rados ls到，我猜测就是这个原因，我们来看下怎么验证这个问题<br><a id="more"></a></p>
<h2 id="验证步骤">验证步骤</h2><h3 id="准备测试数据，并且put进去集群">准备测试数据，并且put进去集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat zp1 </span></span><br><span class="line">sdasdasd</span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados  -p rbd put zp1 zp1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">zp1</span><br></pre></td></tr></table></figure>
<h3 id="找到测试数据并且直接_rm_删除">找到测试数据并且直接 rm 删除</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd map rbd zp1</span></span><br><span class="line">osdmap e90 pool <span class="string">'rbd'</span> (<span class="number">3</span>) object <span class="string">'zp1'</span> -&gt; pg <span class="number">3.43</span>eb7bdb (<span class="number">3.1</span>b) -&gt; up ([<span class="number">0</span>], p0) acting ([<span class="number">0</span>], p0)</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3 </span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> ceph ceph <span class="number">9</span> Apr <span class="number">19</span> <span class="number">14</span>:<span class="number">46</span> /var/lib/ceph/osd/ceph-<span class="number">0</span>/current/<span class="number">3.1</span>b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3</span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-0/current/3.1b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3</span></span><br></pre></td></tr></table></figure>
<h3 id="尝试查询数据，get数据">尝试查询数据，get数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># rados  -p rbd ls</span></span><br><span class="line">[root@lab8106 tmp]<span class="comment"># rados  -p rbd get zp1 zp1</span></span><br><span class="line">[root@lab8106 tmp]<span class="comment"># cat zp1</span></span><br><span class="line">sdasdasd</span><br></pre></td></tr></table></figure>
<p>可以看到数据确实可以查询不到，但是能get下来，并且数据是完整的</p>
<h3 id="验证我的猜测">验证我的猜测</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># lsof |grep zp1</span></span><br><span class="line">ms_pipe_w  <span class="number">4737</span>  <span class="number">5620</span>           ceph   <span class="number">86</span>u      REG               <span class="number">8</span>,<span class="number">33</span>          <span class="number">9</span>  <span class="number">201496748</span> /var/lib/ceph/osd/ceph-<span class="number">0</span>/current/<span class="number">3.1</span>b_head/DIR_B/DIR_D/zp1__head_43EB7BDB__3 (deleted)</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>可以看到这个标记为delete的对象就是我们删除的zp1，这个输出的意思是，进程4737上面删除了一个文件，文件描述符为86的</p>
<p>我们直接去拷贝下这个数据看下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># cp  /proc/4737/fd/86 /tmp/zp_save</span></span><br><span class="line">[root@lab8106 tmp]<span class="comment"># cat /tmp/zp_save </span></span><br><span class="line">sdasdasd</span><br></pre></td></tr></table></figure></p>
<p>可以看到这个数据确实是存在的，还没有释放，所有可以get的到</p>
<p>我们试下重启下这个进程，看下delete的文件是不是会释放<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># lsof |grep zp1</span></span><br></pre></td></tr></table></figure>
<p>可以看到已经没有这个delete了，现在我们尝试下get<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 tmp]<span class="comment"># rados  -p rbd get zp1 zp1</span></span><br><span class="line">error getting rbd/zp1: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到文件释放掉了，问题确实跟我猜测的是一致的，当然这并不是什么问题</p>
<h2 id="总结">总结</h2><p>本篇是对删除了的对象还能get的现象进行了解释</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rm.jpg" alt="rm"><br></center>

<h2 id="前言">前言</h2><p>在很久以前在研究一套文件系统的时候，当时发现一个比较奇怪的现象，没有文件存在，磁盘容量还在增加，在研究了一段时间后，发现这里面有一种比较奇特的处理逻辑</p>
<p>这套文件系统在处理一个文件的时候放入的是一个临时目录，最开始在发送第一个写请求后，在操作系统层面马上进行了一个delete操作，而写还在继续，并且需要处理这个数据的进程一直占着的，一旦使用完这个文件，不需要做处理，这个文件就会自动释放掉，而无需担心临时文件占用空间的问题</p>
<p>在Ceph集群当中，有人碰到了去后台的OSD直接rm一个对象后，在前端通过rados还能get到这个删除的对象，而不能rados ls到，我猜测就是这个原因，我们来看下怎么验证这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph删除OSD上一个异常object]]></title>
    <link href="http://www.zphj1987.com/2017/04/19/ceph-delete-an-error-object/"/>
    <id>http://www.zphj1987.com/2017/04/19/ceph-delete-an-error-object/</id>
    <published>2017-04-19T06:12:59.000Z</published>
    <updated>2017-04-19T06:18:23.297Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/delete.png" alt="delete"><br></center></p>
<h2 id="前言">前言</h2><p>ceph里面的数据是以对象的形式存储在OSD当中的，有的时候因为磁盘的损坏或者其它的一些特殊情况，会引起集群当中的某一个对象的异常，那么我们需要对这个对象进行处理</p>
<p>在对象损坏的情况下，启动OSD有的时候都会有问题，那么通过rados rm的方式是没法发送到这个无法启动的OSD的，也就无法删除，所以需要用其他的办法来处理这个情况<br><a id="more"></a></p>
<h2 id="处理步骤">处理步骤</h2><h3 id="查找对象的路径">查找对象的路径</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd map rbd  rbd_data.857e6b8b4567.00000000000000ba</span></span><br><span class="line">osdmap e53 pool <span class="string">'rbd'</span> (<span class="number">0</span>) object <span class="string">'rbd_data.857e6b8b4567.00000000000000ba'</span> -&gt; pg <span class="number">0.2</span>daee1ba (<span class="number">0.3</span>a) -&gt; up ([<span class="number">1</span>], p1) acting ([<span class="number">1</span>], p1)</span><br></pre></td></tr></table></figure>
<p>先找到这个对象所在的OSD以及PG</p>
<h3 id="设置集群的noout">设置集群的noout</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph osd set noout</span></span><br></pre></td></tr></table></figure>
<p>这个是为了防止osd的停止产生不必要的删除</p>
<h3 id="停止OSD">停止OSD</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ]<span class="comment">#systemctl stop ceph-osd@1</span></span><br></pre></td></tr></table></figure>
<p>如果osd已经是停止的状态就不需要做这一步</p>
<h3 id="使用ceph-objectstore-tool工具删除单个对象">使用ceph-objectstore-tool工具删除单个对象</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ]<span class="comment">#ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --journal-path /var/lib/ceph/osd/ceph-1/journal --pgid 0.3a  rbd_data.857e6b8b4567.00000000000000ba remove</span></span><br></pre></td></tr></table></figure>
<p>如果有多个副本的情况下，最好都删除掉，影响的数据就是包含这个对象的数据，这个操作的前提是这个对象数据已经被破坏了，如果是部分破坏，可以用集群的repair进行修复，这个是无法修复的情况下的删除对象，来实现启动OSD而不影响其它的数据的</p>
<h3 id="启动OSD">启动OSD</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ]<span class="comment"># systemctl start ceph-osd@1</span></span><br></pre></td></tr></table></figure>
<h3 id="解除noout">解除noout</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph osd unset noout</span></span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>一般情况下比较少出现这个情况，如果有这样的删除损坏的对象的需求，就可以这么处理</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/delete.png" alt="delete"><br></center></p>
<h2 id="前言">前言</h2><p>ceph里面的数据是以对象的形式存储在OSD当中的，有的时候因为磁盘的损坏或者其它的一些特殊情况，会引起集群当中的某一个对象的异常，那么我们需要对这个对象进行处理</p>
<p>在对象损坏的情况下，启动OSD有的时候都会有问题，那么通过rados rm的方式是没法发送到这个无法启动的OSD的，也就无法删除，所以需要用其他的办法来处理这个情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[查看ceph集群被哪些客户端连接]]></title>
    <link href="http://www.zphj1987.com/2017/04/13/ceph-cluster-connect-by-which-client/"/>
    <id>http://www.zphj1987.com/2017/04/13/ceph-cluster-connect-by-which-client/</id>
    <published>2017-04-13T06:18:34.000Z</published>
    <updated>2017-04-13T06:28:39.211Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/earth.png" alt="connect"><br></center>

<h2 id="前言">前言</h2><p>我们在使用集群的时候，一般来说比较关注的是后台的集群的状态，但是在做一些更人性化的管理功能的时候，就需要考虑到更多的细节</p>
<p>本篇就是其中的一个点，查询ceph被哪些客户端连接了<br><a id="more"></a></p>
<h2 id="实践">实践</h2><p>从接口上来说，ceph提供了文件，块，和对象的接口，所以不同的接口需要不同的查询方式，因为我接触文件和块比较多，并且文件和块存储属于长连接类型，对象属于请求类型，所以主要关注文件和块存储的连接信息查询</p>
<p>我的集群状态如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">3</span>daaf51a-eeba-<span class="number">43</span>a6-<span class="number">9</span>f58-c26c5796f928</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">6</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">      fsmap e20: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab8106=up:active&#125;</span><br><span class="line">     osdmap e52: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v27223: <span class="number">96</span> pgs, <span class="number">3</span> pools, <span class="number">2579</span> MB data, <span class="number">4621</span> objects</span><br><span class="line">            <span class="number">2666</span> MB used, <span class="number">545</span> GB / <span class="number">548</span> GB avail</span><br><span class="line">                  <span class="number">96</span> active+clean</span><br></pre></td></tr></table></figure></p>
<h3 id="文件接口的连接信息查询">文件接口的连接信息查询</h3><p>文件接口的连接信息是保存在MDS的，所以需要通过跟MDS进行交互查询,我的0h环境的MDS在lab8106，登陆到lab8106这台机器执行下面命令</p>
<figure class="highlight mel"><table><tr><td class="code"><pre><span class="line">[root<span class="variable">@lab8106</span> ~]# ceph daemon mds.lab8106 session <span class="keyword">ls</span>|grep <span class="string">'inst\|hostname\|kernel_version'</span></span><br><span class="line">        <span class="string">"inst"</span>: <span class="string">"client.34157 192.168.8.106:0\/3325402310"</span>,</span><br><span class="line">            <span class="string">"hostname"</span>: <span class="string">"lab8106"</span>,</span><br><span class="line">            <span class="string">"kernel_version"</span>: <span class="string">"4.9.5-1.el7.elrepo.x86_64"</span>,</span><br><span class="line">        <span class="string">"inst"</span>: <span class="string">"client.14118 192.168.8.107:0\/2202227749"</span>,</span><br><span class="line">            <span class="string">"hostname"</span>: <span class="string">"lab8107"</span>,</span><br><span class="line">            <span class="string">"kernel_version"</span>: <span class="string">"4.1.12-37.5.1.el7uek.x86_64"</span></span><br></pre></td></tr></table></figure>
<p>输出结果我做了过滤，主要信息是机器的IP，主机名，和内核版本</p>
<h3 id="块接口的连接信息查询">块接口的连接信息查询</h3><p>块接口也就是rbd的接口的</p>
<p>首先在一台机器上map<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map rbd/zp1</span></span><br></pre></td></tr></table></figure></p>
<p>执行查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd status zp1</span></span><br><span class="line">Watchers:</span><br><span class="line">	watcher=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">0</span>/<span class="number">1837592013</span> client.<span class="number">34246</span> cookie=<span class="number">1844646259873284096</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到是被192.168.8.106使用了，也就是watcher</p>
<h2 id="总结">总结</h2><p>命令都比较简单，如果做成一个监控平台，这种连接信息还是有个地方进行查询比较好</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/earth.png" alt="connect"><br></center>

<h2 id="前言">前言</h2><p>我们在使用集群的时候，一般来说比较关注的是后台的集群的状态，但是在做一些更人性化的管理功能的时候，就需要考虑到更多的细节</p>
<p>本篇就是其中的一个点，查询ceph被哪些客户端连接了<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph如何实现文件系统的横向扩展]]></title>
    <link href="http://www.zphj1987.com/2017/03/29/Ceph-filesystem-scaleout/"/>
    <id>http://www.zphj1987.com/2017/03/29/Ceph-filesystem-scaleout/</id>
    <published>2017-03-29T15:56:13.000Z</published>
    <updated>2017-03-29T16:02:01.812Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/uzqsgxsxm5wo4q1u142sv9ou/box.jpg" alt="box"><br></center>

<h2 id="前言">前言</h2><p>在跟一个朋友聊天的时候，聊到一个技术问题，他们的一个环境上面小文件巨多，是我目前知道的集群里面规模算非常大的了，但是目前有个问题，一方面会进行一倍的硬件的扩容，而文件的数量也在剧烈的增长着，所以有没有什么办法来 缓解这个增长的压力<br><a id="more"></a><br>当时也没想到太多的办法,只是觉得这么用下去风险太大</p>
<p>后来在思考了一段时间后，大概有了一个想法，这个就要看是否能把方案做下去，如果是我自己在用的集群，而非客户，我会这么去用的</p>
<h2 id="方案介绍">方案介绍</h2><h3 id="方案一">方案一</h3><p>也就是默认的方案，一般来说就是一个主MDS，然后几个备用MDS，整个一个挂载点，全局混用的空间</p>
<p>存在问题：</p>
<ul>
<li>扩容以后，有大量的数据迁移</li>
<li>所有的元数据请求，只有一个MDS服务，到了巨型数据的时候，可能出现卡顿或MDS卡掉的问题</li>
</ul>
<p>优点：</p>
<ul>
<li>全局统一命名空间</li>
</ul>
<h3 id="方案二：">方案二：</h3><p>采用分存储池的结构，也就是将集群内的目录树分配到整个集群的多个相互独立的空间里面</p>
<p>存在问题：</p>
<ul>
<li>同样是所有的元数据请求，只有一个MDS服务，到了巨型数据的时候，可能出现卡顿或MDS卡掉的问题</li>
</ul>
<p>优点：</p>
<ul>
<li>全局统一命名空间下面对应目录到不同的存储池当中，在进行扩容的时候，不会影响原有的数据，基本是没有迁移数据</li>
</ul>
<h3 id="方案三：">方案三：</h3><p>物理分存储池的结构并没有解决元数据压力过大的问题，而元数据的处理能力并非横向扩展的，而文件数量和集群规模都是在横向增长，所以必然是一个瓶颈点</p>
<p>这个方案其实很简单，相当于方案二的扩展，我们在方案二中进行了物理存储池的分离，然后把空间映射到子目录，来实现数据的分离，既然规模能够大到分物理空间，那么我们可以考虑部署多套集群，并且来真正的实现了数据处理能力的横向扩展，因为MDS，可以是多个的了，那么比较重要的问题就是统一命名空间的问题了，怎么实现，这个也简单，主要是跟客户沟通好，让客户接受提出的方案</p>
<p>我们在一些商业系统上面可以看到一些限制，比如单卷的大小最大支持多大，在这里我们需要跟客户沟通好，无限的扩展，会带来一些压力的风险，有方案能够解决这种问题，而这种数据量在之前是没有太多的案例可借鉴的，所以需要人为控制一个目录的最大空间，也就是单套集群的大小，下面举例来说明下</p>
<p>假设我们的空间一期规模为2P，二期规模要4P，三期规模6P<br>那么我们的命名空间上就分离出三个逻辑空间，也就是对应三套集群</p>
<p>弄清楚客户的存储的目录结构，一般来说客户并不太关心目录的设计，如果能够引导的情况下，可以引导客户，我们需要弄清楚目录可变化的那个点在哪里，举例说明，假如客户的数据可以去按年进行分类的话，数据就可以是<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2014</span><br><span class="line">2015 </span><br><span class="line">2016</span><br><span class="line">2017</span><br></pre></td></tr></table></figure></p>
<p>这样的增长趋势，并且数据量之前的肯定已知，未来可大概估计，并且集群准备存储多少年的数据，也是可大概预估的，那么这个环境我们就先认为到2017的数据我们放在集群一内，2017年以后的数据放在集群二内，那么挂载点是这样的<br><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="number">192.168</span>.<span class="number">10.101</span><span class="symbol">:/</span><span class="number">2014</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.101</span><span class="symbol">:/</span><span class="number">2015</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.101</span><span class="symbol">:/</span><span class="number">2016</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.101</span><span class="symbol">:/</span><span class="number">2017</span></span><br><span class="line"></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.102</span><span class="symbol">:/</span><span class="number">2018</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.102</span><span class="symbol">:/</span><span class="number">2019</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.102</span><span class="symbol">:/</span><span class="number">2020</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">10.102</span><span class="symbol">:/</span><span class="number">2021</span></span><br></pre></td></tr></table></figure></p>
<p>挂载到本地的服务的机器上<br>本地创建好目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/share/<span class="number">2014</span></span><br><span class="line">/share/<span class="number">2015</span></span><br><span class="line">/share/<span class="number">2016</span></span><br><span class="line">/share/<span class="number">2017</span></span><br><span class="line">/share/<span class="number">2018</span></span><br><span class="line">/share/<span class="number">2019</span></span><br><span class="line">/share/<span class="number">2020</span></span><br><span class="line">/share/<span class="number">2021</span></span><br></pre></td></tr></table></figure></p>
<p>然后把上面的集群挂载点按名称挂载到本地的这些目录上面</p>
<p>本地的共享就把/share共享出去，那么用户看到的就是一个全局命名空间了，这个是用本地子目录映射的方式来实现统一命名空间，技术难度小，难点在于跟客户沟通好数据的层级结构，如果客户能够自己随意增加目录，那么更好实现了，随意的将目录分配到两个集群即可，最终能达到满意的效果就行</p>
<p>当然主要还是需要客户能够接受你的方案，海量小文件的情况下，分开到多个集群当然会更好些，并且集群万一崩溃，也是只会影响局部的集群了</p>
<h2 id="总结">总结</h2><p>我们在利用一些新的技术的时候我们很多时候关注的是他最好的那个点，而这个点有的时候反而阻碍了我们的想法，比如集群，那就是把所有硬盘管理起来，搞成一个集群，那么为什么不能往上再走一层，我用管理的方式把多套集群在管理的层面组合成一个集群池呢？然后从多个集群里面来分配我们需要的资源即可</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-29</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/uzqsgxsxm5wo4q1u142sv9ou/box.jpg" alt="box"><br></center>

<h2 id="前言">前言</h2><p>在跟一个朋友聊天的时候，聊到一个技术问题，他们的一个环境上面小文件巨多，是我目前知道的集群里面规模算非常大的了，但是目前有个问题，一方面会进行一倍的硬件的扩容，而文件的数量也在剧烈的增长着，所以有没有什么办法来 缓解这个增长的压力<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[从hammer到jewel的RGW升级实战-by秦牧羊]]></title>
    <link href="http://www.zphj1987.com/2017/03/24/from-hammerto-jewel-update-by-qinmuyang/"/>
    <id>http://www.zphj1987.com/2017/03/24/from-hammerto-jewel-update-by-qinmuyang/</id>
    <published>2017-03-24T08:11:03.000Z</published>
    <updated>2017-03-24T08:32:29.909Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/update.gif" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇来自秦牧羊的一篇分享，讲述的是从hammer升级到jewel的过程，以及其中的一些故障的处理，是一篇非常详细的实战分享</p>
<h2 id="初始状态">初始状态</h2><h3 id="pool状态">pool状态</h3><figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser# rados lspools</span><br><span class="line">rbd</span><br><span class="line"><span class="title">.cn.rgw.root</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.root</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.domain</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.control</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.gc</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets.index</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets.extra</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets</span></span><br><span class="line"><span class="title">.cn-zone1.log</span></span><br><span class="line"><span class="title">.cn-zone1.intent-log</span></span><br><span class="line"><span class="title">.cn-zone1.usage</span></span><br><span class="line"><span class="title">.cn-zone1.users</span></span><br><span class="line"><span class="title">.cn-zone1.users.email</span></span><br><span class="line"><span class="title">.cn-zone1.users.swift</span></span><br><span class="line"><span class="title">.cn-zone1.users.uid</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="ceph-conf配置">ceph.conf配置</h3><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">[client<span class="class">.radosgw</span><span class="class">.us-zone1</span>]</span><br><span class="line">     rgw dns name = s3<span class="class">.ceph</span><span class="class">.work</span></span><br><span class="line">     rgw frontends = fastcgi</span><br><span class="line">     host = ceph<span class="class">.work</span></span><br><span class="line">     rgw region = cn</span><br><span class="line">     rgw region root pool = <span class="class">.cn</span><span class="class">.rgw</span><span class="class">.root</span></span><br><span class="line">     rgw zone = us-zone1</span><br><span class="line">     rgw zone root pool = <span class="class">.cn-zone1</span><span class="class">.rgw</span><span class="class">.root</span></span><br><span class="line">     keyring = /etc/ceph/ceph<span class="class">.client</span><span class="class">.radosgw</span><span class="class">.keyring</span></span><br><span class="line">     rgw socket path = /home/ceph/var/run/ceph-client<span class="class">.radosgw</span><span class="class">.us-zone1</span><span class="class">.sock</span></span><br><span class="line">     log file = /home/ceph/log/radosgw<span class="class">.us-zone1</span><span class="class">.log</span></span><br><span class="line">     rgw print continue = false</span><br><span class="line">    rgw <span class="attribute">content</span> length compat = true</span><br></pre></td></tr></table></figure>
<h3 id="元数据信息检查">元数据信息检查</h3><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin metadata list user --name client.radosgw.us-zone1</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">"en-user1"</span>,</span><br><span class="line">    <span class="string">"us-zone1"</span>,</span><br><span class="line">    <span class="string">"us-user1"</span>,</span><br><span class="line">    <span class="string">"cn-user1"</span>,</span><br><span class="line">    <span class="string">"en-zone1"</span>,</span><br><span class="line">    <span class="string">"cn-zone1"</span>,</span><br><span class="line">    <span class="string">"cn-user2"</span></span><br><span class="line"></span><br><span class="line">]</span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin metadata list bucket --name client.radosgw.us-zone1</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">"cn-test1"</span>,</span><br><span class="line">    <span class="string">"us-test1"</span>,</span><br><span class="line">    <span class="string">"en-test1"</span>,</span><br><span class="line">    <span class="string">"cn-test2"</span></span><br><span class="line"></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3 id="软件版本及集群状态">软件版本及集群状态</h3><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">0</span>.<span class="number">94.5</span> (<span class="number">9764</span>da52395923e0b32908d83a9f7304401fee43)</span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">23</span>d6f3f9-0b86-<span class="number">432</span>c-bb18-<span class="number">1722</span>f73e93e<span class="number">0</span></span><br><span class="line">     health <span class="constant">HEALTH_OK</span></span><br><span class="line">     monmap <span class="symbol">e1:</span> <span class="number">1</span> mons at &#123;ceph.work=<span class="number">10.63</span>.<span class="number">48.19</span><span class="symbol">:</span><span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">1</span>, quorum <span class="number">0</span> ceph.work</span><br><span class="line">     osdmap <span class="symbol">e43:</span> <span class="number">3</span> <span class="symbol">osds:</span> <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">      pgmap <span class="symbol">v907719:</span> <span class="number">544</span> pgs, <span class="number">16</span> pools, <span class="number">2217</span> kB data, <span class="number">242</span> objects</span><br><span class="line">            <span class="number">3119</span> <span class="constant">MB </span>used, <span class="number">88994</span> <span class="constant">MB </span>/ <span class="number">92114</span> <span class="constant">MB </span>avail</span><br><span class="line">                 <span class="number">544</span> active+clean</span><br></pre></td></tr></table></figure>
<h2 id="ceph升级到最新jewel">ceph升级到最新jewel</h2><p>这里要提醒一点就是如果ceph版本低于0.94.7，直接升级到10.xx会出一些问题，因为低版本的osdmap的数据结构与高版本不兼容，所以先升级到最新的hammer</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser<span class="comment"># vi /etc/apt/sources.list.d/ceph.list </span></span><br><span class="line">deb http://mirrors.163.com/ceph/debian-hammer/ jessie main <span class="comment">#使用163源更新到最新的hammer</span></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># apt-get update</span></span><br><span class="line"><span class="keyword">...</span></span><br><span class="line">Fetched <span class="number">18.7</span> kB <span class="keyword">in</span> 11s (<span class="number">1</span>,<span class="number">587</span> B/s)</span><br><span class="line">Reading package lists... Done</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># apt-cache policy ceph</span></span><br><span class="line">ceph:</span><br><span class="line">  Installed: <span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="comment">#当前已经安装的版本</span></span><br><span class="line">  Candidate: <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="comment">#预备安装的版本</span></span><br><span class="line">  Version table:</span><br><span class="line">     <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="number">0</span></span><br><span class="line">        <span class="number">500</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main amd64 Packages</span><br><span class="line"> *** <span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="number">0</span></span><br><span class="line">        <span class="number">100</span> /var/lib/dpkg/status</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># aptitude install ceph ceph-common ceph-deploy ceph-fs-common ceph-fuse ceph-mds libcephfs1 python-ceph python-cephfs  librados2 libradosstriper1 python-rados  radosgw radosgw-agent librbd1 python-rbd rbd-fuse radosgw radosgw-agent</span></span><br><span class="line">The following packages will be REMOVED:</span><br><span class="line">  daemon&#123;u&#125; mpt-status&#123;u&#125;</span><br><span class="line">The following packages will be upgraded:</span><br><span class="line">  ceph ceph-common ceph-deploy ceph-fs-common ceph-fuse ceph-mds ceph-test libcephfs1 librados2 libradosstriper1 librbd1 python-ceph python-cephfs python-rados python-rbd radosgw radosgw-agent rbd-fuse</span><br><span class="line">The following packages are RECOMMENDED but will NOT be installed:</span><br><span class="line">  btrfs-tools fuse</span><br><span class="line"><span class="number">18</span> packages upgraded, <span class="number">0</span> newly installed, <span class="number">2</span> to remove and <span class="number">185</span> not upgraded.</span><br><span class="line">Need to get <span class="number">75.3</span> MB of archives. After unpacking <span class="number">3</span>,<span class="number">588</span> kB will be used.</span><br><span class="line">Do you want to continue? [Y/n/?] y</span><br><span class="line">Get: <span class="number">1</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main libcephfs1 amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">706</span> kB]</span><br><span class="line">Get: <span class="number">2</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-mds amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">8</span>,<span class="number">053</span> kB]</span><br><span class="line">Get: <span class="number">3</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">11.6</span> MB]</span><br><span class="line">Get: <span class="number">4</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-test amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">28.2</span> MB]</span><br><span class="line">Get: <span class="number">5</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main radosgw amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">576</span> kB]</span><br><span class="line">Get: <span class="number">6</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-common amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">6</span>,<span class="number">526</span> kB]</span><br><span class="line">Get: <span class="number">7</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main librbd1 amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">593</span> kB]</span><br><span class="line">Get: <span class="number">8</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main libradosstriper1 amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">554</span> kB]</span><br><span class="line">Get: <span class="number">9</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main librados2 amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">479</span> kB]</span><br><span class="line">Get: <span class="number">10</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main python-rados amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">895</span> kB]</span><br><span class="line">Get: <span class="number">11</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main python-cephfs amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">886</span> kB]</span><br><span class="line">Get: <span class="number">12</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main python-rbd amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">891</span> kB]</span><br><span class="line">Get: <span class="number">13</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-deploy all <span class="number">1.5</span><span class="number">.37</span> [<span class="number">95.9</span> kB]</span><br><span class="line">Get: <span class="number">14</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-fs-common amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">903</span> kB]</span><br><span class="line">Get: <span class="number">15</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main ceph-fuse amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">2</span>,<span class="number">515</span> kB]</span><br><span class="line">Get: <span class="number">16</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main python-ceph amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">883</span> kB]</span><br><span class="line">Get: <span class="number">17</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main radosgw-agent all <span class="number">1.2</span><span class="number">.7</span> [<span class="number">30.1</span> kB]</span><br><span class="line">Get: <span class="number">18</span> http://mirrors.163.com/ceph/debian-hammer/ jessie/main rbd-fuse amd64 <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> [<span class="number">891</span> kB]</span><br><span class="line">Fetched <span class="number">75.3</span> MB <span class="keyword">in</span> 10s (<span class="number">7</span>,<span class="number">301</span> kB/s)</span><br><span class="line">Reading changelogs... Done</span><br><span class="line">(Reading database <span class="keyword">...</span> <span class="number">74503</span> files and directories currently installed.)</span><br><span class="line">Removing mpt-status (<span class="number">1.2</span><span class="number">.0</span>-<span class="number">8</span>) <span class="keyword">...</span></span><br><span class="line">[ ok ] mpt-statusd is disabled <span class="keyword">in</span> /etc/default/mpt-statusd, not starting..</span><br><span class="line">Removing daemon (<span class="number">0.6</span><span class="number">.4</span>-<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> man-db (<span class="number">2.7</span><span class="number">.0</span><span class="number">.2</span>-<span class="number">5</span>) <span class="keyword">...</span></span><br><span class="line">(Reading database <span class="keyword">...</span> <span class="number">74485</span> files and directories currently installed.)</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libcephfs1_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libcephfs1 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-mds_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-mds (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-test_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-test (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/radosgw_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking radosgw (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-common_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-common (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/librbd1_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking librbd1 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libradosstriper1_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libradosstriper1 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/librados2_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking librados2 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-rados_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-rados (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-cephfs_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-cephfs (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-rbd_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-rbd (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-deploy_1.5.37_all.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-deploy (<span class="number">1.5</span><span class="number">.37</span>) over (<span class="number">1.5</span><span class="number">.28</span>~bpo70+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-fs-common_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-fs-common (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-fuse_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-fuse (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-ceph_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-ceph (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/radosgw-agent_1.2.7_all.deb <span class="keyword">...</span></span><br><span class="line">Unpacking radosgw-agent (<span class="number">1.2</span><span class="number">.7</span>) over (<span class="number">1.2</span><span class="number">.4</span>~bpo70+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/rbd-fuse_0.94.10-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking rbd-fuse (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.5</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> man-db (<span class="number">2.7</span><span class="number">.0</span><span class="number">.2</span>-<span class="number">5</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> systemd (<span class="number">215</span>-<span class="number">17</span>+deb8u2) <span class="keyword">...</span></span><br><span class="line">Setting up libcephfs1 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up librados2 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up librbd1 (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-rados (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-cephfs (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-rbd (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up ceph-common (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Installing new version of config file /etc/init.d/rbdmap <span class="keyword">...</span></span><br><span class="line">Setting up ceph (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Installing new version of config file /etc/init.d/ceph <span class="keyword">...</span></span><br><span class="line">Installing new version of config file /etc/init/ceph-osd.conf <span class="keyword">...</span></span><br><span class="line"></span><br><span class="line">Configuration file <span class="string">'/etc/logrotate.d/ceph'</span> </span><br><span class="line"> ==&gt; Modified (by you or by a script) since installation.</span><br><span class="line"> ==&gt; Package distributor has shipped an updated version.</span><br><span class="line">   What would you like to do about it ?  Your options are:</span><br><span class="line">    Y or I  : install the package maintainer<span class="string">'s version</span><br><span class="line">    N or O  : keep your currently-installed version</span><br><span class="line">      D     : show the differences between the versions</span><br><span class="line">      Z     : start a shell to examine the situation</span><br><span class="line"> The default action is to keep your current version.</span><br><span class="line">*** ceph (Y/I/N/O/D/Z) [default=N] ? N</span><br><span class="line">Setting up ceph-mds (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up libradosstriper1 (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up ceph-test (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up radosgw (0.94.10-1~bpo80+1) ...</span><br><span class="line">Installing new version of config file /etc/init.d/radosgw ...</span><br><span class="line">Installing new version of config file /etc/logrotate.d/radosgw ...</span><br><span class="line">Setting up ceph-deploy (1.5.37) ...</span><br><span class="line">Setting up ceph-fs-common (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up ceph-fuse (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up python-ceph (0.94.10-1~bpo80+1) ...</span><br><span class="line">Setting up radosgw-agent (1.2.7) ...</span><br><span class="line">Installing new version of config file /etc/init.d/radosgw-agent ...</span><br><span class="line">Setting up rbd-fuse (0.94.10-1~bpo80+1) ...</span><br><span class="line">Processing triggers for libc-bin (2.19-18+deb8u1) ...</span><br><span class="line">Processing triggers for systemd (215-17+deb8u2) ...</span><br><span class="line"></span><br><span class="line">Current status: 185 updates [-18].</span></span><br></pre></td></tr></table></figure>
<h3 id="正式升级到最新的hammer">正式升级到最新的hammer</h3><figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser# ceph -v </span><br><span class="line">ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af) #当前软件包版本已经更新</span><br><span class="line">root@demo:/home/demouser# ceph -s </span><br><span class="line"><span class="code">    cluster 23d6f3f9-0b86-432c-bb18-1722f73e93e0</span></span><br><span class="line"><span class="code">     health HEALTH_OK</span></span><br><span class="line"><span class="code">     monmap e1: 1 mons at &#123;ceph.work=10.63.48.19:6789/0&#125;</span></span><br><span class="line"><span class="code">            election epoch 1, quorum 0 ceph.work</span></span><br><span class="line"><span class="code">     osdmap e43: 3 osds: 3 up, 3 in</span></span><br><span class="line"><span class="code">      pgmap v907873: 544 pgs, 16 pools, 2217 kB data, 242 objects</span></span><br><span class="line"><span class="code">            3120 MB used, 88994 MB / 92114 MB avail</span></span><br><span class="line"><span class="code">                 544 active+clean</span></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser# /etc/init.d/ceph  status</span><br><span class="line"><span class="header">=== mon.ceph.work ===</span></span><br><span class="line">mon.ceph.work: running &#123;"version":"0.94.5"&#125; #mon和osd进程还是跑的旧版本</span><br><span class="line"><span class="header">=== osd.0 ===</span></span><br><span class="line">osd.0: running &#123;"version":"0.94.5"&#125;</span><br><span class="line"><span class="header">=== osd.1 ===</span></span><br><span class="line">osd.1: running &#123;"version":"0.94.5"&#125;</span><br><span class="line"><span class="header">=== osd.2 ===</span></span><br><span class="line">osd.2: running &#123;"version":"0.94.5"&#125;</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser# /etc/init.d/ceph  restart #手工重启所有服务，线上环境依次先重启mon再是osd，避免批量重启造成影响</span><br><span class="line"><span class="header">=== mon.ceph.work ===</span></span><br><span class="line"><span class="header">=== mon.ceph.work ===</span></span><br><span class="line">Stopping Ceph mon.ceph.work on ceph.work...kill 2267...done</span><br><span class="line"><span class="header">=== mon.ceph.work ===</span></span><br><span class="line">Starting Ceph mon.ceph.work on ceph.work...</span><br><span class="line"><span class="header">=== osd.0 ===</span></span><br><span class="line"><span class="header">=== osd.0 ===</span></span><br><span class="line">Stopping Ceph osd.0 on ceph.work...kill 1082...kill 1082...done</span><br><span class="line"><span class="header">=== osd.0 ===</span></span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-0</span><br><span class="line">create-or-move updated item name <span class="emphasis">'osd.0'</span> weight 0.03 at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.0 on ceph.work...</span><br><span class="line">starting osd.0 at :/0 osd<span class="emphasis">_data /home/ceph/var/lib/osd/ceph-0 /home/ceph/var/lib/osd/ceph-0/journal</span><br><span class="line">=== osd.1 ===</span><br><span class="line">=== osd.1 ===</span><br><span class="line">Stopping Ceph osd.1 on ceph.work...kill 1262...kill 1262...done</span><br><span class="line">=== osd.1 ===</span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-1</span><br><span class="line">create-or-move updated item name 'osd.1' weight 0.03 at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.1 on ceph.work...</span><br><span class="line">starting osd.1 at :/0 osd_</span>data /home/ceph/var/lib/osd/ceph-1 /home/ceph/var/lib/osd/ceph-1/journal</span><br><span class="line"><span class="header">=== osd.2 ===</span></span><br><span class="line"><span class="header">=== osd.2 ===</span></span><br><span class="line">Stopping Ceph osd.2 on ceph.work...kill 1452...kill 1452...done</span><br><span class="line"><span class="header">=== osd.2 ===</span></span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-2</span><br><span class="line">create-or-move updated item name <span class="emphasis">'osd.2'</span> weight 0.03 at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.2 on ceph.work...</span><br><span class="line">starting osd.2 at :/0 osd<span class="emphasis">_data /home/ceph/var/lib/osd/ceph-2 /home/ceph/var/lib/osd/ceph-2/journal</span></span><br></pre></td></tr></table></figure>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser# /etc/init.d/ceph  status</span><br><span class="line"><span class="header">=== mon.ceph.work ===</span></span><br><span class="line">mon.ceph.work: running &#123;"version":"0.94.10"&#125; #mon和osd都全部更新到最新</span><br><span class="line"><span class="header">=== osd.0 ===</span></span><br><span class="line">osd.0: running &#123;"version":"0.94.10"&#125;</span><br><span class="line"><span class="header">=== osd.1 ===</span></span><br><span class="line">osd.1: running &#123;"version":"0.94.10"&#125;</span><br><span class="line"><span class="header">=== osd.2 ===</span></span><br><span class="line">osd.2: running &#123;"version":"0.94.10"&#125;</span><br><span class="line">root@demo:/home/demouser# ceph -s</span><br><span class="line"><span class="code">    cluster 23d6f3f9-0b86-432c-bb18-1722f73e93e0</span></span><br><span class="line"><span class="code">     health HEALTH_OK </span></span><br><span class="line"><span class="code">     monmap e1: 1 mons at &#123;ceph.work=10.63.48.19:6789/0&#125;</span></span><br><span class="line"><span class="code">            election epoch 1, quorum 0 ceph.work</span></span><br><span class="line"><span class="code">     osdmap e51: 3 osds: 3 up, 3 in</span></span><br><span class="line"><span class="code">      pgmap v907887: 544 pgs, 16 pools, 2217 kB data, 242 objects</span></span><br><span class="line"><span class="code">            3121 MB used, 88992 MB / 92114 MB avail</span></span><br><span class="line"><span class="code">                 544 active+clean</span></span><br></pre></td></tr></table></figure>
<h3 id="升级到最新jewel版本">升级到最新jewel版本</h3><figure class="highlight r"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser<span class="comment"># vi /etc/apt/sources.list.d/ceph.list </span></span><br><span class="line">deb http://mirrors.163.com/ceph/debian-jewel/ jessie main <span class="comment">#使用163源更新到最新的jewel</span></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># apt-get update</span></span><br><span class="line"><span class="keyword">...</span></span><br><span class="line">Fetched <span class="number">18.7</span> kB <span class="keyword">in</span> 11s (<span class="number">1</span>,<span class="number">587</span> B/s)</span><br><span class="line">Reading package lists... Done</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># apt-cache policy ceph</span></span><br><span class="line">ceph:</span><br><span class="line">  Installed: <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="comment">#当前安装的版本</span></span><br><span class="line">  Candidate: <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="comment">#准备安装的最新jewel版本</span></span><br><span class="line">  Version table:</span><br><span class="line">     <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="number">0</span></span><br><span class="line">        <span class="number">500</span> http://mirrors.163.com/ceph/debian-jewel/ jessie/main amd64 Packages</span><br><span class="line"> *** <span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span> <span class="number">0</span></span><br><span class="line">        <span class="number">100</span> /var/lib/dpkg/status</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># aptitude install ceph ceph-common ceph-deploy ceph-fs-common ceph-fuse ceph-mds libcephfs1 python-ceph python-cephfs  librados2 libradosstriper1 python-rados  radosgw radosgw-agent librbd1 python-rbd rbd-fuse radosgw radosgw-agent</span></span><br><span class="line">The following NEW packages will be installed:</span><br><span class="line">  ceph-base&#123;a&#125; ceph-mon&#123;a&#125; ceph-osd&#123;a&#125; libboost-random1.55.0&#123;a&#125; libboost-regex1.55.0&#123;a&#125; librgw2&#123;a&#125; xmlstarlet&#123;a&#125;</span><br><span class="line">The following packages will be upgraded:</span><br><span class="line">  ceph ceph-common ceph-fs-common ceph-fuse ceph-mds ceph-test libcephfs1 librados2 libradosstriper1 librbd1 python-ceph python-cephfs python-rados python-rbd radosgw rbd-fuse</span><br><span class="line">The following packages are RECOMMENDED but will NOT be installed:</span><br><span class="line">  btrfs-tools fuse</span><br><span class="line"><span class="number">16</span> packages upgraded, <span class="number">7</span> newly installed, <span class="number">0</span> to remove and <span class="number">184</span> not upgraded.</span><br><span class="line">Need to get <span class="number">169</span> MB of archives. After unpacking <span class="number">464</span> MB will be used.</span><br><span class="line">Do you want to continue? [Y/n/?] y</span><br><span class="line">....</span><br><span class="line">Fetched <span class="number">169</span> MB <span class="keyword">in</span> 15s (<span class="number">11.0</span> MB/s)</span><br><span class="line">Reading changelogs... Done</span><br><span class="line">Selecting previously unselected package libboost-random1.55.0:amd64.</span><br><span class="line">(Reading database <span class="keyword">...</span> <span class="number">74299</span> files and directories currently installed.)</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libboost-random1.55.0_1.55.0+dfsg-3_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libboost-random1.55.0:amd64 (<span class="number">1.55</span><span class="number">.0</span>+dfsg-<span class="number">3</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package libboost-regex1.55.0:amd64.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libboost-regex1.55.0_1.55.0+dfsg-3_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libboost-regex1.55.0:amd64 (<span class="number">1.55</span><span class="number">.0</span>+dfsg-<span class="number">3</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package xmlstarlet.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/xmlstarlet_1.6.1-1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking xmlstarlet (<span class="number">1.6</span><span class="number">.1</span>-<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libcephfs1_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libcephfs1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-mds_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-mds (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-test_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-test (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/radosgw_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking radosgw (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-common_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-common (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/librbd1_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking librbd1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/libradosstriper1_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking libradosstriper1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/librados2_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking librados2 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package librgw2.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/librgw2_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking librgw2 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-rados_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-rados (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-cephfs_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-cephfs (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-rbd_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-rbd (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package ceph-base.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-base_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-base (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package ceph-mon.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-mon_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-mon (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Selecting previously unselected package ceph-osd.</span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-osd_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-osd (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-fs-common_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-fs-common (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/ceph-fuse_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking ceph-fuse (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/python-ceph_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking python-ceph (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Preparing to unpack <span class="keyword">...</span>/rbd-fuse_10.2.6-<span class="number">1</span>~bpo80+1_amd64.deb <span class="keyword">...</span></span><br><span class="line">Unpacking rbd-fuse (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) over (<span class="number">0.94</span><span class="number">.10</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> man-db (<span class="number">2.7</span><span class="number">.0</span><span class="number">.2</span>-<span class="number">5</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> systemd (<span class="number">215</span>-<span class="number">17</span>+deb8u2) <span class="keyword">...</span></span><br><span class="line">Setting up libboost-random1.55.0:amd64 (<span class="number">1.55</span><span class="number">.0</span>+dfsg-<span class="number">3</span>) <span class="keyword">...</span></span><br><span class="line">Setting up libboost-regex1.55.0:amd64 (<span class="number">1.55</span><span class="number">.0</span>+dfsg-<span class="number">3</span>) <span class="keyword">...</span></span><br><span class="line">Setting up xmlstarlet (<span class="number">1.6</span><span class="number">.1</span>-<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up libcephfs1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up librados2 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up librbd1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up libradosstriper1 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up librgw2 (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-rados (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-cephfs (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-rbd (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up ceph-common (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Installing new version of config file /etc/bash_completion.d/rbd <span class="keyword">...</span></span><br><span class="line">Installing new version of config file /etc/init.d/rbdmap <span class="keyword">...</span></span><br><span class="line">Setting system user ceph properties..usermod: user ceph is currently used by process <span class="number">5312</span></span><br><span class="line">dpkg: error processing package ceph-common (--configure): <span class="comment">#需要重启进程才能更新配置，忽略这里及以下错误</span></span><br><span class="line"> subprocess installed post-installation script returned error exit status <span class="number">8</span></span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-base:</span><br><span class="line"> ceph-base depends on ceph-common (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-base (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-mds:</span><br><span class="line"> ceph-mds depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-mds (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-mon:</span><br><span class="line"> ceph-mon depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-mon (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-osd:</span><br><span class="line"> ceph-osd depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-osd (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph:</span><br><span class="line"> ceph depends on ceph-mon (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-mon is not configured yet.</span><br><span class="line"> ceph depends on ceph-osd (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-osd is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-test:</span><br><span class="line"> ceph-test depends on ceph-common; however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-test (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of radosgw:</span><br><span class="line"> radosgw depends on ceph-common (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package radosgw (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">Setting up ceph-fs-common (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up ceph-fuse (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up python-ceph (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting up rbd-fuse (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> libc-bin (<span class="number">2.19</span>-<span class="number">18</span>+deb8u1) <span class="keyword">...</span></span><br><span class="line">Processing triggers <span class="keyword">for</span> systemd (<span class="number">215</span>-<span class="number">17</span>+deb8u2) <span class="keyword">...</span></span><br><span class="line">Errors were encountered <span class="keyword">while</span> processing:</span><br><span class="line"> ceph-common</span><br><span class="line"> ceph-base</span><br><span class="line"> ceph-mds</span><br><span class="line"> ceph-mon</span><br><span class="line"> ceph-osd</span><br><span class="line"> ceph</span><br><span class="line"> ceph-test</span><br><span class="line"> radosgw</span><br><span class="line">E: Sub-process /usr/bin/dpkg returned an error code (<span class="number">1</span>)</span><br><span class="line">Failed to perform requested operation on package.  Trying to recover:</span><br><span class="line">Setting up ceph-common (<span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>) <span class="keyword">...</span></span><br><span class="line">Setting system user ceph properties..usermod: user ceph is currently used by process <span class="number">5312</span></span><br><span class="line">dpkg: error processing package ceph-common (--configure):</span><br><span class="line"> subprocess installed post-installation script returned error exit status <span class="number">8</span></span><br><span class="line">dpkg: dependency problems prevent configuration of radosgw:</span><br><span class="line"> radosgw depends on ceph-common (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package radosgw (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-test:</span><br><span class="line"> ceph-test depends on ceph-common; however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-test (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-base:</span><br><span class="line"> ceph-base depends on ceph-common (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-common is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-base (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-osd:</span><br><span class="line"> ceph-osd depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-osd (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-mds:</span><br><span class="line"> ceph-mds depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-mds (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph:</span><br><span class="line"> ceph depends on ceph-osd (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-osd is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">dpkg: dependency problems prevent configuration of ceph-mon:</span><br><span class="line"> ceph-mon depends on ceph-base (= <span class="number">10.2</span><span class="number">.6</span>-<span class="number">1</span>~bpo80+<span class="number">1</span>); however:</span><br><span class="line">  Package ceph-base is not configured yet.</span><br><span class="line"></span><br><span class="line">dpkg: error processing package ceph-mon (--configure):</span><br><span class="line"> dependency problems - leaving unconfigured</span><br><span class="line">Errors were encountered <span class="keyword">while</span> processing:</span><br><span class="line"> ceph-common</span><br><span class="line"> radosgw</span><br><span class="line"> ceph-test</span><br><span class="line"> ceph-base</span><br><span class="line"> ceph-osd</span><br><span class="line"> ceph-mds</span><br><span class="line"> ceph</span><br><span class="line"> ceph-mon</span><br><span class="line"></span><br><span class="line">Current status: <span class="number">184</span> updates [-<span class="number">16</span>].</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># /etc/init.d/ceph  status</span></span><br><span class="line">=== mon.ceph.work ===</span><br><span class="line">mon.ceph.work: running &#123;<span class="string">"version"</span>:<span class="string">"0.94.10"</span>&#125; <span class="comment">#当前mon和osd版本还是旧版本</span></span><br><span class="line">=== osd.0 ===</span><br><span class="line">osd.0: running &#123;<span class="string">"version"</span>:<span class="string">"0.94.10"</span>&#125;</span><br><span class="line">=== osd.1 ===</span><br><span class="line">osd.1: running &#123;<span class="string">"version"</span>:<span class="string">"0.94.10"</span>&#125;</span><br><span class="line">=== osd.2 ===</span><br><span class="line">osd.2: running &#123;<span class="string">"version"</span>:<span class="string">"0.94.10"</span>&#125;</span><br><span class="line">root@demo:/home/demouser<span class="comment"># ceph -s</span></span><br><span class="line">    cluster 23d6f3f9-0b86-432c-bb18-1722f73e93e0</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;ceph.work=<span class="number">10.63</span><span class="number">.48</span><span class="number">.19</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">1</span>, quorum <span class="number">0</span> ceph.work</span><br><span class="line">     osdmap e51: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">      pgmap v907893: <span class="number">544</span> pgs, <span class="number">16</span> pools, <span class="number">2217</span> kB data, <span class="number">242</span> objects</span><br><span class="line">            <span class="number">3120</span> MB used, <span class="number">88993</span> MB / <span class="number">92114</span> MB avail</span><br><span class="line">                 <span class="number">544</span> active+clean</span><br><span class="line">root@demo:/home/demouser<span class="comment"># /etc/init.d/ceph restart #手工重启所有服务，线上环境依次先重启mon再是osd，避免批量重启造成影响</span></span><br><span class="line">=== mon.ceph.work ===</span><br><span class="line">=== mon.ceph.work ===</span><br><span class="line">Stopping Ceph mon.ceph.work on ceph.work...kill <span class="number">5312.</span>..done</span><br><span class="line">=== mon.ceph.work ===</span><br><span class="line">Starting Ceph mon.ceph.work on ceph.work...</span><br><span class="line">=== osd.0 ===</span><br><span class="line">=== osd.0 ===</span><br><span class="line">Stopping Ceph osd.0 on ceph.work...kill <span class="number">5677.</span>..kill <span class="number">5677.</span>..done</span><br><span class="line">=== osd.0 ===</span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-<span class="number">0</span></span><br><span class="line">create-or-move updated item name <span class="string">'osd.0'</span> weight <span class="number">0.03</span> at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.0 on ceph.work...</span><br><span class="line">starting osd.0 at :/<span class="number">0</span> osd_data /home/ceph/var/lib/osd/ceph-<span class="number">0</span> /home/ceph/var/lib/osd/ceph-<span class="number">0</span>/journal</span><br><span class="line">=== osd.1 ===</span><br><span class="line">=== osd.1 ===</span><br><span class="line">Stopping Ceph osd.1 on ceph.work...kill <span class="number">6087.</span>..kill <span class="number">6087.</span>..done</span><br><span class="line">=== osd.1 ===</span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-<span class="number">1</span></span><br><span class="line">create-or-move updated item name <span class="string">'osd.1'</span> weight <span class="number">0.03</span> at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.1 on ceph.work...</span><br><span class="line">starting osd.1 at :/<span class="number">0</span> osd_data /home/ceph/var/lib/osd/ceph-<span class="number">1</span> /home/ceph/var/lib/osd/ceph-<span class="number">1</span>/journal</span><br><span class="line">=== osd.2 ===</span><br><span class="line">=== osd.2 ===</span><br><span class="line">Stopping Ceph osd.2 on ceph.work...kill <span class="number">6503.</span>..kill <span class="number">6503.</span>..done</span><br><span class="line">=== osd.2 ===</span><br><span class="line">Mounting xfs on ceph.work:/home/ceph/var/lib/osd/ceph-<span class="number">2</span></span><br><span class="line">create-or-move updated item name <span class="string">'osd.2'</span> weight <span class="number">0.03</span> at location &#123;host=ceph.work,root=default&#125; to crush map</span><br><span class="line">Starting Ceph osd.2 on ceph.work...</span><br><span class="line">starting osd.2 at :/<span class="number">0</span> osd_data /home/ceph/var/lib/osd/ceph-<span class="number">2</span> /home/ceph/var/lib/osd/ceph-<span class="number">2</span>/journal</span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># ceph -s #出现crushmap 兼容性告警</span></span><br><span class="line">    cluster 23d6f3f9-0b86-432c-bb18-1722f73e93e0</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            crush map has legacy tunables (<span class="keyword">require</span> bobtail, min is firefly) </span><br><span class="line">            all OSDs are running jewel or later but the <span class="string">'require_jewel_osds'</span> osdmap flag is not set</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;ceph.work=<span class="number">10.63</span><span class="number">.48</span><span class="number">.19</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">2</span>, quorum <span class="number">0</span> ceph.work</span><br><span class="line">     osdmap e61: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">      pgmap v907906: <span class="number">544</span> pgs, <span class="number">16</span> pools, <span class="number">2217</span> kB data, <span class="number">242</span> objects</span><br><span class="line">            <span class="number">3122</span> MB used, <span class="number">88991</span> MB / <span class="number">92114</span> MB avail</span><br><span class="line">                 <span class="number">544</span> active+clean</span><br><span class="line">                 </span><br><span class="line">                 </span><br><span class="line">                 </span><br><span class="line">root@demo:/home/demouser<span class="comment"># /etc/init.d/ceph status #检查所有服务进程版本是否到最新</span></span><br><span class="line">=== mon.ceph.work ===</span><br><span class="line">mon.ceph.work: running &#123;<span class="string">"version"</span>:<span class="string">"10.2.6"</span>&#125; </span><br><span class="line">=== osd.0 ===</span><br><span class="line">osd.0: running &#123;<span class="string">"version"</span>:<span class="string">"10.2.6"</span>&#125;</span><br><span class="line">=== osd.1 ===</span><br><span class="line">osd.1: running &#123;<span class="string">"version"</span>:<span class="string">"10.2.6"</span>&#125;</span><br><span class="line">=== osd.2 ===</span><br><span class="line">osd.2: running &#123;<span class="string">"version"</span>:<span class="string">"10.2.6"</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser<span class="comment"># ceph osd set require_jewel_osds</span></span><br><span class="line">set require_jewel_osds</span><br><span class="line">root@demo:/home/demouser<span class="comment"># ceph osd crush tunables optimal</span></span><br><span class="line">adjusted tunables profile to optimal</span><br><span class="line">root@demo:/home/demouser<span class="comment"># ceph -s #调整crushmap兼容性参数以后恢复正常</span></span><br><span class="line">    cluster 23d6f3f9-0b86-432c-bb18-1722f73e93e0</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;ceph.work=<span class="number">10.63</span><span class="number">.48</span><span class="number">.19</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">2</span>, quorum <span class="number">0</span> ceph.work</span><br><span class="line">     osdmap e63: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags require_jewel_osds</span><br><span class="line">      pgmap v907917: <span class="number">544</span> pgs, <span class="number">16</span> pools, <span class="number">2217</span> kB data, <span class="number">242</span> objects</span><br><span class="line">            <span class="number">3122</span> MB used, <span class="number">88991</span> MB / <span class="number">92114</span> MB avail</span><br><span class="line">                 <span class="number">544</span> active+clean</span><br></pre></td></tr></table></figure>
<h2 id="rgw服务的修复">rgw服务的修复</h2><h3 id="rgw启动报错">rgw启动报错</h3><figure class="highlight nix"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser<span class="comment"># /etc/init.d/radosgw start #重启失败，发现以下错误log</span></span><br><span class="line"></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.309461</span> <span class="number">7</span>f7f175998c0  <span class="number">0</span> ceph version <span class="number">10.2</span>.<span class="number">6</span> (<span class="number">656</span>b5b63ed7c43bd014bcafd81b001959d5f089f), process radosgw, pid <span class="number">11488</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.317937</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751e30 <span class="variable">obj=</span>.rgw.root:default.realm <span class="variable">state=</span><span class="number">0</span>x7f7f17e93368 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.317943</span> <span class="number">7</span>f7ef17fa700  <span class="number">2</span> RGWDataChangesLog::ChangesRenewThread: start</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.318759</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb7518f0 <span class="variable">obj=</span>.rgw.root:converted <span class="variable">state=</span><span class="number">0</span>x7f7f17e93368 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.319140</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751060 <span class="variable">obj=</span>.rgw.root:default.realm <span class="variable">state=</span><span class="number">0</span>x7f7f17e94398 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.319513</span> <span class="number">7</span>f7f175998c0 <span class="number">10</span> could not read realm id: (<span class="number">2</span>) No such file <span class="constant">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.319858</span> <span class="number">7</span>f7f175998c0 <span class="number">10</span> failed to list objects pool_iterate_begin() returned <span class="variable">r=</span>-<span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.319890</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751290 <span class="variable">obj=</span>.cn-zone1.rgw.root:zone_names.default <span class="variable">state=</span><span class="number">0</span>x7f7f17e94e38 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.321308</span> <span class="number">7</span>f7f175998c0  <span class="number">0</span> error <span class="keyword">in</span> read_id for object name: default : (<span class="number">2</span>) No such file <span class="constant">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.321335</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751290 <span class="variable">obj=</span>.rgw.root:zonegroups_names.default <span class="variable">state=</span><span class="number">0</span>x7f7f17e94e38 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.321725</span> <span class="number">7</span>f7f175998c0  <span class="number">0</span> error <span class="keyword">in</span> read_id for object name: default : (<span class="number">2</span>) No such file <span class="constant">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.321756</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751f60 <span class="variable">obj=</span>.cn.rgw.root:region_map <span class="variable">state=</span><span class="number">0</span>x7f7f17e93368 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.322998</span> <span class="number">7</span>f7f175998c0 <span class="number">10</span>  cannot find current period zonegroup using local zonegroup</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.323018</span> <span class="number">7</span>f7f175998c0 <span class="number">20</span> get_system_obj_state: <span class="variable">rctx=</span><span class="number">0</span>x7ffcdb751d10 <span class="variable">obj=</span>.rgw.root:zonegroups_names.cn <span class="variable">state=</span><span class="number">0</span>x7f7f17e93368 s-&gt;<span class="variable">prefetch_data=</span><span class="number">0</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.323356</span> <span class="number">7</span>f7f175998c0  <span class="number">0</span> error <span class="keyword">in</span> read_id for object name: cn : (<span class="number">2</span>) No such file <span class="constant">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.323371</span> <span class="number">7</span>f7f175998c0  <span class="number">0</span> failed reading zonegroup info: ret -<span class="number">2</span> (<span class="number">2</span>) No such file <span class="constant">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">03</span>:<span class="number">48.324456</span> <span class="number">7</span>f7f175998c0 -<span class="number">1</span> Couldn't init storage provider (RADOS)</span><br></pre></td></tr></table></figure>
<h4 id="检查最新的pool列表">检查最新的pool列表</h4><figure class="highlight puppet"><table><tr><td class="code"><pre><span class="line"><span class="literal">root</span>@demo:/<span class="literal">home</span>/demouser<span class="comment"># rados lspools</span></span><br><span class="line">rbd</span><br><span class="line">.cn.rgw.<span class="literal">root</span></span><br><span class="line">.cn-zone1.rgw.<span class="literal">root</span></span><br><span class="line">.cn-zone1.rgw.<span class="built_in">domain</span></span><br><span class="line">.cn-zone1.rgw.<span class="literal">control</span></span><br><span class="line">.cn-zone1.rgw.gc</span><br><span class="line">.cn-zone1.rgw.buckets.index</span><br><span class="line">.cn-zone1.rgw.buckets.extra</span><br><span class="line">.cn-zone1.rgw.buckets</span><br><span class="line">.cn-zone1.log</span><br><span class="line">.cn-zone1.intent-log</span><br><span class="line">.cn-zone1.usage</span><br><span class="line">.cn-zone1.users</span><br><span class="line">.cn-zone1.users.email</span><br><span class="line">.cn-zone1.users.swift</span><br><span class="line">.cn-zone1.users.<span class="literal">uid</span></span><br><span class="line">.rgw.<span class="literal">root</span></span><br><span class="line"><span class="keyword">default</span>.rgw.<span class="literal">control</span> <span class="comment">#新J版本默认新增的几个pool</span></span><br><span class="line"><span class="keyword">default</span>.rgw.data.<span class="literal">root</span></span><br><span class="line"><span class="keyword">default</span>.rgw.gc</span><br><span class="line"><span class="keyword">default</span>.rgw.log</span><br><span class="line"></span><br><span class="line"><span class="literal">root</span>@demo:/<span class="literal">home</span>/demouser<span class="comment"># ceph df</span></span><br><span class="line"><span class="constant">G</span>LOBAL:</span><br><span class="line">    <span class="constant">S</span>IZE       <span class="constant">A</span>VAIL      <span class="constant">R</span>AW <span class="constant">U</span>SED     %<span class="constant">R</span>AW <span class="constant">U</span>SED</span><br><span class="line">    <span class="number">92114</span>M     <span class="number">88987</span>M        <span class="number">3126</span>M          <span class="number">3.39</span></span><br><span class="line"><span class="constant">P</span>OOLS:</span><br><span class="line">    <span class="constant">N</span>AME                            <span class="constant">I</span>D     <span class="constant">U</span>SED      %<span class="constant">U</span>SED     <span class="constant">M</span>AX <span class="constant">A</span>VAIL     <span class="constant">O</span>BJECTS</span><br><span class="line">    rbd                             <span class="number">0</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn.rgw.<span class="literal">root</span>                    <span class="number">1</span>        <span class="number">338</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">2</span></span><br><span class="line">    .cn-zone1.rgw.<span class="literal">root</span>              <span class="number">2</span>       <span class="number">1419</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">2</span></span><br><span class="line">    .cn-zone1.rgw.<span class="built_in">domain</span>            <span class="number">3</span>       <span class="number">1829</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">9</span></span><br><span class="line">    .cn-zone1.rgw.<span class="literal">control</span>           <span class="number">4</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">8</span></span><br><span class="line">    .cn-zone1.rgw.gc                <span class="number">5</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M          <span class="number">32</span></span><br><span class="line">    .cn-zone1.rgw.buckets.index     <span class="number">6</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M          <span class="number">88</span></span><br><span class="line">    .cn-zone1.rgw.buckets.extra     <span class="number">7</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn-zone1.rgw.buckets           <span class="number">8</span>      <span class="number">2212</span>k         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">5</span></span><br><span class="line">    .cn-zone1.log                   <span class="number">9</span>          <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M          <span class="number">80</span></span><br><span class="line">    .cn-zone1.intent-log            <span class="number">10</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn-zone1.usage                 <span class="number">11</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn-zone1.users                 <span class="number">12</span>        <span class="number">84</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">7</span></span><br><span class="line">    .cn-zone1.users.email           <span class="number">13</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn-zone1.users.swift           <span class="number">14</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    .cn-zone1.users.<span class="literal">uid</span>             <span class="number">15</span>      <span class="number">2054</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">9</span></span><br><span class="line">    .rgw.<span class="literal">root</span>                       <span class="number">16</span>      <span class="number">1588</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">4</span></span><br><span class="line">    <span class="keyword">default</span>.rgw.<span class="literal">control</span>             <span class="number">17</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">8</span></span><br><span class="line">    <span class="keyword">default</span>.rgw.data.<span class="literal">root</span>           <span class="number">18</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    <span class="keyword">default</span>.rgw.gc                  <span class="number">19</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br><span class="line">    <span class="keyword">default</span>.rgw.log                 <span class="number">20</span>         <span class="number">0</span>         <span class="number">0</span>        <span class="number">88554</span>M           <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h4 id="调整默认的zone配置">调整默认的zone配置</h4><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># rados ls -p .rgw.root #新版本的realm、zone、zonegroup的信息都会默认保存在这里，后续需要将原来region和zone的配置从.cn.rgw.root切换到这里，实现新旧版本的集群数据更新</span></span><br><span class="line">zone_info.<span class="number">2</span>f58efaa-<span class="number">3</span>fa2-<span class="number">48</span>b2-b996-<span class="number">7</span>f924ae1215c</span><br><span class="line">zonegroup_info.<span class="number">9</span>d07fb3c-<span class="number">45</span>d7-<span class="number">4</span>d63-a475-fd6ebd41b722</span><br><span class="line">zonegroups_names.default</span><br><span class="line">zone_names.default</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin realm list #默认realm为空</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"default_info"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"realms"</span><span class="symbol">:</span> []</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zonegroups list #默认会新建一个名称为default的zonegroup</span></span><br><span class="line">read_default_id <span class="symbol">:</span> -<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"default_info"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"zonegroups"</span><span class="symbol">:</span> [</span><br><span class="line">        <span class="string">"default"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zone  list #默认会新建一个名称为default的zone</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"default_info"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"zones"</span><span class="symbol">:</span> [</span><br><span class="line">        <span class="string">"default"</span></span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zonegroup get --rgw-zonegroup=default #查看默认的zonegroup配置</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"id"</span><span class="symbol">:</span> <span class="string">"9d07fb3c-45d7-4d63-a475-fd6ebd41b722"</span>,</span><br><span class="line">    <span class="string">"name"</span><span class="symbol">:</span> <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"api_name"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"is_master"</span><span class="symbol">:</span> <span class="string">"true"</span>,</span><br><span class="line">    <span class="string">"endpoints"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"hostnames"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"hostnames_s3website"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"master_zone"</span><span class="symbol">:</span> <span class="string">"2f58efaa-3fa2-48b2-b996-7f924ae1215c"</span>,</span><br><span class="line">    <span class="string">"zones"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"id"</span><span class="symbol">:</span> <span class="string">"2f58efaa-3fa2-48b2-b996-7f924ae1215c"</span>,</span><br><span class="line">            <span class="string">"name"</span><span class="symbol">:</span> <span class="string">"default"</span>,</span><br><span class="line">            <span class="string">"endpoints"</span><span class="symbol">:</span> [],</span><br><span class="line">            <span class="string">"log_meta"</span><span class="symbol">:</span> <span class="string">"false"</span>,</span><br><span class="line">            <span class="string">"log_data"</span><span class="symbol">:</span> <span class="string">"false"</span>,</span><br><span class="line">            <span class="string">"bucket_index_max_shards"</span><span class="symbol">:</span> <span class="number">0</span>,</span><br><span class="line">            <span class="string">"read_only"</span><span class="symbol">:</span> <span class="string">"false"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"placement_targets"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"name"</span><span class="symbol">:</span> <span class="string">"default-placement"</span>,</span><br><span class="line">            <span class="string">"tags"</span><span class="symbol">:</span> []</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"default_placement"</span><span class="symbol">:</span> <span class="string">"default-placement"</span>,</span><br><span class="line">    <span class="string">"realm_id"</span><span class="symbol">:</span> <span class="string">""</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zone get --rgw-zone=default #查看默认的zone配置</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"id"</span><span class="symbol">:</span> <span class="string">"2f58efaa-3fa2-48b2-b996-7f924ae1215c"</span>,</span><br><span class="line">    <span class="string">"name"</span><span class="symbol">:</span> <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"domain_root"</span><span class="symbol">:</span> <span class="string">"default.rgw.data.root"</span>,</span><br><span class="line">    <span class="string">"control_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.control"</span>,</span><br><span class="line">    <span class="string">"gc_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.gc"</span>,</span><br><span class="line">    <span class="string">"log_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.log"</span>,</span><br><span class="line">    <span class="string">"intent_log_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.intent-log"</span>,</span><br><span class="line">    <span class="string">"usage_log_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.usage"</span>,</span><br><span class="line">    <span class="string">"user_keys_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.users.keys"</span>,</span><br><span class="line">    <span class="string">"user_email_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.users.email"</span>,</span><br><span class="line">    <span class="string">"user_swift_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.users.swift"</span>,</span><br><span class="line">    <span class="string">"user_uid_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.users.uid"</span>,</span><br><span class="line">    <span class="string">"system_key"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"access_key"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">        <span class="string">"secret_key"</span><span class="symbol">:</span> <span class="string">""</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"placement_pools"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"key"</span><span class="symbol">:</span> <span class="string">"default-placement"</span>,</span><br><span class="line">            <span class="string">"val"</span><span class="symbol">:</span> &#123;</span><br><span class="line">                <span class="string">"index_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.buckets.index"</span>,</span><br><span class="line">                <span class="string">"data_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.buckets.data"</span>,</span><br><span class="line">                <span class="string">"data_extra_pool"</span><span class="symbol">:</span> <span class="string">"default.rgw.buckets.non-ec"</span>,</span><br><span class="line">                <span class="string">"index_type"</span><span class="symbol">:</span> <span class="number">0</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"metadata_heap"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"realm_id"</span><span class="symbol">:</span> <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zone get --rgw-zone=default &gt; zone.info#导出默认的zone配置</span></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin zone set --rgw-zone=default &lt; zone.info #修改默认配置如下</span></span><br><span class="line">zone id <span class="number">2</span>f58efaa-<span class="number">3</span>fa2-<span class="number">48</span>b2-b996-<span class="number">7</span>f924ae1215c&#123;</span><br><span class="line">    <span class="string">"id"</span><span class="symbol">:</span> <span class="string">"2f58efaa-3fa2-48b2-b996-7f924ae1215c"</span>,</span><br><span class="line">    <span class="string">"name"</span><span class="symbol">:</span> <span class="string">"default"</span>,</span><br><span class="line">    <span class="string">"domain_root"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.domain"</span>,</span><br><span class="line">    <span class="string">"control_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.control"</span>,</span><br><span class="line">    <span class="string">"gc_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.gc"</span>,</span><br><span class="line">    <span class="string">"log_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.log"</span>,</span><br><span class="line">    <span class="string">"intent_log_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.intent-log"</span>,</span><br><span class="line">    <span class="string">"usage_log_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.usage"</span>,</span><br><span class="line">    <span class="string">"user_keys_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.users"</span>, <span class="comment">#这个是之前的users pool，新版本改名了</span></span><br><span class="line">    <span class="string">"user_email_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.users.email"</span>,</span><br><span class="line">    <span class="string">"user_swift_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.users.swift"</span>,</span><br><span class="line">    <span class="string">"user_uid_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.users.uid"</span>,</span><br><span class="line">    <span class="string">"system_key"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"access_key"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">        <span class="string">"secret_key"</span><span class="symbol">:</span> <span class="string">""</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"placement_pools"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"key"</span><span class="symbol">:</span> <span class="string">"default-placement"</span>,</span><br><span class="line">            <span class="string">"val"</span><span class="symbol">:</span> &#123;</span><br><span class="line">                <span class="string">"index_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.buckets.index"</span>,</span><br><span class="line">                <span class="string">"data_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.buckets"</span>,</span><br><span class="line">                <span class="string">"data_extra_pool"</span><span class="symbol">:</span> <span class="string">".cn-zone1.rgw.buckets.extra"</span>,</span><br><span class="line">                <span class="string">"index_type"</span><span class="symbol">:</span> <span class="number">0</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"metadata_heap"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"realm_id"</span><span class="symbol">:</span> <span class="string">""</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="调整ceph-conf配置">调整ceph.conf配置</h4><figure class="highlight applescript"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser<span class="comment"># /etc/init.d/radosgw start #重启失败，发现以下错误log</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">21</span>:<span class="number">42.300998</span> <span class="number">7</span>fc1d58718c0  <span class="number">0</span> ceph <span class="property">version</span> <span class="number">10.2</span><span class="number">.6</span> (<span class="number">656</span>b5b63ed7c43bd014bcafd81b001959d5f089f), process radosgw, pid <span class="number">12586</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">21</span>:<span class="number">42.318848</span> <span class="number">7</span>fc1d58718c0  <span class="number">0</span> <span class="keyword">error</span> <span class="keyword">in</span> read_id <span class="keyword">for</span> object <span class="property">name</span>: default : (<span class="number">2</span>) No such <span class="type">file</span> <span class="keyword">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">21</span>:<span class="number">42.322114</span> <span class="number">7</span>fc1d58718c0  <span class="number">0</span> <span class="keyword">error</span> <span class="keyword">in</span> read_id <span class="keyword">for</span> object <span class="property">name</span>: cn : (<span class="number">2</span>) No such <span class="type">file</span> <span class="keyword">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">21</span>:<span class="number">42.322129</span> <span class="number">7</span>fc1d58718c0  <span class="number">0</span> failed reading zonegroup info: ret -<span class="number">2</span> (<span class="number">2</span>) No such <span class="type">file</span> <span class="keyword">or</span> directory</span><br><span class="line"><span class="number">2017</span>-<span class="number">03</span>-<span class="number">23</span> <span class="number">15</span>:<span class="number">21</span>:<span class="number">42.323295</span> <span class="number">7</span>fc1d58718c0 -<span class="number">1</span> Couldn't init storage provider (RADOS)</span><br></pre></td></tr></table></figure>
<figure class="highlight stata"><table><tr><td class="code"><pre><span class="line">修改ceph.<span class="keyword">conf</span>配置如下</span><br><span class="line">[client.radosgw.<span class="keyword">us</span>-zone1]</span><br><span class="line">     rgw dns name = s3.i.nease.<span class="keyword">net</span></span><br><span class="line">     rgw frontends = fastcgi</span><br><span class="line">     host = ceph.work</span><br><span class="line">     keyring = /etc/ceph/ceph.client.radosgw.keyring</span><br><span class="line">     rgw socket path = /home/ceph/<span class="keyword">var</span>/<span class="keyword">run</span>/ceph-client.radosgw.<span class="keyword">us</span>-zone1.sock</span><br><span class="line">     <span class="keyword">log</span> <span class="keyword">file</span> = /home/ceph/<span class="keyword">log</span>/radosgw.<span class="keyword">us</span>-zone1.<span class="literal">log</span></span><br><span class="line">     rgw <span class="keyword">print</span> <span class="keyword">continue</span> = false</span><br><span class="line">     rgw content length compat = true</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@demo:/home/demouser# /etc/init.<span class="keyword">d</span>/radosgw start #成功启动</span><br></pre></td></tr></table></figure>
<h4 id="测试效果">测试效果</h4><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin metadata list user #user元数据正常</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">"en-user1"</span>,</span><br><span class="line">    <span class="string">"us-zone1"</span>,</span><br><span class="line">    <span class="string">"us-user1"</span>,</span><br><span class="line">    <span class="string">"cn-user1"</span>,</span><br><span class="line">    <span class="string">"en-zone1"</span>,</span><br><span class="line">    <span class="string">"cn-zone1"</span>,</span><br><span class="line">    <span class="string">"cn-user2"</span></span><br><span class="line">]</span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin metadata list bucket #bucket元数据正常</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">"cn-test1"</span>,</span><br><span class="line">    <span class="string">"us-test1"</span>,</span><br><span class="line">    <span class="string">"en-test1"</span>,</span><br><span class="line">    <span class="string">"cn-test2"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin user info --uid=en-user1 #获取用户信息</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span><span class="symbol">:</span> <span class="string">"en-user1"</span>,</span><br><span class="line">    <span class="string">"display_name"</span><span class="symbol">:</span> <span class="string">"en-user1"</span>,</span><br><span class="line">    <span class="string">"email"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span><span class="symbol">:</span> <span class="number">0</span>,</span><br><span class="line">    <span class="string">"max_buckets"</span><span class="symbol">:</span> <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"auid"</span><span class="symbol">:</span> <span class="number">0</span>,</span><br><span class="line">    <span class="string">"subusers"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"keys"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span><span class="symbol">:</span> <span class="string">"en-user1"</span>,</span><br><span class="line">            <span class="string">"access_key"</span><span class="symbol">:</span> <span class="string">"PWDYNWWXXC3GCYLIJUWL"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span><span class="symbol">:</span> <span class="string">"R5kiJPTEroPkUW9TNNM4WWYgXHSMsHoWPxqkRnsG"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"caps"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"op_mask"</span><span class="symbol">:</span> <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"bucket_quota"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"enabled"</span><span class="symbol">:</span> <span class="keyword">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span><span class="symbol">:</span> -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span><span class="symbol">:</span> -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"enabled"</span><span class="symbol">:</span> <span class="keyword">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span><span class="symbol">:</span> -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span><span class="symbol">:</span> -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span><span class="symbol">:</span> []</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root<span class="variable">@demo</span><span class="symbol">:/home/demouser</span><span class="comment"># radosgw-admin user create --uid=demotest --display-name=demotest #新建用户</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span><span class="symbol">:</span> <span class="string">"demotest"</span>,</span><br><span class="line">    <span class="string">"display_name"</span><span class="symbol">:</span> <span class="string">"demotest"</span>,</span><br><span class="line">    <span class="string">"email"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span><span class="symbol">:</span> <span class="number">0</span>,</span><br><span class="line">    <span class="string">"max_buckets"</span><span class="symbol">:</span> <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"auid"</span><span class="symbol">:</span> <span class="number">0</span>,</span><br><span class="line">    <span class="string">"subusers"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"keys"</span><span class="symbol">:</span> [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span><span class="symbol">:</span> <span class="string">"demotest"</span>,</span><br><span class="line">            <span class="string">"access_key"</span><span class="symbol">:</span> <span class="string">"1S9Q6K0P90180M1VFPNR"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span><span class="symbol">:</span> <span class="string">"R123LHsqVzMRe3jvJokPPDSYzmAtIxM5jxywQMTP"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"caps"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"op_mask"</span><span class="symbol">:</span> <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span><span class="symbol">:</span> <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span><span class="symbol">:</span> [],</span><br><span class="line">    <span class="string">"bucket_quota"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"enabled"</span><span class="symbol">:</span> <span class="keyword">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span><span class="symbol">:</span> -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span><span class="symbol">:</span> -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span><span class="symbol">:</span> &#123;</span><br><span class="line">        <span class="string">"enabled"</span><span class="symbol">:</span> <span class="keyword">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span><span class="symbol">:</span> -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span><span class="symbol">:</span> -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span><span class="symbol">:</span> []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>旧版本hammer的rgw管理模型是 region-&gt;zone两级结构，而新版本变成了realm-&gt;zonegroup-&gt;zone,同时部分pool的命名规则也发生了变更，如果总结升级ceph版本，会出现RGW服务启动失败，导致RGW启动失败的因素有两类，一类是pool名称的变更，另外一类是ceph.conf中rgw的配置变更。本文通过真实用例，实现了新旧版本的切换，各位实际环境还是要谨慎操作，毕竟跨版本的升级还是有很大风险。    —-by 秦牧羊</p>
<h2 id="附">附</h2><p>官方升级操作指南：<a href="http://docs.ceph.com/docs/master/radosgw/upgrade_to_jewel/" target="_blank" rel="external">http://docs.ceph.com/docs/master/radosgw/upgrade_to_jewel/</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">dev-广州-秦牧羊</td>
<td style="text-align:center">2017-03-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/update.gif" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇来自秦牧羊的一篇分享，讲述的是从hammer升级到jewel的过程，以及其中的一些故障的处理，是一篇非常详细的实战分享</p>
<h2 id="初始状态">初始状态</h2><h3 id="pool状态">pool状态</h3><figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line">root@demo:/home/demouser# rados lspools</span><br><span class="line">rbd</span><br><span class="line"><span class="title">.cn.rgw.root</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.root</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.domain</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.control</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.gc</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets.index</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets.extra</span></span><br><span class="line"><span class="title">.cn-zone1.rgw.buckets</span></span><br><span class="line"><span class="title">.cn-zone1.log</span></span><br><span class="line"><span class="title">.cn-zone1.intent-log</span></span><br><span class="line"><span class="title">.cn-zone1.usage</span></span><br><span class="line"><span class="title">.cn-zone1.users</span></span><br><span class="line"><span class="title">.cn-zone1.users.email</span></span><br><span class="line"><span class="title">.cn-zone1.users.swift</span></span><br><span class="line"><span class="title">.cn-zone1.users.uid</span></span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[小文件测试数据准备]]></title>
    <link href="http://www.zphj1987.com/2017/03/24/small-file-test-prepare/"/>
    <id>http://www.zphj1987.com/2017/03/24/small-file-test-prepare/</id>
    <published>2017-03-24T06:14:31.000Z</published>
    <updated>2017-03-24T06:44:03.812Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://static.zybuluo.com/zphj1987/fbb4k5ywq1umawkv3epkj89k/upload.jpg" alt="file"><br></center></p>
<h2 id="前言">前言</h2><p>在看一个Linux Vault 2017的资料的时候，看到红帽分享的一个测试的过程，里面关于小文件元数据性能测试的，环境准备的还比较好,可以作为一种测试模型<br><a id="more"></a></p>
<h2 id="测试用例">测试用例</h2><p>测试用例一：</p>
<p><center><br><img src="http://static.zybuluo.com/zphj1987/0mnq6hn37kl4xummn6odqvcv/small1.png" alt="small"><br></center><br>使用find -name 测试 find -size 测试<br>测试用例二：</p>
<p><center><br><img src="http://static.zybuluo.com/zphj1987/30p1h4k2kgxucn23oxkkryij/lardir.png" alt="large"><br></center><br>使用find -name 测试 find -size 测试</p>
<p>测试用例三：</p>
<p><center><br><img src="http://static.zybuluo.com/zphj1987/m7j6azwqnagh2zgf3boykwh5/onlydir.png" alt="onlydir"><br></center><br>使用rmdir进行测试</p>
<h2 id="总结">总结</h2><p>本篇就是记录一个测试模型</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://static.zybuluo.com/zphj1987/fbb4k5ywq1umawkv3epkj89k/upload.jpg" alt="file"><br></center></p>
<h2 id="前言">前言</h2><p>在看一个Linux Vault 2017的资料的时候，看到红帽分享的一个测试的过程，里面关于小文件元数据性能测试的，环境准备的还比较好,可以作为一种测试模型<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[编译的Ceph二进制文件过大问题]]></title>
    <link href="http://www.zphj1987.com/2017/03/23/compile-ceph-binary-big/"/>
    <id>http://www.zphj1987.com/2017/03/23/compile-ceph-binary-big/</id>
    <published>2017-03-23T15:01:23.000Z</published>
    <updated>2017-03-23T15:08:10.520Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/codebug.jpg" alt="binary"><br></center>

<h2 id="前言">前言</h2><p>在ceph的研发群里看到一个cepher提出一个问题，编译的ceph的二进制文件过大，因为我一直用的打包好的rpm包，没有关注这个问题，重新编译了一遍发现确实有这个问题</p>
<p>本篇就是记录如何解决这个问题的<br><a id="more"></a></p>
<h2 id="打rpm包的方式">打rpm包的方式</h2><p>用我自己的环境编译的时候发现一个问题，编译出来的rpm包还是很大，开始怀疑是机器的原因，换了一台发现二进制包就很小了，然后查询了很多资料以后，找到了问题所在</p>
<p>在打rpm包的时候可以通过宏变量去控制是否打出一个的debug的包，这个包的作用就是把二进制文件当中包含的debug的相关的全部抽离出来形成一个新的rpm包，而我的环境不知道什么时候在/root/.rpmmacros添加进去了一个<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">d%ebug_package      %&#123;nil&#125;</span><br></pre></td></tr></table></figure></p>
<p>搜寻资料后确定就是这个的问题,这个变量添加了以后，在打包的时候就不会进行debug相关包的剥离，然后打出的包就是巨大的，可以这样检查自己的rpmbuild的宏变量信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment">#  rpmbuild --showrc|grep debug_package</span></span><br><span class="line">    %&#123;!?__debug_package:</span><br><span class="line">    %&#123;?__debug_package:%&#123;__debug_install_post&#125;&#125;</span><br><span class="line">-<span class="number">14</span>: _<span class="built_in">enable</span>_debug_packages	<span class="number">1</span></span><br><span class="line">-<span class="number">14</span>: debug_package	</span><br><span class="line">%global __debug_package <span class="number">1</span></span><br><span class="line">-<span class="number">14</span>: install	%&#123;?_<span class="built_in">enable</span>_debug_packages:%&#123;?buildsubdir:%&#123;debug_package&#125;&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果开启了debug包抽离（默认就是开启的），那么rpmbuild在打包的过程中会有个调用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/rpm/find-debuginfo.sh --strict-build-id -m --run-dwz --dwz-low-mem-die-limit <span class="number">10000000</span> --dwz-max-die-limit <span class="number">110000000</span> /root/rpmbuild/BUILD/ceph-<span class="number">10.2</span>.<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>这个就是rpmbuild过程中，进行抽离debug信息的操作，也就是缩小二进制的过程，这个并不能直接执行命令，需要用rpmbuild -bb ceph.spec 打包的时候内部自动进行调用的</p>
<p>上面是rpm打包过程中进行的二进制缩小，那么如果我们是源码编译安装时候，如何缩小这个二进制，答案当然是可以的</p>
<h2 id="源码编译安装的方式">源码编译安装的方式</h2><p>./configure 后make生成的二进制文件就在./src下面了<br>我们以ceph-mon为例进行抽离</p>
<p>这个-O3并没有影响到太多的生成的二进制的大小，—with-debug会有一定的影响，关键还是strip的这个操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./configure --with-debug  CXXFLAGS=-O3 CFLAGS=-O3 CCASFLAGS=-O3</span><br></pre></td></tr></table></figure></p>
<p>所以默认的就行</p>
<p>如果整体进行安装就使用make install-strip安装即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># make install-strip</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll /usr/local/bin/ceph-osd</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14266576</span> Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">57</span> /usr/<span class="built_in">local</span>/bin/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll /usr/local/bin/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">57</span> /usr/<span class="built_in">local</span>/bin/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">248</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">54</span> src/ceph-osd</span><br></pre></td></tr></table></figure></p>
<h2 id="关键的strip的用法">关键的strip的用法</h2><p>gcc编译的时候带上-g参数,就是添加了debug的信息</p>
<blockquote>
<p>gcc -g -o</p>
</blockquote>
<h3 id="分离debug_information">分离debug information</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment">#objcopy --only-keep-debug src/ceph-osd src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">248</span>M Mar <span class="number">23</span> <span class="number">17</span>:<span class="number">54</span> src/ceph-osd</span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd.debug -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">235</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">08</span> src/ceph-osd.debug</span><br></pre></td></tr></table></figure>
<p>另外一种方法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># cp src/ceph-osd src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># strip --only-keep-debug src/ceph-osd.debug</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd.debug -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">235</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">10</span> src/ceph-osd.debug</span><br></pre></td></tr></table></figure></p>
<h3 id="从原始文件去掉_debug_information">从原始文件去掉 debug information</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># objcopy --strip-debug src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">18</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">11</span> src/ceph-osd</span><br><span class="line">objcopy --strip-debug main</span><br></pre></td></tr></table></figure>
<p>另外一种方法：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># strip --strip-debug --strip-unneeded src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># ll src/ceph-osd -hl</span></span><br><span class="line">-rwxr-xr-x <span class="number">1</span> root root <span class="number">14</span>M Mar <span class="number">23</span> <span class="number">18</span>:<span class="number">12</span> src/ceph-osd</span><br></pre></td></tr></table></figure></p>
<h3 id="启用debuglink模式">启用debuglink模式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># objcopy --add-gnu-debuglink  src/ceph-osd.debug src/ceph-osd</span></span><br><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># gdb src/ceph-osd</span></span><br></pre></td></tr></table></figure>
<p>或者<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph-<span class="number">10.2</span>.<span class="number">6</span>]<span class="comment"># gdb -s src/ceph-osd.debug -e src/ceph-osd</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>二进制包里面包含了debug的一些相关信息，可以通过strip的方式将内部的debug内容清理掉，这样就可以得到比较小的二进制包了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-23</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/codebug.jpg" alt="binary"><br></center>

<h2 id="前言">前言</h2><p>在ceph的研发群里看到一个cepher提出一个问题，编译的ceph的二进制文件过大，因为我一直用的打包好的rpm包，没有关注这个问题，重新编译了一遍发现确实有这个问题</p>
<p>本篇就是记录如何解决这个问题的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph实现数据的'不拆分']]></title>
    <link href="http://www.zphj1987.com/2017/03/22/ceph-no-distribute-all/"/>
    <id>http://www.zphj1987.com/2017/03/22/ceph-no-distribute-all/</id>
    <published>2017-03-22T07:49:16.000Z</published>
    <updated>2017-04-19T07:34:02.464Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octopus.png" alt="oct"><br></center>

<h2 id="前言">前言</h2><p>之前看过一个朋友一篇文章，讲述的是Vsan为什么使用的是两副本，而ceph则大多数情况下需要三副本，当时个人观点是这个并不是关键点，但是在仔细考虑了问题的出发点以后，这个也可以说是其中的一个点<br><a id="more"></a><br>一个集群数据丢失可以从多方面去看</p>
<ul>
<li>发生丢失数据的事件，这个来说，出现这个事件的概率是一致的，同等硬件情况下没有谁的系统能够说在两副本情况下把这个出现坏盘概率做的比其他系统更低</li>
<li>发生坏盘事件以后，数据丢失波及的范围，这个就是那个朋友提出的一个观点，对于Vsan来说因为文件的不拆分，也就是在丢了的情况下，只是局部数据的丢失，而ceph的数据因为拆分到整个集群，基本上说就是全军覆没了，这一点没有什么争议</li>
</ul>
<p>一般来说，ceph都是配置的分布式文件系统，也就是数据以PG为组合，以对象为最小单元的形式分布到整个集群当中去，通过控制crush能够增加一定的可用概率，但是有没有办法实现真的丢盘的情况下，数据波及没有那么广，答案当然是有的，只是需要做一些更细微的控制，前端的使用的接口也需要做一定的改动，本篇将讲述这个如何去实现，以及前端可能需要的变动</p>
<h2 id="方案实现">方案实现</h2><p>首先来一张示意图，来介绍大致的实现方式，下面再给出操作步骤</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/doeql98mri7dues48uf8hk5y/osd%E4%B8%8D%E6%8B%86%E5%88%86.png" alt="osd不拆分.png-15.7kB"><br></center>

<p>主要包括三步</p>
<ul>
<li>横向划条带 </li>
<li>创建对应规则 </li>
<li>根据规则创建相关存储池</li>
</ul>
<h3 id="横向划条带">横向划条带</h3><p>创建虚拟根<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush add-bucket default<span class="operator">-a</span> root</span><br><span class="line">ceph osd crush add-bucket default-b root</span><br><span class="line">ceph osd crush add-bucket default-c root</span><br><span class="line">ceph osd crush add-bucket default<span class="operator">-d</span> root</span><br></pre></td></tr></table></figure></p>
<p>创建虚拟主机<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph  osd crush add-bucket host1<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host2<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host3<span class="operator">-a</span> host</span><br><span class="line">ceph  osd crush add-bucket host1-b host</span><br><span class="line">ceph  osd crush add-bucket host2-b host</span><br><span class="line">ceph  osd crush add-bucket host3-b host</span><br><span class="line">ceph  osd crush add-bucket host1-c host</span><br><span class="line">ceph  osd crush add-bucket host2-c host</span><br><span class="line">ceph  osd crush add-bucket host3-c host</span><br><span class="line">ceph  osd crush add-bucket host1<span class="operator">-d</span> host</span><br><span class="line">ceph  osd crush add-bucket host2<span class="operator">-d</span> host</span><br><span class="line">ceph  osd crush add-bucket host3<span class="operator">-d</span> host</span><br></pre></td></tr></table></figure></p>
<p>将虚拟主机挪到虚拟根里面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush move host1<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host2<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host3<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush move host1-b root=default-b</span><br><span class="line">ceph osd crush move host2-b root=default-b</span><br><span class="line">ceph osd crush move host3-b root=default-b</span><br><span class="line">ceph osd crush move host1-c root=default-c</span><br><span class="line">ceph osd crush move host2-c root=default-c</span><br><span class="line">ceph osd crush move host3-c root=default-c</span><br><span class="line">ceph osd crush move host1<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush move host2<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush move host3<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>将osd塞入到指定的bucker内<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd  crush create-or-move  osd.<span class="number">0</span> <span class="number">1.83</span>  host=host1<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">4</span> <span class="number">1.83</span>  host=host2<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">8</span> <span class="number">1.83</span>  host=host3<span class="operator">-a</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">1</span> <span class="number">1.83</span>  host=host1-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">5</span> <span class="number">1.83</span>  host=host2-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">9</span> <span class="number">1.83</span>  host=host3-b</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">2</span> <span class="number">1.83</span>  host=host1-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">6</span> <span class="number">1.83</span>  host=host2-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">10</span> <span class="number">1.83</span>  host=host3-c</span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">3</span> <span class="number">1.83</span>  host=host1<span class="operator">-d</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">7</span> <span class="number">1.83</span>  host=host2<span class="operator">-d</span></span><br><span class="line">ceph osd  crush create-or-move  osd.<span class="number">11</span> <span class="number">1.83</span>  host=host3<span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>以上的这么多的操作可以用比较简单的命令实现<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">0</span> <span class="number">1.83</span>  host=host1<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">1</span> <span class="number">1.83</span>  host=host1-b root=default-b</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">2</span> <span class="number">1.83</span>  host=host1-c root=default-c</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">3</span> <span class="number">1.83</span>  host=host1<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">4</span> <span class="number">1.83</span>  host=host2<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">5</span> <span class="number">1.83</span>  host=host2-b root=default-b</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">6</span> <span class="number">1.83</span>  host=host2-c root=default-c</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">7</span> <span class="number">1.83</span>  host=host2<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">8</span> <span class="number">1.83</span>  host=host3<span class="operator">-a</span> root=default<span class="operator">-a</span></span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">9</span> <span class="number">1.83</span>  host=host3-b root=default-b</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">10</span> <span class="number">1.83</span> host=host3-c root=default-c</span><br><span class="line">ceph osd crush <span class="built_in">set</span> osd.<span class="number">11</span> <span class="number">1.83</span> host=host3<span class="operator">-d</span> root=default<span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>查看现在的树<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID  WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line"> -<span class="number">8</span> <span class="number">5.44080</span> root default<span class="operator">-d</span>                                     </span><br><span class="line">-<span class="number">18</span> <span class="number">1.81360</span>     host host1<span class="operator">-d</span>                                   </span><br><span class="line">  <span class="number">3</span> <span class="number">1.81360</span>         osd.<span class="number">3</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">19</span> <span class="number">1.81360</span>     host host2<span class="operator">-d</span>                                   </span><br><span class="line">  <span class="number">7</span> <span class="number">1.81360</span>         osd.<span class="number">7</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">20</span> <span class="number">1.81360</span>     host host3<span class="operator">-d</span>                                   </span><br><span class="line"> <span class="number">11</span> <span class="number">1.81360</span>         osd.<span class="number">11</span>        up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">7</span> <span class="number">5.44080</span> root default-c                                     </span><br><span class="line">-<span class="number">15</span> <span class="number">1.81360</span>     host host1-c                                   </span><br><span class="line">  <span class="number">2</span> <span class="number">1.81360</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">16</span> <span class="number">1.81360</span>     host host2-c                                   </span><br><span class="line">  <span class="number">6</span> <span class="number">1.81360</span>         osd.<span class="number">6</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">17</span> <span class="number">1.81360</span>     host host3-c                                   </span><br><span class="line"> <span class="number">10</span> <span class="number">1.81360</span>         osd.<span class="number">10</span>        up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">6</span> <span class="number">5.44080</span> root default-b                                     </span><br><span class="line">-<span class="number">12</span> <span class="number">1.81360</span>     host host1-b                                   </span><br><span class="line">  <span class="number">1</span> <span class="number">1.81360</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">13</span> <span class="number">1.81360</span>     host host2-b                                   </span><br><span class="line">  <span class="number">5</span> <span class="number">1.81360</span>         osd.<span class="number">5</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">14</span> <span class="number">1.81360</span>     host host3-b                                   </span><br><span class="line">  <span class="number">9</span> <span class="number">1.81360</span>         osd.<span class="number">9</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">5</span> <span class="number">5.44080</span> root default<span class="operator">-a</span>                                     </span><br><span class="line"> -<span class="number">9</span> <span class="number">1.81360</span>     host host1<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">0</span> <span class="number">1.81360</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">10</span> <span class="number">1.81360</span>     host host2<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">4</span> <span class="number">1.81360</span>         osd.<span class="number">4</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">11</span> <span class="number">1.81360</span>     host host3<span class="operator">-a</span>                                   </span><br><span class="line">  <span class="number">8</span> <span class="number">1.81360</span>         osd.<span class="number">8</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> -<span class="number">1</span>       <span class="number">0</span> root default                                       </span><br><span class="line"> -<span class="number">2</span>       <span class="number">0</span>     host host1                                     </span><br><span class="line"> -<span class="number">3</span>       <span class="number">0</span>     host host2                                     </span><br><span class="line"> -<span class="number">4</span>       <span class="number">0</span>     host host3</span><br></pre></td></tr></table></figure></p>
<p>下面老的一些bucket可以清理掉<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool delete rbd rbd  --yes-i-really-really-mean-it</span><br><span class="line">ceph osd crush rule rm replicated_ruleset</span><br><span class="line">ceph osd crush remove host1</span><br><span class="line">ceph osd crush remove host2</span><br><span class="line">ceph osd crush remove host3</span><br><span class="line">ceph osd crush remove default</span><br></pre></td></tr></table></figure></p>
<h3 id="创建对应规则">创建对应规则</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd crush rule create-simple rule048  default<span class="operator">-a</span> host</span><br><span class="line">ceph osd crush rule create-simple rule159  default-b host</span><br><span class="line">ceph osd crush rule create-simple rule2610  default-c host</span><br><span class="line">ceph osd crush rule create-simple rule3711  default<span class="operator">-d</span> host</span><br></pre></td></tr></table></figure>
<p>检查下规则<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd crush rule dump|grep "rule_name\|item_name"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule048"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-a"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule159"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-b"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule2610"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-c"</span></span><br><span class="line">        <span class="string">"rule_name"</span>: <span class="string">"rule3711"</span>,</span><br><span class="line">                <span class="string">"item_name"</span>: <span class="string">"default-d"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="根据规则创建相关存储池">根据规则创建相关存储池</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poola048 64 64 replicated rule048</span></span><br><span class="line">pool <span class="string">'poola048'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poolb159 64 64 replicated rule159</span></span><br><span class="line">pool <span class="string">'poolb159'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poolc2610 64 64 replicated rule2610</span></span><br><span class="line">pool <span class="string">'poolc2610'</span> created</span><br><span class="line">[root@host1 ceph]<span class="comment"># ceph osd pool create poold3711 64 64 replicated rule3711</span></span><br><span class="line">pool <span class="string">'poold3711'</span> created</span><br></pre></td></tr></table></figure>
<p>检查存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@host1 ceph]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">1</span> <span class="string">'poola048'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">145</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">2</span> <span class="string">'poolb159'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">1</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">147</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">3</span> <span class="string">'poolc2610'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">2</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">149</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">4</span> <span class="string">'poold3711'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">3</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">151</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>到这里基本的环境就配置好了，采用的是副本2，但是虚拟组里面留了三个osd，这个后面会解释</p>
<h2 id="如何使用">如何使用</h2><p>假设现在前端需要8个image用来使用了，那么我们创建的时候，就将这个8个平均分布到上面的四个存储里面去，这里是因为是划成了四个条带，在实际环境当中，可以根据需要进行划分，在选择用哪个存储的时候可以去用轮询的算法，进行轮询，也可以自定义去选择在哪个存储池创建，这个都是可以控制的</p>
<h3 id="创建image">创建image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd -p poola048 create image1 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poola048 create image2 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolb159 create image3 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolb159 create image4 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolc2610 create image6 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poolc2610 create image7 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poold3711 create image8 --size <span class="number">1</span>G</span><br><span class="line">rbd -p poold3711 create image9 --size <span class="number">1</span>G</span><br></pre></td></tr></table></figure>
<h3 id="如何跟virsh对接">如何跟virsh对接</h3><p>如果你熟悉virsh配置文件的话，可以看到rbd相关的配置文件是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;<span class="built_in">source</span> protocol=<span class="string">'rbd'</span> name=<span class="string">'volumes/volume-f20fd994-e600-41da-a6d8-6e216044dbb1'</span>&gt;</span><br><span class="line">        &lt;host name=<span class="string">'192.168.10.4'</span> port=<span class="string">'6789'</span>/&gt;</span><br><span class="line">&lt;/<span class="built_in">source</span>&gt;</span><br></pre></td></tr></table></figure></p>
<p>在cinder的相关配置当中虽然我们指定了volume这个存储池值是一个定值，在这个配置文件当中也就读取了这个值，那么需要改造的接口就是在创建云盘的时候，不去将cinder的存储池固定死，volumes/volume-f20fd994-e600-41da-a6d8-6e216044dbb1这样的值可以是上面的poola048/image1,也可以是poolc2610/image6,这个地方就是需要改动的地方，将整个值包含存储池的值作为一个变量，这个改动应该属于可改的</p>
<h2 id="分析">分析</h2><p>按上面的进行处理以后，那么再出现同时坏了两个盘的情况下，数据丢失的波及范围跟Vsan已经是一致了，因为数据打散也只是在这个三个里面打散了，真的出现磁盘损坏波及的也是局部的数据了</p>
<p>问题：<br>1、分布范围小了性能怎么样<br>比完全分布来说性能肯定降低了一些，但是如果说对于负载比较高的情况，每个盘都在跑的情况下，这个性能是一定的，底层的磁盘提供的带宽是一定的，这个跟VSAN一样的</p>
<p>并且这个上面所示的是极端的情况下的，缩小到3个OSD一组条带，也可以自行放宽到6个一个条带，这个只是提供了一种方法，缩小了波及范围</p>
<p>2、副本2为什么留3个osd一个条带<br>比副本数多1的话，这样在坏了一个盘也可以迁移，所以一般来说，至少比副本数多1的故障域</p>
<p>3、如何扩容<br>扩容就增加条带即可，并且可以把老的存储池规则指定到新的磁盘的条带上面</p>
<p>4、这个方法还可以用故障域增加可用性么<br>可以的，可以从每个故障域里面抽出OSD即可，只要保证底层的数据不重叠，实际是两个不同的需求</p>
<h2 id="总结">总结</h2><p>本篇是提供了一种可能性，在实际运行环境当中，可以根据自己的环境进行设计，设计的方法就是，假设一个数据的全部副本都丢了的情况，允许的数据波及范围是多少，如果拆分两份就是波及二分之一，我的测试环境是分成了四个条带，也就是只影响四分之一的数据</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-22</td>
</tr>
<tr>
<td style="text-align:center">补充OSD设置crush的简单方法</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/octopus.png" alt="oct"><br></center>

<h2 id="前言">前言</h2><p>之前看过一个朋友一篇文章，讲述的是Vsan为什么使用的是两副本，而ceph则大多数情况下需要三副本，当时个人观点是这个并不是关键点，但是在仔细考虑了问题的出发点以后，这个也可以说是其中的一个点<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于Docker UI 配置ceph集群]]></title>
    <link href="http://www.zphj1987.com/2017/03/16/base-on-docker-ui-deploy-ceph/"/>
    <id>http://www.zphj1987.com/2017/03/16/base-on-docker-ui-deploy-ceph/</id>
    <published>2017-03-16T10:08:16.000Z</published>
    <updated>2017-03-16T10:18:33.594Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker2.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>前一篇介绍了docker在命令行下面进行的ceph部署，本篇用docker的UI进行ceph的部署，目前来说市面上还没有一款能够比较简单就能直接在OS上面去部署Ceph的管理平台，这是因为OS的环境差异化太大，并且包的版本，以及各种软件的适配都可能造成失败，而docker比较固化环境，因此即使一个通用的UI也能很方便的部署出一个Cpeh集群</p>
<p>本篇就是对Docker UI部署集群做一个实践，对ceph了解，对docker了解，对dokcer的UI操作进行一定的了解的情况下，再做实践会比较好，总体上还是比较简单的<br><a id="more"></a></p>
<h2 id="安装并运行portainer">安装并运行portainer</h2><h3 id="安装软件">安装软件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">wget https://github.com/portainer/portainer/releases/download/<span class="number">1.12</span>.<span class="number">1</span>/portainer-<span class="number">1.12</span>.<span class="number">1</span>-linux-amd64.tar.gz</span><br><span class="line">tar xvpfz portainer-<span class="number">1.12</span>.<span class="number">1</span>-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">cd</span> portainer</span><br></pre></td></tr></table></figure>
<h3 id="运行软件">运行软件</h3><figure class="highlight coffeescript"><table><tr><td class="code"><pre><span class="line">.<span class="regexp">/portainer -H unix:/</span><span class="regexp">//</span><span class="reserved">var</span>/run/docker.sock  -p <span class="string">":9999"</span></span><br></pre></td></tr></table></figure>
<p>注意下这里-H是指定的docker的连接，也就是要控制哪个docker，这个支持本地的sock的方式，也支持远程的tcp的方式，这个进入ui界面后还可以添加更多的<br>-p是指定的访问的接口</p>
<h3 id="扩展知识">扩展知识</h3><p>如何在centos7下面启用 remote api<br>打开文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/docker.service</span><br></pre></td></tr></table></figure></p>
<p>在 <code>$INSECURE_REGISTRY</code> 后面添加 <code>-H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ExecStart=/usr/bin/dockerd-current \</span><br><span class="line">          --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \</span><br><span class="line">          --default-runtime=docker-runc \</span><br><span class="line">          --exec-opt native.cgroupdriver=systemd \</span><br><span class="line">          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \</span><br><span class="line">          <span class="variable">$OPTIONS</span> \</span><br><span class="line">          <span class="variable">$DOCKER_STORAGE_OPTIONS</span> \</span><br><span class="line">          <span class="variable">$DOCKER_NETWORK_OPTIONS</span> \</span><br><span class="line">          <span class="variable">$ADD_REGISTRY</span> \</span><br><span class="line">          <span class="variable">$BLOCK_REGISTRY</span> \</span><br><span class="line">          <span class="variable">$INSECURE_REGISTRY</span>  -H tcp://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">2376</span> -H unix:///var/run/docker.sock</span><br></pre></td></tr></table></figure></p>
<p>修改好了后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#systemctl daemon-reload</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#systemctl restart docker</span></span><br></pre></td></tr></table></figure></p>
<p>检查端口和asok<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep 2376</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">2376</span>                 :::*                    LISTEN      <span class="number">24484</span>/dockerd-curre </span><br><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/run/docker.sock</span></span><br><span class="line">srw-rw---- <span class="number">1</span> root root <span class="number">0</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">39</span> /var/run/docker.sock</span><br></pre></td></tr></table></figure></p>
<p>生成了配置没有问题</p>
<h4 id="portainer的自身数据">portainer的自身数据</h4><p>默认情况下portainer的数据是存储在/data目录下面的，如果想重新配置密码或者内容的话，删除这个目录里面的数据就行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /data/</span></span><br><span class="line">total <span class="number">24</span></span><br><span class="line">-rw------- <span class="number">1</span> root root <span class="number">32768</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">32</span> portainer.db</span><br><span class="line">drwx------ <span class="number">2</span> root root     <span class="number">6</span> Mar <span class="number">16</span> <span class="number">16</span>:<span class="number">32</span> tls</span><br></pre></td></tr></table></figure></p>
<h2 id="UI界面登陆">UI界面登陆</h2><p>直接访问宿主机的<code>http://ip:9999</code><br><img src="http://static.zybuluo.com/zphj1987/0qlepujxmrt4wqadoa01zfg1/image_1bbb4ogmqu1ir8049n1okfq4j9.png" alt="login"><br>输入一个8位数的密码<br>输入好了以后，登陆即可</p>
<p><img src="http://static.zybuluo.com/zphj1987/6grz7nooae5smur1bcb54lb6/image_1bbb4r1eb1qnj0pcjmsbf1ucgm.png" alt="endponit"></p>
<p>检查endpoint，可以看到就是我刚才命令行当中加入的sock</p>
<h2 id="获取image">获取image</h2><p><img src="http://static.zybuluo.com/zphj1987/zvhkf4ujoblfwfvzx6860fxt/image_1bbb4vs5h1ri522q8avkrb1ko716.png" alt="get ceph"></p>
<p>在上面填写<code>ceph/daemon</code> 然后点击pull</p>
<p>有可能会超时，如果多次失败，就去后台命令行执行，这个地方等同于后台的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull ceph/daemon</span><br></pre></td></tr></table></figure></p>
<p>也可以直接在后台执行这个命令<br>可以用dstat -n观察下载的速度</p>
<p>下载好了去页面上看下是否好了<br><img src="http://static.zybuluo.com/zphj1987/pxml52emnm8c3gkzzt90rh27/image_1bbb6c50tip1iud1gfv9m4uku1j.png" alt="download"></p>
<h2 id="配置CEPH集群">配置CEPH集群</h2><p>配置集群可以都在页面做了，因为之前有篇命令行部署docker的ceph，建议先回顾一下，再看这个比较好</p>
<h3 id="创建MON">创建MON</h3><p>点击增加容器<br><img src="http://static.zybuluo.com/zphj1987/myuenisz3pj34mawmdupfifp/image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p>
<p>注意创建好两个目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/ceph</span><br><span class="line">mkdir -p /var/lib/ceph/</span><br></pre></td></tr></table></figure></p>
<p>这两个目录里面不要有任何东西,保持空目录状态</p>
<p><img src="http://static.zybuluo.com/zphj1987/ul8yqkh5dlzx0t9jrkzc84jz/image_1bbb6pbf811pesikkrmemt9du2d.png" alt="ceph mon"></p>
<ul>
<li>填写名称为mon，这个是容器名称，可以自定义</li>
<li>填写Image，这个填写下载好的ceph/daemon</li>
<li>填写command,这个填写mon，为固定值</li>
<li>填写Entry Ponit ,这个填写/entrypoint.sh，为固定值</li>
<li>填写Environment variable，这个填写两个变量<ul>
<li>MON_IP  192.168.8.106</li>
<li>CEPH_PUBLIC_NETWORK 192.168.0.0/16</li>
</ul>
</li>
</ul>
<p>填写完了切换第二个标签页Volumes<br><img src="http://static.zybuluo.com/zphj1987/euivfncy3gw2h25wjyc42ien/image_1bbb6rsb01etg1ebt1hr317lo1met2q.png" alt="volume"></p>
<ul>
<li>填写Volume<ul>
<li>/etc/ceph /etc/ceph</li>
<li>/var/lib/ceph/ /var/lib/ceph/</li>
</ul>
</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/2goxtvodd8fbd2aap5n7uc62/image_1bbb6tgov1kvr1rcc1keg1e0a1i4537.png" alt="network"></p>
<ul>
<li>填写Network为host</li>
<li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li>
</ul>
<p>没出异常的话，就可以进入console进行查询了<br><img src="http://static.zybuluo.com/zphj1987/pqn0zofgbx078kul5wzvn3mw/image_1bbb726491l5it2d1kf31at614lb3k.png" alt="console"><br>点击connect<br><img src="http://static.zybuluo.com/zphj1987/wgvt131186es0erb9kys6mqc/image_1bbb73gjif91s70a6f8pg1vg141.png" alt="image_1bbb73gjif91s70a6f8pg1vg141.png-79.5kB"><br>没有问题</p>
<h3 id="创建OSD">创建OSD</h3><p>点击增加容器<br><img src="http://static.zybuluo.com/zphj1987/myuenisz3pj34mawmdupfifp/image_1bbb6fpgmpgh1enf6pm1kk818q920.png" alt="add comn"></p>
<p><img src="http://static.zybuluo.com/zphj1987/v5kvkfunnbb0y3ueo95gptdr/image_1bbb7a1dm1gv1n4j1odoo3k1n2u4e.png" alt="osd0"></p>
<ul>
<li>填写Name，这个为容器名称，可以自定义</li>
<li>填写Image,这个为ceph/daemon,固定的值</li>
<li>填写command,这个为osd_ceph_disk，为定值</li>
<li>填写Entry Ponit ,这个填写/entrypoint.sh，为固定值</li>
<li>填写Environment variable，这个填写一个OSD磁盘变量<ul>
<li>OSD_DEVICE /dev/sdb</li>
</ul>
</li>
</ul>
<p>切换到第二个Volume标签页</p>
<ul>
<li>填写Volume<ul>
<li>/etc/ceph /etc/ceph</li>
<li>/var/lib/ceph/ /var/lib/ceph/</li>
<li>/dev/ /dev/</li>
</ul>
</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/3r350azqxged9dix7yaesia6/image_1bbb7aqg21jso1ku51mdgajtr0p4r.png" alt="osd0 add"></p>
<p>切换到Network标签页</p>
<ul>
<li>填写Network为host</li>
<li>填写hostname为宿主机的主机名<br>上面都填写完了后就点击create</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/7l50q9mdjffu2qmo0pwsl7yl/image_1bbb7c5d17b21o1uoc1i7h1cr458.png" alt="osdsd add"><br>切换到Security/Host标签页<br>勾选上 <code>privileged</code>,一定要选上，不然没有权限去格式化磁盘</p>
<p><img src="http://static.zybuluo.com/zphj1987/8thiyj24pflih4urdohkn1rt/image_1bbb7okcj8mj1c301tdb16mtecn5l.png" alt="osd addd "><br>上面都填写完了后就点击create<br>没出异常的话，就可以进入console进行查询了<br><img src="http://static.zybuluo.com/zphj1987/hwganai6phk6q2x5xrowfozc/image_1bbb7ufgk12nj1unpoq5taa1iah9.png" alt="good"></p>
<p>基本上一个简单的集群就配置好了，跨主机的情况，就提前把配置文件拷贝到另外一台主机，还有bootstrap keyring也拷贝过去，就可以了，这里就不做过多的赘述</p>
<h2 id="总结">总结</h2><p>本篇基于portainer以及一个现有的ceph容器做的部署实践，从整个操作来说，UI的部署，环境的搭建都非常的简单，这个得益于UI环境的简单，还有docker的封装，更多的玩法可以自己去探索，也可以运用这个UI做更多其他的容器操作</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-16</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker2.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>前一篇介绍了docker在命令行下面进行的ceph部署，本篇用docker的UI进行ceph的部署，目前来说市面上还没有一款能够比较简单就能直接在OS上面去部署Ceph的管理平台，这是因为OS的环境差异化太大，并且包的版本，以及各种软件的适配都可能造成失败，而docker比较固化环境，因此即使一个通用的UI也能很方便的部署出一个Cpeh集群</p>
<p>本篇就是对Docker UI部署集群做一个实践，对ceph了解，对docker了解，对dokcer的UI操作进行一定的了解的情况下，再做实践会比较好，总体上还是比较简单的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于docker部署ceph以及修改docker image]]></title>
    <link href="http://www.zphj1987.com/2017/03/15/base-on-docker-deploy-ceph/"/>
    <id>http://www.zphj1987.com/2017/03/15/base-on-docker-deploy-ceph/</id>
    <published>2017-03-15T04:05:35.000Z</published>
    <updated>2017-03-16T03:42:01.873Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker.png" alt="docker"><br></center>

<h2 id="前言">前言</h2><p>容器和ceph的结合已经在一些生产环境当中做了尝试，容器的好处就是对运行环境的一个封装，传统的方式是集成为ISO，这个需要一定的维护量，而容器的相关操作会简单很多，也就有了一些尝试，个人觉得如果玩的转容器可以考虑，当然得懂ceph，不然两套系统在一起，问题都不知道是哪个的，就比较麻烦了</p>
<p>本篇是基于之前我的填坑群里面的牛鹏举的一个问题，他的环境出现了创建osd的时候权限问题，我这边没遇到，现在实践了一遍，感觉应该是之前目录提前创建了的问题<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><h3 id="安装docker">安装docker</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install docker</span><br></pre></td></tr></table></figure>
<h3 id="下载ceph镜像">下载ceph镜像</h3><p>这个镜像是sebastien维护的，他是redhat的ceph工程师，ceph-ansible的负责人,很多一线的资料都是来自他的分享，这个是一个集成好的镜像<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker pull ceph/daemon</span><br></pre></td></tr></table></figure></p>
<p>准备好一些目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p /etc/ceph</span><br><span class="line">mkdir -p /var/lib/ceph/</span><br></pre></td></tr></table></figure></p>
<p>注意只需要做这个两个目录，不要创建子目录，docker内部有相关的操作</p>
<h3 id="创建一个mon">创建一个mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --net=host  --name=mon \</span><br><span class="line">-v /etc/ceph:/etc/ceph \</span><br><span class="line">-v /var/lib/ceph/:/var/lib/ceph \</span><br><span class="line"><span class="operator">-e</span> MON_IP=<span class="number">192.168</span>.<span class="number">8.106</span> \</span><br><span class="line"><span class="operator">-e</span> CEPH_PUBLIC_NETWORK=<span class="number">192.168</span>.<span class="number">0.0</span>/<span class="number">16</span> \</span><br><span class="line">ceph/daemon mon</span><br></pre></td></tr></table></figure>
<p>MON_IP就是宿主机的IP地址</p>
<p>执行完了后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment">#  docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED              STATUS              PORTS               NAMES</span><br><span class="line"><span class="number">86</span>ed05173432        ceph/daemon         <span class="string">"/entrypoint.sh mon"</span>   About a minute ago   Up <span class="number">59</span> seconds                           mon</span><br></pre></td></tr></table></figure></p>
<p>可以看到退出了，我们来docker logs -f mon看下日志的输出<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker logs -f mon</span></span><br><span class="line">/sbin/ip</span><br><span class="line">creating /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">creating /etc/ceph/ceph.mon.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-osd/ceph.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-mds/ceph.keyring</span><br><span class="line">creating /var/lib/ceph/bootstrap-rgw/ceph.keyring</span><br><span class="line">monmaptool: monmap file /etc/ceph/monmap-ceph</span><br><span class="line">monmaptool: <span class="built_in">set</span> fsid to cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">monmaptool: writing epoch <span class="number">0</span> to /etc/ceph/monmap-ceph (<span class="number">1</span> monitors)</span><br><span class="line">creating /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /etc/ceph/ceph.client.admin.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-osd/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-mds/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /var/lib/ceph/bootstrap-rgw/ceph.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">importing contents of /etc/ceph/ceph.mon.keyring into /tmp/ceph.mon.keyring</span><br><span class="line">ceph-mon: <span class="built_in">set</span> fsid to cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">ceph-mon: created monfs at /var/lib/ceph/mon/ceph-lab8106 <span class="keyword">for</span> mon.lab81</span><br></pre></td></tr></table></figure></p>
<p>提示成功了</p>
<p>我们看下生成的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /etc/ceph</span></span><br><span class="line">total <span class="number">16</span></span><br><span class="line">-rw------- <span class="number">1</span> root  root  <span class="number">137</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.client.admin.keyring</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root  root  <span class="number">285</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.conf</span><br><span class="line">-rw------- <span class="number">1</span> <span class="number">64045</span> <span class="number">64045</span>  <span class="number">77</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> ceph.mon.keyring</span><br><span class="line">-rw-r--r-- <span class="number">1</span> <span class="number">64045</span> <span class="number">64045</span> <span class="number">187</span> Mar <span class="number">14</span> <span class="number">17</span>:<span class="number">53</span> monmap-ceph</span><br></pre></td></tr></table></figure></p>
<p>从这里可以看到内部的cpeh的用户的id是64045，所以在docker宿主机不要随便去给ceph权限，可能id不匹配，容器内部还是无法操作</p>
<h3 id="创建一个osd">创建一个osd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run <span class="operator">-d</span> --net=host --name=myosd1 \</span><br><span class="line">--privileged=<span class="literal">true</span> \</span><br><span class="line">-v /etc/ceph:/etc/ceph \</span><br><span class="line">-v /var/lib/ceph/:/var/lib/ceph \</span><br><span class="line">-v /dev/:/dev/ \</span><br><span class="line"><span class="operator">-e</span> OSD_DEVICE=/dev/sdb \</span><br><span class="line">ceph/daemon osd_ceph_disk</span><br></pre></td></tr></table></figure>
<p>如果查询日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker logs <span class="operator">-f</span> myosd1</span><br></pre></td></tr></table></figure></p>
<p>如果执行命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon ceph <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<p>如果想进入容器内部<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon  /bin/bash</span><br></pre></td></tr></table></figure></p>
<p>修改集群的副本数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mon  ceph osd pool <span class="built_in">set</span> rbd size <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看集群状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker exec -it mon  ceph -s</span></span><br><span class="line">    cluster cb5df106-<span class="number">25</span>b3-<span class="number">4</span>f93-<span class="number">9</span>f54-baca2976a47b</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr no daemons active </span><br><span class="line">     osdmap e7: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds,require_kraken_osds</span><br><span class="line">      pgmap v15: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">34288</span> kB used, <span class="number">279</span> GB / <span class="number">279</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>上面的操作都很顺利，但是某些情况可能出现异常情况，或者镜像内部本身就有问题需要自己修改，这个怎么处理</p>
<h2 id="碰上问题想修改image">碰上问题想修改image</h2><p>我们看下我们运行的docker<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker ps </span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line"><span class="number">874</span>d78ccae55        ceph/daemon         <span class="string">"/entrypoint.sh osd_c"</span>   <span class="number">14</span> hours ago        Up <span class="number">14</span> hours                             myosd1</span><br><span class="line"><span class="number">86</span>ed05173432        ceph/daemon         <span class="string">"/entrypoint.sh mon"</span>     <span class="number">15</span> hours ago        Up <span class="number">15</span> hours                             mon</span><br></pre></td></tr></table></figure></p>
<p>COMMAND这里有个/entrypoint.sh</p>
<p>如果存在ENTRYPOINT和CMD，那么CMD就是ENTRYPOINT的参数，如果没有ENTRYPOINT，则CMD就是默认执行指令<br>也就是容器启动的时候默认是会去执行/entrypoint.sh 这个了</p>
<p>我们不需要他执行这个，就需要加参数了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker run -i -t --entrypoint /bin/bash ceph/daemon</span></span><br></pre></td></tr></table></figure></p>
<p>比如我上次做的一个操作，把ceph用户绑定到root的id<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">64045</span>:<span class="number">64045</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># sed -i 's/64045/0/g' /etc/passwd</span></span><br><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">0</span>:<span class="number">0</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>退出容器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@<span class="number">9</span>b269bf751f9:/<span class="comment"># exit</span></span><br></pre></td></tr></table></figure></p>
<p>查询我们最后运行的容器，修改回entrypoint我们再把容器修改提交到基础image<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES</span><br><span class="line"><span class="number">9</span>b269bf751f9        ceph/daemon         <span class="string">"/bin/bash"</span>         <span class="number">2</span> minutes ago       Exited (<span class="number">0</span>) <span class="number">15</span> seconds ago                       angry_hawking</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#  docker commit 9b269bf751f9 ceph/daemon</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker run -i -t --entrypoint /entrypoint.sh ceph/daemon</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># docker ps -l</span></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES</span><br><span class="line">c2ea602c18ac        ceph/daemon         <span class="string">"/entrypoint.sh"</span>    <span class="number">10</span> seconds ago      Exited (<span class="number">1</span>) <span class="number">7</span> seconds ago                       ecstatic_bartik</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># docker commit c2ea602c18ac ceph/daemon</span></span><br></pre></td></tr></table></figure></p>
<p>再次启动容器,并且检查内容，可以看到已经修改好了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># docker run -i -t --entrypoint /bin/bash ceph/daemon</span></span><br><span class="line">root@<span class="number">65</span>b538fdc61e:/<span class="comment"># cat /etc/passwd|grep ceph</span></span><br><span class="line">ceph:x:<span class="number">0</span>:<span class="number">0</span>:Ceph storage service:/var/lib/ceph:/bin/<span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<p>如果需要做其他的改动，这样改下就行</p>
<h2 id="总结">总结</h2><p>本篇主要是根据sebastien的镜像做的部署，并且给出一些常用的命令，以及如何进入固化的容器的内部进行修改，方便自己调试环境</p>
<h2 id="相关资料">相关资料</h2><p><a href="http://www.sebastien-han.fr/blog/2015/06/23/bootstrap-your-ceph-cluster-in-docker/" target="_blank" rel="external">bootstrap-your-ceph-cluster-in-docker/</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-15</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/docker.png" alt="docker"><br></center>

<h2 id="前言">前言</h2><p>容器和ceph的结合已经在一些生产环境当中做了尝试，容器的好处就是对运行环境的一个封装，传统的方式是集成为ISO，这个需要一定的维护量，而容器的相关操作会简单很多，也就有了一些尝试，个人觉得如果玩的转容器可以考虑，当然得懂ceph，不然两套系统在一起，问题都不知道是哪个的，就比较麻烦了</p>
<p>本篇是基于之前我的填坑群里面的牛鹏举的一个问题，他的环境出现了创建osd的时候权限问题，我这边没遇到，现在实践了一遍，感觉应该是之前目录提前创建了的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[在线动态修改ulimit]]></title>
    <link href="http://www.zphj1987.com/2017/03/06/online-change-ulimit/"/>
    <id>http://www.zphj1987.com/2017/03/06/online-change-ulimit/</id>
    <published>2017-03-06T10:20:26.000Z</published>
    <updated>2017-03-06T10:28:35.755Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/limit.jpg" alt="limit"><br></center>

<h2 id="前言">前言</h2><p>系统中有些地方会进行资源的限制，其中的一个就是open file的限制，操作系统默认限制的是1024,这个值可以通过各种方式修改，本篇主要讲的是如何在线修改，生产上是不可能随便重启进程的<br><a id="more"></a></p>
<h2 id="实践">实践</h2><h3 id="查看系统默认的限制">查看系统默认的限制</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ulimit -a|grep open</span></span><br><span class="line">open files                      (-n) <span class="number">1024</span></span><br></pre></td></tr></table></figure>
<p>默认的打开文件是1024<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ps -ef|grep ceph-osd</span></span><br><span class="line">ceph     <span class="number">28176</span>     <span class="number">1</span>  <span class="number">0</span> <span class="number">18</span>:<span class="number">08</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> /usr/bin/ceph-osd <span class="operator">-f</span> --cluster ceph --id <span class="number">0</span> --setuser ceph --setgroup ceph</span><br><span class="line">root     <span class="number">28619</span> <span class="number">26901</span>  <span class="number">0</span> <span class="number">18</span>:<span class="number">10</span> pts/<span class="number">3</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep --color=auto ceph-osd</span><br><span class="line">[root@lab8106 ~]<span class="comment"># cat /proc/28176/limits |grep open</span></span><br><span class="line">Max open files            <span class="number">1048576</span>              <span class="number">1048576</span>              files</span><br></pre></td></tr></table></figure></p>
<p>ceph osd的进程的这个参数是1048576 </p>
<h3 id="通过配置文件修改">通过配置文件修改</h3><p>这个参数控制是放在：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat  /usr/lib/systemd/system/ceph-osd@.service |grep LimitNOFILE -B 1</span></span><br><span class="line">[Service]</span><br><span class="line">LimitNOFILE=<span class="number">1048576</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方设置的，如果我们有需要修改，那么可以修改这里，这不是本篇的重点，对于运行中的进程如何修改呢</p>
<h3 id="在线修改进程的limit">在线修改进程的limit</h3><p>这里调用的是prlimit进行的在线修改<br>查询指定进程的限制<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176</span></span><br><span class="line">RESOURCE   DESCRIPTION                             SOFT      HARD UNITS</span><br><span class="line">AS         address space <span class="built_in">limit</span>                unlimited unlimited bytes</span><br><span class="line">CORE       max core file size                         <span class="number">0</span> unlimited blocks</span><br><span class="line">CPU        CPU time                           unlimited unlimited seconds</span><br><span class="line">DATA       max data size                      unlimited unlimited bytes</span><br><span class="line">FSIZE      max file size                      unlimited unlimited blocks</span><br><span class="line">LOCKS      max number of file locks held      unlimited unlimited </span><br><span class="line">MEMLOCK    max locked-in-memory address space     <span class="number">65536</span>     <span class="number">65536</span> bytes</span><br><span class="line">MSGQUEUE   max bytes <span class="keyword">in</span> POSIX mqueues            <span class="number">819200</span>    <span class="number">819200</span> bytes</span><br><span class="line">NICE       max nice prio allowed to raise             <span class="number">0</span>         <span class="number">0</span> </span><br><span class="line">NOFILE     max number of open files             <span class="number">1048576</span>   <span class="number">1048576</span> </span><br><span class="line">NPROC      max number of processes              <span class="number">1048576</span>   <span class="number">1048576</span> </span><br><span class="line">RSS        max resident <span class="built_in">set</span> size              unlimited unlimited pages</span><br><span class="line">RTPRIO     max real-time priority                     <span class="number">0</span>         <span class="number">0</span> </span><br><span class="line">RTTIME     timeout <span class="keyword">for</span> real-time tasks        unlimited unlimited microsecs</span><br><span class="line">SIGPENDING max number of pending signals         <span class="number">192853</span>    <span class="number">192853</span> </span><br><span class="line">STACK      max stack size                       <span class="number">8388608</span> unlimited bytes</span><br></pre></td></tr></table></figure></p>
<p>修改指定运行进程的限制<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176 --nofile=104857</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># prlimit --pid 28176 |grep NOFILE</span></span><br><span class="line">NOFILE     max number of open files              <span class="number">104857</span>    <span class="number">104857</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到修改成功了</p>
<h2 id="总结">总结</h2><p>一般来说ulimit这个限制都是在终端上修改对下次生效，本篇用来记录如何在线修改，如果碰到了，可以这样处理</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-03-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/limit.jpg" alt="limit"><br></center>

<h2 id="前言">前言</h2><p>系统中有些地方会进行资源的限制，其中的一个就是open file的限制，操作系统默认限制的是1024,这个值可以通过各种方式修改，本篇主要讲的是如何在线修改，生产上是不可能随便重启进程的<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[python执行rados命令例子]]></title>
    <link href="http://www.zphj1987.com/2017/02/28/python-command-rados-sample/"/>
    <id>http://www.zphj1987.com/2017/02/28/python-command-rados-sample/</id>
    <published>2017-02-28T15:48:15.000Z</published>
    <updated>2017-02-28T16:07:00.766Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/we-love-python-800-600.png" alt="python"><br></center>

<h2 id="前言">前言</h2><p>我们以前的管理平台在python平台下面做的，内部做的一些操作采用的是命令执行，然后解析的方式去做的，ceph自身有python的rados接口，可以直接调用原生接口，然后直接解析json的方式，这样更靠近底层<br><a id="more"></a><br>在看ceph-dash内部的实现的时候，发现里面的获取集群信息的代码可以留存备用</p>
<h2 id="代码实例">代码实例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">from rados import Rados</span><br><span class="line">from rados import Error as RadosError</span><br><span class="line"></span><br><span class="line">class CephClusterCommand(dict):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span><br><span class="line">    Issue a ceph command on the given cluster and provide the returned json</span><br><span class="line">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, cluster, **kwargs):</span><br><span class="line">        dict.__init__(self)</span><br><span class="line">        ret, buf, err = cluster.mon_<span class="built_in">command</span>(json.dumps(kwargs), <span class="string">''</span>, timeout=<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">if</span> ret != <span class="number">0</span>:</span><br><span class="line">            self[<span class="string">'err'</span>] = err</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.update(json.loads(buf))</span><br><span class="line"></span><br><span class="line">config=&#123;<span class="string">'conffile'</span>: <span class="string">'/etc/ceph/ceph.conf'</span>, <span class="string">'conf'</span>: &#123;&#125;&#125;</span><br><span class="line">with Rados(**config) as cluster:</span><br><span class="line">    cluster_status = CephClusterCommand(cluster, prefix=<span class="string">'status'</span>, format=<span class="string">'json'</span>)</span><br><span class="line">    <span class="built_in">print</span> cluster_status</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>调用原生接口的好处在于,只需要很少的库就可以取得监控系统所需要的值</p>
<p>最近在研究系统的时候发现一个问题</p>
<blockquote>
<p>跟着错误的文档实践只会掉进同一个坑</p>
</blockquote>
<p>在遇到一个小的错误的时候，翻到了一个github的Issue，然后看到一个人把自己的配置过程和配置文件详详细细的都写在Issue下面，然后就跟着他的过程走了一遍，发现不论怎么弄都是同样的错误</p>
<p>而返回去根据另一个正确的文档又走一遍的时候，发现终于跑通了，回顾了一遍，发现是那个错误的过程里面的配置文件里面是有配置项目，不兼容的，而软件也没有抛出相关的错误，然后在同一个地方找了两天</p>
<p>所以如果有碰到无法解决的操作步骤文档的时候，就尽量不要去根据那个文档操作了，除非自己对细节弄的很清楚了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-02-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/we-love-python-800-600.png" alt="python"><br></center>

<h2 id="前言">前言</h2><p>我们以前的管理平台在python平台下面做的，内部做的一些操作采用的是命令执行，然后解析的方式去做的，ceph自身有python的rados接口，可以直接调用原生接口，然后直接解析json的方式，这样更靠近底层<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
