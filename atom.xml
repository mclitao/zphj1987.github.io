<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-10-17T07:29:44.528Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Ceph技能树]]></title>
    <link href="http://www.zphj1987.com/2016/10/17/Ceph%E6%8A%80%E8%83%BD%E6%A0%91/"/>
    <id>http://www.zphj1987.com/2016/10/17/Ceph技能树/</id>
    <published>2016-10-17T06:32:50.000Z</published>
    <updated>2016-10-17T07:29:44.528Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>
<a id="more"></a>
<h2 id="二、技能图">二、技能图</h2><p><center></center></p>
<p><iframe width="1050" height="720" src="https://coggle.it/diagram/WAR1HK8sAvMFM_kd/a0b529bd1fc87c52d12a2b18a5b602929218d60384ca9664cc243e1cbcf72649" frameborder="0" allowfullscreen></iframe><br><br>本图支持下载为pdf,PNG，或者mm文件，Chrome支持滚轮缩放，下载PDF效果还不错</p>
<h2 id="三、变更记录">三、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephskill/cephskill.png" alt="cephskill"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前在ceph中国社区看到有一个ceph的技能树，因为是一个图片，所以没法编辑，修改，所以完全copy了一份，方便查看，如果有侵犯到原作者的版权，欢迎沟通，从内容来看，感觉出自有云的一位工程师</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中国社区QQ群数据分析]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph中国社区QQ群数据分析/</id>
    <published>2016-10-13T15:59:58.000Z</published>
    <updated>2016-10-13T16:09:33.153Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>
<a id="more"></a>
<h2 id="三、数据展示">三、数据展示</h2><p>第一张图包含信息如下</p>
<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephchina/ceph%E7%A4%BE%E5%8C%BA.png" alt=""><br></center>
</li>
<li><p>男性居多</p>
</li>
<li>80后居多</li>
</ul>
<p>图示的好处就是告诉你，多，大概多成一个什么程度</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>最近对数据比较感兴趣，正好想练下手，看下从给定的数据当中能提取到多少信息，数据分析主要借助的是可视化的工具，这个在一般进行性能测试当中是很有用的，当然数据的采集，数据的提取，数据的展现，都是非常细的活</p>
<blockquote>
<p>状态：本篇未完成，进行中</p>
</blockquote>
<h2 id="二、数据来源">二、数据来源</h2><p>本次数据来源来自Ceph中国社区QQ群，是基于群成员填写的信息来进行分析，这中间就不对填写的信息进行真伪的辨别，基于能提取到的数据进行数据处理</p>
<p>目前实现下面的数据</p>
<ul>
<li>性别分布</li>
<li>Q龄分布</li>
<li><p>年龄分布</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph的参数mon_osd_down_out_subtree_limit细解]]></title>
    <link href="http://www.zphj1987.com/2016/10/13/Ceph%E7%9A%84%E5%8F%82%E6%95%B0mon-osd-down-out-subtree-limit%E7%BB%86%E8%A7%A3/"/>
    <id>http://www.zphj1987.com/2016/10/13/Ceph的参数mon-osd-down-out-subtree-limit细解/</id>
    <published>2016-10-13T03:34:29.000Z</published>
    <updated>2016-10-13T03:53:59.910Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br><a id="more"></a><br>参数，那么这个参数做什么用的，我们来看看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph --show-config|grep mon_osd_down_out_subtree_limit</span></span><br><span class="line">mon_osd_down_out_subtree_<span class="built_in">limit</span> = rack</span><br></pre></td></tr></table></figure></p>
<p>首先解释下这个参数是做什么的，这个是控制标记为out的最小子树(bucket)，默认的这个为rack，这个可能我们平时感知不到这个有什么作用，大部分情况下，我们一般都为主机分组或者做了故障域，也很少做到测试去触发它，本篇文章将告诉你这个参数在什么情况下生效，对我们又有什么作用</p>
<p>准备两个物理节点，每个节点上3个osd，一共六个osd，上面的down out的时间已经修改为20s，那么会在20s后出现out的情况</p>
<h2 id="三、测试过程">三、测试过程</h2><h3 id="3-1_测试默认参数停止一台主机单个OSD">3.1 测试默认参数停止一台主机单个OSD</h3><p>首先用默认的<code>mon_osd_down_out_subtree_limit = rack</code>去做测试<br>开启几个监控终端方便观察<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph -w</span><br><span class="line">watch ceph osd tree</span><br></pre></td></tr></table></figure></p>
<p><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/mutiscreen1.png" alt="screen"></p>
<p>在其中的一台上执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:15:39.673898 mon.0 [INF] osd.5 out (down for 20.253201)&#10;2016-10-13 10:15:39.757399 mon.0 [INF] osdmap e60: 6 osds: 5 up, 5 in</span><br></pre></td></tr></table></figure></p>
<p>停止一个后正常out</p>
<h3 id="3-2_测试默认参数停止掉一台主机所有osd">3.2 测试默认参数停止掉一台主机所有osd</h3><p>我们再来停止一台主机所有osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:17:09.699129 mon.0 [INF] osd.3 out (down for 23.966959)&#10;2016-10-13 10:17:09.699178 mon.0 [INF] osd.4 out (down for 23.966958)&#10;2016-10-13 10:17:09.699222 mon.0 [INF] osd.5 out (down for 23.966958)</span><br></pre></td></tr></table></figure></p>
<p>可以看到这台主机上的节点全部都正常out了</p>
<h3 id="3-3_测试修改参数后停止一台主机单个OSD">3.3 测试修改参数后停止一台主机单个OSD</h3><p>我们再调整下参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = rack</span><br></pre></td></tr></table></figure></p>
<p>将这个参数设置为host<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_subtree_limit = host</span><br></pre></td></tr></table></figure></p>
<p>重启所有的进程，让配置生效，我们测试下只断一个osd的时候能不能out<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd@<span class="number">5</span></span><br></pre></td></tr></table></figure></p>
<p>停止掉osd.5<br>测试输出<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">2016-10-13 10:48:45.612206 mon.0 [INF] osd.5 out (down for 21.966238)</span><br></pre></td></tr></table></figure></p>
<p>可以看到可以osd.5可以正常的out</p>
<h3 id="3-4_测试修改参数后停止一台主机所有OSD">3.4 测试修改参数后停止一台主机所有OSD</h3><p>我们再来停止lab8107的所有的osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>停止掉 lab8107 所有的osd,可以看到没有out了,这个是因为把故障out设置为host级别了，这个地方出现host级别故障的时候，就不进行迁移了</p>
<h2 id="四、总结">四、总结</h2><p>关键的地方在于总结了，首先我们要想一想，ceph机器的迁移开不开（noout），关于这个问题，一定有两个答案</p>
<ul>
<li>开，不开的话，盘再坏怎么办，就会丢数据了</li>
<li>不开，人工触发，默认的情况下迁移数据会影响前端业务</li>
</ul>
<p>这里这个参数其实就是将我们的问题更加细腻的控制了，我们现在根据这个参数就能做到，迁移可以开，坏掉一个盘的时候我让它迁移，一个盘的数据恢复影响和时间是可以接受的，主机损坏我不让他迁移，为什么？主机损坏你去让他迁移，首先会生成一份数据，等主机好了，数据又要删除一份数据，这个对于磁盘都是消耗，主机级别的故障一定是可修复的，这个地方主机down机，主机电源损坏，这部分数据都是在的，那么这个地方就是需要人工去做这个修复的工作的，对于前端的服务是透明的，默认的控制是down rack才不去标记out，这个当然你也可以控制为这个，比如有个rack掉电，就不做恢复，如果down了两台主机，让他去做恢复，当然个人不建议这么做，这个控制就是自己去判断这个地方需要做不</p>
<p>ceph里面还是提供了一些细微粒度的控制，值得去与实际的应用场景结合，当然默认的参数已经能应付大部分的场景，控制的更细只是让其变得更好</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/screen/roadmap.png" alt="参数"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前跟一个朋友沟通一个其他的问题的时候，发现了有一个参数 <code>mon osd down out subtree limit</code> 一直没有接触到，看了一下这个参数还是很有作用的，本篇将讲述这个参数的作用和使用的场景</p>
<h2 id="二、测试环境准备">二、测试环境准备</h2><p>首先配置一个集群环境，配置基本参数<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">mon_osd_down_out_interval = 20</span><br></pre></td></tr></table></figure></p>
<p>调整这个参数为20s,默认为300s,默认一个osd,down超过300s就会标记为out，然后触发迁移,这个是为了方便尽快看到测试的效果，很多测试都是可以这样缩短测试周期的</p>
<p>本次测试关心的是这个参数<code>mon osd down out subtree limit</code><br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[创建一个自定义名称的Ceph集群]]></title>
    <link href="http://www.zphj1987.com/2016/10/12/%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E7%9A%84Ceph%E9%9B%86%E7%BE%A4/"/>
    <id>http://www.zphj1987.com/2016/10/12/创建一个自定义名称的Ceph集群/</id>
    <published>2016-10-12T02:44:17.000Z</published>
    <updated>2016-10-12T02:45:39.610Z</updated>
    <content type="html"><![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>
<a id="more"></a>
<h2 id="三、简单的创建过程">三、简单的创建过程</h2><p>创建mon<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph mon create lab8107</span><br></pre></td></tr></table></figure></p>
<p>获取部署密钥<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph gatherkeys lab8107</span><br></pre></td></tr></table></figure></p>
<p>部署osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy  --cluster myceph osd prepare lab8107:/dev/sdb</span><br><span class="line">ceph-deploy  --cluster myceph osd activate lab8107:/dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>查询集群状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph --cluster myceph <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<h2 id="四、总结">四、总结</h2><p>最简单的修改名称主要步骤就在这里了，关键部分就是修改那几个文件里面的集群的名称，这个里面是用一个变量写成了ceph，根据自己的需要进行修改即可</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="一、前言">一、前言</h2><p>这里有个条件，系统环境是Centos 7 ,Ceph 的版本为Jewel版本，因为这个组合下是由systemctl来进行服务控制的，所以需要做稍微的改动即可实现</p>
<h2 id="二、准备工作">二、准备工作</h2><p>部署mon的时候需要修改这个几个文件<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">/usr/lib/systemd/system/ceph-mon@.service&#10;/usr/lib/systemd/system/ceph-create-keys@.service&#10;/usr/lib/systemd/system/ceph-osd@.service&#10;/usr/lib/systemd/system/ceph-mds@.service</span><br></pre></td></tr></table></figure></p>
<p>将 <code>Environment=CLUSTER=ceph</code> 改成 <code>Environment=CLUSTER=myceph</code> 后面的myceph可以为你自定义的名称</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[编译一个支持多线程的php安装包]]></title>
    <link href="http://www.zphj1987.com/2016/10/10/%E7%BC%96%E8%AF%91%E4%B8%80%E4%B8%AA%E6%94%AF%E6%8C%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84php%E5%AE%89%E8%A3%85%E5%8C%85/"/>
    <id>http://www.zphj1987.com/2016/10/10/编译一个支持多线程的php安装包/</id>
    <published>2016-10-10T04:27:18.000Z</published>
    <updated>2016-10-10T04:44:29.648Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br><a id="more"></a></p>
<h2 id="二、获取源码">二、获取源码</h2><p>地址：<br><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="symbol">https:</span>/<span class="regexp">/uk.repo.webtatic.com/yum</span><span class="regexp">/el7/</span><span class="constant">SRPMS/RPMS/</span></span><br></pre></td></tr></table></figure></p>
<p>这个是webtatic发行的php版本，做了一些修改和优化</p>
<p>选择需要的版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># wget https://uk.repo.webtatic.com/yum/el7/SRPMS/RPMS/php56w-5.6.26-1.w7.src.rpm</span></span><br></pre></td></tr></table></figure></p>
<p>解压安装包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpm2cpio php56w-5.6.26-1.w7.src.rpm |cpio -div</span></span><br></pre></td></tr></table></figure></p>
<p>解压完成了后，当前目录下面会有很多文件<br>修改当前目录下面的php56.spec<br>在编译相关的configure后面增加</p>
<blockquote>
<p>—enable-maintainer-zts</p>
</blockquote>
<p>拷贝解压和修改的文件到源码编译目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># cp -ra * /root/rpmbuild/SOURCES/</span></span><br></pre></td></tr></table></figure></p>
<h2 id="三、编译rpm包">三、编译rpm包</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 myphp]<span class="comment"># rpmbuild -bb php56.spec</span></span><br></pre></td></tr></table></figure>
<p>如果提示缺依赖，就把相关的依赖包安装好就可以了，编译环境最好跟最终使用环境是一样的环境，执行完成了以后，会生成rpm安装包</p>
<h2 id="四、增加多线程支持">四、增加多线程支持</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pecl install pthreads-<span class="number">2.0</span>.<span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>这个会下载源码，然后自动编译成可用的内核模块，将这个内核模块的配置文件和模块文件拷贝到最终使用环境即可</p>
<p>检查是否安装成功<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># php -m|grep pth</span></span><br><span class="line">pthreads</span><br></pre></td></tr></table></figure></p>
<p>可用看到已经支持了</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/php/php-elephant-01.png" alt=""><br></center></p>
<h2 id="一、前言">一、前言</h2><p>因为项目上的需要，需要用到php，一般来说，用默认的版本和配置就可以满足大多数的场景，因为需要加入多线程，所以需要自己编译一个包</p>
<p>一般来说，发行的包的版本的配置选项和代码都是最稳定的，所以在大多数情况下，我都不会直接去拿原始的源码做编译，这里我的经验是用别人发布版本的源码包，然后根据自己的需要，做修改，然后打包，这次的处理方法还是一样<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[fio测试数据的可视化]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/fio%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://www.zphj1987.com/2016/09/28/fio测试数据的可视化/</id>
    <published>2016-09-28T10:13:04.000Z</published>
    <updated>2016-09-28T10:17:48.586Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br><a id="more"></a></p>
<h2 id="图例">图例</h2><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/fio/fiokeshuhua.png" alt=""><br></center>

<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>近期会做一个事情，把fio的数据可视化，目前有gfio可以动态的获取状态，希望能够对已经产生的数据进行分析</p>
<p>目前处于起步数据分析阶段，通过python获取需要的数据输出到csv，然后对csv进行综合的输出，从而能够清楚的从大量数据当中得到想要的效果<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python生成csv中文乱码解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/09/28/Python%E7%94%9F%E6%88%90csv%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/28/Python生成csv中文乱码的解决办法/</id>
    <published>2016-09-28T06:36:43.000Z</published>
    <updated>2016-09-28T06:39:34.698Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br><a id="more"></a></p>
<h2 id="二、处理过程">二、处理过程</h2><h3 id="原始代码">原始代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line"><span class="comment">#import codecs</span></span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line"><span class="comment">#    csvfile.write(codecs.BOM_UTF8)</span></span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>运行以后：<br>Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/2cve2nr8jyy4chs7kvur5wwt/image_1atnnp5i41b7lf7tumgj6175k9.png" alt="image_1atnnp5i41b7lf7tumgj6175k9.png-4.3kB"></p>
<h3 id="修改代码">修改代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import csv</span><br><span class="line">import codecs</span><br><span class="line">with open(<span class="string">'test.csv'</span>, <span class="string">'wb'</span>) as csvfile:</span><br><span class="line">    csvfile.write(codecs.BOM_UTF8)</span><br><span class="line">    spamwriter = csv.writer(csvfile, dialect=<span class="string">'excel'</span>)</span><br><span class="line">    spamwriter.writerow([<span class="string">'测试'</span>] * <span class="number">5</span> + [<span class="string">'Baked Beans'</span>])</span><br><span class="line">    spamwriter.writerow([<span class="string">'Spam'</span>, <span class="string">'Lovely Spam'</span>, <span class="string">'Wonderful Spam'</span>])</span><br></pre></td></tr></table></figure>
<p>跟上面的代码相比，引入了两行代码</p>
<blockquote>
<p>import codecs<br>csvfile.write(codecs.BOM_UTF8)</p>
</blockquote>
<p>我们再来看效果Linux下的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat test.csv </span></span><br><span class="line">测试,测试,测试,测试,测试,Baked Beans</span><br><span class="line">Spam,Lovely Spam,Wonderful Spam</span><br></pre></td></tr></table></figure></p>
<p>Windows下打开的效果<br><img src="http://static.zybuluo.com/zphj1987/k9m15wfa83wbrhuc6b0xyftg/image_1atnnsp1713931d1h1e641l4f13kim.png" alt="image_1atnnsp1713931d1h1e641l4f13kim.png-3.5kB"><br>问题解决</p>
<h2 id="三、总结">三、总结</h2><p>网上找了一些资料，这个方式比较快而简单，就先用这个方式解决，方法有很多</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/csv/parsing-csv-dribbble.gif" alt="csv"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>在Linux下面用python进行数据处理，然后输出为csv格式，如果没有中文一切正常，但是如果有中文，就会出现乱码的问题,本篇将讲述怎么处理这个问题<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[不小心清空了Ceph的OSD的分区表如何恢复]]></title>
    <link href="http://www.zphj1987.com/2016/09/24/%E4%B8%8D%E5%B0%8F%E5%BF%83%E6%B8%85%E7%A9%BA%E4%BA%86Ceph%E7%9A%84OSD%E7%9A%84%E5%88%86%E5%8C%BA%E8%A1%A8%E5%A6%82%E4%BD%95%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/09/24/不小心清空了Ceph的OSD的分区表如何恢复/</id>
    <published>2016-09-23T16:56:27.000Z</published>
    <updated>2016-09-23T17:07:47.286Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br><a id="more"></a></p>
<h2 id="二、实践过程">二、实践过程</h2><p>我们现在有一个正常的集群，我们假设这些分区都是一致的，用的是默认的分区的方式，我们先来看看默认的分区方式是怎样的</p>
<h3 id="2-1_破坏环境">2.1 破坏环境</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list</span></span><br><span class="line">···</span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 ceph data, active, cluster ceph, osd.<span class="number">0</span>, journal /dev/sdb2</span><br><span class="line"> /dev/sdb2 ceph journal, <span class="keyword">for</span> /dev/sdb1</span><br><span class="line">···</span><br></pre></td></tr></table></figure>
<p>查看分区情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>来一个破坏，这里是破坏 <code>osd.0</code>，对应盘符 <code>/dev/sdb</code><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-deploy disk zap lab8106:/dev/sdb</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">34</span>): /usr/bin/ceph-deploy disk zap lab8106:/dev/sdb</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] Warning: The kernel is still using the old partition table.</span><br><span class="line">[lab8106][DEBUG ] The new table will be used at the next reboot.</span><br><span class="line">[lab8106][DEBUG ] GPT data structures destroyed! You may now partition the disk using fdisk or</span><br><span class="line">[lab8106][DEBUG ] other utilities.</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>即使这个 osd 被使用在，还是被破坏了，这里假设上面的就是一个误操作，我们看下带来了哪些变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">58</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">02</span> /var/lib/ceph/osd/ceph-<span class="number">0</span>/journal -&gt; /dev/disk/by-partuuid/bd81471d-<span class="number">13</span>ff-<span class="number">44</span>ce-<span class="number">8</span>a33-<span class="number">92</span>a8df9e8eee</span><br></pre></td></tr></table></figure></p>
<p>如果你用命令行看，就可以看到上面的链接已经变红了，分区没有了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-disk  list </span></span><br><span class="line">/dev/sdb :</span><br><span class="line"> /dev/sdb1 other, xfs, mounted on /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line"> /dev/sdb2 other</span><br></pre></td></tr></table></figure></p>
<p>已经跟上面有变化了，没有ceph的相关信息了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br></pre></td></tr></table></figure></p>
<p>分区表完全没有信息了，到这我们可以确定分区表完全没了，如果现在重启将会发生什么？重启以后这个磁盘就是一个裸盘，没有分区的裸盘</p>
<h4 id="2-2_处理办法">2.2 处理办法</h4><p>首先一个办法就是当这个OSD坏了，然后直接按照删除节点，添加节点就可以了，这个应该是最主流，最通用的处理办法，但是这个在生产环境环境当中造成的数据迁移还是非常大的，我们尝试做恢复，这就是本篇主要讲的东西</p>
<h5 id="关闭迁移">关闭迁移</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd set noout</span></span><br></pre></td></tr></table></figure>
<h5 id="停止OSD">停止OSD</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>现在的OSD还是有进程的，所以需要停止掉再做处理<br>通过其他节点查看分区的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdc  unit s print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdc: <span class="number">585937500</span>s</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start     End         Size        File system  Name          Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">2048</span>s     <span class="number">2097152</span>s    <span class="number">2095105</span>s                 ceph journal</span><br><span class="line"> <span class="number">1</span>      <span class="number">2099200</span>s  <span class="number">585937466</span>s  <span class="number">583838267</span>s  xfs          ceph data</span><br></pre></td></tr></table></figure></p>
<p>我们现在进行分区表的恢复，记住上面的数值，我print的时候是加了unit s这个是要精确的值的,下面的创建会用到的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2099200s 585937466s</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  mkpart  primary  2048s 2097152s</span></span><br></pre></td></tr></table></figure>
<p>我们再来检查下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted -s /dev/sdb  print</span></span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdb: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name     Flags</span><br><span class="line"> <span class="number">2</span>      <span class="number">1049</span>kB  <span class="number">1074</span>MB  <span class="number">1073</span>MB               primary</span><br><span class="line"> <span class="number">1</span>      <span class="number">1075</span>MB  <span class="number">300</span>GB   <span class="number">299</span>GB   xfs          primary</span><br></pre></td></tr></table></figure></p>
<p>分区表已经回来了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># umount /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># partprobe</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新挂载看看，没有问题，还要做下其他的处理<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-0/journal</span></span><br></pre></td></tr></table></figure></p>
<p>我们先删除掉journal的链接文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-osd -i 0 --osd-journal=/dev/sdb2 --mkjournal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">09</span>-<span class="number">24</span> <span class="number">00</span>:<span class="number">36</span>:<span class="number">06.595992</span> <span class="number">7</span>f9d0afbc880 -<span class="number">1</span> created new journal /dev/sdb2 <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ln -s /dev/sdb2 /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">9</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">37</span> journal -&gt; /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>上面操作就是创建journal相关的,注意下我上面的操作—osd-journal=/dev/sdb2这个地方，我是便于识别，这个地方要写上dev/sdb2的uuid的路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># ll /dev/disk/by-partuuid/03fc6039-ad80-4b8d-86ec-aeee14fb3bb6 </span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Sep <span class="number">24</span> <span class="number">00</span>:<span class="number">33</span> /dev/disk/by-partuuid/<span class="number">03</span><span class="built_in">fc</span>6039-ad80-<span class="number">4</span>b8d-<span class="number">86</span>ec-aeee14fb3bb6 -&gt; ../../sdb2</span><br></pre></td></tr></table></figure></p>
<p>也就是这个链接的这一串，这个防止盘符串了情况下journal无法找到的问题</p>
<h4 id="启动osd">启动osd</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">0</span>]<span class="comment"># systemctl start ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>检查下，到这osd就正常的恢复了</p>
<h2 id="三、为什么有这篇">三、为什么有这篇</h2><p>一直都知道分区表是可以恢复的，也一直知道会有误操作，但是一直没有去把ceph中完整流程走下来，前两天一个哥们环境副本一，然后自己给搞错了，出现不得不恢复的情况，正好自己一直想把这个问题的处理办法给记录下来，所以就有了这篇，万一哪天有人碰到了，就把这篇发给他</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-24</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery/recuvaicon.png" alt="disk"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>如果你是新手，应该出现过敲盘符的时候，敲错的情况，有些操作可能没什么问题，查询类的操作都没问题，但是写入的情况，就可能比较麻烦了，当然老手也可能有误操作，本篇将讲述在误操作把分区表给弄丢了的情况，来看看我们应该如何恢复<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph的Mon数据重新构建工具]]></title>
    <link href="http://www.zphj1987.com/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/"/>
    <id>http://www.zphj1987.com/2016/09/20/Ceph的Mon数据重新构建工具/</id>
    <published>2016-09-20T08:09:53.000Z</published>
    <updated>2016-10-12T09:11:02.645Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心 Mon 万一三个同时都挂掉了怎么办，那么集群所有的数据是不是都丢了，关于后台真实数据恢复，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，Ceph 的中国（Redhat）的一位开发者 <a href="https://github.com/tchaikov" target="_blank" rel="external">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到 Ceph 的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292" target="_blank" rel="external">issue</a></p>
<a id="more"></a>
<h2 id="打包一个合进新代码的master版本的ceph包">打包一个合进新代码的master版本的ceph包</h2><h3 id="从github上面获取代码">从github上面获取代码</h3><p>默认的分支就是master的直接去clone就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># git clone https://github.com/ceph/ceph.git</span></span><br></pre></td></tr></table></figure></p>
<h3 id="检查是否是master分支">检查是否是master分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># git branch</span></span><br><span class="line">* master</span><br></pre></td></tr></table></figure>
<h3 id="检查代码是否是合进需要的代码了">检查代码是否是合进需要的代码了</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat ceph/doc/rados/troubleshooting/troubleshooting-mon.rst |grep rebuild</span></span><br><span class="line">  <span class="comment"># rebuild the monitor store from the collected map, if the cluster does not</span></span><br><span class="line">  <span class="comment"># i.e. use "ceph-monstore-tool /tmp/mon-store rebuild" instead</span></span><br><span class="line">  ceph-monstore-tool /tmp/mon-store rebuild -- --keyring /path/to/admin.keyring</span><br><span class="line"><span class="comment">#. then rebuild the store</span></span><br></pre></td></tr></table></figure>
<p>因为这个代码是最近才合进去的 ，所以一定要检查代码的正确性</p>
<h3 id="创建一个源码包">创建一个源码包</h3><p>进入到代码的根目录，修改make-dist文件里面的一个地方(第46行)，否则打出来的包可能没有版本号，因为打包的时候检查了有没有git目录<br>修改下面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tar cvf $outfile.version.tar $outfile/src/.git_version $outfile/src/ceph_ver.h $outfile/ceph.spec</span></span><br><span class="line">tar cvf <span class="variable">$outfile</span>.version.tar <span class="variable">$outfile</span>/src/.git_version <span class="variable">$outfile</span>/src/ceph_ver.h <span class="variable">$outfile</span>/ceph.spec <span class="variable">$outfile</span>/.git</span><br></pre></td></tr></table></figure></p>
<h4 id="如果不改，就可能出现">如果不改，就可能出现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version HEAD-HASH-NOTFOUND (GITDIR-NOTFOUND)</span><br></pre></td></tr></table></figure>
<h4 id="创建源码包">创建源码包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment">#cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#./make-dist</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp ceph-11.0.0-2460-g22053d0.tar.bz2 /root/rpmbuild/SOURCES/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp -f ceph.spec /root/rpmbuild/SPECS/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># rpmbuild -bb /root/rpmbuild/SPECS/ceph.spec</span></span><br></pre></td></tr></table></figure>
<p>执行完了以后就去这个路径取包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /root/rpmbuild/RPMS/x86_64/</span></span><br><span class="line">total <span class="number">1643964</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">1972</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">42259096</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-base-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">320843080</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">35</span> ceph-common-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">58138088</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">36</span> ceph-mds-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<h3 id="准备测试环境">准备测试环境</h3><p>使用打好的包进行集群的配置，创建一个正常的集群，这里就不讲述怎么配置集群了</p>
<h4 id="模拟mon损坏">模拟mon损坏</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-mon@lab8106</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mv /var/lib/ceph/mon/ceph-lab8106/  /var/lib/ceph/mon/ceph-lab8106bk</span></span><br></pre></td></tr></table></figure>
<p>按上面的操作以后，mon的数据相当于全部丢失了，本测试环境是单mon的，多mon原理一样</p>
<h4 id="重构数据">重构数据</h4><p>创建一个临时目录,停止掉所有的osd，这个地方因为mon已经完全挂掉了,所以停止所有osd也没什么大的影响了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mkdir /tmp/mon-store</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br></pre></td></tr></table></figure></p>
<p>注意如果有多台OSD机器，那么在一台台的OSD主机进行上面的操作，这个目录的数据要保持递增的，也就是一直对着这个目录弄，假如换了一台机器那么先把这个数据传递到另外一台机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8106 ~]<span class="comment"># rsync -avz /tmp/mon-store 192.168.8.107:/tmp/mon-store</span></span><br><span class="line">sending incremental file list</span><br><span class="line">created directory /tmp/mon-store</span><br><span class="line">mon-store/</span><br><span class="line">mon-store/kv_backend</span><br><span class="line">mon-store/store.db/</span><br><span class="line">mon-store/store.db/<span class="number">000005</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000008</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000009</span>.log</span><br><span class="line">mon-store/store.db/CURRENT</span><br><span class="line">mon-store/store.db/LOCK</span><br><span class="line">mon-store/store.db/MANIFEST-<span class="number">000007</span></span><br><span class="line"></span><br><span class="line">sent <span class="number">11490</span> bytes  received <span class="number">153</span> bytes  <span class="number">7762.00</span> bytes/sec</span><br><span class="line">total size is <span class="number">74900</span>  speedup is <span class="number">6.43</span></span><br></pre></td></tr></table></figure></p>
<p>等192.168.8.106的机器全部做完了，然后这个/tmp/mon-store传递到了192.168.8.107的机器上，然后再开始做192.168.8.107这台机器的，等全部做外了，把这个/tmp/mon-store弄到需要恢复mon的机器上</p>
<h3 id="根据获得的数据进行重构">根据获得的数据进行重构</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-monstore-tool /tmp/mon-store rebuild</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cp -ra /tmp/mon-store/* /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/done</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/systemd</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph -R /var/lib/ceph/mon/</span></span><br></pre></td></tr></table></figure>
<h3 id="启动mon">启动mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-mon@lab8106</span></span><br></pre></td></tr></table></figure>
<p>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到可以好了，在实践过程中，发现如果对修复的数据，马上进行破坏，再次进行修复的时候，就无法恢复了，应该是个bug，已经提交给作者 Issue:<a href="https://github.com/ceph/ceph/pull/11126" target="_blank" rel="external">11226</a></p>
<h3 id="无法恢复的数据">无法恢复的数据</h3><ul>
<li>pg settings: the full ratio and nearfull ratio 设置会丢失，这个无关紧要，再设置一次就可以了</li>
<li>MDS Maps: the MDS maps are lost.</li>
</ul>
<h2 id="总结">总结</h2><p>因为工具才出来，可能难免有些bug，这个是为未来提供一种恢复数据的方式，使得 Ceph 变得更加的健壮</p>
<h2 id="附加知识">附加知识</h2><p>如果指定ceph版本进行编译<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph.git</span><br><span class="line">git checkout -b myceph v10.<span class="number">2.3</span></span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure></p>
<p>v10.2.3为发行版本的tag，也就是release的版本号码，这个操作是切换到指定的tag，并且下载依赖的一些模块</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-20</td>
</tr>
<tr>
<td style="text-align:center">增加git版本选择</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-10-12</td>
</tr>
</tbody>
</table>
<h2 id="For_me">For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心 Mon 万一三个同时都挂掉了怎么办，那么集群所有的数据是不是都丢了，关于后台真实数据恢复，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，Ceph 的中国（Redhat）的一位开发者 <a href="https://github.com/tchaikov">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到 Ceph 的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292">issue</a></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[替换OSD操作的优化与分析]]></title>
    <link href="http://www.zphj1987.com/2016/09/19/%E6%9B%BF%E6%8D%A2OSD%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BC%98%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/09/19/替换OSD操作的优化与分析/</id>
    <published>2016-09-19T02:56:54.000Z</published>
    <updated>2016-09-19T03:07:09.282Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/" target="_blank" rel="external">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br><a id="more"></a></p>
<h2 id="二、测试三种方法">二、测试三种方法</h2><h3 id="方法一：首先_out_一个_OSD，然后剔除_OSD，然后增加_OSD">方法一：首先 out 一个 OSD，然后剔除 OSD，然后增加 OSD</h3><ol>
<li>停止指定 OSD 进程</li>
<li>out 指定 OSD</li>
<li>crush remove 指定 OSD</li>
<li>增加一个新的 OSD</li>
</ol>
<p>一般生产环境会设置为 <code>noout</code>，当然不设置也可以，那就交给程序去控制节点的 out，默认是在进程停止后的五分钟，总之这个地方如果有 out 触发，不管是人为触发，还是自动触发数据流是一定的，我们这里为了便于测试，使用的是人为触发，上面提到的预制环境就是设置的 <code>noout</code></p>
<p>开始测试前获取最原始的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布,保存到文件pg1.txt，这个 PG 分布记录是 PG 所在的 OSD，记录下来，方便后面进行比较，从而得出需要迁移的数据 </p>
<h4 id="停止指定的_OSD_进程">停止指定的 OSD 进程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@15</span></span><br></pre></td></tr></table></figure>
<p>停止进程并不会触发迁移，只会引起 PG 状态的变化，比如原来主 PG 在停止的 OSD 上，那么停止掉 OSD 以后，原来的副本的那个 PG 就会角色升级为主 PG 了</p>
<h4 id="out_掉一个_OSD">out 掉一个 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd out 15</span></span><br></pre></td></tr></table></figure>
<p>在触发 out 以前，当前的 PG 状态应该有 <code>active+undersized+degraded</code>,触发 out 以后，所有的 PG 的状态应该会慢慢变成 <code>active+clean</code>,等待集群正常后，再次查询当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg2.txt</span></span><br></pre></td></tr></table></figure></p>
<p>保存当前的 PG 分布为pg2.txt<br>比较 out 前后的 PG 的变化情况，下面是比较具体的变化情况，只列出变化的部分<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们关心的是变动的数目，只统计变动的 PG 的数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">102</span></span><br></pre></td></tr></table></figure></p>
<p>第一次 out 以后有102个 PG 的变动,这个数字记住，后面的统计会用到</p>
<h4 id="从_crush_里面删除_OSD">从 crush 里面删除 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<p>crush 删除以后同样会触发迁移，等待 PG 的均衡，也就是全部变成 <code>active+clean</code> 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布的状态<br>现在来比较 crush remove 前后的 PG 变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg2.txt pg3.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">137</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新加上新的 OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure></p>
<p>加完以后统计当前的新的 PG 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg4.txt</span></span><br></pre></td></tr></table></figure></p>
<p>比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg3.txt pg4.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">167</span></span><br></pre></td></tr></table></figure></p>
<p>整个替换流程完毕，统计上面的 PG 总的变动</p>
<blockquote>
<p>102 +137 +167 = 406</p>
</blockquote>
<p>也就是按这个方法的变动为406个 PG，因为是只有双主机，里面可能存在某些放大问题，这里不做深入的讨论，因为我的三组测试环境都是一样的情况，只做横向比较，原理相通，这里是用数据来分析出差别</p>
<h3 id="方法二：先crush_reweight_0_，然后out，然后再增加osd">方法二：先crush reweight 0 ，然后out，然后再增加osd</h3><p>首先恢复环境为测试前的环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>记录最原始的 PG 分布情况</p>
<h4 id="crush_reweight_指定OSD">crush reweight 指定OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.16 0</span></span><br><span class="line">reweighted item id <span class="number">16</span> name <span class="string">'osd.16'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>等待平衡了以后记录当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg2.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<p>比较前后的变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg1.txt 2pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">166</span></span><br></pre></td></tr></table></figure></p>
<h4 id="crush_remove_指定_OSD">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.16</span></span><br><span class="line">removed item id <span class="number">16</span> name <span class="string">'osd.16'</span> from crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为上面 crush 已经是0了所以删除也不会引起 PG 变动<br>然后直接 <code>ceph osd rm osd.16</code> 同样没有 PG 变动</p>
<h4 id="增加新的_OSD">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>等待平衡以后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>来比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg2.txt 2pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">159</span></span><br></pre></td></tr></table></figure></p>
<p>总的 PG 变动为</p>
<blockquote>
<p>166+159=325</p>
</blockquote>
<h3 id="方法3：开始做norebalance，然后做crush_remove，然后做add">方法3：开始做norebalance，然后做crush remove，然后做add</h3><p>恢复环境为初始环境，然后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg1.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<h4 id="给集群做多种标记，防止迁移">给集群做多种标记，防止迁移</h4><p>设置为 norebalance，nobackfill，norecover,后面是有地方会解除这些设置的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norebalance</span></span><br><span class="line"><span class="built_in">set</span> norebalance</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set nobackfill</span></span><br><span class="line"><span class="built_in">set</span> nobackfill</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norecover</span></span><br><span class="line"><span class="built_in">set</span> norecover</span><br></pre></td></tr></table></figure></p>
<h4 id="crush_reweight_指定_OSD">crush reweight 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.15 0</span></span><br><span class="line">reweighted item id <span class="number">15</span> name <span class="string">'osd.15'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为已经做了上面的标记，所以只会出现状态变化，而没有真正的迁移，我们也先统计一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg2.txt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg2.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">158</span></span><br></pre></td></tr></table></figure></p>
<p>注意这里只是计算了，并没有真正的数据变动，可以通过监控两台的主机的网络流量来判断,所以这里的变动并不用计算到需要迁移的 PG 数目当中</p>
<h4 id="crush_remove_指定_OSD-1">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<h4 id="删除指定的_OSD">删除指定的 OSD</h4><p>删除以后同样是没有 PG 的变动的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd rm osd.<span class="number">15</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方有个小地方需要注意一下，不做 ceph auth del osd.15 把15的编号留着，这样好判断前后的 PG 的变化，不然相同的编号，就无法判断是不是做了迁移了</p>
<h4 id="增加新的_OSD-1">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>我的环境下，新增的 OSD 的编号为16了</p>
<h4 id="解除各种标记">解除各种标记</h4><p>我们放开上面的设置，看下数据的变动情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norebalance</span></span><br><span class="line"><span class="built_in">unset</span> norebalance</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset nobackfill</span></span><br><span class="line"><span class="built_in">unset</span> nobackfill</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norecover</span></span><br><span class="line"><span class="built_in">unset</span> norecover</span><br></pre></td></tr></table></figure></p>
<p>设置完了后数据才真正开始变动了，可以通过观察网卡流量看到，来看下最终pg变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg3.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">195</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们只需要跟最开始的 PG 分布状况进行比较就可以了，因为中间的状态实际上都没有做数据的迁移，所以不需要统计进去，可以看到这个地方动了195个 PG<br>总共的 PG 迁移量为</p>
<blockquote>
<p>195</p>
</blockquote>
<h2 id="三、数据汇总">三、数据汇总</h2><p>现在通过表格来对比下三种方法的迁移量的比较(括号内为迁移 PG 数目)</p>
<table>
<thead>
<tr>
<th style="text-align:center">　</th>
<th style="text-align:left">方法一</th>
<th style="text-align:left">方法二</th>
<th style="text-align:left">方法三</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">所做操作</td>
<td style="text-align:left">stop osd (0)<br>out osd(102)<br>crush remove osd (137)<br> add osd(167)</td>
<td style="text-align:left">crush reweight osd(166)<br>out osd(0)<br>crush remove osd (0)<br>add osd(159)</td>
<td style="text-align:left">set 标记(0)<br>crush reweight osd(0)<br>crush remove osd (0)<br>add osd(195)</td>
</tr>
<tr>
<td style="text-align:center">PG迁移数量</td>
<td style="text-align:left">406</td>
<td style="text-align:left">325</td>
<td style="text-align:left">195</td>
</tr>
</tbody>
</table>
<p>可以很清楚的看到三种不同的方法，最终的触发的迁移量是不同的，处理的好的话，能节约差不多一半的迁移的数据量，这个对于生产环境来说还是很好的，关于这个建议先在测试环境上进行测试，然后再操作，上面的操作只要不对磁盘进行格式化，操作都是可逆的，也就是可以比较放心的做，记住所做的操作，每一步都做完都去检查 PG 的状态是否是正常的</p>
<h2 id="四、总结">四、总结</h2><p>从我自己的操作经验来看，最开始是用的第一种方法，后面就用第二种方法减少了一部分迁移量，最近看到资料写做剔除OSD的时候可以关闭迁移防止无效的过多的迁移，然后就测试了一下，确实能够减少不少的迁移量，这个减少在某些场景下还是很好的，当然如果不太熟悉，用哪一种都可以，最终能达到的目的是一样的</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-19</td>
</tr>
</tbody>
</table>
<h2 id="六、For_me">六、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos7下Jewel版本radosgw服务启动]]></title>
    <link href="http://www.zphj1987.com/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/09/12/Centos7下Jewel版本radosgw服务启动/</id>
    <published>2016-09-12T05:47:47.000Z</published>
    <updated>2016-09-12T06:00:17.703Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>
<a id="more"></a>
<h2 id="二、预备环境">二、预备环境</h2><h3 id="一个完整的集群">一个完整的集群</h3><p>拥有一个正常的集群是需要提前准备好的，ceph -s检查正确的输出</p>
<h3 id="关闭各种auth">关闭各种auth</h3><p>这个地方也可以不关闭，注意配置好用户认证就可以了，这里关闭了，配置起来方便，我是从来不开的,也避免了新手不会配置用户造成认证的各种异常<br>关闭认证就是在ceph.conf里面添加下面字段<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br></pre></td></tr></table></figure></p>
<h3 id="安装ceph-radosgw的包">安装ceph-radosgw的包</h3><p>这个因为默认不会安装，所以要安装好<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<h2 id="三、默认启动过程">三、默认启动过程</h2><p>我们先什么都不配置，看下一般的会怎么处理</p>
<h3 id="3-1_启动服务">3.1 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-radosgw.target</span><br></pre></td></tr></table></figure>
<h3 id="3-2_检查服务的状态">3.2 检查服务的状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw.target </span></span><br><span class="line">● ceph-radosgw.target - ceph target allowing to start/stop all ceph-radosgw@.service instances at once</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw.target; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> CST; <span class="number">51</span>s ago</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Stopping ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Starting ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">51</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br></pre></td></tr></table></figure>
<p>可以看到进程是启动的，没有任何异常</p>
<h3 id="3-3_检查端口是否启动">3.3 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br></pre></td></tr></table></figure>
<p>但是并没有生成任何端口，这个是因为还没有配置实例,这个地方就是新手经常卡住的地方</p>
<h2 id="四、下面开始配置默认单实例">四、下面开始配置默认单实例</h2><h3 id="4-1_写配置文件">4.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写一个最简配置文件<br>注意下面的client.radosgw1这个包起来的，这个是固定写法，在 <code>systemctl</code> 启动服务的时候 <code>@</code> 取后面的radosgw1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4-2_启动服务">4.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3_检查服务状态">4.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> CST; <span class="number">17</span>s ago</span><br><span class="line"> Main PID: <span class="number">19996</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">19996</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="4-4_检查端口是否启动">4.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7480</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">19996</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到默认的端口是7480</p>
<h2 id="五、配置多个自定义端口实例">五、配置多个自定义端口实例</h2><h3 id="5-1_写配置文件">5.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写下配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7481</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[client.radosgw2]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7482</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方配置两个实例，用了不同的名称，用了不同的端口</p>
<h3 id="5-2_启动服务">5.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw2</span></span><br></pre></td></tr></table></figure>
<h3 id="5-3_检查服务状态">5.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> CST; <span class="number">1</span>min <span class="number">4</span>s ago</span><br><span class="line"> Main PID: <span class="number">20509</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">20509</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw2</span></span><br><span class="line">● ceph-radosgw@radosgw2.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> CST; <span class="number">1</span>min <span class="number">3</span>s ago</span><br><span class="line"> Main PID: <span class="number">20696</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw2.service</span><br><span class="line">           └─<span class="number">20696</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw2 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="5-4_检查端口是否启动">5.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7481</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20509</span>/radosgw       </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7482</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20696</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到服务和端口都能正常的启动了</p>
<p>好了，关于centos7下jewel版本的radosgw配置的启动已经介绍完了，这里不涉及更多深入的东西，其他的东西可以参照其他文档配置即可，这个地方只是对启动服务这里专门的介绍一下</p>
<h2 id="六、总结">六、总结</h2><p>从上面的过程可以看出大致的流程如下</p>
<ul>
<li>安装软件</li>
<li>启动服务</li>
<li>检查服务状态</li>
<li>检查服务端口</li>
</ul>
<p>这些很多都是基础的做法，在centos7下面虽然比6做了一些改变，但是掌握了一些通用的排查方法后，是很容易举一反三的，因为看到有新手不熟悉启动，所以写下这篇文章，自己因为也没经常用，所以也写下当个笔记了</p>
<h2 id="七、For_me">七、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何统计Ceph的RBD真实使用容量]]></title>
    <link href="http://www.zphj1987.com/2016/09/08/%E5%A6%82%E4%BD%95%E7%BB%9F%E8%AE%A1Ceph%E7%9A%84RBD%E7%9C%9F%E5%AE%9E%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F/"/>
    <id>http://www.zphj1987.com/2016/09/08/如何统计Ceph的RBD真实使用容量/</id>
    <published>2016-09-08T09:17:08.000Z</published>
    <updated>2016-09-12T05:54:30.809Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>
<a id="more"></a>
<p>详细介绍</p>
<h3 id="2-1_方法一：使用rbd_du查询">2.1 方法一：使用rbd du查询</h3><p>这个参考我之前的文章：<a href="http://www.zphj1987.com/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/" target="_blank" rel="external">查询rbd的使用容量</a></p>
<h3 id="2-2_方法二：使用rbd_diff">2.2 方法二：使用rbd diff</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd diff rbd/zp | awk '&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<h3 id="2-3_方法三：根据对象统计的方法进行统计">2.3 方法三：根据对象统计的方法进行统计</h3><p>这个是本篇着重介绍的一点，在集群非常大的时候，再去按上面的一个个的查询，需要花很长的时间，并且需要时不时的跟集群进行交互，这里采用的方法是把统计数据一次获取下来，然后进行数据的统计分析，从而获取结果，获取的粒度是以存储池为基准的</p>
<h4 id="拿到所有对象的信息">拿到所有对象的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<p>这个获取的时间长短是根据对象的多少来的，如果担心出问题，可以换个终端查看进度<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tail <span class="operator">-f</span>  obj.txt</span><br></pre></td></tr></table></figure></p>
<h4 id="获取RBD的镜像列表">获取RBD的镜像列表</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd ls</span></span><br><span class="line"><span class="built_in">test</span>1</span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<h3 id="获取RBD的镜像的prefix">获取RBD的镜像的prefix</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `rbd -p rbd ls`;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$a</span> ;rbd -p rbd info <span class="variable">$a</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span> ;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="获取指定RBD镜像的大小">获取指定RBD镜像的大小</h3><p>查询 test1 的镜像大小<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat obj.txt |grep rbd_data.3ac16b8b4567|awk  '&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">4014.27</span> MB</span><br></pre></td></tr></table></figure></p>
<h3 id="将上面的汇总，使用脚本一次查询出所有的">将上面的汇总，使用脚本一次查询出所有的</h3><h4 id="第一步获取：">第一步获取：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="第二步计算：">第二步计算：</h4><p>创建一个获取的脚本getused.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line"><span class="comment">##default pool name use rbd,you can change it </span></span><br><span class="line"><span class="comment">##default objfile is obj.txt,you can change it</span></span><br><span class="line">objfile=obj.txt</span><br><span class="line">Poolname=rbd</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> `rbd -p <span class="variable">$Poolname</span> ls`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">Imagename=<span class="variable">$image</span></span><br><span class="line">Prefix=`rbd  -p <span class="variable">$Poolname</span> info <span class="variable">$image</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line">Used=`cat <span class="variable">$objfile</span> |grep <span class="variable">$Prefix</span>|awk <span class="string">'&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$Imagename</span> <span class="variable">$Prefix</span></span><br><span class="line"><span class="built_in">echo</span> Used: <span class="variable">$Used</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h4 id="我的输出如下：">我的输出如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sh getused.sh </span></span><br><span class="line"><span class="built_in">test</span>1 rbd_data.<span class="number">3</span>ac16b8b4567</span><br><span class="line">Used: <span class="number">4014.27</span> MB</span><br><span class="line">zp rbd_data.<span class="number">11</span>f66b8b4567</span><br><span class="line">Used: <span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意这里只统计了image里面的真实容量，如果是用了快速clone的,存在容量复用的问题，需要自己看是否需要统计那一部分的对象，方法同上</p>
</blockquote>
<h2 id="三、总结">三、总结</h2><p>对于已存在的系统，并且数据量很大的系统，不要频繁的去做请求，最好把统计请求，集中起来，并且就单线程的处理，慢一点不要紧，然后拉取到数据后，慢慢处理，这样能把影响降低到最少，可以在最不忙的时候去进行相关的操作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-08</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中的Copyset概念和使用方法]]></title>
    <link href="http://www.zphj1987.com/2016/09/06/Ceph%E4%B8%AD%E7%9A%84Copyset%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/06/Ceph中的Copyset概念和使用方法/</id>
    <published>2016-09-06T09:39:15.000Z</published>
    <updated>2016-09-07T03:31:05.791Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>
<a id="more"></a>
<h2 id="二、copyset的概念">二、copyset的概念</h2><p>首先我们要理解copyset的概念，用通俗的话说就是，包含一个数据的所有副本的节点，也就是一个copyset损坏的情况下，数据就是全丢的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomcopy.png" alt="radomcopy"><br></center><br>如上图所示，这里的copyset就是：<br>{1,5,6}，{2,6,8} 两组</p>
<p>如果不做特殊的设置，那么基本上就是会随机的去分布</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomall.png" alt="allcopy"><br></center></p>
<h3 id="2-1_最大copyset">2.1 最大copyset</h3><p>如上图的所示，一般来说，最终组合将是一个最大的随机组合，比如这样的一个9个node随机组合3个的，这样的组合数有：<br>从 n个元素中取出  k个元素， k个元素的组合数量为：</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/gongshi.png" alt="计算公式"><br></center><br>9个随机3个的组合为84<br>如果3个节点down掉，那么有数据丢失概率就是100%</p>
<h3 id="2-2_最小copyset">2.2 最小copyset</h3><p>如果存在一种情况，分布是这样的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/mincopy.png" alt="mincopy"><br></center><br>那么copyset为<br>{1,5,7},{2,4,9},{3,6,8}<br>如果3个节点down掉,只有正好是上面的3种组合中的一种出现的时候，才会出现数据丢失<br>那么数据丢失的概率为 3/84</p>
<p>最小copyset可能带来的不好的地方</p>
<ul>
<li>真出现丢失的时候（概率极低），丢失的数据量将是最大化的，这个是因为出现丢的时候，那么三个上面的组合配对为100%，其他情况不是100%</li>
<li>失效恢复时间将会增大一些，根据facebook的报告100GB的39节点的HDFS随机分布恢复时间在60s,最小分布为700s，这个是因为可用于恢复的点相对减少了，恢复时间自然长了</li>
</ul>
<h3 id="2-3_比较好的处理方式">2.3 比较好的处理方式</h3><p>比较好的方式就是取copyset值为介于纯随机和最小之间的数，那么失效的概率计算方式就是：</p>
<blockquote>
<p>当前的copyset数目/最大copyset</p>
</blockquote>
<h2 id="三、这个概念在ceph当中的实现">三、这个概念在ceph当中的实现</h2><p>其实这个概念在ceph当中就是bucket的概念，PG为最小故障单元，PG就可以理解为上图当中的node上的元素，默认的分组方式为host，这个copyset就是全随机的在这些主机当中进行组合，我们在提升故障域为rack的时候，实际上就是将copyset进行了减少，一个rack之内的主机是形成不了copyset，这样down掉rack的时候，就不会数据丢失了，这个地方的实际可以做的控制方式有三种，下面将详细的介绍三种模式</p>
<h3 id="3-1、缩小最小主机单位">3.1、缩小最小主机单位</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hostzu.png" alt="最小主机组"><br></center><br>默认的为主机组，这样的主机间的copyset为<br>{1,2}，{1,3}，{1,4}，{2,3}，{2,4}，{3,4}<br>这样的有六组</p>
<p>现在我们对host进行一个合并看下</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hebing.png" alt="此处输入图片的描述"><br></center><br>注意这个地方并不是往上加了一层bucket，而是把最底层的host给拆掉了，加入一台机器有24个osd，那么这里的vhost1里面的osd个数实际是48个osd，那么当前的copyset为<br>{vhost1,vhost2}<br>copyset已经为上面默认情况的1/6<br>这样会带来两个好处</p>
<ul>
<li>减少了copyset，减少的好处就见上面的分析</li>
<li>增加可接收恢复的osd数目，之前坏了一个osd的时候，能接收数据的osd为n-1,那么现在坏一个osd，可接收的osd为2n-1(n为单node上的osd个数)</li>
</ul>
<h3 id="3-2、增加分组">3.2、增加分组</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/rackfenzu.png" alt="rack分组"><br></center><br>这个地方是增加了rack分组的，同一个rack里面不会出现copyset，那么当前的模式的copyset就是<br>{1,3}，{1,4}，{2,3}，{2,4}</p>
<p>同没有处理相比copyset为4/6</p>
<h3 id="3-3、增加分组的情况进行PG分流">3.3、增加分组的情况进行PG分流</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/zone.png" alt="zone"><br></center><br>这里看上去跟上面的分组很像，但是在做crush的时候是有区别的，上面的分组以后，会让PG分布在两个rack当中，这里的crush写的时候会让PG只在一个zone当中，在进入zone的下层再去进行分离主副PG，那么这种方式的copyset为<br>{1,2} {3,4}<br>为上面默认情况的2/6</p>
<h2 id="四、总结">四、总结</h2><p>关于ceph中的ceph的copyset的三种模式已经总结完了，需要补充的是，上面的node都是一个虚拟的概念，你可以扩充为row，或者rack都行，这里只是说明了不同的处理方式，针对每个集群都可以有很多种组合，这个关键看自己怎么处理，减少copyset会明显的减低机器上的线程数目和资源的占用，这一点可以自行研制，从原理上来说少了很多配对的通信，crush的是非常灵活的一个分布控制，可以做很精细的控制，当然也会增加了维护的难度</p>
<h2 id="五、参考资料：">五、参考资料：</h2><p><a href="https://www.ustack.com/blog/build-block-storage-service/" target="_blank" rel="external">打造高性能高可靠块存储系统</a><br><a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/cidon" target="_blank" rel="external">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
<h2 id="七、For_me">七、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Proftp最简匿名访问配置]]></title>
    <link href="http://www.zphj1987.com/2016/09/01/Proftp%E6%9C%80%E7%AE%80%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.zphj1987.com/2016/09/01/Proftp最简匿名访问配置/</id>
    <published>2016-09-01T06:04:24.000Z</published>
    <updated>2016-09-01T06:24:34.153Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>
<a id="more"></a>
<h2 id="配置proftp过程">配置proftp过程</h2><p>1、配置过程尽量少的动原配置文件，需要共享的为/share/a目录，首先修改默认的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DefaultRoot                     ~ !adm</span><br></pre></td></tr></table></figure></p>
<p>修改为:<br><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="title">DefaultRoot</span>                     /share</span><br></pre></td></tr></table></figure></p>
<p>让默认的根目录为 /share,默认的为用户的根目录，匿名用户对应的ftp用户的根目录</p>
<p>2、修改匿名用户的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous ~ftp&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous /share&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改原匿名用户ftp的用户目录为/share</p>
<p>3、修改默认屏蔽权限WRITE<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit WRITE SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>默认会屏蔽掉写的操作，就没法上传了</p>
<p>5、配置访问的目录<br>默认启用了vroot，所以写路径的时候写相对路径即可，添加如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br><span class="line">&lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure></p>
<p>/a就代表的是/share/a</p>
<p>6、开启匿名<br>修改配置vim /etc/sysconfig/proftpd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>改成:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">"-DANONYMOUS_FTP"</span></span><br></pre></td></tr></table></figure></p>
<p>7、给目录访问权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chown ftp:ftp /share/a</span><br><span class="line">chmod <span class="number">755</span>  /share/a</span><br></pre></td></tr></table></figure></p>
<p>8、启动proftp服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart proftpd</span><br></pre></td></tr></table></figure></p>
<h2 id="完整配置文件">完整配置文件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ServerName			<span class="string">"ProFTPD server"</span></span><br><span class="line">ServerIdent			on <span class="string">"FTP Server ready."</span></span><br><span class="line">ServerAdmin			root@localhost</span><br><span class="line">DefaultServer			on</span><br><span class="line">DefaultRoot			~ !adm</span><br><span class="line">AuthPAMConfig			proftpd</span><br><span class="line">AuthOrder			mod_auth_pam.c* mod_auth_unix.c</span><br><span class="line">UseReverseDNS			off</span><br><span class="line">User				nobody</span><br><span class="line">Group				nobody</span><br><span class="line">MaxInstances			<span class="number">20</span></span><br><span class="line">UseSendfile			off</span><br><span class="line">LogFormat			default	<span class="string">"%h %l %u %t \"%r\" %s %b"</span></span><br><span class="line">LogFormat			auth	<span class="string">"%v [%P] %h %t \"%r\" %s"</span></span><br><span class="line">LoadModule mod_ctrls_admin.c</span><br><span class="line">LoadModule mod_vroot.c</span><br><span class="line">ModuleControlsACLs		insmod,rmmod allow user root</span><br><span class="line">ModuleControlsACLs		lsmod allow user *</span><br><span class="line">ControlsEngine			on</span><br><span class="line">ControlsACLs			all allow user root</span><br><span class="line">ControlsSocketACL		allow user *</span><br><span class="line">ControlsLog			/var/<span class="built_in">log</span>/proftpd/controls.log</span><br><span class="line">&lt;IfModule mod_ctrls_admin.c&gt;</span><br><span class="line">  AdminControlsEngine		on</span><br><span class="line">  AdminControlsACLs		all allow user root</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfModule mod_vroot.c&gt;</span><br><span class="line">  VRootEngine			on</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfDefine TLS&gt;</span><br><span class="line">  TLSEngine			on</span><br><span class="line">  TLSRequired			on</span><br><span class="line">  TLSRSACertificateFile		/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSRSACertificateKeyFile	/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSCipherSuite		ALL:!ADH:!DES</span><br><span class="line">  TLSOptions			NoCertRequest</span><br><span class="line">  TLSVerifyClient		off</span><br><span class="line">  TLSLog			/var/<span class="built_in">log</span>/proftpd/tls.log</span><br><span class="line">  &lt;IfModule mod_tls_shmcache.c&gt;</span><br><span class="line">    TLSSessionCache		shm:/file=/var/run/proftpd/sesscache</span><br><span class="line">  &lt;/IfModule&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine DYNAMIC_BAN_LISTS&gt;</span><br><span class="line">  LoadModule			mod_ban.c</span><br><span class="line">  BanEngine			on</span><br><span class="line">  BanLog			/var/<span class="built_in">log</span>/proftpd/ban.log</span><br><span class="line">  BanTable			/var/run/proftpd/ban.tab</span><br><span class="line">  BanOnEvent			MaxLoginAttempts <span class="number">2</span>/<span class="number">00</span>:<span class="number">10</span>:<span class="number">00</span> <span class="number">01</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">  BanMessage			<span class="string">"Host %a has been banned"</span></span><br><span class="line">  BanControlsACLs		all allow user ftpadm</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine QOS&gt;</span><br><span class="line">  LoadModule			mod_qos.c</span><br><span class="line">  QoSOptions			dataqos throughput ctrlqos lowdelay</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;Global&gt;</span><br><span class="line">  Umask				<span class="number">022</span></span><br><span class="line">  AllowOverwrite		yes</span><br><span class="line">  &lt;Limit ALL SITE_CHMOD&gt;</span><br><span class="line">    AllowAll</span><br><span class="line">  &lt;/Limit&gt;</span><br><span class="line">&lt;/Global&gt;</span><br><span class="line">&lt;IfDefine ANONYMOUS_FTP&gt;</span><br><span class="line">  &lt;Anonymous /share/&gt;</span><br><span class="line">    User			ftp</span><br><span class="line">    Group			ftp</span><br><span class="line">    AccessGrantMsg		<span class="string">"Anonymous login ok, restrictions apply."</span></span><br><span class="line">    UserAlias			anonymous ftp</span><br><span class="line">    MaxClients			<span class="number">10</span> <span class="string">"Sorry, max %m users -- try again later"</span></span><br><span class="line">    DisplayLogin		/welcome.msg</span><br><span class="line">    DisplayChdir		.message</span><br><span class="line">    DisplayReadme		README*</span><br><span class="line">    DirFakeUser			on ftp</span><br><span class="line">    DirFakeGroup		on ftp</span><br><span class="line">    &lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">      DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;IfModule mod_vroot.c&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">	       AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">        &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">              AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">          AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line">    WtmpLog			off</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/access.log WRITE,READ default</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/auth.log AUTH auth</span><br><span class="line">  &lt;/Anonymous&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>最简配置就完成了，也可以根据需要再去做更复杂的配置，这里就不做过多的介绍，比较容易错误的点就是容易出现权限问题无法访问，或者是上下的设置关联错误，可以开启调试模式进行调试<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">proftpd   -n <span class="operator">-d</span> <span class="number">10</span> -c /etc/proftpd.conf -DANONYMOUS_FTP</span><br></pre></td></tr></table></figure></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-01</td>
</tr>
</tbody>
</table>
<h2 id="For_me">For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday北京总结-August 20, 2016（未完待续）]]></title>
    <link href="http://www.zphj1987.com/2016/08/29/Cephday%E5%8C%97%E4%BA%AC%E6%80%BB%E7%BB%93-August-20-2016%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/"/>
    <id>http://www.zphj1987.com/2016/08/29/Cephday北京总结-August-20-2016（未完待续）/</id>
    <published>2016-08-29T15:57:52.000Z</published>
    <updated>2016-09-06T04:19:47.616Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>
<a id="more"></a>
<h2 id="分享的PPT">分享的PPT</h2><h3 id="开幕致辞-张建">开幕致辞-张建</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/00-%E5%BC%80%E5%B9%95%E8%87%B4%E8%BE%9E-%E5%BC%A0%E5%BB%BA.pdf" width="850" height="700"></center>

<p>首先说下这位分享者，之前在2015的Ceph Hackathon上，就是他最先发现的老版本的ceph与 TCMalloc结合的一个bug，然后提出了用jemalloc获取了随机IO的提升，并且降低了资源占用， 这对于老版本的环境提升还是比较大的，在新的环境下，差别没有那么大了，不过分享者还是非常无私的分享了他们的发现<br>本篇主要讲了下面几点：ceph在中国很火，intel投入很多，并且参与了很多的功能的开发，这只是一个致辞，发出的信号就是Intel 很关注ceph</p>
<h3 id="Ceph社区进展-Patrick">Ceph社区进展-Patrick</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/01-Ceph%E7%A4%BE%E5%8C%BA%E8%BF%9B%E5%B1%95-Patrick.pdf" width="850" height="700"></center>

<p>Patrick是红帽的ceph社区的总监，负责推进ceph各方面的发展，<br>本篇主要讲了：ceph当前的发展情况，各大厂对ceph的关注，ceph的固定的活动，cephfs在jewel版本会稳定下来</p>
<h3 id="Ceph中国社区-孙琦">Ceph中国社区-孙琦</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/02-Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BA-%E5%AD%99%E7%90%A6.pdf" width="850" height="700"></center><br>这篇是由孙琦进行的演讲，他对推动ceph在中国的发展做了很多工作<br>本篇主要讲了：ceph中国社区在中国做了哪些推广方面的活动，主要是建立圈子，关注的人很多，发布了一本翻译的技术书籍，未来会做的事情，需要关注的是社区自己写的书会在10月份出来<br><br>###SSD-Ceph在360游戏云的应用-谷忠言<br><br><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/03-SSD%20Ceph%E5%9C%A8360%E6%B8%B8%E6%88%8F%E4%BA%91%E7%9A%84%E5%BA%94%E7%94%A8-%E8%B0%B7%E5%BF%A0%E8%A8%80.pdf" width="850" height="700"></center>

<p>本篇是由360游戏的谷忠言进行演讲的，主要讲述了ceph在360游戏中使用的经验<br>提出了IO容量计算模型；概括了ceph主要调优的方法;相同负载情况下分池对线程和资源的占用帮助很大;如果不限流很有可能因为过载造成心跳超时，进程自杀了;扩容采用扩池的方式避免数据大量变动；网络问题不好定位，根据osd的提交时间的异常来追踪问题（这个数据看下采集方法）；图形化监控采用的是grafana；纯ssd才能满足360游戏主机对性能的需求</p>
<h3 id="SPDK加速Ceph-XSKY_Bluestore案例分享-扬子夜-王豪迈">SPDK加速Ceph-XSKY Bluestore案例分享-扬子夜-王豪迈</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/04-SPDK%E5%8A%A0%E9%80%9FCeph-XSKY%20Bluestore%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB-%E6%89%AC%E5%AD%90%E5%A4%9C-%E7%8E%8B%E8%B1%AA%E8%BF%88.pdf" width="850" height="700"></center><br>本篇是由xsky的扬子夜-王豪迈进行演讲的，ceph设计是在低性能硬件基础上设计的，现在的网络磁盘都是高性能的，软件设计和实现是性能瓶颈，介绍了底层对象存储的写入模型的优点和缺点；设计了新的写入方式，解决这些问题，介绍了spdk；spdk的nvme 驱动比内核的nvme驱动带来了6倍随机读性能的提升；spdk对iscsi场景也能带来很大性能提升；替换内核驱动NVME SSD的OSD为spdk驱动，OSD网络用dpdk替换；介绍bluestore，性能全线提升，当前还在完善功能；总之这个地方会对性能提升很多，但是目前资料太少，目前还没普及，只能有一定功底研发实力的才能参与进来，目前主要是xsky和Intel还有Redhat等大厂在进行驱动在</p>
<h3 id="Ceph_Tiering高性能架构-Thor_Chin">Ceph Tiering高性能架构-Thor Chin</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/05-Ceph%20Tiering%E9%AB%98%E6%80%A7%E8%83%BD%E6%9E%B6%E6%9E%84-Thor%20Chin.pdf" width="850" height="700"></center><br>本篇是由 Thor Chin 进行的演讲，首先介绍了自己的使用场景，然后介绍了下crushmap文件里面各个字段的意思；介绍了各种测试的工具；然后给出了测试的情况，这里作者没有给出测试模型，并且没有说明是否在cache满载的情况下，这个方案是可以使用的，但是性能数据就不做过多的评价</p>
<h3 id="Ceph在视频应用上的性能优化-何营">Ceph在视频应用上的性能优化-何营</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/06-Ceph%E5%9C%A8%E8%A7%86%E9%A2%91%E5%BA%94%E7%94%A8%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%BD%95%E8%90%A5.pdf" width="850" height="700"></center><br>本篇来自浪潮的何营的演讲，主要介绍了ceph在视频行业的运用，并且提出了直接纠删码的实现方法，研发可以看看，实现起来代码量还是很高的</p>
<h3 id="借助当今的NVM_Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘">借助当今的NVM Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/07-%E5%80%9F%E5%8A%A9%E5%BD%93%E4%BB%8A%E7%9A%84NVM%20Express%E5%9B%BA%E6%80%81%E7%9B%98%E5%92%8C%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%8B%B1%E7%89%B9%E5%B0%94Optane%E6%8A%80%E6%9C%AF%E6%89%93%E9%80%A0%E7%BB%8F%E6%B5%8E%E9%AB%98%E6%95%88%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%AD%98%E5%82%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E5%91%A8%E6%B8%8A-%E5%BC%A0%E7%BC%98.pdf" width="850" height="700"></center><br>本篇来自Intel的周渊-张缘作的演讲，开始介绍了Intel在ceph上的贡献，有三大工具，CeTune性能调优工具，Vsm部署管理工具，COSbench压力测试工具，三大工具目前都是开源可部署的，然后介绍了基于Intel的硬件的Ceph方案，介绍了几个调优点，4K随机写提高了6倍，4K随机读提高了16倍，介绍了Bluestore和Filestore在Intel硬件上性能的差别，根据火焰图的输出提出rocksdb需要调优；介绍了Intel的3D Xpoint，最后给出了参考配置文件，这个是针对全闪存的调优</p>
<h3 id="基于ARM的Ceph可扩展高效解决方案-罗旭">基于ARM的Ceph可扩展高效解决方案-罗旭</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/08-%E5%9F%BA%E4%BA%8EARM%E7%9A%84Ceph%E5%8F%AF%E6%89%A9%E5%B1%95%E9%AB%98%E6%95%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E7%BD%97%E6%97%AD.pdf " width="850" height="700"></center><br>本篇来自罗旭的演讲，介绍了arm在存储方面的应用，社区也在发布arm版本的发行包，提供了ceph的arm基本解决方案，介绍了功耗的优势，西数之前做了一个504个osd的arm的测试，arm目前在国内还属于起步概念的东西，很多人想上，但是因为因为不是通用平台，目前的成本其实并没有太大的优势，未来还是值得期待，在冷数据存储的场景上，还是大有可为的，还有一个原因，国内在功耗这一块并没有特别的重视</p>
<h3 id="Ceph存储设备案例研究与S3对象存储性能优化-刘志刚">Ceph存储设备案例研究与S3对象存储性能优化-刘志刚</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/09-Ceph%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E4%B8%8ES3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%88%98%E5%BF%97%E5%88%9A.pdf " width="850" height="700"></center><br>本篇来自富士通的刘志刚演讲，这也是本次分享里面唯一的RGW方面的分享，开始介绍了富士通在ceph上的投入，介绍了cache tiering存在性能衰减的问题，介绍了一些方案上调优的点，介绍了ownCloud与对象存储对接的方案和性能，介绍了cosbench测试出来的性能的情况，介绍了rgw调优的参数</p>
<h3 id="Ceph全闪存存储-周皓">Ceph全闪存存储-周皓</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/10-Ceph%E5%85%A8%E9%97%AA%E5%AD%98%E5%AD%98%E5%82%A8-%E5%91%A8%E7%9A%93.pdf" width="850" height="700"></center><br>本篇来自SanDisk的周皓的演讲，介绍了Sandisk的全闪存ceph的方案InfiniFlash，性能确实非常的好，并且TCO非常的低；介绍了一些调优的点BlueStore，KV Store，Memory allocation等等</p>
<h3 id="将Ceph引入企业-在30分钟安装一个50T移动集群">将Ceph引入企业-在30分钟安装一个50T移动集群</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/11-%E5%B0%86Ceph%E5%BC%95%E5%85%A5%E4%BC%81%E4%B8%9A-%E5%9C%A830%E5%88%86%E9%92%9F%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AA50T%E7%A7%BB%E5%8A%A8%E9%9B%86%E7%BE%A4-Alex%20Lau.pdf" width="850" height="700"></center><br>本篇来自Suse的劉俊賢的演讲，主要讲的是快速部署一个50T的ceph集群，介绍了Suse的iscsi的方案，目前Suse这块做的不错，商业版本提供了基于rbd的iscsi方案的高可用，介绍了基于LIO的LRBD，介绍了openATTIC，这个是之前一家做存储管理平台的公司，后来和Suse合作比较紧密，这个在openATTIC更稳定一点我会写下部署的相关文档，介绍了基于salt的快速部署</p>
<h2 id="完结">完结</h2><p>现在一些大厂的分享都会带上一些优化的方法和优化的参数，这个比前几年已经好了很多，这些参数建议都自己在自己的环境上跑一跑，因为优化是基于当前环境的优化，如果有通用优化，那不用优化了，直接固定参数值就行了，举个简单的例子，在ssd场景上常用的一个优化需要调高IO的线程，如果直接参数硬套到sata的场景，性能不会提高，反而增大了延时，IO一般在增大到一个峰值后，就不会增大，延时反而会增大，所以调优就是找到自己环境的最适合的参数，上面只是简单的介绍了PPT里面的内容点，如果感兴趣可以深挖里面的东西</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-29</td>
</tr>
<tr>
<td style="text-align:center">完成</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重写ceph-lazy]]></title>
    <link href="http://www.zphj1987.com/2016/08/28/%E9%87%8D%E5%86%99ceph-lazy/"/>
    <id>http://www.zphj1987.com/2016/08/28/重写ceph-lazy/</id>
    <published>2016-08-28T15:58:04.000Z</published>
    <updated>2016-08-29T01:05:09.111Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/" target="_blank" rel="external">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>
<a id="more"></a>
<h2 id="项目地址">项目地址</h2><p>原作者项目地址：<a href="https://github.com/gcharot/ceph-lazy" target="_blank" rel="external">https://github.com/gcharot/ceph-lazy</a><br>我重写的地址：<a href="https://github.com/zphj1987/ceph-lazy/tree/lazy-python" target="_blank" rel="external">https://github.com/zphj1987/ceph-lazy/tree/lazy-python</a></p>
<h3 id="安装方法">安装方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -O /sbin/ceph-lazy https://raw.githubusercontent.com/zphj1987/ceph-lazy/lazy-python/ceph-lazy.py</span><br><span class="line">chmod <span class="number">777</span> /sbin/ceph-lazy</span><br></pre></td></tr></table></figure>
<h3 id="详细使用说明">详细使用说明</h3><h4 id="列出节点上的所有的OSD">列出节点上的所有的OSD</h4><p>命令：ceph-lazy host-get-osd {hostname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-osd lab8106</span></span><br><span class="line">osd.<span class="number">0</span> </span><br><span class="line">osd.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有的存储主机节点">列出所有的存储主机节点</h4><p>命令：ceph-lazy host-get-nodes<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-nodes </span></span><br><span class="line">lab8106</span><br><span class="line">lab8107</span><br></pre></td></tr></table></figure></p>
<h4 id="列出存储节点上的存储使用的情况(detail看详细信息)">列出存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-osd-usage {hostname} {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106</span></span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106 detail</span></span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有存储节点上的存储使用的情况(detail看详细信息)">列出所有存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-all-usage {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage detail</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">2</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">3.8</span>GB | Available:<span class="number">274.4</span>GB</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出PG所在的节点(first_is_primary)">列出PG所在的节点(first is primary)</h4><p>命令： ceph-lazy pg-get-host {pg_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-get-host   10.2</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host :lab8107</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(_operations_number)">列出写操作最多的PG ( operations number)</h4><p>命令：ceph-lazy pg-most-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-most-write</span></span><br><span class="line">PG:<span class="number">10.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(_operations_number)">列出写操作最少的PG ( operations number)</h4><p>命令：ceph-lazy pg-less-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-less-write</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(data_written)">列出写操作最多的PG (data written)</h4><p>命令：ceph-lazy pg-most-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-write-kb</span></span><br><span class="line">PG:<span class="number">10.0</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(data_written)">列出写操作最少的PG (data written)</h4><p>命令：ceph-lazy pg-less-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-write-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(operations_number)">列出读操作最多的PG (operations number)</h4><p>命令：ceph-lazy pg-most-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read</span></span><br><span class="line">PG:<span class="number">10.1</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(operations_number)">列出读操作最少的PG (operations number)</h4><p>命令：ceph-lazy pg-less-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(data_read)">列出读操作最多的PG (data read)</h4><p>命令：ceph-lazy pg-most-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read-kb</span></span><br><span class="line">PG:<span class="number">10.4</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(data_read)">列出读操作最少的PG (data read)</h4><p>命令：ceph-lazy pg-less-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出空的PG_(没有存储对象)">列出空的PG (没有存储对象)</h4><p>命令：ceph-lazy pg-empty<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-empty</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">11.2</span></span><br><span class="line"><span class="number">11.1</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">11.4</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的prefix">列出RBD的prefix</h4><p>命令：ceph-lazy rbd-prefix {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-prefix rbd zp</span></span><br><span class="line">rbd_data.<span class="number">1</span>b93a6b8b4567</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的对象数目">列出RBD的对象数目</h4><p>命令：ceph-lazy rbd-count {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-count rbd zp</span></span><br><span class="line"></span><br><span class="line">    RBD image rbd/zp has prefix rbd_data.<span class="number">1</span>b93a6b8b4567; now couning objects...</span><br><span class="line">    count: <span class="number">27</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的存储主机">列出RBD的Primary所在的存储主机</h4><p>命令：ceph-lazy rbd-host {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-host rbd zp</span></span><br><span class="line">Primary Host: lab8107</span><br><span class="line">Primary Host: lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的OSD节点">列出RBD的Primary所在的OSD节点</h4><p>命令：ceph-lazy rbd-osd {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-osd rbd zp</span></span><br><span class="line">Primary Osd: <span class="number">0</span></span><br><span class="line">Primary Osd: <span class="number">1</span></span><br><span class="line">Primary Osd: <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Image的真实大小">列出RBD的Image的真实大小</h4><p>命令：ceph-lazy rbd-size rbd zp<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-size rbd zp</span></span><br><span class="line">Pool: rbd | Image:zp | Real_size:<span class="number">71.5586</span> MB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最多的OSD">列出容量使用最多的OSD</h4><p>命令：ceph-lazy osd-most-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-most-used</span></span><br><span class="line">OSD:osd.<span class="number">1</span> | Host: lab8106 | Used: <span class="number">8</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最少的OSD">列出容量使用最少的OSD</h4><p>命令：ceph-lazy osd-less-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-less-used</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host: lab8107 | Used: <span class="number">3</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上所有的primary_PG">列出指定OSD上所有的primary PG</h4><p>命令： ceph-lazy osd-get-ppg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-ppg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上的所有PG">列出指定OSD上的所有PG</h4><p>命令：ceph-lazy osd-get-pg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-pg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.2</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">10.1</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">10.4</span></span><br><span class="line"><span class="number">10.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定对象所在的主机（第一个是主）">列出指定对象所在的主机（第一个是主）</h4><p>命令：ceph-lazy object-get-host   {poolname} {obj_name}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy object-get-host   rbd rbd_data.1b93a6b8b4567.00000000000000a0</span></span><br><span class="line">Pg: <span class="number">10.4</span></span><br><span class="line">OSD:osd.<span class="number">0</span> | Host :lab8106</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h3 id="总结">总结</h3><p>本篇只是暂时结束了，目前完成了原作者的一些想法，等有空再写点自己比较注重的数据</p>
<p>最近一直在关注冯大辉的事情，看完后还是原来的感觉，在利益面前，公司总是会追求最大化，当出现分离的时候，总会显得无情，还是自己让自己强大一点，拿到属于自己的那一部分就好</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
<h3 id="For_me">For me</h3><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux配置邮件发送信息]]></title>
    <link href="http://www.zphj1987.com/2016/08/19/Linux%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF/"/>
    <id>http://www.zphj1987.com/2016/08/19/Linux配置邮件发送信息/</id>
    <published>2016-08-18T16:48:17.000Z</published>
    <updated>2016-08-28T16:08:27.386Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="编写一个发送的脚本">编写一个发送的脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /root/sendmail.sh </span><br><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line">timeout <span class="number">20</span> date &gt; /tmp/mail</span><br><span class="line">timeout <span class="number">20</span> ceph <span class="operator">-s</span> &gt;&gt; /tmp/mail</span><br><span class="line">timeout <span class="number">600</span> mail <span class="operator">-s</span> <span class="string">"cephstatus-`date`"</span> zbkc2016@sina.com &lt; /tmp/mail</span><br></pre></td></tr></table></figure>
<h3 id="在crontab中添加定期执行">在crontab中添加定期执行</h3><p>修改crontab配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim crontab</span><br><span class="line">*/<span class="number">5</span> * * * *  root  sh /root/sendmail.sh  <span class="number">2</span>&gt;&amp;<span class="number">1</span>  &gt; /dev/null</span><br></pre></td></tr></table></figure></p>
<p>让crontab服务生效<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crontab crontab</span><br><span class="line">/etc/init.d/crontab restart</span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>这个东西很简单，不过自己真去配置的时候，还是找半天资料，还是自己写好文档，方便以后使用，最快最简单的实现需求</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph 状态报警告 pool rbd has many more objects per pg than average (too few pgs?)]]></title>
    <link href="http://www.zphj1987.com/2016/07/27/Ceph-%E7%8A%B6%E6%80%81%E6%8A%A5%E8%AD%A6%E5%91%8A-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/"/>
    <id>http://www.zphj1987.com/2016/07/27/Ceph-状态报警告-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/</id>
    <published>2016-07-27T13:42:05.000Z</published>
    <updated>2016-07-27T14:45:55.641Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>
<a id="more"></a>
<p>这个地方查看集群详细信息：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph health detail</span></span><br><span class="line">HEALTH_WARN pool rbd has many more objects per pg than average (too few pgs?); mon.lab8106 low disk space</span><br><span class="line">pool rbd objects per pg (<span class="number">1912</span>) is more than <span class="number">50.3158</span> <span class="built_in">times</span> cluster average (<span class="number">38</span>)</span><br></pre></td></tr></table></figure></p>
<p>看下集群的pool的对象状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    <span class="number">556</span>G      <span class="number">538</span>G       <span class="number">18668</span>M          <span class="number">3.28</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME       ID     USED       %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd        <span class="number">6</span>      <span class="number">16071</span>M      <span class="number">2.82</span>          <span class="number">536</span>G       <span class="number">15296</span> </span><br><span class="line">    pool1      <span class="number">7</span>        <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool2      <span class="number">8</span>        <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool3      <span class="number">9</span>        <span class="number">188</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">48</span> </span><br><span class="line">    pool4      <span class="number">10</span>       <span class="number">192</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">49</span> </span><br><span class="line">    pool5      <span class="number">11</span>       <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool6      <span class="number">12</span>       <span class="number">148</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">38</span> </span><br><span class="line">    pool7      <span class="number">13</span>       <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool8      <span class="number">14</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool9      <span class="number">15</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool10     <span class="number">16</span>       <span class="number">248</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">63</span> </span><br><span class="line">    pool11     <span class="number">17</span>       <span class="number">232</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">59</span> </span><br><span class="line">    pool12     <span class="number">18</span>       <span class="number">264</span>M      <span class="number">0.05</span>          <span class="number">536</span>G          <span class="number">67</span></span><br></pre></td></tr></table></figure></p>
<p>查看存储池的pg个数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">6</span> <span class="string">'rbd'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">8</span> pgp_num <span class="number">8</span> last_change <span class="number">132</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">7</span> <span class="string">'pool1'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">134</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">8</span> <span class="string">'pool2'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">136</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">9</span> <span class="string">'pool3'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">138</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">10</span> <span class="string">'pool4'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">140</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">11</span> <span class="string">'pool5'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">142</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">12</span> <span class="string">'pool6'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">144</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">13</span> <span class="string">'pool7'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">146</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">14</span> <span class="string">'pool8'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">148</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">15</span> <span class="string">'pool9'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">150</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">16</span> <span class="string">'pool10'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">152</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">17</span> <span class="string">'pool11'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">154</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">18</span> <span class="string">'pool12'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">200</span> pgp_num <span class="number">200</span> last_change <span class="number">156</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>我们看下这个是怎么得到的</p>
<blockquote>
<p>pool rbd objects per pg (1912) is more than 50.3158 times cluster average (38)</p>
</blockquote>
<p>rbd objects_per_pg = 15296 / 8 = 1912<br>objects_per_pg = 15920 /417  ≈ 38<br>50.3158 =  rbd objects_per_pg / objects_per_pg =  1912 / 38 </p>
<p>也就是出现其他pool的对象太少，而这个pg少，对象多，就会提示这个了，我们看下代码里面的判断</p>
<p><a href="https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">int average_objects_per_pg = pg_map.pg_sum.stats.sum.num_objects / pg_map.pg_stat.size();</span><br><span class="line">     <span class="keyword">if</span> (average_objects_per_pg &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">         pg_map.pg_sum.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_objects &amp;&amp;</span><br><span class="line">         p-&gt;second.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_pool_objects) &#123;</span><br><span class="line">int objects_per_pg = p-&gt;second.stats.sum.num_objects / pi-&gt;get_pg_num();</span><br><span class="line"><span class="built_in">float</span> ratio = (<span class="built_in">float</span>)objects_per_pg / (<span class="built_in">float</span>)average_objects_per_pg;</span><br><span class="line"><span class="keyword">if</span> (g_conf-&gt;mon_pg_warn_max_object_skew &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">    ratio &gt; g_conf-&gt;mon_pg_warn_max_object_skew) &#123;</span><br><span class="line">  ostringstream ss;</span><br><span class="line">  ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" has many more objects per pg than average (too few pgs?)"</span>;</span><br><span class="line">  summary.push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  <span class="keyword">if</span> (detail) &#123;</span><br><span class="line">    ostringstream ss;</span><br><span class="line">    ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" objects per pg ("</span></span><br><span class="line">       &lt;&lt; objects_per_pg &lt;&lt; <span class="string">") is more than "</span> &lt;&lt; ratio &lt;&lt; <span class="string">" times cluster average ("</span></span><br><span class="line">       &lt;&lt; average_objects_per_pg &lt;&lt; <span class="string">")"</span>;</span><br><span class="line">    detail-&gt;push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>主要下面的几个限制条件</p>
<blockquote>
<p>mon_pg_warn_min_objects = 10000   //总的对象超过10000<br>mon_pg_warn_min_pool_objects = 1000     //存储池对象超过1000<br>mon_pg_warn_max_object_skew = 10        //就是上面的存储池的平均对象与所有pg的平均值的倍数关系</p>
</blockquote>
<h2 id="解决问题">解决问题</h2><p>有三个方法解决这个警告的提示：</p>
<ul>
<li><p>删除无用的存储池<br>如果集群中有一些不用的存储池，并且相对的pg数目还比较高，那么可以删除一些这样的存储池，从而降低<code>mon_pg_warn_max_object_skew</code>这个值，警告就会没有了</p>
</li>
<li><p>增加提示的pool的pg数目<br>有可能的情况就是，这个存储池的pg数目从一开始就不够，增加pg和pgp数目，同样降低了<code>mon_pg_warn_max_object_skew</code>这个值了</p>
</li>
<li>增加<code>mon_pg_warn_max_object_skew</code>的参数值<br>如果集群里面已经有足够多的pg了，再增加pg会不稳定，如果想去掉这个警告，就可以增加这个参数值，默认为10</li>
</ul>
<h2 id="总结">总结</h2><p>这个警告是比较的是存储池中的对象数目与整个集群的pg的平均对象数目的偏差，如果偏差太大就会发出警告</p>
<p>检查的步骤：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">ceph df</span><br><span class="line">ceph osd dump | grep pool</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>mon_pg_warn_max_object_skew = 10.0</p>
</blockquote>
<p>((objects/pg_num) in the affected pool)/(objects/pg_num in the entire system) &gt;= 10.0 警告就会出现</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何替换Ceph的Journal]]></title>
    <link href="http://www.zphj1987.com/2016/07/26/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2Ceph%E7%9A%84Journal/"/>
    <id>http://www.zphj1987.com/2016/07/26/如何替换Ceph的Journal/</id>
    <published>2016-07-26T14:32:18.000Z</published>
    <updated>2016-07-26T17:31:44.315Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>
<a id="more"></a>
<h2 id="第一种方法">第一种方法</h2><p>在开始处理前，最开始要设置OSD状态为<code>noout</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set noout</span></span><br><span class="line"><span class="built_in">set</span> noout</span><br></pre></td></tr></table></figure>
<p>停止需要替换journal的osd(这里是osd.1)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>我的版本是jewel的，如果是hammer版本，就使用 /etc/init.d/ceph stop osd.1</p>
</blockquote>
<p>下刷journal到osd，使用 -i 指定需要替换journal的 osd的编号</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --flush-journal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">26</span> <span class="number">22</span>:<span class="number">47</span>:<span class="number">20.185292</span> <span class="number">7</span><span class="built_in">fc</span>54a6c3800 -<span class="number">1</span> flushed journal /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="创建一个新的journal">创建一个新的journal</h3><p>删除原来的journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">58</span> Jul <span class="number">25</span> <span class="number">09</span>:<span class="number">25</span> /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal -&gt; /dev/disk/by-partuuid/<span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7</span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-1/journal</span></span><br></pre></td></tr></table></figure></p>
<p>准备一个新的分区</p>
<p>我的环境准备使用/dev/sdd1,分区大小为10G，这个注意磁盘大小比参数设置的要大一点即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ls -l /dev/disk/by-partuuid/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">25</span> <span class="number">14</span>:<span class="number">25</span> <span class="number">4766</span>ce93<span class="operator">-a</span>476-<span class="number">4</span>e97-<span class="number">9</span>aac-<span class="number">894</span>d461b367e -&gt; ../../sdb2</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">51</span> <span class="number">5</span>bb48687-<span class="number">6</span>be6-<span class="number">4</span>aef-<span class="number">82</span>f6-<span class="number">5</span>af822c3fad8 -&gt; ../../sdd1</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">47</span> <span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7 -&gt; ../../sdc2</span><br></pre></td></tr></table></figure>
<p>我的新的journal的uuid的路径为<code>/dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8</code></p>
<p>将这个磁盘的分区链接到原始路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ln -s /dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8 /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo 5bb48687-6be6-4aef-82f6-5af822c3fad8 &gt; /var/lib/ceph/osd/ceph-1/journal_uuid</span></span><br></pre></td></tr></table></figure></p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<p>启动后检查集群的状态</p>
<hr>
<h2 id="第二种方法">第二种方法</h2><p>这个属于备份和转移分区表的方法<br>首先进行上面方法的停进程，下刷journal</p>
<p>备份需要替换journal的分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --backup=/tmp/backup_journal_sdd /dev/sdd</span></span><br></pre></td></tr></table></figure></p>
<p>还原分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --load-backup=/tmp/backup_journal_sde /dev/sde</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># parted -s /dev/sde print</span></span><br></pre></td></tr></table></figure></p>
<p>新的journal磁盘现在跟老的journal的磁盘的分区表一样的了。这意味着新的分区的UUID和老的相同的。如果选择的是这种备份还原分布的方法，那么journal的那个软连接是不需要进行修改的，因为两个磁盘的uuid是一样的，所以需要注意将老的磁盘拔掉或者清理掉分区，以免冲突</p>
<p>在做完这个以后同样跟上面的方法一样需要重建journal</p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<h2 id="第一种方法的实践记录">第一种方法的实践记录</h2><p>这样你可以看到完整的操作过程，而不是枯燥的文档了，虽然命令行看上去也是那么的枯燥</p>
<iframe src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2journal.html" height="530px" width="90%" align="center"></iframe>

<p>支持暂停复制，是不是很屌？</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[modprobe: FATAL: Module ceph not found解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found解决办法/</id>
    <published>2016-07-24T14:48:23.000Z</published>
    <updated>2016-07-24T15:10:25.730Z</updated>
    <content type="html"><![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>
<a id="more"></a>
<pre><code class="bash">[root@lab8106 ~]<span class="comment"># uname -a</span>
Linux ciserver <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 <span class="comment">#1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux</span>
[root@lab8106 ~]<span class="comment"># modinfo ceph</span>
filename:       /lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64/kernel/fs/ceph/ceph.ko
license:        GPL
description:    Ceph filesystem <span class="keyword">for</span> Linux
author:         Patience Warnick &lt;patience@newdream.net&gt;
author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
author:         Sage Weil &lt;sage@newdream.net&gt;
<span class="built_in">alias</span>:          fs-ceph
rhelversion:    <span class="number">7.1</span>
srcversion:     <span class="number">2086</span>D500AFAF47B7260E08A
depends:        libceph
intree:         Y
vermagic:       <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 SMP mod_unload modversions 
signer:         CentOS Linux kernel signing key
sig_key:        A6:<span class="number">2</span>A:<span class="number">0</span>E:<span class="number">1</span>D:<span class="number">6</span>A:<span class="number">6</span>E:<span class="number">48</span>:<span class="number">4</span>E:<span class="number">9</span>B:FD:<span class="number">73</span>:<span class="number">68</span>:AF:<span class="number">34</span>:<span class="number">08</span>:<span class="number">10</span>:<span class="number">48</span>:E5:<span class="number">35</span>:E5
sig_hashalgo:   sha256
</code></pre>
<p>可以从上面的输出可以看到有个路径为 <code>/lib/modules/3.10.0-229.el7.x86_64/kernel/fs/ceph/ceph.ko</code> 的内核模块，这个就是 cephfs 客户端需要使用到的模块</p>
<h3 id="二、解决办法">二、解决办法</h3><p>解决这个缺失的模块的办法就是升级内核，并且在编译内核的时候需要选上这个模块，在某些商用的 Ceph 里面都是默认把这个模块给屏蔽了，这是因为 Cephfs 并没有达到稳定的标准，而这个在后端版本升级到 10.2 版本（jewel）版本，才正式宣布为第一个稳定版本，当然这个还是慎用为好，除非有比较强大的技术力量支撑，否则也不会出现那么多的大的商用厂家也不开放 Cephfs。</p>
<h3 id="三、总结">三、总结</h3><p>Cephfs这块是比rbd和radosgw这两个部分都复杂的部分，而真正能控制住这个开发的目前主要是 Intel 的<code>zhengyan</code>，从邮件列表里面可以看到主要都是他在修bug，这一块未知的可能性太多，任何小的故障抖动都可能是致命的</p>
<p>Bug不会自己消失，都是在那里的，只是看你有没有碰到</p>
<h3 id="四、变更记录">四、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-24</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
