<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[止于至善]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2018-05-29T16:01:56.305Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[cephfs元数据池故障的恢复]]></title>
    <link href="http://www.zphj1987.com/2018/05/29/cephfs-metadatapool-disaster-recover/"/>
    <id>http://www.zphj1987.com/2018/05/29/cephfs-metadatapool-disaster-recover/</id>
    <published>2018-05-29T15:37:52.000Z</published>
    <updated>2018-05-29T16:01:56.305Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>cephfs 在L版本已经比较稳定了，这个稳定的意义个人觉得是在其故障恢复方面的成熟，一个文件系统可恢复是其稳定必须具备的属性，本篇就是根据官网的文档来实践下这个恢复的过程</p>
<h2 id="实践过程">实践过程</h2><h3 id="部署一个ceph_Luminous集群">部署一个ceph  Luminous集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">12.2</span>.<span class="number">5</span> (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br></pre></td></tr></table></figure>
<p>创建filestore<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd create  lab102  --filestore  --data /dev/sdb1  --journal /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>这里想用filestore进行测试就按上面的方法去创建osd即可</p>
<a id="more"></a>
<p>传入测试数据</p>
<ul>
<li>doc </li>
<li>pic</li>
<li>vidio<br>这里提供下载链接</li>
</ul>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg" target="_blank" rel="external">https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg</a> 密码：ugjo</p>
</blockquote>
<p>这个是网上下载的模板的数据，方便进行真实的文件的模拟，dd产生的是空文件，有的时候会影响到测试</p>
<p>需要更多的测试文档推荐可以从下面网站下载</p>
<p>视频下载：</p>
<blockquote>
<p><a href="https://videos.pexels.com/popular-videos" target="_blank" rel="external">https://videos.pexels.com/popular-videos</a></p>
</blockquote>
<p>图片下载：</p>
<blockquote>
<p><a href="https://www.pexels.com/" target="_blank" rel="external">https://www.pexels.com/</a></p>
</blockquote>
<p>文档下载：</p>
<blockquote>
<p><a href="http://office.mmais.com.cn/Template/Home.shtml" target="_blank" rel="external">http://office.mmais.com.cn/Template/Home.shtml</a></p>
</blockquote>
<h3 id="元数据模拟故障">元数据模拟故障</h3><p>跟元数据相关的故障无非就是mds无法启动，或者元数据pg损坏了，这里我们模拟的比较极端的情况，把metadata的元数据对象全部清空掉，这个基本能覆盖到最严重的故障了，数据的损坏不在元数据损坏的范畴</p>
<p>清空元数据存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> object <span class="keyword">in</span> `rados -p metadata ls`;<span class="keyword">do</span> rados -p metadata rm <span class="variable">$object</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>重启下mds进程，应该mds是无法恢复正常的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster:</span><br><span class="line">    id:     <span class="number">9</span>ec7768a-<span class="number">5</span>e7c-<span class="number">4</span>f8e-<span class="number">8</span>a85-<span class="number">89895</span>e338cca</span><br><span class="line">    health: HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> filesystem is degraded</span><br><span class="line">            <span class="number">1</span> mds daemon damaged</span><br><span class="line">            too few PGs per OSD (<span class="number">16</span> &lt; min <span class="number">30</span>)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: <span class="number">1</span> daemons, quorum lab102</span><br><span class="line">    mgr: lab102(active)</span><br><span class="line">    mds: ceph-<span class="number">0</span>/<span class="number">1</span>/<span class="number">1</span> up , <span class="number">1</span> up:standby, <span class="number">1</span> damaged</span><br><span class="line">    osd: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>准备开始我们的修复过程</p>
<h3 id="元数据故障恢复">元数据故障恢复</h3><p>设置允许多文件系统<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph fs flag <span class="built_in">set</span> <span class="built_in">enable</span>_multiple <span class="literal">true</span> --yes-i-really-mean-it</span><br></pre></td></tr></table></figure></p>
<p>创建一个新的元数据池，这里是为了不去动原来的metadata的数据，以免损坏原来的元数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create recovery <span class="number">8</span></span><br></pre></td></tr></table></figure></p>
<p>将老的存储池data和新的元数据池recovery关联起来并且创建一个新的recovery-fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph fs new recovery-fs recovery data --allow-dangerous-metadata-overlay</span></span><br><span class="line">new fs with metadata pool <span class="number">3</span> and data pool <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>做下新的文件系统的初始化相关工作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#cephfs-data-scan init --force-init --filesystem recovery-fs --alternate-pool recovery</span></span><br></pre></td></tr></table></figure></p>
<p>reset下新的fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#ceph fs reset recovery-fs --yes-i-really-mean-it</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset session</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset snap</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset inode</span></span><br></pre></td></tr></table></figure></p>
<p>做相关的恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_extents --force-pool --alternate-pool recovery --filesystem ceph  data</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_inodes --alternate-pool recovery --filesystem ceph --force-corrupt --force-init data</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_links --filesystem recovery-fs</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># systemctl start ceph-mds@lab102</span></span><br><span class="line">等待mds active 以后再继续下面操作</span><br><span class="line">[root@lab102 ~]<span class="comment"># ceph daemon mds.lab102 scrub_path / recursive repair</span></span><br></pre></td></tr></table></figure>
<p>设置成默认的fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph fs set-default recovery-fs</span></span><br></pre></td></tr></table></figure></p>
<p>挂载检查数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#  mount -t ceph 192.168.19.102:/ /mnt</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># ll /mnt</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">1</span> Jan  <span class="number">1</span>  <span class="number">1970</span> lost+found</span><br><span class="line">[root@lab102 ~]<span class="comment"># ll /mnt/lost+found/</span></span><br><span class="line">total <span class="number">226986</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">569306</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000001</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root <span class="number">16240627</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000002</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1356367</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000003</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">137729</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000004</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">155163</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000005</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">118909</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000006</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1587656</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000007</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">252705</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000008</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1825192</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000009</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">156990</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>a</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">3493435</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>b</span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">342390</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>c</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1172247</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>d</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">2516169</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>e</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">3218770</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>f</span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">592729</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000010</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到在lost+found里面就有数据了<br><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000010</span> </span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000010</span>: Microsoft PowerPoint <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000011</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000011</span>: Microsoft Word <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000012</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000012</span>: Microsoft Word <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000013</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000013</span>: Microsoft PowerPoint <span class="number">2007</span>+</span><br></pre></td></tr></table></figure></p>
<p>这个生成的文件名称就是实际文件存储的数据的prifix，也就是通过原始inode进行的运算得到的</p>
<p>如果提前备份好了原始的元数据信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph daemon mds.lab102 dump cache &gt; /tmp/mdscache</span></span><br></pre></td></tr></table></figure></p>
<p>那么可以比较轻松的找到丢失的文件</p>
<h2 id="总结">总结</h2><p>在我另外一篇文章当中已经写过了，通过文件的inode可以把文件跟后台的对象结合起来，在以前我的恢复的思路是，把后台的对象全部抓出来，然后自己手动去对对象进行拼接，实际是数据存在的情况下，反向把文件重新link到一个路径，这个是官方提供的的恢复方法，mds最大的担心就是mds自身的元数据的损坏可能引起整个文件系统的崩溃，而现在，基本上只要data的数据还在的话，就不用担心数据丢掉，即使文件路径信息没有了，但是文件还在</p>
<p>通过备份mds cache可以把文件名称，路径，大小和inode关联起来，而恢复的数据是对象前缀，也就是备份好了mds cache 就可以把整个文件信息串联起来了</p>
<p>虽然cephfs的故障不是常发生，但是万一呢</p>
<p>后续准备带来一篇关于cephfs从挂载点误删除数据后的数据恢复的方案，这个目前已经进行了少量文件的恢复试验了，等后续进行大量文件删除的恢复后，再进行分享</p>
<h2 id="参考文档">参考文档</h2><p><a href="http://docs.ceph.com/docs/luminous/cephfs/disaster-recovery/" target="_blank" rel="external">disaster-recovery</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-05-29</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>cephfs 在L版本已经比较稳定了，这个稳定的意义个人觉得是在其故障恢复方面的成熟，一个文件系统可恢复是其稳定必须具备的属性，本篇就是根据官网的文档来实践下这个恢复的过程</p>
<h2 id="实践过程">实践过程</h2><h3 id="部署一个ceph_Luminous集群">部署一个ceph  Luminous集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">12.2</span>.<span class="number">5</span> (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br></pre></td></tr></table></figure>
<p>创建filestore<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd create  lab102  --filestore  --data /dev/sdb1  --journal /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>这里想用filestore进行测试就按上面的方法去创建osd即可</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[cosbench使用方法]]></title>
    <link href="http://www.zphj1987.com/2018/04/12/cosbench-how-to-use/"/>
    <id>http://www.zphj1987.com/2018/04/12/cosbench-how-to-use/</id>
    <published>2018-04-11T17:18:33.000Z</published>
    <updated>2018-04-11T17:38:02.798Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/f1bbph3hxt9twexfcgmh3wtp/cosbenchnnnn.png" alt="cosbench.png-10.4kB"><br></center>

<h2 id="前言">前言</h2><p>cosbench的功能很强大，但是配置起来可能就有点不是太清楚怎么配置了，本篇将梳理一下这个测试的配置过程，以及一些测试注意项目，以免无法完成自己配置模型的情况</p>
<h2 id="安装">安装</h2><p>cosbench模式是一个控制端控制几个driver向后端rgw发起请求</p>
<p>下载最新版本</p>
<blockquote>
<p><a href="https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip" target="_blank" rel="external">https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip</a></p>
</blockquote>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 cosbench]<span class="comment"># unzip 0.4.2.zip</span></span><br><span class="line">[root@lab102 cosbench]<span class="comment"># yum install java-1.7.0-openjdk nmap-ncat</span></span><br></pre></td></tr></table></figure>
<p>同时可以执行的workloads的个数通过下面的control参数控制</p>
<blockquote>
<p>concurrency=1</p>
</blockquote>
<p>默认是一个，这个为了保证单机的硬件资源足够，保持单机启用一个workload</p>
<p>创建一个s3用户<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># radosgw-admin user create --uid=test1 --display-name="test1" --access-key=test1  --secret-key=test1</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span>: <span class="string">"test1"</span>,</span><br><span class="line">    <span class="string">"display_name"</span>: <span class="string">"test1"</span>,</span><br><span class="line">    <span class="string">"email"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"max_buckets"</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"auid"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"subusers"</span>: [],</span><br><span class="line">    <span class="string">"keys"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span>: <span class="string">"test1"</span>,</span><br><span class="line">            <span class="string">"access_key"</span>: <span class="string">"test1"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span>: <span class="string">"test1"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span>: [],</span><br><span class="line">    <span class="string">"caps"</span>: [],</span><br><span class="line">    <span class="string">"op_mask"</span>: <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span>: [],</span><br><span class="line">    <span class="string">"bucket_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span>: -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span>: -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="配置相关">配置相关</h2><p>cosbench的配置文件结构<br><img src="http://static.zybuluo.com/zphj1987/1ftoz8yntuixd6eqmvxm4old/image.png" alt="image.png-47.7kB"></p>
<ul>
<li>一个workload 可以定义一个或者多个work stages</li>
<li>执行多个work stages是顺序的，执行同一个work stage里面的work是可以并行执行的</li>
<li>每个work里面，worker是来调整负载的</li>
<li>认证可以多个级别的定义，低级别的认证会覆盖高级别的配置</li>
</ul>
<p>可以通过配置多个work的方式来实现并发，而在work内通过增加worker的方式增加并发，从而实现多对多的访问，worker的分摊是分到了driver上面，注意多work的时候的containers不要重名，划分好bucker的空间</p>
<p><img src="http://static.zybuluo.com/zphj1987/1rbgzhq4j0wojgy6x6vd9vik/image.png" alt="image.png-144.5kB"></p>
<p>work相关的说明</p>
<ul>
<li>可以通过写入时间，写入容量，写入iops来控制什么时候结束</li>
<li>interval默认是5s是用来对性能快照的间隔，可以理解为采样点</li>
<li>division 控制workers之间的分配工作的方式是bucket还是对象还是none</li>
<li>默认全部的driver参与工作，也可以通过参数控制部分driver参与</li>
<li>时间会控制执行，如果时间没到，但是指定的对象已经写完了的话就会去进行复写的操作，这里要注意是进行对象的控制还是时间的控制进行的测试</li>
</ul>
<p>如果读取测试的时候，如果没有那个对象，会中断的提示，所以测试读之前需要把测试的对象都填充完毕（最好检查下先）</p>
<h2 id="单项的配置文件">单项的配置文件</h2><h3 id="通过单网关创建bucket">通过单网关创建bucket</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,32);containers=r(1,32);objects=r(0,0);sizes=c(0)B;containers=r(1,32)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure>
<p>如上配置的时候，如果设置的是workers=1,那么就会从当前的driver中挑选一个driver出来，然后选择配置storage进行bucket的创建，如果设置的是workers=2，那么就会挑选两个driver出来进行创建，一个driver负责一半的工作，相当于两个客户端同时向一个网关发起创建的操作</p>
<p>rgw的网关是对等的关系，那么这里肯定就有另外一种配置，我想通过不只一个网关进行创建的操作，那么这个地方是通过增加work的配置来实现的，我们看下配置</p>
<h3 id="通过多网关创建bucket">通过多网关创建bucket</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);containers=r(1,16);objects=r(0,0);sizes=c(0)B;containers=r(1,16)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);containers=r(17,32);objects=r(0,0);sizes=c(0)B;containers=r(17,32)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure>
<p>以上配置就实现了通过两个网关进行创建bucket的配置了，下面是做prepare的相关配置，在cosbench里面有两个部分可以进行写操作，在prepare stage里面和 main stage里面<br>这个地方这样设置的理由是：<br>如果有读和写混合测试的时候，那么就需要提前进行读数据的准备，然后再开始进行读写并发的测试，所以会有一个prepare的阶段，这个在配置文件里面只是type设置的不同，其他没区别，我们可以看下这里web界面里面提供的配置项目，下面其他项目默认都是采取双并发的模式</p>
<p><img src="http://static.zybuluo.com/zphj1987/e11r5pzxy4hpexsodbnj49bi/image.png" alt="prepare"></p>
<p>在写的部分是一样的</p>
<h3 id="通过多网关写数据">通过多网关写数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"putobject"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw1-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"container"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">            config=<span class="string">"containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw2-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"container"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">            config=<span class="string">"containers=u(17,32);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;			</span><br><span class="line">&lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<p>这里有几个参数可以注意一下：</p>
<blockquote>
<p>containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB</p>
</blockquote>
<p>控制写入的bucket的名称的，是全部散列还是把负载均分可以自己去控制，objects是指定写入bucke里面的对象的名称的，sizes是指定大小的，如果两个值不同，就是设置的范围，相同就是设置的指定大小的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span> afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span></span><br></pre></td></tr></table></figure></p>
<p>这个是控制写入什么时候中止的，可以通过时间，也可以通过总的ops，或者总的大小来控制，这个需求可以自己定，afr是控制允许的失效率的，单位为1百万分之<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">interval=<span class="string">"5"</span></span><br></pre></td></tr></table></figure></p>
<p>这个是控制抓取性能数据的周期的</p>
<p>写入的配置就完了</p>
<h3 id="并发读取的配置">并发读取的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"getobj"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw1-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"none"</span> runtime=<span class="string">"30"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">            config=<span class="string">"containers=u(1,16);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw2-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"none"</span> runtime=<span class="string">"30"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">            config=<span class="string">"containers=u(17,32);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;			</span><br><span class="line">&lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<h3 id="删除对象的配置">删除对象的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"cleanup"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">          &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">          &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw1-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);objects=r(1,5);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                  config=<span class="string">"containers=r(1,16);objects=r(1,5);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw2-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);objects=r(1,5);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                  config=<span class="string">"containers=r(17,32);objects=r(1,5);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">      &lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<h3 id="删除bucket的配置">删除bucket的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"dispose"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">          &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">          &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw1-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                  division=<span class="string">"container"</span></span><br><span class="line">                  config=<span class="string">"containers=r(1,16);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw2-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                  division=<span class="string">"container"</span></span><br><span class="line">                  config=<span class="string">"containers=r(17,32);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;			</span><br><span class="line">      &lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<p>上面的workstage一共包括下面几种</p>
<ul>
<li>init 创建bucket</li>
<li>normal write 写入对象</li>
<li>normal read  读取对象</li>
<li>cleanup  清理对象</li>
<li>dispose  清理bucket</li>
</ul>
<p>division是控制多个worker之间的操作怎么去分的控制，最好在operation那层进行控制</p>
<h2 id="测试前自我提问">测试前自我提问</h2><ul>
<li>单机用了几个workload（默认一般一个，保证单个测试资源的独占）</li>
<li>采用了几个driver（决定了客户端的发起是有几个客户端，单机一个就可以）</li>
<li>测试了哪几个项目（init,prepare or normal,remove），单独测试还是混合测试</li>
<li>单个项目的workstage里面启动了几个work（work可以控制请求发向哪里）</li>
<li>单个work里面采用了几个workers(这个是控制几个driver进行并发的)</li>
<li>测试的ceph集群有多少个rgw网关，创建了多少个bucket测试</li>
<li>设置的写入每个bucket的对象为多少？对象大小为多少？测试时间为多久？</li>
</ul>
<p>测试很多文件的时候，可以用ops控制，并且将ops设置大于想测试的文件数目，保证能写入那么多的数据，或者比较确定性能，也可以通过时间控制</p>
<p>那么我来根据自己的需求来进行一个测试模型说明，然后根据说明进行配置</p>
<ul>
<li>采用两个客户端测试，那么准备两个driver</li>
<li>准备配置两个rgw的网关，那么在配置workstage的时候配置两个work对应到两个storage</li>
<li>测试创建，写入，读取，删除对象，删除bucket一套完整测试</li>
<li>wokers设置为2的倍数，初始值为2，让每个driver分得一半的负载，在进行一轮测试后，成倍的增加driver的数目，来增大并发，在性能基本不增加，时延在增加的时候，记录性能值和参数值，这个为本环境的最大性能</li>
<li>创建32个bucket，每个bucket写入5个2M的对象，测试写入时间为600s，读取时间为60s</li>
</ul>
<p>简单框架图<br><img src="http://static.zybuluo.com/zphj1987/6onskxt6rtoqqbq6zwba48bi/cosbench-1.png" alt="cosbench"></p>
<p>配置文件如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-create"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);objects=r(0,0);sizes=c(0)B"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-create"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);objects=r(0,0);sizes=c(0)B"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">		</span><br><span class="line">        &lt;workstage name=<span class="string">"putobject"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"600"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"600"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=u(17,32);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line"></span><br><span class="line">        &lt;workstage name=<span class="string">"getobj"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"none"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">                    config=<span class="string">"containers=u(1,16);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"none"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">                    config=<span class="string">"containers=u(17,32);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">		</span><br><span class="line">		&lt;workstage name=<span class="string">"cleanup"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);objects=r(1,100);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);objects=r(1,100);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);objects=r(1,100);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);objects=r(1,100);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line"></span><br><span class="line">		&lt;workstage name=<span class="string">"dispose"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                    division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                    division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;		</span><br><span class="line">		</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure></p>
<p>上面的测试是为了做测试模板，所以采用了比较小的对象数目和比较小的测试时间</p>
<p>可以根据自己的硬件环境或者客户的要求来设计测试模型，环境够大的时候，提供足够的rgw和足够的客户端才能测出比较大的性能值</p>
<p>测试的时候，尽量把写入和读取的测试分开，也就是分两次测试，避免第一次的写入没写足够对象，读取的时候读不到中断了，对于长达数小时的测试的时候，中断是很令人头疼的，分段可以减少这种中断后的继续测试的时间</p>
<p>写入的测试在允许的范围内，尽量写入多点对象，尽量避免复写，也能够在读取的时候尽量能够足够散列</p>
<p>测试时间能够长尽量长</p>
<h2 id="测试结果">测试结果</h2><p><img src="http://static.zybuluo.com/zphj1987/kp625z85jbve3bvw3aoqbzq3/image.png" alt="result"><br><img src="http://static.zybuluo.com/zphj1987/h92rb9bgrw1sq9fs5jyq883i/image.png" alt="graph"></p>
<p>可以通过线图来看指定测试项目的中间情况，一般是去关注是否出现比较大的抖动    ，相同性能下，抖动越小越好</p>
<h2 id="其他调优">其他调优</h2><p>在硬件环境一定的情况下，可以通过增加nginx负载均衡，或者lvs负载均衡来尝试增加性能值，这个不在本篇的讨论范围内</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-04-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/f1bbph3hxt9twexfcgmh3wtp/cosbenchnnnn.png" alt="cosbench.png-10.4kB"><br></center>

<h2 id="前言">前言</h2><p>cosbench的功能很强大，但是配置起来可能就有点不是太清楚怎么配置了，本篇将梳理一下这个测试的配置过程，以及一些测试注意项目，以免无法完成自己配置模型的情况</p>
<h2 id="安装">安装</h2><p>cosbench模式是一个控制端控制几个driver向后端rgw发起请求</p>
<p>下载最新版本</p>
<blockquote>
<p><a href="https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip">https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip</a></p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的ISCSI GATEWAY]]></title>
    <link href="http://www.zphj1987.com/2018/04/11/ceph-ISCSI-GATEWAY/"/>
    <id>http://www.zphj1987.com/2018/04/11/ceph-ISCSI-GATEWAY/</id>
    <published>2018-04-11T06:57:55.000Z</published>
    <updated>2018-04-11T07:09:14.189Z</updated>
    <content type="html"><![CDATA[<center><br><br><img src="http://static.zybuluo.com/zphj1987/w0jdxwzox8vxnjagipexheoa/gateway.jpg" alt="gateway"><br><br></center>






<h2 id="前言">前言</h2><p>最开始接触这个是在L版本的监控平台里面看到的，有个iscsi网关，但是没看到有类似的介绍，然后通过接口查询到了一些资料，当时由于有比较多的东西需要新内核，新版本的支持，所以并没有配置出来，由于内核已经更新迭代了几个小版本了，经过测试验证可以跑起来了，这里只是把东西跑起来，性能相关的对比需要根据去做</p>
<a id="more"></a>
<h2 id="实践过程">实践过程</h2><h3 id="架构图">架构图</h3><p><img src="http://static.zybuluo.com/zphj1987/hnknjcp8kkiha844m4fyejm4/Ceph_iSCSI_HA_424879_1116_ECE-01.png" alt="Ceph_iSCSI_HA_424879_1116_ECE-01.png-79.4kB"></p>
<p>这个图是引用的红帽的架构图，可以理解为一个多路径的实现方式，那么这个跟之前的有什么不同</p>
<p>主要是有个新的tcmu-runner来处理LIO TCM后端存储的用户空间端的守护进程，这个是在内核之上多了一个用户态的驱动层，这样只需要根据tcmu的标准来对接接口就可以了，而不用去直接跟内核进行交互</p>
<h3 id="需要的软件">需要的软件</h3><p>Ceph Luminous 版本的集群或者更新的版本<br>RHEL/CentOS 7.5或者Linux kernel v4.16或者更新版本的内核<br>其他控制软件</p>
<blockquote>
<p>targetcli-2.1.fb47 or newer package<br> ython-rtslib-2.1.fb64 or newer package<br> cmu-runner-1.3.0 or newer package<br> eph-iscsi-config-2.4 or newer package<br> eph-iscsi-cli-2.5 or newer package</p>
</blockquote>
<p>以上为配置这个环境需要的软件，下面为我使用的版本的软件，统一打包放在一个下载路径<br>我安装的版本如下：</p>
<blockquote>
<p>kernel-4.16.0-0.rc5.git0.1<br>targetcli-fb-2.1.fb48<br>python-rtslib-2.1.67<br>tcmu-runner-1.3.0-rc4<br>ceph-iscsi-config-2.5<br>ceph-iscsi-cli-2.6</p>
</blockquote>
<p>下载链接：</p>
<blockquote>
<p>链接:<a href="https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg" target="_blank" rel="external">https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg</a> 密码:m09k</p>
</blockquote>
<p>如果环境之前有安装过其他版本，需要先卸载掉，并且需要提前部署好一个Luminous 最新版本的集群<br>官方建议调整的参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_client_watch_timeout 15'</span></span><br><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_heartbeat_grace 20'</span></span><br><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_heartbeat_interval 5'</span></span><br></pre></td></tr></table></figure></p>
<h3 id="配置过程">配置过程</h3><p>创建一个存储池<br>需要用到rbd存储池，用来存储iscsi的配置文件，提前创建好一个名字是rbd的存储池</p>
<p>创建iscsi-gateway配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch /etc/ceph/iscsi-gateway.cfg</span><br></pre></td></tr></table></figure></p>
<p>修改iscsi-gateway.cfg配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[config]</span><br><span class="line"><span class="comment"># Name of the Ceph storage cluster. A suitable Ceph configuration file allowing</span></span><br><span class="line"><span class="comment"># access to the Ceph storage cluster from the gateway node is required, if not</span></span><br><span class="line"><span class="comment"># colocated on an OSD node.</span></span><br><span class="line">cluster_name = ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place a copy of the ceph cluster's admin keyring in the gateway's /etc/ceph</span></span><br><span class="line"><span class="comment"># drectory and reference the filename here</span></span><br><span class="line">gateway_keyring = ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># API settings.</span></span><br><span class="line"><span class="comment"># The API supports a number of options that allow you to tailor it to your</span></span><br><span class="line"><span class="comment"># local environment. If you want to run the API under https, you will need to</span></span><br><span class="line"><span class="comment"># create cert/key files that are compatible for each iSCSI gateway node, that is</span></span><br><span class="line"><span class="comment"># not locked to a specific node. SSL cert and key files *must* be called</span></span><br><span class="line"><span class="comment"># 'iscsi-gateway.crt' and 'iscsi-gateway.key' and placed in the '/etc/ceph/' directory</span></span><br><span class="line"><span class="comment"># on *each* gateway node. With the SSL files in place, you can use 'api_secure = true'</span></span><br><span class="line"><span class="comment"># to switch to https mode.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To support the API, the bear minimum settings are:</span></span><br><span class="line">api_secure = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Additional API configuration options are as follows, defaults shown.</span></span><br><span class="line"><span class="comment"># api_user = admin</span></span><br><span class="line"><span class="comment"># api_password = admin</span></span><br><span class="line"><span class="comment"># api_port = 5001</span></span><br><span class="line"><span class="comment"># trusted_ip_list = 192.168.0.10,192.168.0.11</span></span><br></pre></td></tr></table></figure></p>
<p>最后一行的trusted_ip_list修改为用来配置网关的主机IP，我的环境为</p>
<blockquote>
<p>trusted_ip_list =192.168.219.128,192.168.219.129</p>
</blockquote>
<p>所有网关节点的这个配置文件的内容需要一致，修改好一台直接scp到每个网关节点上</p>
<p>启动API服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 install]<span class="comment"># systemctl daemon-reload</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl enable rbd-target-api</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl start rbd-target-api</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl status rbd-target-api</span></span><br><span class="line">● rbd-target-api.service - Ceph iscsi target configuration API</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rbd-target-api.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu <span class="number">2018</span>-<span class="number">03</span>-<span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> CST; <span class="number">18</span>min ago</span><br><span class="line"> Main PID: <span class="number">1493</span> (rbd-target-api)</span><br><span class="line">   CGroup: /system.slice/rbd-target-api.service</span><br><span class="line">           └─<span class="number">1493</span> /usr/bin/python /usr/bin/rbd-target-api</span><br><span class="line"></span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> lab101 systemd[<span class="number">1</span>]: Started Ceph iscsi target configuration API.</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> lab101 systemd[<span class="number">1</span>]: Starting Ceph iscsi target configuration API...</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]: Started the configuration object watcher</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]: Checking <span class="keyword">for</span> config object changes every <span class="number">1</span>s</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]:  * Running on http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">5000</span>/</span><br></pre></td></tr></table></figure></p>
<p>配置iscsi<br>执行gwcli命令<br><img src="http://static.zybuluo.com/zphj1987/b74q5lumh96ui9uhknf2329i/image.png" alt="image.png-23kB"></p>
<p>默认是这样的</p>
<p>进入icsi-target创建一个target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/&gt; <span class="built_in">cd</span> iscsi-target </span><br><span class="line">/iscsi-target&gt; create iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建iSCSI网关。以下使用的IP是用于iSCSI数据传输的IP,它们可以与trusted_ip_list中列出的用于管理操作的IP相同，也可以不同，看有没有做多网卡分离<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target&gt; <span class="built_in">cd</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw/</span><br><span class="line">/iscsi-target...-gw:iscsi-igw&gt; <span class="built_in">cd</span> gateways </span><br><span class="line">/iscsi-target...-igw/gateways&gt; create lab101 <span class="number">192.168</span>.<span class="number">219.128</span> skipchecks=<span class="literal">true</span></span><br><span class="line">OS version/package checks have been bypassed</span><br><span class="line">Adding gateway, syncing <span class="number">0</span> disk(s) and <span class="number">0</span> client(s)</span><br><span class="line">  /iscsi-target...-igw/gateways&gt; create lab102 <span class="number">192.168</span>.<span class="number">219.129</span> skipchecks=<span class="literal">true</span></span><br><span class="line">OS version/package checks have been bypassed</span><br><span class="line">Adding gateway, sync<span class="string">'ing 0 disk(s) and 0 client(s)</span><br><span class="line">ok</span><br><span class="line">/iscsi-target...-igw/gateways&gt; ls</span><br><span class="line">o- gateways ............. [Up: 2/2, Portals: 2]</span><br><span class="line">  o- lab101 ............. [192.168.219.128 (UP)]</span><br><span class="line">  o- lab102 ............. [192.168.219.129 (UP)]</span></span><br></pre></td></tr></table></figure></p>
<p>创建一个rbd设备disk_1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...-igw/gateways&gt; <span class="built_in">cd</span> /disks </span><br><span class="line">/disks&gt; create pool=rbd image=disk_1 size=<span class="number">100</span>G</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建一个客户端名称iqn.1994-05.com.redhat:75c3d5efde0<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/disks&gt; <span class="built_in">cd</span> /iscsi-target/iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw/hosts </span><br><span class="line">/iscsi-target...csi-igw/hosts&gt; create iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建chap的用户名密码，由于用户名密码都有特殊要求，如果你不确定，就按我给的去设置，并且chap必须设置，否则服务端是禁止连接的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; auth chap=iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0/admin@a_12a-bb</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>chap的命名规则可以这样查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; <span class="built_in">help</span> auth</span><br><span class="line"></span><br><span class="line">SYNTAX</span><br><span class="line">======</span><br><span class="line">auth [chap] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DESCRIPTION</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">Client authentication can be <span class="built_in">set</span> to use CHAP by supplying the</span><br><span class="line">a string of the form &lt;username&gt;/&lt;password&gt;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">auth chap=username/password | nochap</span><br><span class="line"></span><br><span class="line">username ... the username is <span class="number">8</span>-<span class="number">64</span> character string. Each character</span><br><span class="line">             may either be an alphanumeric or use one of the following</span><br><span class="line">             special characters .,:,-,@.</span><br><span class="line">             Consider using the hosts <span class="string">'shortname'</span> or the initiators IQN</span><br><span class="line">             value as the username</span><br><span class="line"></span><br><span class="line">password ... the password must be between <span class="number">12</span>-<span class="number">16</span> chars <span class="keyword">in</span> length</span><br><span class="line">             containing alphanumeric characters, plus the following</span><br><span class="line">             special characters @,_,-</span><br><span class="line"></span><br><span class="line">WARNING: Using unsupported special characters may result <span class="keyword">in</span> truncation,</span><br><span class="line">         resulting <span class="keyword">in</span> failed logins.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Specifying <span class="string">'nochap'</span> will remove chap authentication <span class="keyword">for</span> the client</span><br><span class="line">across all gateways.</span><br></pre></td></tr></table></figure></p>
<p>增加磁盘到客户端<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; disk add rbd.disk_1</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>到这里就配置完成了，我们看下最终应该是怎么样的<br><img src="http://static.zybuluo.com/zphj1987/erpd7tv1gymtrdkny0h33afy/image.png" alt="image.png-38.5kB"></p>
<h2 id="windows客户端配置">windows客户端配置</h2><p>这个地方我配置的时候用的win10配置的时候出现了无法连接的情况，可能是windows10自身的认证要求跟服务端冲突了，这里用windows server 2016 进行连接测试</p>
<p>windows server开启下Multipath IO</p>
<p>修改windows iscsi客户端的名称<br><img src="http://static.zybuluo.com/zphj1987/fqt0s8h75jm5joffglsm7z4t/image.png" alt="image.png-47.5kB"><br>修改为上面创建的客户端名称</p>
<p>发现门户<br><img src="http://static.zybuluo.com/zphj1987/yp7pteay72mms7fbf7jf6z1u/image.png" alt="image.png-37.7kB"><br>点击发现门户，填写好服务端的IP后直接点确定，这里先不用高级里面的配置</p>
<p><img src="http://static.zybuluo.com/zphj1987/3r2crtvesqir0f2qzhqow8dq/image.png" alt="image.png-35.1kB"></p>
<p>这个时候目标里面已经有一个发现的目标了，显示状态是不活动的，准备点击连接</p>
<p><img src="http://static.zybuluo.com/zphj1987/2oliqvnxb9rogfwimy8ogvx6/image.png" alt="image.png-80.7kB"><br>点击高级，选择门户IP，填写chap登陆信息，然后chap名称就是上面设置的用户名称，因为跟客户端名称设置的一致，也就是客户端的名称，密码就是上面设置的admin@a_12a-bb</p>
<p><img src="http://static.zybuluo.com/zphj1987/138747o4l4cxch52sb5z2z5w/image.png" alt="image.png-21.9kB"></p>
<p>切换到卷和设备，点击自动配置<br><img src="http://static.zybuluo.com/zphj1987/qn4zzyofhh2546fv4nxczbs3/image.png" alt="image.png-47.4kB"></p>
<p>可以看到已经装载设备了</p>
<p>在服务管理器，文件存储服务，卷，磁盘里面查看设备<br><img src="http://static.zybuluo.com/zphj1987/hi5flzjcsbdhv0wglrrkenda/image.png" alt="image.png-92.8kB"></p>
<p>可以看到是配置的LIO-ORG TCMU设备，对设备进行格式化即可</p>
<p><img src="http://static.zybuluo.com/zphj1987/h7yj7k87fllhxuqfz5utc4mv/image.png" alt="image.png-42.6kB"></p>
<p>完成了连接了</p>
<h2 id="Linux的客户端连接">Linux的客户端连接</h2><p>Linux客户端选择建议就选择3.10默认内核，选择高版本的内核的时候在配置多路径的时候碰到内核崩溃的问题</p>
<p>安装连接软件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># yum install iscsi-initiator-utils</span></span><br><span class="line">[root@lab103 ~]<span class="comment"># yum install device-mapper-multipath</span></span><br></pre></td></tr></table></figure></p>
<p>配置多路径</p>
<p>开启服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># mpathconf --enable --with_multipathd y</span></span><br></pre></td></tr></table></figure></p>
<p>修改配置文件/etc/multipath.conf<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">devices &#123;</span><br><span class="line">        device &#123;</span><br><span class="line">                vendor                 <span class="string">"LIO-ORG"</span></span><br><span class="line">                hardware_handler       <span class="string">"1 alua"</span></span><br><span class="line">                path_grouping_policy   <span class="string">"failover"</span></span><br><span class="line">                path_selector          <span class="string">"queue-length 0"</span></span><br><span class="line">                failback               <span class="number">60</span></span><br><span class="line">                path_checker           tur</span><br><span class="line">                prio                   alua</span><br><span class="line">                prio_args              exclusive_pref_bit</span><br><span class="line">                fast_io_fail_tmo       <span class="number">25</span></span><br><span class="line">                no_path_retry          queue</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>重启多路径服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># systemctl reload multipathd</span></span><br></pre></td></tr></table></figure></p>
<p>配置chap的认证</p>
<p>修改配置客户端的名称为上面设置的名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># cat /etc/iscsi/initiatorname.iscsi </span></span><br><span class="line">InitiatorName=iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br></pre></td></tr></table></figure></p>
<p>修改认证的配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># cat /etc/iscsi/iscsid.conf |grep "node.session.auth.username\|node.session.auth.password\|node.session.auth.authmethod"</span></span><br><span class="line">node.session.auth.authmethod = CHAP</span><br><span class="line">node.session.auth.username = iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br><span class="line">node.session.auth.password = admin@a_12a-bb</span><br></pre></td></tr></table></figure></p>
<p>查询iscsi target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># iscsiadm -m discovery -t st -p 192.168.219.128</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">219.128</span>:<span class="number">3260</span>,<span class="number">1</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line"><span class="number">192.168</span>.<span class="number">219.129</span>:<span class="number">3260</span>,<span class="number">2</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br></pre></td></tr></table></figure></p>
<p>连接target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># iscsiadm -m node -T iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw -l</span></span><br><span class="line">Logging <span class="keyword">in</span> to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] (multiple)</span><br><span class="line">Logging <span class="keyword">in</span> to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] (multiple)</span><br><span class="line">Login to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] successful.</span><br><span class="line">Login to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] successful.</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># multipath -ll</span></span><br><span class="line">mpathb (<span class="number">360014052</span><span class="built_in">fc</span>39ba627874fdba9aefcf6c) dm-<span class="number">4</span> LIO-ORG ,TCMU device     </span><br><span class="line">size=<span class="number">100</span>G features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'1 alua'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'queue-length 0'</span> prio=<span class="number">10</span> status=active</span><br><span class="line">| `- <span class="number">5</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span> sdc <span class="number">8</span>:<span class="number">32</span> active ready running</span><br><span class="line">`-+- policy=<span class="string">'queue-length 0'</span> prio=<span class="number">10</span> status=enabled</span><br><span class="line">  `- <span class="number">6</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span> sdd <span class="number">8</span>:<span class="number">48</span> active ready running</span><br></pre></td></tr></table></figure>
<p>查看盘符<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># parted -s /dev/mapper/mpathb print</span></span><br><span class="line">Model: Linux device-mapper (multipath) (dm)</span><br><span class="line">Disk /dev/mapper/mpathb: <span class="number">107</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name                          Flags</span><br><span class="line"> <span class="number">1</span>      <span class="number">17.4</span>kB  <span class="number">134</span>MB  <span class="number">134</span>MB               Microsoft reserved partition  msftres</span><br><span class="line"> <span class="number">2</span>      <span class="number">135</span>MB   <span class="number">107</span>GB  <span class="number">107</span>GB  ntfs         Basic data partition</span><br></pre></td></tr></table></figure></p>
<p>直接使用这个/dev/mapper/mpathb设备即可</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-04-11</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><br><img src="http://static.zybuluo.com/zphj1987/w0jdxwzox8vxnjagipexheoa/gateway.jpg" alt="gateway"><br><br></center>






<h2 id="前言">前言</h2><p>最开始接触这个是在L版本的监控平台里面看到的，有个iscsi网关，但是没看到有类似的介绍，然后通过接口查询到了一些资料，当时由于有比较多的东西需要新内核，新版本的支持，所以并没有配置出来，由于内核已经更新迭代了几个小版本了，经过测试验证可以跑起来了，这里只是把东西跑起来，性能相关的对比需要根据去做</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[parted会启动你的ceph osd，意外不？]]></title>
    <link href="http://www.zphj1987.com/2018/03/23/parted-may-start-your-osd/"/>
    <id>http://www.zphj1987.com/2018/03/23/parted-may-start-your-osd/</id>
    <published>2018-03-23T15:54:55.000Z</published>
    <updated>2018-03-23T16:13:59.765Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yellowhelmet.png" alt="disk"><br></center>

<h2 id="前言">前言</h2><p>如果看到标题，你是不是第一眼觉得写错了，这个怎么可能，完全就是两个不相关的东西，最开始我也是这么想的，直到我发现真的是这样的时候，也是很意外，还是弄清楚下比较好，不然在某个操作下，也许就会出现意想不到的情况</p>
<a id="more"></a>
<h2 id="定位">定位</h2><p>如果你看过我的博客，正好看过这篇<a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/" target="_blank" rel="external">ceph在centos7下一个不容易发现的改变</a>，那么应该还记得这个讲的是centos 7 下面通过udev来实现了osd的自动挂载，这个自动挂载就是本篇需要了解的前提<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># systemctl stop ceph-osd@1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># umount /dev/sdf1 </span></span><br><span class="line">[root@lab101 ~]<span class="comment"># parted -l &amp;&gt;/dev/null</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># ps -ef|grep osd</span></span><br><span class="line">ceph      <span class="number">62701</span>      <span class="number">1</span>  <span class="number">1</span> <span class="number">23</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> /usr/bin/ceph-osd <span class="operator">-f</span> --cluster ceph --id <span class="number">1</span> --setuser ceph --setgroup ceph</span><br><span class="line">root      <span class="number">62843</span>  <span class="number">35114</span>  <span class="number">0</span> <span class="number">23</span>:<span class="number">25</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep --color=auto osd</span><br></pre></td></tr></table></figure></p>
<p>看这个操作过程，是不是很神奇，是不是很意外，不管怎么说，parted -l的一个操作把我们的osd给自动mount 起来了，也自动给启动了</p>
<p>出现这个以后，我们先看下日志怎么出的，大概看起来的是这样的<br><img src="http://static.zybuluo.com/zphj1987/u7va8eisqwexkyx19zxjhemk/parted.gif" alt="parted.gif-4083.1kB"></p>
<p>可以看到确实是实时去触发的</p>
<p>服务器上面是有一个这个服务的 </p>
<blockquote>
<p>systemd-udevd.service<br>看到在做parted -l 后就会起一个这个子进程的<br><img src="http://static.zybuluo.com/zphj1987/xh8q4om2hyc7obw4xn09w8yx/image.png" alt="image.png-146.3kB"></p>
</blockquote>
<p>在尝试关闭这个服务后，再做parted -l操作就不会出现自动启动进程</p>
<h2 id="原因">原因</h2><p>执行parted -l 对指定设备发起parted命令的时候，就会对内核做一个trigger，而我们的</p>
<blockquote>
<p>/lib/udev/rules.d/95-ceph-osd.rules<br>这个文件一旦触发是会去调用<br>/usr/sbin/ceph-disk —log-stdout -v trigger /dev/$name</p>
</blockquote>
<p>也就是自动挂载加上启动osd的的操作了</p>
<h3 id="可能带来什么困扰">可能带来什么困扰</h3><p>其实这个我也不知道算不算bug，至少在正常使用的时候是没有问题的，以至于这个功能已经有了这么久，而我并没有察觉到，也没有感觉到它给我带来的干扰，那么作为一名测试人员，现在来构思一种可能出现的破坏场景，只要按照正常操作去做的，还会出现的，就是有可能发生的事情<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /var/lib/ceph/osd/</span><br><span class="line">[root@lab101 osd]<span class="comment"># df -h|grep osd</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># systemctl stop ceph-osd@1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># umount /dev/sdf1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># parted -l  &amp;&gt;/dev/null</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># rm -rf ceph-1/</span></span><br><span class="line">rm: cannot remove ‘ceph-<span class="number">1</span>/’: Device or resource busy</span><br><span class="line">[root@lab101 osd]<span class="comment"># ll ceph-1/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">33</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到除了上面的parted -l以外，其他操作都是一个正常的操作，umount掉挂载点，然后清理掉这个目录，然后数据就被删了，当然正常情况下也许没人在正好那个点来了一个parted,但是不是完全没有可能</p>
<p>还有种情况就是我是要做维护，我想umount掉挂载点，不想进程起来，执行parted是很常规的操作了，结果自己给我拉起来了，这个操作应该比较常见的</p>
<h3 id="如何解决这个情况">如何解决这个情况</h3><p>第一种方法<br>什么都不动，你知道这个事情就行，执行过parted后再加上个df多检查下</p>
<p>第二种方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop systemd-udevd</span><br></pre></td></tr></table></figure></p>
<p>这个会带来其他什么影响，暂时不好判断，还没深入研究，影响应该也只会在硬件变动和一些udev触发的需求，不确定的情况可以不改，不推荐此方法</p>
<p>第三种方法<br>不用这个/lib/udev/rules.d/95-ceph-osd.rules做控制了，自己去写配置文件，或者写fstab，都可以，保证启动后能够自动mount，服务能够正常启动就可以了，个人从维护角度还是偏向于第三种方法，记录的信息越多，维护的时候越方便，这个是逼着记录了一些信息，虽然可以什么信息也不记</p>
<h2 id="总结">总结</h2><p>其实这个问题梳理清楚了也还好，最可怕的也许就是不知道为什么，特别是觉得完全不搭边的东西相互起了关联，至少在我们的研发跟我描述这个问题的时候，我想的是，还有这种神操作，是不是哪里加入了钩子程序什么的，花了点时间查到了原因，也方便在日后碰到不那么惊讶了</p>
<p>ceph北京大会已经顺利开完了，等PPT出来以后再学习一下新的东西，内容应该还是很多的，其实干货不干货，都在于你发现了什么，如果有一个PPT里面你提取到了一个知识点，你都是赚到了，何况分享的人并没有告知的义务的，所以每次看到有分享都是很感谢分享者的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-03-23</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yellowhelmet.png" alt="disk"><br></center>

<h2 id="前言">前言</h2><p>如果看到标题，你是不是第一眼觉得写错了，这个怎么可能，完全就是两个不相关的东西，最开始我也是这么想的，直到我发现真的是这样的时候，也是很意外，还是弄清楚下比较好，不然在某个操作下，也许就会出现意想不到的情况</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[REDHAT 7.5beta 新推出的VDO功能]]></title>
    <link href="http://www.zphj1987.com/2018/02/10/REDHAT-7-5beta-with-VDO/"/>
    <id>http://www.zphj1987.com/2018/02/10/REDHAT-7-5beta-with-VDO/</id>
    <published>2018-02-10T08:25:08.000Z</published>
    <updated>2018-02-11T07:17:27.655Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/hat.jpg" alt="network"><br></center>

<h2 id="前言">前言</h2><h3 id="关于VDO">关于VDO</h3><p>VDO的技术来源于收购的Permabit公司，一个专门从事重删技术的公司，所以技术可靠性是没有问题的</p>
<p>VDO是一个内核模块，目的是通过重删减少磁盘的空间占用，以及减少复制带宽，VDO是基于块设备层之上的，也就是在原设备基础上映射出mapper虚拟设备，然后直接使用即可，功能的实现主要基于以下技术：</p>
<ul>
<li><p>零区块的排除：</p>
<p>在初始化阶段，整块为0的会被元数据记录下来，这个可以用水杯里面的水和沙子混合的例子来解释，使用滤纸（零块排除），把沙子（非零空间）给过滤出来，然后就是下一个阶段的处理</p>
</li>
</ul>
<ul>
<li><p>重复数据删除：</p>
<p>在第二阶段，输入的数据会判断是不是冗余数据（在写入之前就判断），这个部分的数据通过UDS内核模块来判断（U niversal D eduplication S ervice），被判断为重复数据的部分不会被写入，然后对元数据进行更新，直接指向原始已经存储的数据块即可</p>
</li>
<li><p>压缩：</p>
<p>一旦消零和重删完成，LZ4压缩会对每个单独的数据块进行处理，然后压缩好的数据块会以固定大小4KB的数据块存储在介质上，由于一个物理块可以包含很多的压缩块，这个也可以加速读取的性能</p>
</li>
</ul>
<p>上面的技术看起来很容易理解，但是实际做成产品还是相当大的难度的，技术设想和实际输出还是有很大距离，不然redhat也不会通过收购来获取技术，而不是自己去重新写一套了<br><a id="more"></a></p>
<h3 id="如何获取VDO">如何获取VDO</h3><p>主要有两种方式，一种是通过申请测试版的方式申请redhat 7.5的ISO，这个可以进行一个月的测试</p>
<p>另外一种方式是申请测试版本，然后通过源码在你正在使用的ISO上面进行相关的测试，从适配方面在自己的ISO上面进行测试能够更好的对比，由于基于redhat的源码做分发会涉及法律问题，这里就不做过多讲解，也不提供rpm包，自行申请测试即可</p>
<h2 id="实践过程">实践过程</h2><h3 id="安装VDO">安装VDO</h3><p>安装的操作系统为CentOS Linux release 7.4.1708<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># lsb_release -a</span></span><br><span class="line">LSB Version:	:core-<span class="number">4.1</span>-amd64:core-<span class="number">4.1</span>-noarch</span><br><span class="line">Distributor ID:	CentOS</span><br><span class="line">Description:	CentOS Linux release <span class="number">7.4</span>.<span class="number">1708</span> (Core) </span><br><span class="line">Release:	<span class="number">7.4</span>.<span class="number">1708</span></span><br><span class="line">Codename:	Core</span><br></pre></td></tr></table></figure></p>
<p>内核版本如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># uname -a</span></span><br><span class="line">Linux lab101 <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 <span class="comment">#1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -qa|grep kernel</span></span><br><span class="line">kernel-tools-libs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64</span><br><span class="line">abrt-addon-kerneloops-<span class="number">2.1</span>.<span class="number">11</span>-<span class="number">48</span>.el7.centos.x86_64</span><br><span class="line">kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64</span><br></pre></td></tr></table></figure>
<p>我们把内核升级一下，因为这个模块比较新，所以选择目前updates里面最新的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/<span class="number">7</span>/updates/x86_64/Packages/kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693.17</span>.<span class="number">1</span>.el7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>大版本一致，小版本不同，直接安装即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh kernel-3.10.0-693.17.1.el7.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693.17</span>.<span class="number">1</span>.el7       <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># grub2-set-default 'CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)'</span></span><br></pre></td></tr></table></figure></p>
<p>重启服务器<br>安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh kmod-kvdo-6.1.0.98-11.el7.centos.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:kmod-kvdo-<span class="number">6.1</span>.<span class="number">0.98</span>-<span class="number">11</span>.el7.centos <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># yum install PyYAML   </span></span><br><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh vdo-6.1.0.98-13.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:vdo-<span class="number">6.1</span>.<span class="number">0.98</span>-<span class="number">13</span>                  <span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure></p>
<p>到这里安装就完成了</p>
<h3 id="配置VDO">配置VDO</h3><p>创建一个vdo卷<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo create --name=my_vdo  --device=/dev/sdb1   --vdoLogicalSize=80G --writePolicy=sync</span></span><br><span class="line">Creating VDO my_vdo</span><br><span class="line">Starting VDO my_vdo</span><br><span class="line">Starting compression on VDO my_vdo</span><br><span class="line">VDO instance <span class="number">0</span> volume is ready at /dev/mapper/my_vdo</span><br></pre></td></tr></table></figure></p>
<p>参数解释：<br>name是创建的vdo名称，也就是生成的新设备的名称，device是指定的设备，vdoLogicalSize是指定新生成的设备的大小，因为vdo是支持精简配置的，也就是你原来1T的物理空间，这里可以创建出超过1T的逻辑空间，因为内部支持重删，可以根据数据类型进行放大，writePolicy是指定写入的模式的</p>
<p>如果磁盘设备是write back模式的可以设置为aysnc，如果没有的话就设置为sync模式</p>
<p>如果磁盘没有写缓存或者有write throuth cache的时候设置为sync模式<br>如果磁盘有write back cache的时候就必须设置成async模式</p>
<p>默认是sync模式的，这里的同步异步实际上是告诉vdo，我们的底层存储是不是有写缓存，有缓存的话就要告诉vdo我们底层是async的，没有缓存的时候就是sync</p>
<p>检查我们的磁盘的写入方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># cat /sys/block/sdb/device/scsi_disk/0\:0\:1\:0/cache_type </span></span><br><span class="line">write through</span><br></pre></td></tr></table></figure></p>
<p>这个输出的根据上面的规则，我们设置为sync模式</p>
<p>修改缓存模式的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vdo changeWritePolicy --writePolicy=sync_or_async --name=vdo_name</span><br></pre></td></tr></table></figure></p>
<p>格式化硬盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># mkfs.xfs -K /dev/mapper/my_vdo </span></span><br><span class="line">meta-data=/dev/mapper/my_vdo     isize=<span class="number">512</span>    agcount=<span class="number">4</span>, agsize=<span class="number">5242880</span> blks</span><br><span class="line">         =                       sectsz=<span class="number">4096</span>  attr=<span class="number">2</span>, projid32bit=<span class="number">1</span></span><br><span class="line">         =                       crc=<span class="number">1</span>        finobt=<span class="number">0</span>, sparse=<span class="number">0</span></span><br><span class="line">data     =                       bsize=<span class="number">4096</span>   blocks=<span class="number">20971520</span>, imaxpct=<span class="number">25</span></span><br><span class="line">         =                       sunit=<span class="number">0</span>      swidth=<span class="number">0</span> blks</span><br><span class="line">naming   =version <span class="number">2</span>              bsize=<span class="number">4096</span>   ascii-ci=<span class="number">0</span> ftype=<span class="number">1</span></span><br><span class="line"><span class="built_in">log</span>      =internal <span class="built_in">log</span>           bsize=<span class="number">4096</span>   blocks=<span class="number">10240</span>, version=<span class="number">2</span></span><br><span class="line">         =                       sectsz=<span class="number">4096</span>  sunit=<span class="number">1</span> blks, lazy-count=<span class="number">1</span></span><br><span class="line">realtime =none                   extsz=<span class="number">4096</span>   blocks=<span class="number">0</span>, rtextents=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>使用-K参数是加速了格式化的操作，也就是不发送丢弃的请求，因为之前创建了vdo，已经将其初始化为0了，所以可以采用这个操作</p>
<p>我们挂载的时候最好能加上discard的选项，精简配置的设备需要对之前的空间进行回收，一般来说有在线的和离线的回收，离线的就通过fstrim来进行回收即可</p>
<p>挂载设备<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># mount -o discard /dev/mapper/my_vdo /myvod/</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdostats --human-readable </span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.0</span>G     <span class="number">46.0</span>G   <span class="number">8</span>%           <span class="number">99</span>%</span><br></pre></td></tr></table></figure>
<p>默认创建完vdo设备就会占用4G左右的空间，这个用来存储UDS和VDO的元数据</p>
<p>检查重删和压缩是否开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo status -n my_vdo|grep Deduplication</span></span><br><span class="line">    Deduplication: enabled</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdo status -n my_vdo|grep Compress</span></span><br><span class="line">    Compression: enabled</span><br></pre></td></tr></table></figure></p>
<p>如果没有开启，可以通过下面的命令开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vdo <span class="built_in">enable</span>Compression -n &lt;vdo_vol_name&gt;</span><br><span class="line">vdo <span class="built_in">enable</span>Deduplication -n &lt;vdo_vol_name&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="验证重删功能">验证重删功能</h3><figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">[root@lab<span class="number">101</span> ~]<span class="title"># df -h|grep vdo</span><br><span class="line">/dev/mapper/my_vdo   80</span>G   <span class="number">33</span>M   <span class="number">80</span>G   <span class="number">1</span><span class="preprocessor">%</span> /myvod</span><br><span class="line">[root@lab<span class="number">101</span> ~]<span class="title"># vdostats --hu</span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       50</span><span class="number">.0</span>G      <span class="number">4.0</span>G     <span class="number">46.0</span>G   <span class="number">8</span><span class="preprocessor">%</span>           <span class="number">99</span><span class="preprocessor">%</span></span><br></pre></td></tr></table></figure>
<p>传入一个ISO文件CentOS-7-x86_64-NetInstall-1708.iso 422M的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep vdo</span></span><br><span class="line">/dev/mapper/my_vdo   <span class="number">80</span>G  <span class="number">455</span>M   <span class="number">80</span>G   <span class="number">1</span>% /myvod</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdostats --hu</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.4</span>G     <span class="number">45.6</span>G   <span class="number">8</span>%            <span class="number">9</span>%</span><br></pre></td></tr></table></figure></p>
<p>然后重复传入3个相同文件，一共四个文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep vdo</span></span><br><span class="line">/dev/mapper/my_vdo   <span class="number">80</span>G  <span class="number">1.7</span>G   <span class="number">79</span>G   <span class="number">3</span>% /myvod</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdostats --hu</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.4</span>G     <span class="number">45.6</span>G   <span class="number">8</span>%           <span class="number">73</span>%</span><br></pre></td></tr></table></figure></p>
<p>可以看到后面传入的文件，并没有占用底层存储的实际空间</p>
<h3 id="验证压缩功能">验证压缩功能</h3><p>测试数据来源 silesia的资料库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://sun.aei.polsl.pl/~sdeor/corpus/silesia.zip</span><br></pre></td></tr></table></figure>
<p>通过资料库里面的文件来看下对不同类型的数据的压缩情况</p>
<table>
<thead>
<tr>
<th style="text-align:center">Filename</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">原始空间（KB）</th>
<th style="text-align:center">实际占用空间（KB）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">dickens</td>
<td style="text-align:center">狄更斯文集</td>
<td style="text-align:center">英文原文</td>
<td style="text-align:center">9953</td>
<td style="text-align:center">9948</td>
</tr>
<tr>
<td style="text-align:center">mozilla</td>
<td style="text-align:center">Mozilla的1.0可执行文件</td>
<td style="text-align:center">执行程序</td>
<td style="text-align:center">50020</td>
<td style="text-align:center">33228</td>
</tr>
<tr>
<td style="text-align:center">mr</td>
<td style="text-align:center">医用resonanse图像</td>
<td style="text-align:center">图片</td>
<td style="text-align:center">9736</td>
<td style="text-align:center">9272</td>
</tr>
<tr>
<td style="text-align:center">nci</td>
<td style="text-align:center">结构化的化学数据库</td>
<td style="text-align:center">数据库</td>
<td style="text-align:center">32767</td>
<td style="text-align:center">10168</td>
</tr>
<tr>
<td style="text-align:center">ooffice</td>
<td style="text-align:center">Open Office.org 1.01 DLL</td>
<td style="text-align:center">可执行程序</td>
<td style="text-align:center">6008</td>
<td style="text-align:center">5640</td>
</tr>
<tr>
<td style="text-align:center">osdb</td>
<td style="text-align:center">基准测试用的MySQL格式示例数据库</td>
<td style="text-align:center">数据库</td>
<td style="text-align:center">9849</td>
<td style="text-align:center">9824</td>
</tr>
<tr>
<td style="text-align:center">reymont</td>
<td style="text-align:center">瓦迪斯瓦夫·雷蒙特的书</td>
<td style="text-align:center">PDF</td>
<td style="text-align:center">6471</td>
<td style="text-align:center">6312</td>
</tr>
<tr>
<td style="text-align:center">samba</td>
<td style="text-align:center">samba源代码</td>
<td style="text-align:center">src源码</td>
<td style="text-align:center">21100</td>
<td style="text-align:center">11768</td>
</tr>
<tr>
<td style="text-align:center">sao</td>
<td style="text-align:center">星空数据</td>
<td style="text-align:center">天文格式的bin文件</td>
<td style="text-align:center">7081</td>
<td style="text-align:center">7036</td>
</tr>
<tr>
<td style="text-align:center">webster</td>
<td style="text-align:center">辞海</td>
<td style="text-align:center">HTML</td>
<td style="text-align:center">40487</td>
<td style="text-align:center">40144</td>
</tr>
<tr>
<td style="text-align:center">xml</td>
<td style="text-align:center">XML文件</td>
<td style="text-align:center">HTML</td>
<td style="text-align:center">5220</td>
<td style="text-align:center">2180</td>
</tr>
<tr>
<td style="text-align:center">x-ray</td>
<td style="text-align:center">透视医学图片</td>
<td style="text-align:center">医院数据</td>
<td style="text-align:center">8275</td>
<td style="text-align:center">8260</td>
</tr>
</tbody>
</table>
<p>可以看到都有不同程度的压缩，某些类型的数据压缩能达到50%的比例</p>
<p>停止vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo stop  -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<p>启动vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo start  -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<p>删除vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo remove -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<h2 id="VDO和CEPH能产生什么火花？">VDO和CEPH能产生什么火花？</h2><p>在ceph里面可以用到vdo的地方有两个，一个是作为Kernel rbd的前端，在块设备的上层，另外一个是作为OSD的底层，也就是把VDO当OSD来使用，我们看下怎么使用</p>
<h3 id="作为rbd的上层">作为rbd的上层</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># rbd create testvdorbd --size 20G</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># rbd map testvdorbd</span></span><br></pre></td></tr></table></figure>
<p>创建rbd的vdo<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd</span></span><br><span class="line">Creating VDO rbd_vdo</span><br><span class="line">vdo: ERROR -   Device /dev/rbd/rbd/testvdorbd not found (or ignored by filtering).</span><br></pre></td></tr></table></figure></p>
<p>被默认排除掉了，这个以前正好见过类似的问题，比较好处理</p>
<p>这个地方因为vdo添加存储的时候内部调用了lvm相关的配置，然后lvm默认会排除掉rbd，这里修改下lvm的配置文件即可<br>在/etc/lvm/lvm.conf的修改如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">types = [ <span class="string">"fd"</span>, <span class="number">16</span> ,<span class="string">"rbd"</span>, <span class="number">64</span> ]</span><br></pre></td></tr></table></figure></p>
<p>把types里面增加下rbd 的文件类型即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd</span></span><br><span class="line">Creating VDO rbd_vdo</span><br><span class="line">Starting VDO rbd_vdo</span><br><span class="line">Starting compression on VDO rbd_vdo</span><br><span class="line">VDO instance <span class="number">2</span> volume is ready at /dev/mapper/rbd_vdo</span><br></pre></td></tr></table></figure>
<p>挂载<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -o discard /dev/mapper/rbd_vdo /mnt</span><br></pre></td></tr></table></figure></p>
<p>查看容量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># vdostats --human-readable</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/rbd_vdo      <span class="number">20.0</span>G      <span class="number">4.4</span>G     <span class="number">15.6</span>G  <span class="number">22</span>%            <span class="number">3</span>%</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">57316</span>M     <span class="number">49409</span>M        <span class="number">7906</span>M         <span class="number">13.79</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd      <span class="number">0</span>      <span class="number">566</span>M      <span class="number">1.20</span>        <span class="number">46543</span>M         <span class="number">148</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">57316</span>M     <span class="number">48699</span>M        <span class="number">8616</span>M         <span class="number">15.03</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME     ID     USED      %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd      <span class="number">0</span>      <span class="number">1393</span>M      <span class="number">2.95</span>        <span class="number">45833</span>M         <span class="number">355</span></span><br></pre></td></tr></table></figure>
<p>多次传入相同的时候可以看到对于ceph内部来说还是会产生对象的，只是这个在vdo的文件系统来看是不占用物理空间的</p>
<p>对镜像做下copy<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rbd cp testvdorbd testvdorbdclone</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment">#rbd map  testvdorbdclone</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># cat /etc/vdoconf.yml |grep device</span></span><br><span class="line">      device: /dev/rbd/rbd/testvdorbdclone</span><br></pre></td></tr></table></figure>
<p>修改配置文件为对应的设备，就可以启动了,这个操作说明vdo设备是不绑定硬件的，只需要有相关的配置文件，即可对文件系统进行启动</p>
<p>那么这个在一个数据转移用途下，就可以利用vdo对数据进行重删压缩，然后把整个img转移到远端去，这个也符合现在的私有云和公有云之间的数据传输量的问题，会节省不少空间</p>
<h3 id="vdo作为ceph的osd">vdo作为ceph的osd</h3><p>ceph对设备的属性有要求，这里直接采用目录部署的方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name sdb1 --device=/dev/sdb1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name sdb2 --device=/dev/sdb2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkfs.xfs -K -f /dev/mapper/sdb1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkfs.xfs -K -f /dev/mapper/sdb2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkdir /osd1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkdir /osd2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mount /dev/mapper/sdb1 /osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mount /dev/mapper/sdb2 /osd2/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># chown ceph:ceph /osd1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># chown ceph:ceph /osd2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd prepare lab101:/osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd prepare lab101:/osd2/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd activate lab101:/osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd activate lab101:/osd2/</span></span><br></pre></td></tr></table></figure></p>
<p>写入测试数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># rados  -p rbd bench 60 write --no-cleanup</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># df -h</span></span><br><span class="line">Filesystem        Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda2          <span class="number">56</span>G  <span class="number">2.0</span>G   <span class="number">54</span>G   <span class="number">4</span>% /</span><br><span class="line">devtmpfs          <span class="number">983</span>M     <span class="number">0</span>  <span class="number">983</span>M   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs             <span class="number">992</span>M     <span class="number">0</span>  <span class="number">992</span>M   <span class="number">0</span>% /dev/shm</span><br><span class="line">tmpfs             <span class="number">992</span>M  <span class="number">8.8</span>M  <span class="number">983</span>M   <span class="number">1</span>% /run</span><br><span class="line">tmpfs             <span class="number">992</span>M     <span class="number">0</span>  <span class="number">992</span>M   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1        <span class="number">1014</span>M  <span class="number">151</span>M  <span class="number">864</span>M  <span class="number">15</span>% /boot</span><br><span class="line">tmpfs             <span class="number">199</span>M     <span class="number">0</span>  <span class="number">199</span>M   <span class="number">0</span>% /run/user/<span class="number">0</span></span><br><span class="line">/dev/mapper/sdb1   <span class="number">22</span>G  <span class="number">6.5</span>G   <span class="number">16</span>G  <span class="number">30</span>% /osd1</span><br><span class="line">/dev/mapper/sdb2   <span class="number">22</span>G  <span class="number">6.5</span>G   <span class="number">16</span>G  <span class="number">30</span>% /osd2</span><br><span class="line">[root@lab101 ceph]<span class="comment"># vdostats --human-readable </span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/sdb2         <span class="number">25.0</span>G      <span class="number">3.0</span>G     <span class="number">22.0</span>G  <span class="number">12</span>%           <span class="number">99</span>%</span><br><span class="line">/dev/mapper/sdb1         <span class="number">25.0</span>G      <span class="number">3.0</span>G     <span class="number">22.0</span>G  <span class="number">12</span>%           <span class="number">99</span>%</span><br></pre></td></tr></table></figure>
<p>可以看到虽然在df看到了空间的占用，实际上因为rados bench写入的是填充的空洞数据，vdo作为osd对数据直接进行了重删了，测试可以看到vdo是可以作为ceph osd的，由于我的测试环境是在vmware虚拟机里面的，所以并不能做性能测试，有硬件环境的情况下可以对比下开启vdo和不开启的情况的性能区别</p>
<h2 id="参考文档">参考文档</h2><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-qs-creating-a-volume" target="_blank" rel="external">vdo-qs-creating-a-volume</a><br><a href="https://rhelblog.redhat.com/tag/vdo/" target="_blank" rel="external">Determining the space savings of virtual data optimizer (VDO) in RHEL 7.5 Beta</a></p>
<h2 id="总结">总结</h2><p>本篇从配置和部署以及适配方面对vdo进行一次比较完整的实践，从目前的测试情况来看，配置简单，对环境友好，基本是可以作为一个驱动层嵌入到任何块设备之上的，未来应该有广泛的用途，目前还不清楚红帽是否会把这个属性放到centos下面去，目前可以通过在<a href="https://access.redhat.com/downloads/" target="_blank" rel="external">https://access.redhat.com/downloads/</a> 申请测试版本的ISO进行功能的测试</p>
<p>应该是农历年前的最后一篇文章了，祝新春快乐！</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-02-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/hat.jpg" alt="network"><br></center>

<h2 id="前言">前言</h2><h3 id="关于VDO">关于VDO</h3><p>VDO的技术来源于收购的Permabit公司，一个专门从事重删技术的公司，所以技术可靠性是没有问题的</p>
<p>VDO是一个内核模块，目的是通过重删减少磁盘的空间占用，以及减少复制带宽，VDO是基于块设备层之上的，也就是在原设备基础上映射出mapper虚拟设备，然后直接使用即可，功能的实现主要基于以下技术：</p>
<ul>
<li><p>零区块的排除：</p>
<p>在初始化阶段，整块为0的会被元数据记录下来，这个可以用水杯里面的水和沙子混合的例子来解释，使用滤纸（零块排除），把沙子（非零空间）给过滤出来，然后就是下一个阶段的处理</p>
</li>
</ul>
<ul>
<li><p>重复数据删除：</p>
<p>在第二阶段，输入的数据会判断是不是冗余数据（在写入之前就判断），这个部分的数据通过UDS内核模块来判断（U niversal D eduplication S ervice），被判断为重复数据的部分不会被写入，然后对元数据进行更新，直接指向原始已经存储的数据块即可</p>
</li>
<li><p>压缩：</p>
<p>一旦消零和重删完成，LZ4压缩会对每个单独的数据块进行处理，然后压缩好的数据块会以固定大小4KB的数据块存储在介质上，由于一个物理块可以包含很多的压缩块，这个也可以加速读取的性能</p>
</li>
</ul>
<p>上面的技术看起来很容易理解，但是实际做成产品还是相当大的难度的，技术设想和实际输出还是有很大距离，不然redhat也不会通过收购来获取技术，而不是自己去重新写一套了<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[定位一个网络问题引起的ceph异常]]></title>
    <link href="http://www.zphj1987.com/2018/01/16/catch-a-problem-with-network-in-ceph/"/>
    <id>http://www.zphj1987.com/2018/01/16/catch-a-problem-with-network-in-ceph/</id>
    <published>2018-01-16T15:10:59.000Z</published>
    <updated>2018-01-16T15:45:03.002Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ne.png" alt="network"><br></center>


<h2 id="前言">前言</h2><p>有一个ceph环境出现了异常，状态就是恢复异常的慢，但是所有数据又都在走，只是非常的慢，本篇将记录探测出问题的过程，以便以后处理类似的问题有个思路<br><a id="more"></a></p>
<h2 id="处理过程">处理过程</h2><p>问题的现象是恢复的很慢，但是除此以外并没有其它的异常，通过iostat监控磁盘，也没有出现异常的100%的情况，暂时排除了是osd底层慢的问题</p>
<h3 id="检测整体写入的速度">检测整体写入的速度</h3><p>通过rados bench写入<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd bench <span class="number">5</span> write</span><br></pre></td></tr></table></figure></p>
<p>刚开始写入的时候没问题，但是写入了以后不久就会出现一只是0的情况，可以判断在写入某些对象的时候出现了异常</p>
<p>本地生成一些文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">seq <span class="number">0</span> <span class="number">30</span>|xargs -i dd <span class="keyword">if</span>=/dev/zero of=benchmarkzp&#123;&#125; bs=<span class="number">4</span>M count=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>通过rados put 命令把对象put进去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `ls ./`;<span class="keyword">do</span> time rados -p rbd put <span class="variable">$a</span> <span class="variable">$a</span>;<span class="built_in">echo</span> <span class="variable">$a</span>;ceph osd map rbd <span class="variable">$a</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>得到的结果里面会有部分是好的，部分是非常长的时间，对结果进行过滤，分为bad 和good</p>
<p>开始怀疑会不会是固定的盘符出了问题，首先把磁盘组合分出来，完全没问题的磁盘全部排除，结果最后都排除完了，所以磁盘本省是没问题的</p>
<h3 id="根据pg的osd组合进行主机分类">根据pg的osd组合进行主机分类</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">4</span>  ok</span><br><span class="line"><span class="number">3</span>  <span class="number">1</span>   <span class="number">2</span>  bad</span><br><span class="line"><span class="number">2</span>  <span class="number">4</span>   <span class="number">1</span> ok</span><br><span class="line"><span class="number">3</span>  <span class="number">1</span> <span class="number">2</span>   bad</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">2</span>  bad</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>上面的编号是写入对象所在的pg对应的osd所在的主机，严格按照顺序写入，第一个主机为发送数据方，第二个和第三个为接收数据方，并且使用了cluster network</p>
<p>通过上面的结果发现了从3往2进行发送副本数据的时候出现了问题，然后去主机上排查网络</p>
<p>在主机2上面做iperf -s<br>在主机3上面做iperf -c host2然后就发现了网络异常了</p>
<p>最终还是定位在了网络上面</p>
<p>已经在好几个环境上面发现没装可以监控实时网络流量dstat工具或者ifstat的动态监控，做操作的时候监控下网络，可以发现一些异常</p>
<h2 id="总结">总结</h2><p>这个环境在最开始的时候就怀疑是网络可能有问题，但是没有去进行全部服务器的网络的检测，这个在出现一些奇奇怪怪的异常的时候，还是可能出现在网络上面，特别是这种坏掉又不是完全坏掉，只是掉速的情况，通过集群的一些内部告警还没法完全体现出来，而主机很多的时候，又没有多少人愿意一个个的去检测，就容易出现这种疏漏了</p>
<p>在做一个ceph的管理平台的时候，对整个集群做全员对等网络带宽测试还是很有必要的，如果有一天我来设计管理平台，一定会加入这个功能进去</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-16</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ne.png" alt="network"><br></center>


<h2 id="前言">前言</h2><p>有一个ceph环境出现了异常，状态就是恢复异常的慢，但是所有数据又都在走，只是非常的慢，本篇将记录探测出问题的过程，以便以后处理类似的问题有个思路<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CTDB使用rados object作为lock file]]></title>
    <link href="http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/"/>
    <id>http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/</id>
    <published>2018-01-06T15:29:59.000Z</published>
    <updated>2018-01-06T16:06:18.867Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/object.jpg" alt="object"><br></center></p>
<h2 id="前言">前言</h2><p>服务器的服务做HA有很多种方式，其中有一种就是是用CTDB，之前这个是独立的软件来做HA的，现在已经跟着SAMBA主线里面了，也就是跟着samba发行包一起发行</p>
<p>之前CTDB的模式是需要有一个共享文件系统，并且在这个共享文件系统里面所有的节点都去访问同一个文件，会有一个Master会获得这个文件的锁</p>
<p>在cephfs的使用场景中可以用cephfs的目录作为这个锁文件的路径，这个有个问题就是一旦有一个节点down掉的时候，可能客户端也会卡住目录，这个目录访问会被卡住，文件锁在其他机器无法获取到，需要等到这个锁超时以后，其它节点才能获得到锁，这个切换的周期就会长一点了</p>
<p>CTDB在最近的版本当中加入了cluster mutex helper using Ceph RADOS的支持，本篇将介绍这个方式锁文件配置方式<br><a id="more"></a></p>
<h2 id="实践过程">实践过程</h2><h3 id="安装CTDB">安装CTDB</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># yum install samba ctdb</span></span><br></pre></td></tr></table></figure>
<p>检查默认包里面是否有rados的支持<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -qpl ctdb-4.6.2-12.el7_4.x86_64.rpm</span></span><br><span class="line">…</span><br><span class="line">usr/libexec/ctdb</span><br><span class="line">/usr/libexec/ctdb/ctdb_event</span><br><span class="line">/usr/libexec/ctdb/ctdb_eventd</span><br><span class="line">/usr/libexec/ctdb/ctdb_killtcp</span><br><span class="line">/usr/libexec/ctdb/ctdb_lock_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_lvs</span><br><span class="line">/usr/libexec/ctdb/ctdb_mutex_fcntl_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_natgw</span><br><span class="line">/usr/libexec/ctdb/ctdb_recovery_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_takeover_helper</span><br><span class="line">/usr/libexec/ctdb/smnotify</span><br><span class="line">…</span><br></pre></td></tr></table></figure></p>
<p>这个可以看到默认并没有包含这个rados的支持，这个很多通用软件都会这么处理，因为支持第三方插件的时候需要开发库，而开发库又有版本的区别，所以默认并不支持，需要支持就自己编译即可，例如fio支持librbd的接口就是这么处理的，等到插件也通用起来的时候，可能就会默认支持了</p>
<p>很多软件的编译可以采取源码的编译方式，如果不是有很强的代码合入和patch跟踪能力，直接用发行包的方式是最稳妥的，所以为了不破坏这个稳定性，本篇采用的是基于发行版本，增加模块的方式，这样不会破坏核心组件的稳定性，并且后续升级也是比较简单的，这个也是个人推荐的方式</p>
<p>查询当前使用的samba版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -qa|grep samba</span></span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.x86_64</span><br></pre></td></tr></table></figure></p>
<h3 id="打包新的CTDB">打包新的CTDB</h3><p>可以查询得到这个的源码包为samba-4.6.2-12.el7_4.src.rpm,进一步搜索可以查询的到这个src源码rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://vault.centos.org/<span class="number">7.4</span>.<span class="number">1708</span>/updates/Source/SPackages/samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>下载这个rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># wget http://vault.centos.org/7.4.1708/updates/Source/SPackages/samba-4.6.2-12.el7_4.src.rpm</span></span><br></pre></td></tr></table></figure></p>
<p>如果下载比较慢的话就用迅雷下载，会快很多，国内的源里面把源码包的rpm都删除掉了，上面的是官网会有最全的包</p>
<p>解压这个rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm2cpio samba-4.6.2-12.el7_4.src.rpm |cpio -div</span></span><br></pre></td></tr></table></figure></p>
<p>检查包的内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># ls</span></span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12150</span>.patch                                 samba-v4-<span class="number">6</span>-fix-cross-realm-refferals.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12151</span>.patch                                 samba-v4-<span class="number">6</span>-fix-kerberos-debug-message.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12163</span>.patch                                 samba-v4-<span class="number">6</span>-fix_net_ads_changetrustpw.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">14746</span>.patch                                 samba-v4-<span class="number">6</span>-fix-net-ads-keytab-handling.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">15275</span>.patch                                 samba-v4-<span class="number">6</span>-fix_path_substitutions.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">7494</span>.patch                                  samba-v4-<span class="number">6</span>-fix_smbclient_session_setup_info.patch</span><br><span class="line">gpgkey-<span class="number">52</span>FBC0B86D954B0843324CDC6F33915B6568B7EA.gpg  samba-v4-<span class="number">6</span>-fix_smbclient_username_parsing.patch</span><br><span class="line">pam_winbind.conf                                     samba-v4.<span class="number">6</span>-fix_smbpasswd_user_<span class="built_in">pwd</span>_change.patch</span><br><span class="line">README.dc                                            samba-v4-<span class="number">6</span>-fix-spoolss-<span class="number">32</span>bit-driver-upload.patch</span><br><span class="line">README.downgrade                                     samba-v4-<span class="number">6</span>-fix-vfs-expand-msdfs.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.src.rpm                         samba-v4-<span class="number">6</span>-fix_winbind_child_crash.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>.tar.asc                                  samba-v4-<span class="number">6</span>-fix_winbind_normalize_names.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>.tar.xz                                   samba-v4.<span class="number">6</span>-graceful_fsctl_validate_negotiate_info.patch</span><br><span class="line">samba.log                                            samba-v4.<span class="number">6</span>-gss_krb5_import_cred.patch</span><br><span class="line">samba.pamd                                           samba-v4.<span class="number">6</span>-lib-crypto-implement-samba.crypto-Python-module-for-.patch</span><br><span class="line">samba.spec                                           samba-v4.<span class="number">7</span>-config-dynamic-rpc-port-range.patch</span><br><span class="line">samba-v4.<span class="number">6</span>-credentials-fix-realm.patch               smb.conf.example</span><br><span class="line">samba-v4-<span class="number">6</span>-fix-building-with-new-glibc.patch         smb.conf.vendor</span><br></pre></td></tr></table></figure></p>
<p>可以看到在源码包基础上还打入了很多的patch，内部的编译采用的是waf编译的方式，内部的过程就不做太多介绍了，这里只去改动我们需要的部分即可，也就是去修改samba.spec文件</p>
<p>我们先获取相关的编译选项，这个我最开始的时候打算独立编译ctdb的rpm包，发现有依赖关系太多，后来多次验证后，发现直接可以在samba编译里面增加选项的，选项获取方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab211 samba-<span class="number">4.6</span>.<span class="number">2</span>]<span class="comment"># ./configure --help|grep ceph</span></span><br><span class="line">  --with-libcephfs=LIBCEPHFS_DIR</span><br><span class="line">            Directory under <span class="built_in">which</span> libcephfs is installed</span><br><span class="line">  --enable-cephfs</span><br><span class="line">            Build with cephfs support (default=yes)</span><br><span class="line">  --enable-ceph-reclock</span><br></pre></td></tr></table></figure></p>
<p>这个可以知道需要添加ceph-reclock的支持就添加这个选项，我们把这个选项添加到samba.spec当中<br>修改samba.spec文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">…</span><br><span class="line">%configure \</span><br><span class="line">        --enable-fhs \</span><br><span class="line">        --with-piddir=/run \</span><br><span class="line">        --with-sockets-dir=/run/samba \</span><br><span class="line">        --with-modulesdir=%&#123;_libdir&#125;/samba \</span><br><span class="line">        --with-pammodulesdir=%&#123;_libdir&#125;/security \</span><br><span class="line">        --with-lockdir=/var/lib/samba/lock \</span><br><span class="line">        --with-statedir=/var/lib/samba \</span><br><span class="line">        --with-cachedir=/var/lib/samba \</span><br><span class="line">        --disable-rpath-install \</span><br><span class="line">        --with-shared-modules=%&#123;_samba4_modules&#125; \</span><br><span class="line">        --bundled-libraries=%&#123;_samba4_libraries&#125; \</span><br><span class="line">        --with-pam \</span><br><span class="line">        --with-pie \</span><br><span class="line">        --with-relro \</span><br><span class="line">        --enable-ceph-reclock \</span><br><span class="line">        --without-fam \</span><br><span class="line">…</span><br><span class="line">%dir %&#123;_libexecdir&#125;/ctdb</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_event</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_eventd</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_killtcp</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_lock_helper</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_lvs</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_fcntl_helper</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">…</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdb.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdb_diagnostics.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdbd.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/onnode.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ltdbtool.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ping_pong.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdbd_wrapper.<span class="number">1</span>.gz</span><br><span class="line">…</span><br></pre></td></tr></table></figure></p>
<p>这个文件当中一共添加了三行内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--enable-ceph-reclock \</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br></pre></td></tr></table></figure></p>
<p>把解压后的目录里面的所有文件都拷贝到源码编译目录,就是上面ls列出的那些文件，以及修改好的samba.spec文件都一起拷贝过去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># cp -ra * /root/rpmbuild/SOURCES/</span></span><br></pre></td></tr></table></figure></p>
<p>安装librados2的devel包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># yum install librados2-devel</span></span><br></pre></td></tr></table></figure></p>
<p>如果编译过程缺其他的依赖包就依次安装即可，这个可以通过解压源码先编译一次的方式来把依赖包找全，然后再打rpm包</p>
<p>开始编译rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># rpmbuild -bb samba.spec</span></span><br></pre></td></tr></table></figure></p>
<p>这个可以就在当前的目录执行即可</p>
<p>检查生成的包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># rpm -qpl /root/rpmbuild/RPMS/x86_64/ctdb-4.6.2-12.el7.centos.x86_64.rpm|grep rados</span></span><br><span class="line">/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">/usr/share/man/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br></pre></td></tr></table></figure></p>
<p>可以看到已经生成了这个，把这个包拷贝到需要更新的机器上面</p>
<h3 id="配置ctdb">配置ctdb</h3><p>首先要升级安装下新的ctdb包，因为名称有改变，会提示依赖问题,这里忽略依赖的问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -Uvh ctdb-4.6.2-12.el7.centos.x86_64.rpm --nodeps</span></span><br></pre></td></tr></table></figure></p>
<p>添加一个虚拟IP配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/public_addresses </span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.99</span>/<span class="number">16</span> ens33</span><br></pre></td></tr></table></figure></p>
<p>添加node配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/nodes </span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.18</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.201</span></span><br></pre></td></tr></table></figure></p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/ctdbd.conf|grep -v "#"</span></span><br><span class="line"> CTDB_RECOVERY_LOCK=<span class="string">"!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb"</span></span><br><span class="line"> CTDB_NODES=/etc/ctdb/nodes</span><br><span class="line"> CTDB_PUBLIC_ADDRESSES=/etc/ctdb/public_addresses</span><br><span class="line"> CTDB_LOGGING=file:/var/<span class="built_in">log</span>/log.ctdb</span><br><span class="line"><span class="comment"># CTDB_DEBUGLEVEL=debug</span></span><br></pre></td></tr></table></figure></p>
<p>上面为了调试，我开启了debug来查看重要的信息</p>
<blockquote>
<p>CTDB_RECOVERY_LOCK=”!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb”<br>最重要的是这行配置文件规则是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CTDB_RECOVERY_LOCK=<span class="string">"!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper [Cluster] [User] [Pool] [Object]"</span></span><br><span class="line">Cluster: Ceph cluster name (e.g. ceph)</span><br><span class="line">User: Ceph cluster user name (e.g. client.admin)</span><br><span class="line">Pool: Ceph RADOS pool name</span><br><span class="line">Object: Ceph RADOS object name</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>在ctdb的机器上面准备好librados2和ceph配置文件，这个配置的rbd的lockctdb对象会由ctdb去生成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># systemctl restart ctdb</span></span><br></pre></td></tr></table></figure></p>
<p>配置好了以后就可以启动进程了，上面的/etc/ctdb/ctdbd.conf配置文件最好是修改好一台机器的，然后scp到其它机器，里面内容有一点点偏差都会判断为异常的，所以最好是相同的配置文件</p>
<p>查看进程状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ceph]<span class="comment"># ctdb status</span></span><br><span class="line">Number of nodes:<span class="number">2</span></span><br><span class="line">pnn:<span class="number">0</span> <span class="number">192.168</span>.<span class="number">0.18</span>     OK (THIS NODE)</span><br><span class="line">pnn:<span class="number">1</span> <span class="number">192.168</span>.<span class="number">0.201</span>    OK</span><br><span class="line">Generation:<span class="number">1662303628</span></span><br><span class="line">Size:<span class="number">2</span></span><br><span class="line"><span class="built_in">hash</span>:<span class="number">0</span> lmaster:<span class="number">0</span></span><br><span class="line"><span class="built_in">hash</span>:<span class="number">1</span> lmaster:<span class="number">1</span></span><br><span class="line">Recovery mode:NORMAL (<span class="number">0</span>)</span><br><span class="line">Recovery master:<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看/var/log/log.ctdb日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399849</span> ctdb-recoverd[<span class="number">129134</span>]: Node:<span class="number">1</span> was <span class="keyword">in</span> recovery mode. Start recovery process</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399879</span> ctdb-recoverd[<span class="number">129134</span>]: ../ctdb/server/ctdb_recoverd.c:<span class="number">1267</span> Starting <span class="keyword">do</span>_recovery</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399903</span> ctdb-recoverd[<span class="number">129134</span>]: Attempting to take recovery lock (!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb)</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.400657</span> ctdb-recoverd[<span class="number">129134</span>]: ../ctdb/server/ctdb_cluster_mutex.c:<span class="number">251</span> Created PIPE FD:<span class="number">17</span></span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.579865</span> ctdbd[<span class="number">129038</span>]: ../ctdb/server/ctdb_daemon.c:<span class="number">907</span> client request <span class="number">40</span> of <span class="built_in">type</span> <span class="number">7</span> length <span class="number">72</span> from node <span class="number">1</span> to <span class="number">4026531841</span></span><br></pre></td></tr></table></figure></p>
<p>日志中可以看到ctdb-recoverd已经是采用的ctdb_mutex_ceph_rados_helper来获取的recovery lock</p>
<p>停掉ctdb的进程，IP可以正常的切换，到这里，使用对象作为lock文件的功能就实现了，其他更多的ctdb的高级控制就不在这个里作过多的说明</p>
<h2 id="总结">总结</h2><p>本篇是基于发行版本的ctdb包进行模块的加入重新发包，并且把配置做了一次实践，这个可以作为一个ctdb的方案之一，具体跟之前的方案相比切换时间可以改善多少，需要通过数据进行对比，这个进行测试即可</p>
<h2 id="资源">资源</h2><p>已经打好包的ctdb共享一下，可以直接使用</p>
<blockquote>
<p><a href="http://7xweck.com1.z0.glb.clouddn.com/ctdb-4.6.2-12.el7.centos.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/ctdb-4.6.2-12.el7.centos.x86_64.rpm</a></p>
</blockquote>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/object.jpg" alt="object"><br></cemter></p>
<h2 id="前言">前言</h2><p>服务器的服务做HA有很多种方式，其中有一种就是是用CTDB，之前这个是独立的软件来做HA的，现在已经跟着SAMBA主线里面了，也就是跟着samba发行包一起发行</p>
<p>之前CTDB的模式是需要有一个共享文件系统，并且在这个共享文件系统里面所有的节点都去访问同一个文件，会有一个Master会获得这个文件的锁</p>
<p>在cephfs的使用场景中可以用cephfs的目录作为这个锁文件的路径，这个有个问题就是一旦有一个节点down掉的时候，可能客户端也会卡住目录，这个目录访问会被卡住，文件锁在其他机器无法获取到，需要等到这个锁超时以后，其它节点才能获得到锁，这个切换的周期就会长一点了</p>
<p>CTDB在最近的版本当中加入了cluster mutex helper using Ceph RADOS的支持，本篇将介绍这个方式锁文件配置方式<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Kernel RBD的QOS配置方案]]></title>
    <link href="http://www.zphj1987.com/2018/01/05/Kernel-RBD-QOS/"/>
    <id>http://www.zphj1987.com/2018/01/05/Kernel-RBD-QOS/</id>
    <published>2018-01-05T07:23:30.000Z</published>
    <updated>2018-01-06T15:42:00.936Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/io.png" alt="io"><br></center>

<h2 id="前言">前言</h2><p>关于qos的讨论有很多，ceph内部也正在实现着一整套的基于dmclock的qos的方案，这个不是本篇的内容，之前在社区的邮件列表看过有研发在聊qos的相关的实现的，当时一个研发就提出了在使用kernel rbd的时候，可以直接使用linux的操作系统qos来实现，也就是cgroup来控制读取写入</p>
<p>cgroup之前也有接触过，主要测试了限制cpu和内存相关的，没有做io相关的测试，这个当然可以通过ceph内部来实现qos，但是有现成的解决方案的时候，可以减少很多开发周期，以及测试的成本</p>
<p>本篇将介绍的是kernel rbd的qos方案<br><a id="more"></a></p>
<h2 id="时间过长">时间过长</h2><p>首先介绍下几个测试qos相关的命令，用来比较设置前后的效果<br>验证写入IOPS命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=write -ioengine=libaio -bs=<span class="number">4</span>K -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证写入带宽的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=write -ioengine=libaio -bs=<span class="number">4</span>M -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证读取IOPS命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=<span class="built_in">read</span> -ioengine=libaio -bs=<span class="number">4</span>K -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证读取带宽命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=<span class="built_in">read</span> -ioengine=libaio -bs=<span class="number">4</span>M -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>上面为什么会设置不同的块大小，这个是因为测试的存储是会受到带宽和iops的共同制约的，当测试小io的时候，这个时候的峰值是受到iops的限制的，测试大io的时候，受到的是带宽限制，所以在做测试的时候，需要测试iops是否被限制住的时候就使用小的bs=4K，需要测试大的带宽的限制的时候就采用bs=4M来测试</p>
<p>测试的时候都是，开始不用做qos来进行测试得到一个当前不配置qos的性能数值，然后根据需要进行qos设置后通过上面的fio去测试是否能限制住</p>
<p>启用cgroup的blkio模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p  /cgroup/blkio/</span><br><span class="line">mount -t cgroup -o blkio blkio /cgroup/blkio/</span><br></pre></td></tr></table></figure></p>
<p>获取rbd磁盘的major/minor numbers<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab211 ~]<span class="comment"># lsblk </span></span><br><span class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">rbd0   <span class="number">252</span>:<span class="number">0</span>    <span class="number">0</span>  <span class="number">19.5</span>G  <span class="number">0</span> disk </span><br><span class="line">sda      <span class="number">8</span>:<span class="number">0</span>    <span class="number">1</span> <span class="number">238.4</span>G  <span class="number">0</span> disk </span><br><span class="line">├─sda4   <span class="number">8</span>:<span class="number">4</span>    <span class="number">1</span>     <span class="number">1</span>K  <span class="number">0</span> part </span><br><span class="line">├─sda2   <span class="number">8</span>:<span class="number">2</span>    <span class="number">1</span>  <span class="number">99.9</span>G  <span class="number">0</span> part </span><br><span class="line">├─sda5   <span class="number">8</span>:<span class="number">5</span>    <span class="number">1</span>     <span class="number">8</span>G  <span class="number">0</span> part [SWAP]</span><br><span class="line">├─sda3   <span class="number">8</span>:<span class="number">3</span>    <span class="number">1</span>     <span class="number">1</span>G  <span class="number">0</span> part /boot</span><br><span class="line">├─sda1   <span class="number">8</span>:<span class="number">1</span>    <span class="number">1</span>   <span class="number">100</span>M  <span class="number">0</span> part </span><br><span class="line">└─sda6   <span class="number">8</span>:<span class="number">6</span>    <span class="number">1</span> <span class="number">129.4</span>G  <span class="number">0</span> part /</span><br></pre></td></tr></table></figure></p>
<p>通过lsblk命令可以获取到磁盘对应的major number和minor number,这里可以看到rbd0对应的编号为252:0</p>
<p>设置rbd0的iops的qos为10<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10"</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device</span><br></pre></td></tr></table></figure></p>
<p>如果想清理这个规则,把后面的数值设置为0就清理了，后面几个配置也是相同的方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 0"</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device</span><br></pre></td></tr></table></figure></p>
<p>限制写入的带宽为10MB/s<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10485760"</span> &gt; /cgroup/blkio/blkio.throttle.write_bps_device</span><br></pre></td></tr></table></figure></p>
<p>限制读取的qos为10<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10"</span> &gt; /cgroup/blkio/blkio.throttle.read_iops_device</span><br></pre></td></tr></table></figure></p>
<p>限制读取的带宽为10MB/s<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10485760"</span> &gt; /cgroup/blkio/blkio.throttle.read_bps_device</span><br></pre></td></tr></table></figure></p>
<p>以上简单的设置就完成了kernel rbd的qos设置了，我测试了下，确实是生效了的</p>
<h2 id="总结">总结</h2><p>这个知识点很久前就看到了，一直没总结，现在记录下，个人观点是能快速，有效，稳定的实现功能是最好的，所以使用这个在kernel rbd方式下可以不用再进行qos的开发了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-05</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/io.png" alt="io"><br></center>

<h2 id="前言">前言</h2><p>关于qos的讨论有很多，ceph内部也正在实现着一整套的基于dmclock的qos的方案，这个不是本篇的内容，之前在社区的邮件列表看过有研发在聊qos的相关的实现的，当时一个研发就提出了在使用kernel rbd的时候，可以直接使用linux的操作系统qos来实现，也就是cgroup来控制读取写入</p>
<p>cgroup之前也有接触过，主要测试了限制cpu和内存相关的，没有做io相关的测试，这个当然可以通过ceph内部来实现qos，但是有现成的解决方案的时候，可以减少很多开发周期，以及测试的成本</p>
<p>本篇将介绍的是kernel rbd的qos方案<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph对象主本损坏的修复方法]]></title>
    <link href="http://www.zphj1987.com/2018/01/02/ceph-primary-object-damage-recover/"/>
    <id>http://www.zphj1987.com/2018/01/02/ceph-primary-object-damage-recover/</id>
    <published>2018-01-02T14:21:23.000Z</published>
    <updated>2018-01-02T14:41:54.296Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dog.jpg" alt="dog"><br></center>

<h2 id="前言">前言</h2><p>问题的触发是在进行一个目录的查询的时候，osd就会挂掉，开始以为是osd操作超时了，后来发现每次访问这个对象都有问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">log</span> [WRN] ： slow request <span class="number">60.793196</span> seconds old, received at osd_op(mds.<span class="number">0.188</span>:<span class="number">728345234100006</span>c6ddc.<span class="number">00000000</span> [o map-get-header <span class="number">0</span>-<span class="number">0</span>,omap-get-vals <span class="number">0</span>~<span class="number">16</span>,getxattr parent] snapc <span class="number">0</span>=[] ack+<span class="built_in">read</span>+known_<span class="keyword">if</span>_redirected+full_force e218901) currently started</span><br><span class="line">heartbeat_map is_healthy  ··· osd_op_tp thread ··· had timed out after <span class="number">60</span></span><br></pre></td></tr></table></figure></p>
<p>这个对象是元数据的一个空对象，保留数据在扩展属性当中<br><a id="more"></a><br>然后做了一个操作判断是对象损坏了:</p>
<p>直接列取omapkeys</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p metadata listomapvals <span class="number">100006</span>c6ddc.<span class="number">00000000</span></span><br></pre></td></tr></table></figure>
<p>发现会卡住，然后关闭这个osd再次做操作，就可以了，启动后还是不行，这里可以判断是主本的对象已经有问题了，本篇将讲述多种方法来解决这个问题</p>
<h2 id="处理办法">处理办法</h2><p>本章将会根据操作粒度的不同来讲述三种方法的恢复，根据自己的实际情况，和风险的判断来选择自己的操作</p>
<h3 id="方法一：通过repair修复">方法一：通过repair修复</h3><p>首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span></span><br></pre></td></tr></table></figure></p>
<p>通过ceph-object-tool进行移除的时候有bug,无法移除metadata的对象，已经提了一个<a href="http://tracker.ceph.com/issues/22553" target="_blank" rel="external">bug</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># mv 100.00000000__head_C5265AB3__2 ../</span></span><br></pre></td></tr></table></figure></p>
<p>注意一下在老版本的时候，对对象进行删除以后，可能元数据里面记录了对象信息，而对象又不在的时候可能会引起osd无法启动，这个在10.2.10是没有这个问题</p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>对pg做scrub<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph pg scrub 2.0</span></span><br><span class="line">instructing pg <span class="number">2.0</span> on osd.<span class="number">0</span> to scrub</span><br></pre></td></tr></table></figure></p>
<p>这种方法就是需要做scrub的操作，如果对象特别多，并且是线上环境，可能不太好去做scrub的操作<br>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">03580</span>f14-<span class="number">9906</span>-<span class="number">4257</span>-<span class="number">9182</span>-<span class="number">65</span>c886e7f5a7</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> pgs inconsistent</span><br><span class="line">            <span class="number">1</span> scrub errors</span><br><span class="line">            too few PGs per OSD (<span class="number">3</span> &lt; min <span class="number">30</span>)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab71=<span class="number">20.20</span>.<span class="number">20.71</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab71</span><br><span class="line">      fsmap e30: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab71=up:active&#125;</span><br><span class="line">     osdmap e101: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v377: <span class="number">3</span> pgs, <span class="number">3</span> pools, <span class="number">100814</span> bytes data, <span class="number">41</span> objects</span><br><span class="line">            <span class="number">70196</span> kB used, <span class="number">189</span> GB / <span class="number">189</span> GB avail</span><br><span class="line">                   <span class="number">2</span> active+clean</span><br><span class="line">                   <span class="number">1</span> active+clean+inconsistent</span><br></pre></td></tr></table></figure></p>
<p>发起修复请求<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph pg repair 2.0</span></span><br><span class="line">instructing pg <span class="number">2.0</span> on osd.<span class="number">0</span> to repair</span><br></pre></td></tr></table></figure></p>
<p>修复完成后检查集群状态和对象，到这里可以恢复正常了</p>
<h3 id="方法二：通过rsync拷贝数据方式恢复">方法二：通过rsync拷贝数据方式恢复</h3><p>跟上面一样这里首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span></span><br></pre></td></tr></table></figure></p>
<p>移除对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># mv 100.00000000__head_C5265AB3__2 ../</span></span><br></pre></td></tr></table></figure></p>
<p>在副本的机器上执行rsync命令，这里我们直接从副本拷贝对象过来，注意下不能直接使用scp会掉扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab72 <span class="number">2.0</span>_head]<span class="comment"># rsync  -avXH  /var/lib/ceph/osd/ceph-1/current/2.0_head/100.00000000__head_C5265AB3__2 20.20.20.71:/var/lib/ceph/osd/ceph-0/current/2.0_head/100.00000000__head_C5265AB3__2</span></span><br></pre></td></tr></table></figure></p>
<p>在主本机器检查扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># getfattr 100.00000000__head_C5265AB3__2 </span></span><br><span class="line"><span class="comment"># file: 100.00000000__head_C5265AB3__2</span></span><br><span class="line">user.ceph._</span><br><span class="line">user.ceph._@<span class="number">1</span></span><br><span class="line">user.ceph.snapset</span><br><span class="line">user.cephos.spill_out</span><br></pre></td></tr></table></figure></p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>检查对象的扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># rados -p metadata listomapvals 100.00000000</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方法三：通过删除PG的方式恢复">方法三：通过删除PG的方式恢复</h3><p>这个方式是删除PG，然后重新启动的方式<br>这种方式操作比较危险，所以提前备份好pg的数据，最好主备pg都备份下，万一出了问题或者数据不对，可以根据需要再导入<br>备份PG<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-objectstore-tool --pgid <span class="number">2.0</span> --op <span class="built_in">export</span> --data-path /var/lib/ceph/osd/ceph-<span class="number">0</span>/ --journal-path   /var/lib/ceph/osd/ceph-<span class="number">0</span>/journal --file /root/<span class="number">2.0</span></span><br></pre></td></tr></table></figure></p>
<p>删除PG的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment"># ceph-objectstore-tool --pgid 2.0  --op remove --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">00</span> c0 <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">00</span> c0 <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"> marking collection <span class="keyword">for</span> removal</span><br><span class="line">setting <span class="string">'_remove'</span> omap key</span><br><span class="line">finish_remove_pgs <span class="number">2.0</span>_head removing <span class="number">2.0</span></span><br><span class="line">Remove successful</span><br></pre></td></tr></table></figure></p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>等待回复即可</p>
<p>本方法里面还可以衍生一种就是，通过导出的副本的PG数据,在主本删除了相应的PG以后,进行导入的方法，这样就不会产生迁移<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment">#  ceph-objectstore-tool --pgid 2.0  --op import --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal --file /root/2.0</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>上面用三种方法来实现了副本向主本同步的操作，判断主本是否有问题的方法就是主动的把主本所在的OSD停掉，然后检查请求是否可达，在确定主本已经坏掉的情况下，就可以做将副本同步到主本的操作，可以根据PG的对象的多少来选择需要做哪种操作</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dog.jpg" alt="dog"><br></center>

<h2 id="前言">前言</h2><p>问题的触发是在进行一个目录的查询的时候，osd就会挂掉，开始以为是osd操作超时了，后来发现每次访问这个对象都有问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">log</span> [WRN] ： slow request <span class="number">60.793196</span> seconds old, received at osd_op(mds.<span class="number">0.188</span>:<span class="number">728345234100006</span>c6ddc.<span class="number">00000000</span> [o map-get-header <span class="number">0</span>-<span class="number">0</span>,omap-get-vals <span class="number">0</span>~<span class="number">16</span>,getxattr parent] snapc <span class="number">0</span>=[] ack+<span class="built_in">read</span>+known_<span class="keyword">if</span>_redirected+full_force e218901) currently started</span><br><span class="line">heartbeat_map is_healthy  ··· osd_op_tp thread ··· had timed out after <span class="number">60</span></span><br></pre></td></tr></table></figure></p>
<p>这个对象是元数据的一个空对象，保留数据在扩展属性当中<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mds的cpu占用问题分析以及解决办法]]></title>
    <link href="http://www.zphj1987.com/2017/12/04/mds-use-too-more-cpu/"/>
    <id>http://www.zphj1987.com/2017/12/04/mds-use-too-more-cpu/</id>
    <published>2017-12-04T14:47:04.000Z</published>
    <updated>2017-12-04T15:06:37.402Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ganesha.png" alt="ganesha"><br></center>

<h2 id="前言">前言</h2><p>mds是ceph里面处理文件接口的组件，一旦使用文件系统，不可避免的会出现一种场景就是目录很多，目录里面的文件很多，而mds是一个单进程的组件，现在虽然有了muti mds，但稳定的使用的大部分场景还是单acitve mds的</p>
<p>这就会出现一种情况，一旦一个目录里面有很多文件的时候，去查询这个目录里的文件就会在当前目录做一次遍历，这个需要一个比较长的时间，如果能比较好的缓存文件信息，也能避免一些过载情况，本篇讲述的是内核客户端正常，而export nfs后mds的负载长时间过高的情况<br><a id="more"></a></p>
<h2 id="问题复现">问题复现</h2><h3 id="准备测试数据,准备好监控环境">准备测试数据,准备好监控环境</h3><p>监控mds cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pidstat -u  <span class="number">1</span> -p <span class="number">27076</span> &gt; /tmp/mds.cpu.log</span><br><span class="line">UserParameter=mds.cpu,cat /tmp/mds.cpu.log|tail -n <span class="number">1</span>|grep -v Average| awk <span class="string">'&#123;print $8&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>整个测试避免屏幕的打印影响时间统计,把输出需要重定向<br>测试一：<br>内核客户端写入10000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time seq 10000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line">real	<span class="number">0</span>m30.<span class="number">121</span>s</span><br><span class="line">user	<span class="number">0</span>m1.<span class="number">901</span>s</span><br><span class="line">sys	<span class="number">0</span>m10.<span class="number">420</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/3no4iobedvhwujvtr8jfmrf7/aa.png" alt="aa.png-32.5kB"></p>
<p>测试二：<br>内核客户端写入20000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc20000]<span class="comment"># time seq 20000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line">real	<span class="number">1</span>m38.<span class="number">233</span>s</span><br><span class="line">user	<span class="number">0</span>m3.<span class="number">761</span>s</span><br><span class="line">sys	<span class="number">0</span>m21.<span class="number">510</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/ual9inmekpeth163muql8dyn/bbb.png" alt="bbb.png-39kB"><br>测试三：<br>内核客户端写入40000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc40000]<span class="comment">#  time seq 40000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">2</span>m55.<span class="number">261</span>s</span><br><span class="line">user	<span class="number">0</span>m7.<span class="number">699</span>s</span><br><span class="line">sys	<span class="number">0</span>m42.<span class="number">410</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/pnqr3lpjuydb8dj182cw6ffa/cccc.png" alt="cccc.png-57.3kB"></p>
<p>测试4：<br>内核客户端列目录10000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">228</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">063</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">048</span>s</span><br></pre></td></tr></table></figure></p>
<p>内核客户端列目录20000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">737</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">141</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">092</span>s</span><br></pre></td></tr></table></figure></p>
<p>内核客户端列目录40000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">658</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">286</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">196</span>s</span><br></pre></td></tr></table></figure></p>
<p>都是比较快的返回，CPU可以忽略不计</p>
<p>现在重启mds后再次列目录<br>客户端如果不umount,直接重启mds的话,还是会缓存在<br>新版本这个地方好像已经改了（重启了mds 显示inode还在，但是随着时间的增长inode会减少，说明还是有周期，会释放，这个还不知道哪个地方控制，用什么参数控制，这个不是本篇着重关注的地方，后续再看下,jewel版本已经比hammer版本的元数据时间快了很多了）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">380</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">065</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">041</span>s</span><br><span class="line">[root@nfsserver kc10000]<span class="comment"># cd ../kc20000/</span></span><br><span class="line">[root@nfsserver kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">868</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">154</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">074</span>s</span><br><span class="line">[root@nfsserver kc20000]<span class="comment"># cd ../kc40000/</span></span><br><span class="line">[root@nfsserver kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">947</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">300</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">166</span>s</span><br></pre></td></tr></table></figure></p>
<p>测试都是看到很快的返回，以上都是正常的，下面开始将这个目录exportnfs出去，看下是个什么情况</p>
<h3 id="负载问题复现">负载问题复现</h3><p>从nfs客户端第一次列10000个小文件的目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m4.<span class="number">038</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">095</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">069</span>s</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/zphj1987/2zkcsg31i5r0972clmsnwu9p/nfs10000.png" alt="nfs10000.png-36.7kB"></p>
<p>从nfs客户端第一次列20000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m17.<span class="number">446</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">175</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">141</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/jsd4ropqf7s80olmib2bm16t/nfs20000.png" alt="nfs20000.png-43.2kB"><br>从nfs客户端第二次列20000个小文件目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m21.<span class="number">215</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">182</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">151</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/im9wby3cyze0fsvnvoifgita/nfs200002.png" alt="nfs200002.png-56.7kB"></p>
<p>从nfs客户端第三次列20000个小文件目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m16.<span class="number">222</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">189</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">143</span>s</span><br></pre></td></tr></table></figure></p>
<p>可以看到在20000量级的时候列目录维持在20000左右，CPU维持一个高位</p>
<p>从nfs客户端列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">7</span>m15.<span class="number">663</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">319</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">581</span>s</span><br><span class="line">[root@nfsclient kc40000]<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/rflb49cxesc1g6uv0cuyfyse/nfs40000.png" alt="nfs40000.png-77.2kB"><br>第一次列完，马上第二次列看下情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">1</span>m12.<span class="number">816</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">163</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">142</span>s</span><br></pre></td></tr></table></figure></p>
<p>可以看到第二次列的时间已经缩短了，再来第三次<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">1</span>m33.<span class="number">549</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">162</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">183</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/nhx161bnwwt3ggiaommufvfx/nfs400003.png" alt="nfs400003.png-61.3kB"><br>可以看到在后面列的时候时间确实缩短了，但是还是维持一个非常高CPU的占用，以及比较长的一个时间，这个很容易造成过载</p>
<p>这个地方目前看应该是内核客户端与内核NFS的结合的问题</p>
<h2 id="解决办法:用ganesha的ceph用户态接口替代kernel_nfs">解决办法:用ganesha的ceph用户态接口替代kernel nfs</h2><p>我们看下另外一种方案用户态的NFS+ceph同样的环境下测试结果：</p>
<p>从nfs客户端第一次列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m3.<span class="number">289</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">335</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">386</span>s</span><br></pre></td></tr></table></figure></p>
<p>从nfs客户端第二次列40000个小文件的目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">686</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">351</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">389</span>s</span><br></pre></td></tr></table></figure>
<p>从nfs客户端第三次列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">675</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">320</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">391</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/mw0j9nj322p0slwymyicffvc/ganesha.png" alt="ganesha.png-51.5kB"><br>基本mds无多余的负载，非常快的返回</p>
<p>可以从上面的测试看到差别是非常的大的，这个地方应该是内核模块与内核之间的问题，而采用用户态的以后解决了列目录慢以及卡顿的问题</p>
<h2 id="如何配置ganesha支持ceph的nfs接口">如何配置ganesha支持ceph的nfs接口</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b V2.<span class="number">3</span>-stable https://github.com/nfs-ganesha/nfs-ganesha.git</span><br><span class="line"><span class="built_in">cd</span> nfs-ganesha/</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> nfs-ganesha/</span><br><span class="line">ll src/FSAL/FSAL_CEPH/</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">mkdir mybuild</span><br><span class="line"><span class="built_in">cd</span> mybuild/</span><br><span class="line">cmake -DUSE_FSAL_CEPH=ON ../nfs-ganesha/src/</span><br><span class="line">ll FSAL/FSAL_CEPH/</span><br><span class="line">make</span><br><span class="line">make -j <span class="number">12</span></span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>vim /etc/ganesha/ganesha.conf<br>修改配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">EXPORT</span><br><span class="line">&#123;</span><br><span class="line">    Export_ID=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    Path = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">    Pseudo = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">    Access_Type = RW;</span><br><span class="line"></span><br><span class="line">    NFS_Protocols = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    Transport_Protocols = TCP;</span><br><span class="line"></span><br><span class="line">    FSAL &#123;</span><br><span class="line">        Name = CEPH;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>停止掉原生的nfs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop nfs</span><br></pre></td></tr></table></figure></p>
<p>启用ganesha nfs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start  nfs-ganesha.service</span><br></pre></td></tr></table></figure></p>
<p>然后在客户端进行nfs的挂载即可</p>
<h2 id="总结">总结</h2><p>ganesha在需要用到cephfs又正好是要用到nfs接口的时候，可以考虑这个方案，至少在缓存文件，降低负载上面能够比kernel client有更好的效果，这个可以根据测试情况用数据来做比较</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-12-04</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ganesha.png" alt="ganesha"><br></center>

<h2 id="前言">前言</h2><p>mds是ceph里面处理文件接口的组件，一旦使用文件系统，不可避免的会出现一种场景就是目录很多，目录里面的文件很多，而mds是一个单进程的组件，现在虽然有了muti mds，但稳定的使用的大部分场景还是单acitve mds的</p>
<p>这就会出现一种情况，一旦一个目录里面有很多文件的时候，去查询这个目录里的文件就会在当前目录做一次遍历，这个需要一个比较长的时间，如果能比较好的缓存文件信息，也能避免一些过载情况，本篇讲述的是内核客户端正常，而export nfs后mds的负载长时间过高的情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CentOS GRUB损坏修复方法]]></title>
    <link href="http://www.zphj1987.com/2017/11/30/recovery-from-grub-damage/"/>
    <id>http://www.zphj1987.com/2017/11/30/recovery-from-grub-damage/</id>
    <published>2017-11-30T14:51:55.000Z</published>
    <updated>2017-11-30T15:22:39.682Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/grub.jpg" alt="grub"><br></center>

<h2 id="前言">前言</h2><p>博客很久没有更新了，一个原因就是原来存放部署博客的环境坏了，硬盘使用的是SSD，只要读取到某个文件，整个磁盘就直接识别不到了，还好博客环境之前有做备份，最近一直没有把部署环境做下恢复，今天抽空把环境做下恢复并且记录一篇基础的GRUB的处理文档</p>
<p>这两天正好碰到GRUB损坏的事，很久前处理过，但是没留下文档，正好现在把流程梳理一下，来解决grub.cfg损坏的情况,或者无法启动的情况<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><p>安装操作系统的时候会有多种可能分区的方法，一个直接的分区，一个是用了lvm,本篇将几种分区的情况分别写出来</p>
<h3 id="lvm分区的情况">lvm分区的情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem               Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root   <span class="number">17</span>G  <span class="number">927</span>M   <span class="number">17</span>G   <span class="number">6</span>% /</span><br><span class="line">devtmpfs                 <span class="number">901</span>M     <span class="number">0</span>  <span class="number">901</span>M   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs                    <span class="number">912</span>M     <span class="number">0</span>  <span class="number">912</span>M   <span class="number">0</span>% /dev/shm</span><br><span class="line">tmpfs                    <span class="number">912</span>M  <span class="number">8.6</span>M  <span class="number">904</span>M   <span class="number">1</span>% /run</span><br><span class="line">tmpfs                    <span class="number">912</span>M     <span class="number">0</span>  <span class="number">912</span>M   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               <span class="number">1014</span>M  <span class="number">143</span>M  <span class="number">872</span>M  <span class="number">15</span>% /boot</span><br><span class="line">tmpfs                    <span class="number">183</span>M     <span class="number">0</span>  <span class="number">183</span>M   <span class="number">0</span>% /run/user/<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>模拟/boot/grub2/grub.cfg的破坏</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># mv /boot/grub2/grub.cfg /boot/grub2/grub.cfgbk</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># reboot</span></span><br></pre></td></tr></table></figure>
<p>重启后就会出现这个</p>
<p><img src="http://static.zybuluo.com/zphj1987/agdslyms36u15eaar4h4d8gr/image.png" alt="image.png-13.4kB"></p>
<p>使用ls查询当前的分区情况</p>
<p><img src="http://static.zybuluo.com/zphj1987/fv6pjy9a3aw09ut819k8lvj9/image.png" alt="image.png-7.7kB"><br>查询分区情况<br><img src="http://static.zybuluo.com/zphj1987/uuj04u8y2dguvhqg85iavbe5/image.png" alt="image.png-29.1kB"></p>
<p>可以看到(hd0,msdos1)可以列出/boot里面的内容，可以确定这个就是启动分区</p>
<p>设置root<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos1)</span><br></pre></td></tr></table></figure></p>
<p>命令后面的路径可以用tab键补全,/dev/mapper/centos-root为根分区，因为当前的分区模式是lvm的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/mapper/centos-root</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h3 id="一个完整/分区形式">一个完整/分区形式</h3><p>这种情况，整个安装的系统就一个分区，boot是作为/分区的一个子目录的情况<br>ls 查询分区<br><img src="http://static.zybuluo.com/zphj1987/o3d1wegh1nfpm6a3u7w5q8yo/image.png" alt="image.png-4.6kB"></p>
<p>设置根分区<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos3)</span><br></pre></td></tr></table></figure></p>
<p>可以看到上面是msdos3分区对应的就是root=/dev/sda3,下面就设置这个root</p>
<p>设置linux16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /root/vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/sda3</span><br></pre></td></tr></table></figure></p>
<p>设置initrd16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /root/initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure></p>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h3 id="/分区和/boot分区独立分区的情况">/分区和/boot分区独立分区的情况</h3><p><img src="http://static.zybuluo.com/zphj1987/bd481u09kfonua77zwc87jsc/image.png" alt="image.png-16.3kB"></p>
<p>设置根分区<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos1)</span><br></pre></td></tr></table></figure></p>
<p>根据/分区为msdos2可以知道root分区为/dev/sda2<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/sda2</span><br></pre></td></tr></table></figure></p>
<p>设置initrd16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure></p>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h2 id="总结">总结</h2><p>主要的处理流程如下：</p>
<ul>
<li>首先通过<code>ls</code>得到分区的情况</li>
<li>通过<code>set</code>设置/boot所在的分区为root</li>
<li>分别设置linux16，initrd16并且指定root分区为/分区所在的目录</li>
<li>重启后重新生成grub即可</li>
</ul>
<p>本篇作为一个总结以备不时之需</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-11-30</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/grub.jpg" alt="grub"><br></center>

<h2 id="前言">前言</h2><p>博客很久没有更新了，一个原因就是原来存放部署博客的环境坏了，硬盘使用的是SSD，只要读取到某个文件，整个磁盘就直接识别不到了，还好博客环境之前有做备份，最近一直没有把部署环境做下恢复，今天抽空把环境做下恢复并且记录一篇基础的GRUB的处理文档</p>
<p>这两天正好碰到GRUB损坏的事，很久前处理过，但是没留下文档，正好现在把流程梳理一下，来解决grub.cfg损坏的情况,或者无法启动的情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[推荐一本书《Ceph设计原理与实现》]]></title>
    <link href="http://www.zphj1987.com/2017/09/28/a-new-ceph-book/"/>
    <id>http://www.zphj1987.com/2017/09/28/a-new-ceph-book/</id>
    <published>2017-09-28T14:23:47.000Z</published>
    <updated>2017-11-30T14:47:50.773Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/book-rocket.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇不是一篇技术文，而是推荐的一本书，对于写书来说，在多年以前觉得是一件可望而不可及的事情，而看到几本经典书籍的作者在讲述自己写书的过程的时候，都是自己注入了大量的精力的，所以我自己目前也只能是一个个知识点的以博客的方式进行记录</p>
<p>对于买书来说，很多人会觉得很贵，其实一本书几百面，只要里面的书有两面是能够帮助到你的，书的价值其实已经回来了，所以对于技术书籍来说，基本不评价书的好坏，而是去看多少能提取的东西，多少未知的东西</p>
<p>ceph的书籍最初的起步也是国外的一个作者写的，社区进行翻译，社区自己也出了一本书，还有ZETTAKIT（泽塔云）的研发常涛也出了一本《Ceph源码分析》，这些都是很好的书籍，之前也都有推荐，这些书籍都是一线的研发在繁忙之中抽出空余的时间写下来的</p>
<p>本篇推荐的一篇是来自中兴的书籍，中兴也是国内ceph开发里面代码提交量很高的公司</p>
<p>目前没能拿到书籍，所以只能从目录来讲下本书会提供哪些相关的知识了<br><a id="more"></a></p>
<h2 id="书籍简介">书籍简介</h2><h3 id="straw及straw2相关内容">straw及straw2相关内容</h3><p>这个是ceph里面的crush算法的内容，straw2算法优化了再平衡的时候的数据迁移量，以及能提供更好的分布，让数据更平均，相关内容里面还讲了数据分布的相关知识，整个能解决的应该是数据平均分布相关的知识，让你的数据更加平衡</p>
<h3 id="BlueStore_相关内容">BlueStore 相关内容</h3><p>BlueStore 是Ceph Luminous版本作为默认存储的新型的底层存储，这个是用来替换掉linux下的底层的文件系统的，而实现的一个新型的文件系统，这个是为了带来一个更好的性能的提升的，目前是测试可用，生产慎用的情况，应该会越来越稳定的</p>
<h3 id="纠删码原理与overwrites支持">纠删码原理与overwrites支持</h3><p>纠删码是为了解决副本的空间占用的问题，用更少的空间损失来获取更大的安全性，相当于计算换空间，纠删这个在很久以前就接触过一个另外一套文件系统，使用场景个人觉得是冷数据比较合适，而如果性能足够好，计算能力足够强，也能支撑比较大的带宽的<br>在之前的版本当中，ec的启用必须启动缓冲池，需要副本缓冲池的缓冲池做一层转发，这个转发实际上意味着写放大，并且还会出现缓冲池下刷数据的时候性能急剧下降的问题<br>在新版本中加入了overwrites支持，这个现在新版的bluestore的已经支持数据直接写到ec存储池了，也就是无需缓冲了</p>
<h3 id="PG_读写流程与状态迁移详解">PG 读写流程与状态迁移详解</h3><p>PG在恢复过程中会有各种状态，什么情况下会出现什么状态，什么状态进行什么处理，什么情况下不能乱动，这些都是需要好好的了解PG状态再进行操作的，否则把PG状态弄坏了，意味着数据也就无法读取了</p>
<h3 id="存储服务质量QoS">存储服务质量QoS</h3><p>ceph里面一直没有qos这个，也就是对读写相关的限流，kernel rbd的场景下是可以用cgroup进行qos相关的控制的，其他场景就没有什么好的方法了,所以在比较新的版本里面引入了dmclock来进行限流的相关的控制，这个以后可以在恢复以及写入当中做更精准的控制了，qos也是商用存储里面必要的功能，所以说ceph在功能完善方面更进了一步，需求推动研发</p>
<h3 id="存储RBD">存储RBD</h3><p>这个讲了rbd相关的一些知识，结构和功能方面的</p>
<h3 id="对象存储网关RGW">对象存储网关RGW</h3><p>这个讲了对象存储方面的一些功能特性和相关的操作</p>
<h3 id="分布式文件系统_CephFS">分布式文件系统 CephFS</h3><p>这个讲了cephfs相关的一些知识，讲了负载均衡和故障恢复的相关内容，负载均衡是相对于多active mds的场景的，可以对目录进行mds的负载划分，把负载分摊到多个mds上面，这个在新版本已经可以使用了，并且目前已经是生产可用</p>
<h3 id="定时scrub">定时scrub</h3><p>scrub这个不要让默认触发，自己做相关的策略，指定时间一个个PG的去scrub就可以了，书中应该会提及相关的具体做法</p>
<h3 id="Full的紧急处理">Full的紧急处理</h3><p>这个是集群出现Full以后的紧急处理，对于full以后的情况，一般不要乱动，因为full以后，其他osd也会是快full的状态，并且还有backfill full的控制，所以需要比较精准的控制，相当于游戏里面的微操了，书中应该会系统的讲解</p>
<h3 id="快照在增量备份中的应用">快照在增量备份中的应用</h3><p>通过快照的方式可以进行增量的备份，从而减少备份的需要获取的数据量，这个之前也有介绍过</p>
<h3 id="异常watcher的处理">异常watcher的处理</h3><p>这个应该是通过黑名单的方式进行watcher的相关的处理，这个建议是先处理能处理的，最后无法处理的异常情况用黑名单处理，这个等书出来以后可以看到更详细的内容</p>
<h2 id="作者简介">作者简介</h2><p>谢型果<br>中兴通讯资深软件工程师，5年存储开发经验，精通本地文件系统ZFS和分布式存储系统Ceph。2014 年开始研究 Ceph，2015 年加入 Ceph 开源社区，目前是 Ceph 开源社区的 Ceph Member。</p>
<p>任焕文<br>中兴通讯高级软件工程师，有10余年研发经验，曾就职于浪潮和华为，擅长数据库、网络和存储相关技术。Ceph Member成员，现主要负责Ceph文件系统、NAS存储和分布式一致性方面的研发工作。</p>
<p>严　军<br>中兴通讯高级软件工程师，从事存储系统开发工作多年，熟悉DPDK开发框架；2015年加入Ceph开源项目，对分布式存储系统QoS有深入研究，目前是Ceph开源社区的积极贡献者。</p>
<p>罗润兵<br>华中科技大学微电子专业研究生，中兴通讯高级软件工程师，精通TCP/IP协议栈和分布式存储系统，2014年开始接触并参与Ceph开源项目，目前是Ceph开源社区的积极贡献者。</p>
<p>韦巧苗<br>中兴通讯高级软件工程师，擅长C/C++编程，有5年存储系统研发经验，对Ceph RGW模块有深入研究，同时在Cache技术及性能优化上也有丰富的经验。</p>
<p>骆科学<br>中兴通讯高级软件工程师，有5年存储产品相关开发经验，擅长虚拟化及存储相关技术，2016年于Ceph中国社区年终盛典中被评为“2016年度社区十佳贡献者”。<br>总结</p>
<p>从目录上面看书中的内容包含的方面很多，可以看到这些很多都是我之前在生产环境当中使用到了或者接触过的东西，所以可以很系统的把这些知识提取出来，这样可以更了解整个系统<br>购买地址：</p>
<blockquote>
<p><a href="http://item.jd.com/12196497.html" target="_blank" rel="external">http://item.jd.com/12196497.html</a></p>
</blockquote>
<p>或者扫描二维码购买</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/miyt9s24b9e9x2jzf8arkzzq/scancode.png" alt=""><br></center>

<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-28</td>
</tr>
<tr>
<td style="text-align:center">更正常涛的公司名称</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/book-rocket.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇不是一篇技术文，而是推荐的一本书，对于写书来说，在多年以前觉得是一件可望而不可及的事情，而看到几本经典书籍的作者在讲述自己写书的过程的时候，都是自己注入了大量的精力的，所以我自己目前也只能是一个个知识点的以博客的方式进行记录</p>
<p>对于买书来说，很多人会觉得很贵，其实一本书几百面，只要里面的书有两面是能够帮助到你的，书的价值其实已经回来了，所以对于技术书籍来说，基本不评价书的好坏，而是去看多少能提取的东西，多少未知的东西</p>
<p>ceph的书籍最初的起步也是国外的一个作者写的，社区进行翻译，社区自己也出了一本书，还有ZETTAKIT（泽塔云）的研发常涛也出了一本《Ceph源码分析》，这些都是很好的书籍，之前也都有推荐，这些书籍都是一线的研发在繁忙之中抽出空余的时间写下来的</p>
<p>本篇推荐的一篇是来自中兴的书籍，中兴也是国内ceph开发里面代码提交量很高的公司</p>
<p>目前没能拿到书籍，所以只能从目录来讲下本书会提供哪些相关的知识了<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[掉电后osdmap丢失无法启动osd的解决方案]]></title>
    <link href="http://www.zphj1987.com/2017/09/27/lost-osdmap-recovery/"/>
    <id>http://www.zphj1987.com/2017/09/27/lost-osdmap-recovery/</id>
    <published>2017-09-27T06:03:59.000Z</published>
    <updated>2017-09-27T08:48:41.122Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recuva.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇讲述的是一个比较极端的故障的恢复场景，在整个集群全部服务器突然掉电的时候，osd里面的osdmap可能会出现没刷到磁盘上的情况，这个时候osdmap的最新版本为空或者为没有这个文件</p>
<p>还有一种情况就是机器宕机了，没有马上处理，等了一段时间以后，服务器机器启动了起来，而这个时候osdmap已经更新了，全局找不到需要的旧版本的osdmap和incmap，osd无法启动</p>
<p>一般情况下能找到的就直接从其他osd上面拷贝过来，然后就可以启动了，本篇讲述的是无法启动的情况<br><a id="more"></a></p>
<h2 id="解决方案">解决方案</h2><h3 id="获取运行的ceph集群当前版本">获取运行的ceph集群当前版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">10.2</span>.<span class="number">9</span> (<span class="number">2</span>ee413f77150c0f375ff6f10edd6c8f9c7d060d0)</span><br></pre></td></tr></table></figure>
<p>获取最新的osdmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph osd getmap -o /tmp/productosdmap</span></span><br><span class="line">got osdmap epoch <span class="number">142</span></span><br></pre></td></tr></table></figure></p>
<p>通过osdmap可以得到crushmap，fsid，osd，存储池，pg等信息</p>
<p>提取crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool /tmp/productosdmap --export-crush /tmp/productcrushmap</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'/tmp/productosdmap'</span></span><br><span class="line">osdmaptool: exported crush map to /tmp/productcrushmap</span><br></pre></td></tr></table></figure></p>
<p>拷贝到开发环境的机器上面</p>
<p>通过osdmap获取集群的fsid<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep fsid</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">fsid d153844c-<span class="number">16</span>f5-<span class="number">4</span>f48-<span class="number">829</span>d-<span class="number">87</span>fb49120bbe</span><br></pre></td></tr></table></figure></p>
<p>获取存储池相关的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep  pool</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">pool <span class="number">0</span> <span class="string">'rbd'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">1</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>获取osd相关的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep  osd</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">flags sortbitwise,require_jewel_osds</span><br><span class="line">max_osd <span class="number">3</span></span><br><span class="line">osd.<span class="number">0</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">135</span> up_thru <span class="number">141</span> down_at <span class="number">127</span> last_clean_interval [<span class="number">23</span>,<span class="number">24</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6801</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6802</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6803</span>/<span class="number">28245</span> exists,up d8040272-<span class="number">7</span>afb-<span class="number">49</span>c0-bb78-<span class="number">9</span>ff13cf7d31b</span><br><span class="line">osd.<span class="number">1</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">140</span> up_thru <span class="number">141</span> down_at <span class="number">131</span> last_clean_interval [<span class="number">33</span>,<span class="number">130</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6808</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6809</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6810</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6811</span>/<span class="number">28698</span> exists,up c6ac4c7a-<span class="number">0227</span>-<span class="number">4</span>af4-ac3f-bd844b2480f8</span><br><span class="line">osd.<span class="number">2</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">137</span> up_thru <span class="number">141</span> down_at <span class="number">133</span> last_clean_interval [<span class="number">29</span>,<span class="number">132</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6804</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6805</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6806</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6807</span>/<span class="number">28549</span> exists,up <span class="number">2170260</span>b-bb05-<span class="number">4965</span>-baf2-<span class="number">12</span>d1c41b3ba0</span><br></pre></td></tr></table></figure></p>
<h3 id="构建新集群">构建新集群</h3><p>下载这个版本的源码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS/ceph-<span class="number">10.2</span>.<span class="number">9</span>-<span class="number">0</span>.el7.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>放到一台独立的机器上面</p>
<p>解压rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 bianyi]<span class="comment"># rpm2cpio ceph-10.2.9-0.el7.src.rpm |cpio -div</span></span><br><span class="line">[root@lab8106 bianyi]<span class="comment"># tar -xvf ceph-10.2.9.tar.bz2</span></span><br></pre></td></tr></table></figure></p>
<p>编译环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ceph</span><br><span class="line">./install-deps.sh</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make -j <span class="number">12</span></span><br><span class="line"><span class="built_in">cd</span> src</span><br></pre></td></tr></table></figure></p>
<p>修改vstart.sh里面的fsid<br>启动集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./vstart.sh -n  --mon_num <span class="number">1</span> --osd_num <span class="number">3</span> --mds_num <span class="number">0</span>  --short  <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>检查集群状态：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf -s</span></span><br><span class="line">    cluster d153844c-<span class="number">16</span>f5-<span class="number">4</span>f48-<span class="number">829</span>d-<span class="number">87</span>fb49120bbe</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;a=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> a</span><br><span class="line">     osdmap e12: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v16: <span class="number">8</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">115</span> GB used, <span class="number">1082</span> GB / <span class="number">1197</span> GB avail</span><br><span class="line">                   <span class="number">8</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>导入crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd setcrushmap -i /root/rpmbuild/bianyi/productcrushmap </span></span><br><span class="line"><span class="built_in">set</span> crush map</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">14</span>:<span class="number">13</span>:<span class="number">29.052246</span> <span class="number">7</span>f19fd01d700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>设置PG<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">64</span></span><br><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pgp_num <span class="number">64</span></span><br></pre></td></tr></table></figure></p>
<p>模拟正式集群上的故障<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/osdmap.153__0_AC977A95__none  /tmp/</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/inc\\uosdmap.153__0_C67D77C2__none  /tmp/</span></span><br></pre></td></tr></table></figure></p>
<p>相当于无法读取这个osdmap和incmap了</p>
<p>尝试启动osd<br>设置debug_osd=20后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-osd@<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>检查日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/bj2fr68otco4oh6lloy9j7ly/image.png" alt="image.png-56.9kB"></p>
<p>可以看到153 epoch的osdmap是有问题的，那么我们需要的就是这个版本的osdmap</p>
<p>检查当前开发集群的osdmap的版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e18: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>那么先快速把osdmap版本提高到153附近，这里我选择120<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd thrash 120</span></span><br><span class="line">will thrash map <span class="keyword">for</span> <span class="number">120</span> epochs</span><br></pre></td></tr></table></figure></p>
<p>检查快速变化后的osdmap epoch<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e138: <span class="number">3</span> osds: <span class="number">2</span> up, <span class="number">1</span> <span class="keyword">in</span>; <span class="number">64</span> remapped pgs</span><br></pre></td></tr></table></figure></p>
<p>做了上面的thrash后，集群的osd会是比较乱的，比如我的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">0.80338</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.80338</span>     host lab8107                                   </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26779</span>         osd.<span class="number">0</span>         up        <span class="number">0</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>       down        <span class="number">0</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">43</span>:<span class="number">24.817177</span> <span class="number">7</span>fbcc7cdb700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>做下恢复，启动下相关osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd reweight 0 1</span></span><br><span class="line">reweighted osd.<span class="number">0</span> to <span class="number">1</span> (<span class="number">10000</span>)</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">01.439009</span> <span class="number">7</span>f56c147b700  <span class="number">0</span> lockdep stop</span><br><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd reweight 1 1</span></span><br><span class="line">reweighted osd.<span class="number">1</span> to <span class="number">1</span> (<span class="number">10000</span>)</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">04.020686</span> <span class="number">7</span>fea3345c700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>注意提取下开发集群上面新生成的osdmap的文件（多次执行以免刷掉了）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment">#rsync -qvzrtopg   dev/osd0/current/meta/ /root/meta/</span></span><br></pre></td></tr></table></figure></p>
<p>重启一遍开发集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./vstart.sh   --mon_num 1 --osd_num 3 --mds_num 0  --short  -d</span></span><br></pre></td></tr></table></figure></p>
<p>注意这里少了一个参数 -n,n是重建集群，这里我们只需要重启即可<br>再次检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e145: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>还是不够，不够的时候就执行上面的这个多次即可，一直到epoch到满足即可</p>
<p>将得到的osdmap拷贝到无法启动的osd的主机上面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># scp /root/meta/osdmap.153__0_AC977A95__none 192.168.8.107:/root</span></span><br><span class="line">osdmap.<span class="number">153</span>__0_AC977A95__none                            <span class="number">100</span>% <span class="number">2824</span>     <span class="number">2.8</span>KB/s   <span class="number">00</span>:<span class="number">00</span>    </span><br><span class="line">[root@lab8106 src]<span class="comment"># scp /root/meta/inc\\uosdmap.153__0_C67D77C2__none 192.168.8.107:/root</span></span><br><span class="line">inc\uosdmap.<span class="number">153</span>__0_C67D77C2__none                       <span class="number">100</span>%  <span class="number">198</span>     <span class="number">0.2</span>KB/s   <span class="number">00</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure></p>
<p>拷贝到osdmap的路径下面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># cp /root/osdmap.153__0_AC977A95__none ./</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># cp /root/inc\\uosdmap.153__0_C67D77C2__none ./</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># chown ceph:ceph osdmap.153__0_AC977A95__none </span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># chown ceph:ceph inc\\uosdmap.153__0_C67D77C2__none</span></span><br></pre></td></tr></table></figure></p>
<p>启动并且观测<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># systemctl start ceph-osd@0</span></span><br><span class="line">[root@lab8107 meta]<span class="comment">#tailf /var/log/ceph/ceph-osd.0.log</span></span><br></pre></td></tr></table></figure></p>
<p>检查集群状态，可以看到已经可以启动了</p>
<h2 id="总结">总结</h2><p>一般来说，出问题的时候都会说一句，如果备份了，就没那多事情，在一套生产环境当中，可以考虑下，什么是可以备份的，备份对环境的影响大不大，这种关键数据，并且可以全局共用，数据量也不大的数据，就需要备份好，比如上面的osdmap就可以在一个osd节点上面做一个实时的备份，或者短延时备份</p>
<p>本篇讲的是已经没有备份的情况下的做的一个恢复，掉电不是没有可能发生，至少解决了一个在osdmap无法找回的情况下的恢复办法</p>
<p>当然这里如果能够通过直接基于最新的osdmap和incmap做一定的解码，修改，编码，这样的方式应该也是可行的，这个就需要有一定的开发基础了，如果后面有找到这个方法会补充进本篇文章</p>
<p>你备份osdmap了么？</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recuva.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇讲述的是一个比较极端的故障的恢复场景，在整个集群全部服务器突然掉电的时候，osd里面的osdmap可能会出现没刷到磁盘上的情况，这个时候osdmap的最新版本为空或者为没有这个文件</p>
<p>还有一种情况就是机器宕机了，没有马上处理，等了一段时间以后，服务器机器启动了起来，而这个时候osdmap已经更新了，全局找不到需要的旧版本的osdmap和incmap，osd无法启动</p>
<p>一般情况下能找到的就直接从其他osd上面拷贝过来，然后就可以启动了，本篇讲述的是无法启动的情况<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[Luminous监控界面中文语言包]]></title>
    <link href="http://www.zphj1987.com/2017/09/13/maybe-the-first-chinese-for-luminous-dashboard/"/>
    <id>http://www.zphj1987.com/2017/09/13/maybe-the-first-chinese-for-luminous-dashboard/</id>
    <published>2017-09-13T09:30:32.000Z</published>
    <updated>2017-09-13T10:11:06.694Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/china.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前有各种ceph的管理平台，在部署方面大部分都比较麻烦，现在在luminous版本当中有一个原生的dashboard，虽然目前这个只能看，但是从界面上面，从接口方面都是非常不错的一个版本</p>
<p>原生版本目前没有语言的选择，虽然IT方面都是推荐用英语去做，但是在数据展示方面因为毕竟是要人来看，所以这里做了一个中文的语言包，方便转换成中文的界面，这个语言包是跟着ceph版本走的，因为界面可能会调整，所以只能一一匹配，同时提供了原版语言包，可以方便的回退回去，如果版本有更新以最后一个链接为准</p>
<p>如果有翻译的建议，欢迎在下面留言，或者其他方式告知我<br><a id="more"></a></p>
<h2 id="语言包">语言包</h2><h3 id="ceph版本（ceph_version_12-2-0_(32ce2a3ae5239ee33d6150705cdb24d43bab910c)_luminous_(rc)">ceph版本（ceph version 12.2.0 (32ce2a3ae5239ee33d6150705cdb24d43bab910c) luminous (rc)</h3><p>中文包：</p>
<p><a href="http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-chinese-12.2.0-1.0-1.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-chinese-12.2.0-1.0-1.x86_64.rpm</a></p>
<p>英文原版包：<br><a href="http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-english-12.2.0-1.0-1.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-english-12.2.0-1.0-1.x86_64.rpm</a></p>
<h3 id="安装方法">安装方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -Uvh  http://xxxxx.rpm --force</span><br></pre></td></tr></table></figure>
<h2 id="在线预览">在线预览</h2><p>为了方便看到效果，专门在本篇博客内放了一个预览，可以看看效果，数据是离线的，但是可以点击</p>
<div class="video-container"> <object><br><embed src="http://ow7obg32z.bkt.clouddn.com" <="" embed=""></object><br></div>

<h2 id="总结">总结</h2><p>一直有这个想法，花了点时间去实现，慢慢优化</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/china.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前有各种ceph的管理平台，在部署方面大部分都比较麻烦，现在在luminous版本当中有一个原生的dashboard，虽然目前这个只能看，但是从界面上面，从接口方面都是非常不错的一个版本</p>
<p>原生版本目前没有语言的选择，虽然IT方面都是推荐用英语去做，但是在数据展示方面因为毕竟是要人来看，所以这里做了一个中文的语言包，方便转换成中文的界面，这个语言包是跟着ceph版本走的，因为界面可能会调整，所以只能一一匹配，同时提供了原版语言包，可以方便的回退回去，如果版本有更新以最后一个链接为准</p>
<p>如果有翻译的建议，欢迎在下面留言，或者其他方式告知我<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[怎样禁止Ceph OSD的自动挂载]]></title>
    <link href="http://www.zphj1987.com/2017/09/07/how-to-disable-Ceph-OSD-automount/"/>
    <id>http://www.zphj1987.com/2017/09/07/how-to-disable-Ceph-OSD-automount/</id>
    <published>2017-09-06T16:29:55.000Z</published>
    <updated>2017-09-06T16:31:34.346Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mount.png" alt="此处输入图片的描述"><br></center>

<h2 id="前言">前言</h2><p>本篇来源于群里一个人的问题，有没有办法让ceph的磁盘不自动挂载，一般人的问题都是怎样让ceph能够自动挂载，在centos 7 平台下 ceph jewel版本以后都是有自动挂载的处理的，这个我之前也写过两篇文章 <a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/" target="_blank" rel="external">ceph在centos7下一个不容易发现的改变</a>和<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/" target="_blank" rel="external">Ceph数据盘怎样实现自动挂载</a>，来讲述这个自动挂载的<br><a id="more"></a><br>这里讲下流程：</p>
<blockquote>
<p>开机后udev匹配95-ceph-osd.rules规则，触发ceph-disk  trigger，遍历磁盘，匹配到磁盘的标记后就触发了自动挂载</p>
</blockquote>
<p>为什么要取消挂载？<br>也许一般都会想：不就是停掉osd，然后umount掉，检查磁盘吗<br>这个想法如果放在一般情况下都没有问题，但是为什么有这个需求就是有不一般的情况，这个我在很久前遇到过，所以对这个需求的场景比较清楚</p>
<p>在很久以前碰到过一次，机器启动都是正常的，但是只要某个磁盘一挂载，机器就直接挂掉了，所以这个是不能让它重启机器自动挂载的，也许还有其他的情况，这里总结成一个简单的需求就是不想它自动挂载</p>
<h2 id="解决方法">解决方法</h2><p>从上面的自启动后的自动挂载流程里面，我们可以知道这里可以有两个方案去解决这个问题，第一种是改变磁盘的标记，第二种就是改变udev的rule的规则匹配，这里两个方法都行，一个是完全不动磁盘，一个是动了磁盘的标记</p>
<h3 id="修改udev规则的方式">修改udev规则的方式</h3><p>这个因为曾经有一段时间看过udev相关的一些东西，所以处理起来还是比较简单的，这里顺便把调试过程也记录下来<br>/lib/udev/rules.d/95-ceph-osd.rules这个文件里面就是集群自动挂载的触发规则，所以在这里我们在最开始匹配上我们需要屏蔽的盘，然后绕过内部的所有匹配规则，具体办法就是<br>在这个文件里面第一行加上</p>
<blockquote>
<p>KERNEL==”sdb1|sdb2”, GOTO=”not_auto_mount”</p>
</blockquote>
<p>在最后一行加上</p>
<blockquote>
<p>LABEL=”not_auto_mount”</p>
</blockquote>
<p>验证规则是否正确<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">udevadm <span class="built_in">test</span> /sys/block/sdb/sdb1</span><br></pre></td></tr></table></figure></p>
<p>我们先看下正常的可以挂载的盘符的触发测试显示<br><img src="http://static.zybuluo.com/zphj1987/4fuopv2z3ys36e3462svo72t/image.png" alt="image.png-17.2kB"><br>再看下屏蔽了后的规则是怎样的<br><img src="http://static.zybuluo.com/zphj1987/3phv4b3x8d2nf6mhaio68zk4/image.png" alt="image.png-16kB"><br>可以看到在加入屏蔽条件以后，就没有触发挂载了，这里要注意，做屏蔽规则的时候需要把这个osd相关的盘都屏蔽，不然在触发相关分区的时候可能顺带挂载起来了，上面的sdb1就是数据盘，sdb2就是bluestore的block盘</p>
<p>测试没问题后就执行下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">udevadm control --reload-rules</span><br></pre></td></tr></table></figure></p>
<p>重启后验证是否自动挂载了</p>
<h3 id="修改磁盘标记的方式">修改磁盘标记的方式</h3><p>查询磁盘的标记typecode,也就是ID_PART_ENTRY_TYPE这个属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># blkid -o udev -p /dev/sdb1</span></span><br><span class="line">ID_FS_UUID=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_UUID_ENC=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_TYPE=xfs</span><br><span class="line">ID_FS_USAGE=filesystem</span><br><span class="line">ID_PART_ENTRY_SCHEME=gpt</span><br><span class="line">ID_PART_ENTRY_NAME=ceph\x20data</span><br><span class="line">ID_PART_ENTRY_UUID=<span class="number">7</span>b321ca3-<span class="number">402</span>c-<span class="number">4557</span>-b121-<span class="number">887266</span>a1e1b8</span><br><span class="line">ID_PART_ENTRY_TYPE=<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff05d</span><br><span class="line">ID_PART_ENTRY_NUMBER=<span class="number">1</span></span><br><span class="line">ID_PART_ENTRY_OFFSET=<span class="number">2048</span></span><br><span class="line">ID_PART_ENTRY_SIZE=<span class="number">204800</span></span><br><span class="line">ID_PART_ENTRY_DISK=<span class="number">8</span>:<span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p>匹配到这个属性就认为是集群的节点，可以挂载的，那么我们先改变这个<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff0f9 -- /dev/sdb</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># blkid -o udev -p /dev/sdb1</span></span><br><span class="line">ID_FS_UUID=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_UUID_ENC=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_TYPE=xfs</span><br><span class="line">ID_FS_USAGE=filesystem</span><br><span class="line">ID_PART_ENTRY_SCHEME=gpt</span><br><span class="line">ID_PART_ENTRY_NAME=ceph\x20data</span><br><span class="line">ID_PART_ENTRY_UUID=<span class="number">7</span>b321ca3-<span class="number">402</span>c-<span class="number">4557</span>-b121-<span class="number">887266</span>a1e1b8</span><br><span class="line">ID_PART_ENTRY_TYPE=<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff0f9</span><br><span class="line">ID_PART_ENTRY_NUMBER=<span class="number">1</span></span><br><span class="line">ID_PART_ENTRY_OFFSET=<span class="number">2048</span></span><br><span class="line">ID_PART_ENTRY_SIZE=<span class="number">204800</span></span><br><span class="line">ID_PART_ENTRY_DISK=<span class="number">8</span>:<span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到type的属性已经被修改了<br>再次测试，可以看到已经不匹配了<br><img src="http://static.zybuluo.com/zphj1987/ek3ocgg9w584u07x0pg8lqc0/image.png" alt="image.png-14.1kB"></p>
<p>如果需要恢复就执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb</span></span><br></pre></td></tr></table></figure></p>
<p>这里同样需要改掉相关的block盘的标记，否则一样被关联的挂载起来了</p>
<h2 id="总结">总结</h2><p>本篇用两种方法来实现了ceph osd的盘符的不自动挂载，这个一般情况下都不会用到，比较特殊的情况遇到了再这么处理就可以了，或者比较暴力的方法就是直接把挂载的匹配的规则全部取消掉，使用手动触发挂载的方式也行，这个方法很多，能够快速，简单的满足需求即可</p>
<p>此mount非彼mount，题图无关</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-07</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mount.png" alt="此处输入图片的描述"><br></center>

<h2 id="前言">前言</h2><p>本篇来源于群里一个人的问题，有没有办法让ceph的磁盘不自动挂载，一般人的问题都是怎样让ceph能够自动挂载，在centos 7 平台下 ceph jewel版本以后都是有自动挂载的处理的，这个我之前也写过两篇文章 <a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/">ceph在centos7下一个不容易发现的改变</a>和<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/">Ceph数据盘怎样实现自动挂载</a>，来讲述这个自动挂载的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph OSD服务失效自动启动控制]]></title>
    <link href="http://www.zphj1987.com/2017/09/06/Ceph-OSD-autorestart-when-fail/"/>
    <id>http://www.zphj1987.com/2017/09/06/Ceph-OSD-autorestart-when-fail/</id>
    <published>2017-09-06T04:32:41.000Z</published>
    <updated>2017-09-06T05:34:31.655Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/restart.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>服务器上面的服务会因为各种各样的原因失败，磁盘故障，权限问题，或者是服务过载引起超时，这些都可能引起</p>
<p>这个在ceph里面systemctl unit 默认有个on-fail restart,默认的可能并不适合所有的场景，所以自动化的服务应该是尽量去适配你手动处理的过程，手动怎么处理的，就怎么去设置<br><a id="more"></a></p>
<h2 id="启动分析">启动分析</h2><p>如果有osd失败了，一般上去会先启动一次，尽快让服务启动，然后去检查是否有故障，如果失败了，就开启调试日志，再次重启，在问题解决之前，是不会再启动了，所以这里我们的自动启动设置也这么设置</p>
<h2 id="参数配置">参数配置</h2><p>ceph的osd的启动配置在这个配置文件</p>
<blockquote>
<p>/usr/lib/systemd/system/ceph-osd@.service</p>
</blockquote>
<p>默认参数：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Restart=on-failure</span><br><span class="line">StartLimitInterval=<span class="number">30</span>min</span><br><span class="line">StartLimitBurst=<span class="number">30</span></span><br><span class="line">RestartSec=<span class="number">20</span>s</span><br></pre></td></tr></table></figure></p>
<p>默认的参数意思是<br>在30min的周期内，如果没启动成功，那么在失败后20s进行启动，这样的启动尝试30次</p>
<p>这个在启动机器的时候，是尽量在osd启动失败的情况下，能够在30min分钟内尽量把服务都启动起来，这个对于关机启动后的控制是没问题的</p>
<p>参数解释：<br>StartLimitInterval不能设置太小，在osd崩溃的情况里面有一种是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，所以RestartSec*StartLimitBurst 必须小于StartLimitInterval，否则可能出现无限重启的情况</p>
<p>restart的触发条件</p>
<table>
<thead>
<tr>
<th style="text-align:left">Restart settings/Exit causes</th>
<th style="text-align:center">always</th>
<th style="text-align:center">on-success</th>
<th style="text-align:center">on-failure</th>
<th style="text-align:center">on-abnormal</th>
<th style="text-align:center">on-abort</th>
<th style="text-align:center">on-watchdog</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Clean exit code or signal</td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Unclean exit code</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Unclean signal</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Timeout</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Watchdog</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
</tr>
</tbody>
</table>
<p>可调整项目<br>Restart=always就是只要非正常的退出了，就满足重启的条件，kill -9 进程也能够自动启动</p>
<p>在osd崩溃的情况里面有一种情况是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，这种崩溃的情况我们不需要尝试多次重启,所以适当降低重启频率<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">StartLimitBurst=<span class="number">3</span></span><br><span class="line">RestartSec=<span class="number">10</span>s</span><br></pre></td></tr></table></figure></p>
<p>这个设置后能够在运行的集群当中比较好的处理异常退出的情况，但是设置后就要注意关机osd osd启动的问题，一般关机的时候肯定是有人在维护的，所以这个问题不大，人为处理下就行</p>
<p>所以建议的参数是</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=<span class="number">30</span>min</span><br><span class="line">StartLimitBurst=<span class="number">3</span></span><br><span class="line">RestartSec=<span class="number">10</span>s</span><br></pre></td></tr></table></figure>
<p>可以根据自己的需要进行设置，这个设置下，停止osd就用systemctl 命令去 stop，然后其他的任何异常退出情况都会把osd给拉起来</p>
<h2 id="总结">总结</h2><p>systemctl在服务控制方面有着很丰富的功能，可以根据自己的需求进行调整，特别是对启动条件有约束的场景，这个是最适合的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/restart.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>服务器上面的服务会因为各种各样的原因失败，磁盘故障，权限问题，或者是服务过载引起超时，这些都可能引起</p>
<p>这个在ceph里面systemctl unit 默认有个on-fail restart,默认的可能并不适合所有的场景，所以自动化的服务应该是尽量去适配你手动处理的过程，手动怎么处理的，就怎么去设置<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[osd磁盘空间足够无法写入数据的分析与解决]]></title>
    <link href="http://www.zphj1987.com/2017/09/04/osd-has-inode-cannot-write/"/>
    <id>http://www.zphj1987.com/2017/09/04/osd-has-inode-cannot-write/</id>
    <published>2017-09-04T15:06:17.000Z</published>
    <updated>2017-09-05T02:42:23.055Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/full.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个问题的来源是ceph社区里面一个群友的环境出现在85%左右的时候，启动osd报错，然后在本地文件系统当中进行touch文件的时候也是报错，df -i查询inode也是没用多少，使用的也是inode64挂载的，开始的时候排除了配置原因引起的，在ceph的邮件列表里面有一个相同<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2016-October/013929.html" target="_blank" rel="external">问题</a>，也是没有得到解决</p>
<p>看到这个问题比较感兴趣，就花了点时间来解决来定位和解决这个问题，现在分享出来，如果有类似的生产环境，可以提前做好检查预防工作</p>
<h2 id="现象描述">现象描述</h2><p>ceph版本</p>
<blockquote>
<p>[root@lab8107 mnt]# ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br>我复现的环境为这个版本<br><a id="more"></a></p>
</blockquote>
<p>查询使用空间</p>
<p><img src="http://static.zybuluo.com/zphj1987/mqyxyf1tthpo3596f0gt6ujj/image.png" alt="image.png-19.8kB"><br>可以看到空间才使用了54%<br><img src="http://static.zybuluo.com/zphj1987/434y23gzh7mhy3sjzmv9f8wg/image.png" alt="image.png-28kB"><br>可以看到，inode剩余比例很多，而文件确实无法创建</p>
<p>这个时候把一个文件mv出来，然后又可以创建了，并且可以写入比mv出来的文件更大的文件，写完一个无法再写入更多文件了</p>
<p>这里有个初步判断，不是容量写完了，而是文件的个数限制住了</p>
<p>那么来查询下文件系统的inode还剩余多少，xfs文件系统的inode是动态分配的，我们先检查无法写入的文件系统的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs_db -r -c <span class="string">"sb 0"</span> -c <span class="string">"p"</span> -c <span class="string">"freesp -s"</span> /dev/sdb1|grep ifree</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/eqn4pcu0bj7rz5bn2rfum7pj/image.png" alt="image.png-5.1kB"><br>可以看到剩余的inode确实为0，这里确实是没有剩余inode了，所以通过df -i来判断inode是否用完并不准确，那个是已经使用值与理论值的相除的结果</p>
<p>查询xfs碎片，也是比例很低</p>
<h2 id="定位问题">定位问题</h2><p>首先查看xfs上面的数据结构<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs_db -r -c <span class="string">"sb 0"</span> -c <span class="string">"p"</span> -c <span class="string">"freesp -s "</span> /dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/2a48824pncg1g9up7n1npud3/image.png" alt="image.png-13.7kB"></p>
<p>上面的输出结果这里简单解释一下，这里我也是反复比对和查看资料才理解这里的意思，这里有篇<a href="https://www.novell.com/support/kb/doc.php?id=7014320" target="_blank" rel="external">novell</a>的资料有提到这个，这里我再拿一个刚刚格式化完的分区结果来看下<br><img src="http://static.zybuluo.com/zphj1987/p1c3mb2e5kfypt4kw2ow9tm6/image.png" alt="image.png-14.3kB"></p>
<p>这里用我自己的理解来描述下，这个extents的剩余数目是动态变化的，刚分完区的那个，有4个1048576-1220608左右的逻辑区间，而上面的无法写入数据的数据结构，剩下的extent的平均大小为22个block，而这样的blocks总数有1138886个，占总体的99.85，也就是剩余的空间的的extents所覆盖的区域全部是16个block到31个block的这种空洞，相当于蛋糕被切成很多小块了，大的都拿走了，剩下的总量还很多，但是都是很小的碎蛋糕，所以也没法取了</p>
<p>默认来说inode chunk 为64 ，也就是需要64*inodesize的存储空间来存储inode，这个剩下的空间已经不够分配了</p>
<h2 id="解决办法">解决办法</h2><p>下个段落会讲下为什么会出现上面的情况，现在先说解决办法，把文件mv出来，然后mv进去，这个是在其他场景下的一个解决方法，这个操作要小心，因为有扩展属性，操作不小心会弄掉了，这里建议用另外一个办法xfs_dump的方法</p>
<p>我的环境比较小，20G的盘，如果盘大就准备大盘,这里是验证是否可行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfsdump -L osd0 -M osd0 <span class="operator">-f</span> /mnt/osd0 /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>还原回去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph-<span class="number">0</span>]<span class="comment"># xfsrestore -f /mnt/osd0 /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">xfsrestore: using file dump (drive_simple) strategy</span><br><span class="line">xfsrestore: version <span class="number">3.1</span>.<span class="number">4</span> (dump format <span class="number">3.0</span>) - <span class="built_in">type</span> ^C <span class="keyword">for</span> status and control</span><br><span class="line">xfsrestore: ERROR: unable to create /var/lib/ceph/osd/ceph-<span class="number">0</span>/xfsrestorehousekeepingdir: No space left on device</span><br><span class="line">xfsrestore: Restore Status: ERROR</span><br></pre></td></tr></table></figure></p>
<p>直接还原还是会有问题,没有可以写的地方了，这里因为已经dump了一份，这里就mv pg的数据目录出去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv /var/lib/ceph/osd/ceph-<span class="number">0</span>/current/ /mnt</span><br></pre></td></tr></table></figure></p>
<p>开始还原<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfsrestore  -o <span class="operator">-f</span> /mnt/osd0 /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>还原以后如果有权限需要处理的就处理下权限，先检查下文件系统的数据结构<br><img src="http://static.zybuluo.com/zphj1987/r8fmdzz923pju8p48gqq0ma3/image.png" alt="image.png-19.6kB"><br>可以看到数据结构已经很理想了<br>然后启动osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-osd@<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>然后检查下数据是不是都可以正常写进去了</p>
<ul>
<li>如果出现了上面的空间已经满了的情况，处理的时候需要注意</li>
<li>备份好数据</li>
<li>单个盘进行处理</li>
<li>备份的数据先保留好以防万一</li>
<li>启动好了后，验证下集群的状态后再继续，可以尝试get下数据检查数据</li>
</ul>
<h2 id="为什么会出现这样">为什么会出现这样</h2><p>我们在本地文件系统里面连续写100个文件<br>准备一个a文件里面有每行150个a字符，700行，这个文件大小就是100K<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 <span class="built_in">test</span>]<span class="comment"># seq 100|xargs -i dd if=a of=a&#123;&#125; bs=100K count=1</span></span><br></pre></td></tr></table></figure></p>
<p>检查文件的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 <span class="built_in">test</span>]<span class="comment"># seq 100|xargs -i xfs_bmap -v a&#123;&#125; |less</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/cnx9c7dwm2vm1njym9c2ogc2/image.png" alt="image.png-47.1kB"></p>
<p>大部分情况下这个block的分配是连续的</p>
<p>先检查下当前的数据结构<br><img src="http://static.zybuluo.com/zphj1987/9065j88etksn793ewezr6fh3/image.png" alt="image.png-30.8kB"></p>
<p>我们把刚刚的100个对象put到集群里面去，监控下集群的数据目录的写入情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">inotifywait -m --timefmt <span class="string">'%Y %B %d %H:%M:%S'</span> --format <span class="string">'%T %w %e %f'</span> -r -m /var/lib/ceph/osd/ceph-<span class="number">0</span>/</span><br></pre></td></tr></table></figure></p>
<p>put数据进去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `ls ./`;<span class="keyword">do</span> rados -p rbd put <span class="variable">$a</span> <span class="variable">$a</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/l2bfn1hg02ot8oiz1bkpj2n4/image.png" alt="image.png-53.7kB"><br><img src="http://static.zybuluo.com/zphj1987/7myxv8qwe9cjrybah3gnoj5v/image.png" alt="image.png-64.2kB"><br>查看对象的数据，里面并没有连续起来，并且写入的数据的方式是:<br>打开文件，设置扩展属性，填充内容，设置属性，关闭，很多并发在一起做</p>
<p>写完的数据结构<br><img src="http://static.zybuluo.com/zphj1987/9njqf9rlqfd8mfefub9sqp39/image.png" alt="image.png-30.9kB"></p>
<p>结果就是在100K这个数据模型下，会产生很多小的block空隙，最后就是无法写完文件的情况，这里产生空隙并不是很大的问题，问题是这里剩下的空隙无法完成inode的动态分配的工作，这里跟一个格式化选项的变化有关</p>
<p>准备一个集群<br>然后写入(一直写)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd bench -b <span class="number">100</span>K <span class="number">6000</span> write --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>就可以必现这个问题，可以看到上面的从16-31 block的区间从 12 extents涨到了111 extents</p>
<h2 id="解决办法-1">解决办法</h2><p>用deploy在部署的时候默认的格式化参数为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/sbin/mkfs -t xfs <span class="operator">-f</span> -i size=<span class="number">2048</span> -- /dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>这个isize设置的是2048，这个在后面剩余的空洞比较小的时候就无法写入新的数据了，所以在ceph里面存储100K这种小文件的场景的时候，把mkfs.xfs的isize改成默认的256就可以提前避免这个问题<br>修改 /usr/lib/python2.7/site-packages/ceph_disk/main.py的256行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs=[</span><br><span class="line">    <span class="comment"># xfs insists on not overwriting previous fs; even if we wipe</span></span><br><span class="line">    <span class="comment"># partition table, we often recreate it exactly the same way,</span></span><br><span class="line">    <span class="comment"># so we'll see ghosts of filesystems past</span></span><br><span class="line">    <span class="string">'-f'</span>,</span><br><span class="line">    <span class="string">'-i'</span>, <span class="string">'size=2048'</span>,</span><br><span class="line">],</span><br></pre></td></tr></table></figure></p>
<p>改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">'-i'</span>, <span class="string">'size=256'</span>,</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/crdjd24yzed9s0tdh8og3d1e/image.png" alt="image.png-24.4kB"><br>这个地方检查下是不是对的，然后就可以避免这个问题了，可以测试下是不是一直可以写到很多，我的这个测试环境写到91%还没问题</p>
<h2 id="总结">总结</h2><p>在特定的数据写入模型下，可能出现一些可能无法预料的问题，而参数的改变可能也没法覆盖所有场景，本篇就是其中的一个比较特殊的问题，定位好问题，在遇到的时候能够解决，或者提前避免掉</p>
<h2 id="后续">后续</h2><p>在升级了内核到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># uname  -a</span></span><br><span class="line">Linux lab8107 <span class="number">4.13</span>.<span class="number">0</span>-<span class="number">1</span>.el7.elrepo.x86_64 <span class="comment">#1 SMP Sun Sep 3 19:07:24 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux</span></span><br></pre></td></tr></table></figure></p>
<p>升级xfsprogs到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># rpm -qa|grep xfsprogs</span></span><br><span class="line">xfsprogs-<span class="number">4.12</span>.<span class="number">0</span>-<span class="number">4</span>.el7.centos.x86_64</span><br></pre></td></tr></table></figure></p>
<p>重新部署osd，还是一样的isize=2048，一样的写入模型<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># df -h /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sdb1       <span class="number">9.4</span>G  <span class="number">9.0</span>G  <span class="number">395</span>M  <span class="number">96</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">meta_uuid = <span class="number">00000000</span>-<span class="number">0000</span>-<span class="number">0000</span>-<span class="number">0000</span>-<span class="number">000000000000</span></span><br><span class="line">   from      to extents  blocks    pct</span><br><span class="line">      <span class="number">1</span>       <span class="number">1</span>     <span class="number">545</span>     <span class="number">545</span>   <span class="number">0.50</span></span><br><span class="line">      <span class="number">2</span>       <span class="number">3</span>     <span class="number">665</span>    <span class="number">1666</span>   <span class="number">1.52</span></span><br><span class="line">      <span class="number">4</span>       <span class="number">7</span>    <span class="number">1624</span>    <span class="number">8927</span>   <span class="number">8.12</span></span><br><span class="line">      <span class="number">8</span>      <span class="number">15</span>    <span class="number">1853</span>   <span class="number">19063</span>  <span class="number">17.34</span></span><br><span class="line">     <span class="number">16</span>      <span class="number">31</span>      <span class="number">19</span>     <span class="number">352</span>   <span class="number">0.32</span></span><br><span class="line">   <span class="number">4096</span>    <span class="number">8191</span>       <span class="number">1</span>    <span class="number">7694</span>   <span class="number">7.00</span></span><br><span class="line">  <span class="number">16384</span>   <span class="number">32767</span>       <span class="number">3</span>   <span class="number">71659</span>  <span class="number">65.20</span></span><br><span class="line">total free extents <span class="number">4710</span></span><br><span class="line">total free blocks <span class="number">109906</span></span><br><span class="line">average free extent size <span class="number">23.3346</span></span><br><span class="line">[root@lab8107 ~]<span class="comment"># xfs_db -r -c "sb 0" -c "p" -c "freesp -s " /dev/sdb1</span></span><br></pre></td></tr></table></figure>
<p>可以看到已经很少的稀疏空间了，留下比较大的空间，这个地方应该是优化了底层数据存储的算法</p>
<p>另外，xfs的inode是动态分配的,xfs官方也考虑到了这个可能空洞太多无法分配inode问题，这个是最新的mkfs.xfs的man page<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sparse[=value]</span><br><span class="line">  Enable sparse inode chunk allocation. The value is either <span class="number">0</span> or <span class="number">1</span>, with <span class="number">1</span> signifying that sparse allocation is enabled.  If  the value  is omitted, <span class="number">1</span> is assumed. Sparse inode allocation is disabled by default. This feature is only available <span class="keyword">for</span> filesystems formatted with -m crc=<span class="number">1</span>.</span><br><span class="line">  </span><br><span class="line">   When enabled, sparse inode allocation allows the filesystem to allocate smaller than the  standard  <span class="number">64</span>-inode  chunk  when  free space  is  severely  limited. This feature is useful <span class="keyword">for</span> filesystems that might fragment free space over time such that no free extents are large enough to accommodate a chunk of <span class="number">64</span> inodes. Without this feature enabled, inode allocations can fail with out of space errors under severe fragmented free space conditions.</span><br></pre></td></tr></table></figure></p>
<p>是以64个inode为chunk来进行动态分配的，应该是有两个chunk，也就是动态查询看到的是128个inode以下，在更新到最新的版本以后，因为已经没有那么多空洞了，所以即使在没开这个稀疏inode的情况下，ceph的小文件也能够把磁盘写满</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-04</td>
</tr>
<tr>
<td style="text-align:center">增加更新内核和xfsprogs的验证</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-05</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/full.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个问题的来源是ceph社区里面一个群友的环境出现在85%左右的时候，启动osd报错，然后在本地文件系统当中进行touch文件的时候也是报错，df -i查询inode也是没用多少，使用的也是inode64挂载的，开始的时候排除了配置原因引起的，在ceph的邮件列表里面有一个相同<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2016-October/013929.html">问题</a>，也是没有得到解决</p>
<p>看到这个问题比较感兴趣，就花了点时间来解决来定位和解决这个问题，现在分享出来，如果有类似的生产环境，可以提前做好检查预防工作</p>
<h2 id="现象描述">现象描述</h2><p>ceph版本</p>
<blockquote>
<p>[root@lab8107 mnt]# ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br>我复现的环境为这个版本<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[为什么关不掉所有的OSD]]></title>
    <link href="http://www.zphj1987.com/2017/08/21/why-can-not-stop-allosd/"/>
    <id>http://www.zphj1987.com/2017/08/21/why-can-not-stop-allosd/</id>
    <published>2017-08-21T05:39:09.000Z</published>
    <updated>2017-08-21T06:03:03.343Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>碰到一个cepher问了一个问题：</p>
<blockquote>
<p>为什么我的OSD关闭到最后有92个OSD无法关闭,总共的OSD有300个左右</p>
</blockquote>
<p>想起来在很久以前帮人处理过一次问题，当时环境是遇上了一个BUG，需要升级到新版本进行解决，然后当时我来做操作，升级以后，发现osd无法启动，进程在，状态无法更新，当时又回滚回去，就可以了，当时好像是K版本升级到J版本，想起来之前看过这个版本里面有数据结构的变化，需要把osd全部停掉以后才能升级，然后就stop掉所有osd，当时发现有的osd还是无法stop，然后就手动去标记了，然后顺利升级<br><a id="more"></a><br>今天这个现象应该跟当时是一个问题，然后搜索了一番参数以后，最后定位在确实是参数进行了控制</p>
<h2 id="实践">实践</h2><p>我的一个8个osd的单机环境，对所有OSD进行stop以后就是这个状态，还有2个状态无法改变<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">49</span>ee8a7f-fb7c-<span class="number">4239</span><span class="operator">-a</span>4b7-acf0bc37430d</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">295</span> pgs are stuck inactive <span class="keyword">for</span> more than <span class="number">300</span> seconds</span><br><span class="line">            <span class="number">295</span> pgs stale</span><br><span class="line">            <span class="number">295</span> pgs stuck stale</span><br><span class="line">            too many PGs per OSD (<span class="number">400</span> &gt; max <span class="number">300</span>)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e77: <span class="number">8</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span>; <span class="number">178</span> remapped pgs</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v296: <span class="number">400</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">76440</span> kB used, <span class="number">548</span> GB / <span class="number">548</span> GB avail</span><br><span class="line">                 <span class="number">295</span> stale+active+clean</span><br><span class="line">                 <span class="number">105</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>看下这组参数：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_osd_min_up_ratio = <span class="number">0.3</span></span><br><span class="line">mon_osd_min_<span class="keyword">in</span>_ratio = <span class="number">0.3</span></span><br></pre></td></tr></table></figure></p>
<p>我们修改成0 后再测试</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_osd_min_up_ratio = <span class="number">0</span></span><br><span class="line">mon_osd_min_<span class="keyword">in</span>_ratio = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>停止进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>查看状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">49</span>ee8a7f-fb7c-<span class="number">4239</span><span class="operator">-a</span>4b7-acf0bc37430d</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">48</span> pgs are stuck inactive <span class="keyword">for</span> more than <span class="number">300</span> seconds</span><br><span class="line">            <span class="number">85</span> pgs degraded</span><br><span class="line">            <span class="number">15</span> pgs peering</span><br><span class="line">            <span class="number">400</span> pgs stale</span><br><span class="line">            <span class="number">48</span> pgs stuck inactive</span><br><span class="line">            <span class="number">48</span> pgs stuck unclean</span><br><span class="line">            <span class="number">85</span> pgs undersized</span><br><span class="line">            <span class="number">8</span>/<span class="number">8</span> <span class="keyword">in</span> osds are down</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e86: <span class="number">8</span> osds: <span class="number">0</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v310: <span class="number">400</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">286</span> MB used, <span class="number">2193</span> GB / <span class="number">2194</span> GB avail</span><br><span class="line">                 <span class="number">300</span> stale+active+clean</span><br><span class="line">                  <span class="number">85</span> stale+undersized+degraded+peered</span><br><span class="line">                  <span class="number">15</span> stale+peering</span><br></pre></td></tr></table></figure></p>
<p>可以看到状态已经可以正常全部关闭了</p>
<h2 id="分析">分析</h2><p>这里不清楚官方做这个的理由，个人推断是这样的，默认的副本为3，那么在集群有三分之二的OSD都挂掉了以后，再出现OSD挂掉的情况下，这个集群其实就是一个废掉的状态的集群，而这个时候，还去触发down和out，对于环境来说已经是无效的操作了，触发的迁移也属于无效的迁移了，这个时候保持一个最终的可用的osdmap状态，对于整个环境的恢复也有一个基准点</p>
<p>在Luminous版本中已经把这个参数改成</p>
<blockquote>
<p>mon_osd min_up_ratio = 0.3<br>mon_osd_min_in_ratio = 0.75</p>
</blockquote>
<p>来降低其他异常情况引起的down，来避免过量的迁移</p>
<h2 id="总结">总结</h2><p>本篇就是一个参数的实践</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-21</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>碰到一个cepher问了一个问题：</p>
<blockquote>
<p>为什么我的OSD关闭到最后有92个OSD无法关闭,总共的OSD有300个左右</p>
</blockquote>
<p>想起来在很久以前帮人处理过一次问题，当时环境是遇上了一个BUG，需要升级到新版本进行解决，然后当时我来做操作，升级以后，发现osd无法启动，进程在，状态无法更新，当时又回滚回去，就可以了，当时好像是K版本升级到J版本，想起来之前看过这个版本里面有数据结构的变化，需要把osd全部停掉以后才能升级，然后就stop掉所有osd，当时发现有的osd还是无法stop，然后就手动去标记了，然后顺利升级<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于scrub的详细分析和建议]]></title>
    <link href="http://www.zphj1987.com/2017/08/19/about-scrub-suggestion/"/>
    <id>http://www.zphj1987.com/2017/08/19/about-scrub-suggestion/</id>
    <published>2017-08-19T15:08:56.000Z</published>
    <updated>2017-08-21T06:02:44.778Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/scrub.png" alt="scrub"><br></center>

<h2 id="前言">前言</h2><p>关于scrub这块一直想写一篇文章的，这个在很久前，就做过一次测试，当时是看这个scrub到底有多大的影响，当时看到的是磁盘读占很高，启动deep-scrub后会有大量的读,前端可能会出现 slow request,这个是当时测试看到的现象，一个比较简单的处理办法就是直接给scrub关掉了，当然关掉了就无法检测底层到底有没有对象不一致的问题<br>关于这个scrub生产上是否开启，仁者见仁，智者见智，就是选择的问题了，这里不做讨论，个人觉得开和关都有各自的道理，本篇是讲述的如果想开启的情况下如何把scrub给控制住<br><a id="more"></a><br>最近在ceph群里看到一段大致这样的讨论：</p>
<blockquote>
<p>scrub是个坑<br>小文件多的场景一定要把scrub关掉<br>单pg的文件量达到一定规模，scrub一开就会有slow request<br>这个问题解决不了</p>
</blockquote>
<p>上面的说法有没有问题呢？在一般情况下来看，确实如此，但是我们是否能尝试去解决下这个问题，或者缓解下呢？那么我们就来尝试下</p>
<h2 id="scrub的一些追踪">scrub的一些追踪</h2><p>下面的一些追踪并不涉及代码，仅仅从配置和日志的观测来看看scrub到底干了什么</p>
<h3 id="环境准备">环境准备</h3><p>我的环境为了便于观测，配置的是一个pg的存储池，然后往这个pg里面put了100个对象，然后对这个pg做deep-scrub，deep-scrub比scrub对磁盘的压力要大些，所以本篇主要是去观测的deep-scrub</p>
<h4 id="开启对pg目录的访问的监控">开启对pg目录的访问的监控</h4><p>使用的是inotifywait，我想看下deep-scrub的时候，pg里面的对象到底接收了哪些请求</p>
<p>inotifywait -m 1.0_head<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a16__head_8FA46F40__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a16__head_8FA46F40__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a39__head_621FD720__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a39__head_621FD720__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a30__head_655287E0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a30__head_655287E0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a91__head_B02EE3D0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a91__head_B02EE3D0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a33__head_9E9E3E30__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a33__head_9E9E3E30__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a92__head_6AFC6B30__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a92__head_6AFC6B30__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a22__head_AC48AAB0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a22__head_AC48AAB0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a42__head_76B90AC8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a42__head_76B90AC8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a5__head_E5A1A728__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a5__head_E5A1A728__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a34__head_4D9ABA68__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a34__head_4D9ABA68__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a69__head_7AF2B6E8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a69__head_7AF2B6E8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a95__head_BD3695B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a95__head_BD3695B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a67__head_6BCD37B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a67__head_6BCD37B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a10__head_F0F08AF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a10__head_F0F08AF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a3__head_88EF0BF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a3__head_88EF0BF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a82__head_721BC094__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a82__head_721BC094__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a48__head_27A729D4__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a48__head_27A729D4__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a36__head_F63E6AF4__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a36__head_F63E6AF4__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a29__head_F06D540C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a29__head_F06D540C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a31__head_AC83164C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a31__head_AC83164C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a59__head_884F9B6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a59__head_884F9B6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a58__head_06954F6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a58__head_06954F6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a55__head_2A42E61C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a55__head_2A42E61C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a90__head_1B88FEDC__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a90__head_1B88FEDC__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a100__head_C29E0C42__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a100__head_C29E0C42__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a15__head_87123BE2__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a15__head_87123BE2__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a23__head_AABFFB92__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a23__head_AABFFB92__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a41__head_4EA9A5D2__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a41__head_4EA9A5D2__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a85__head_83760D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a85__head_83760D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a72__head_8A105D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a72__head_8A105D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a60__head_5536480A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a60__head_5536480A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a73__head_F1819D0A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a73__head_F1819D0A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a78__head_6929D12A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a78__head_6929D12A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a57__head_2C43153A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a57__head_2C43153A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a1__head_51903B7A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a1__head_51903B7A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a53__head_DF95B716__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a53__head_DF95B716__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a13__head_E09E0896__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a13__head_E09E0896__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a27__head_7ED31896__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a27__head_7ED31896__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a43__head_7052A656__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a43__head_7052A656__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a28__head_E6257CD6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a28__head_E6257CD6__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a35__head_ACABD736__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a35__head_ACABD736__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a54__head_B9482876__1</span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_WRITE,CLOSE a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a54__head_B9482876__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a4__head_F12ACA76__1</span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_WRITE,CLOSE a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a4__head_F12ACA76__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a84__head_B033038E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a84__head_B033038E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a19__head_D6A64F9E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a19__head_D6A64F9E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a93__head_F54E757E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a93__head_F54E757E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a7__head_1F08F77E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a7__head_1F08F77E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a9__head_635C6201__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a9__head_635C6201__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a11__head_12780121__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a11__head_12780121__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a50__head_5E524321__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a50__head_5E524321__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a75__head_27E1CB21__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a75__head_27E1CB21__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a21__head_69ACD1A1__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a21__head_69ACD1A1__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a25__head_698E7751__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a25__head_698E7751__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a44__head_57E29949__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a44__head_57E29949__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a66__head_944E79C9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a66__head_944E79C9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a52__head_DAC6BF29__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a52__head_DAC6BF29__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a14__head_295EA1A9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a14__head_295EA1A9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a70__head_62941259__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a70__head_62941259__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a18__head_53B48959__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a18__head_53B48959__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a17__head_7D103759__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a17__head_7D103759__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a6__head_9505BEF9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a6__head_9505BEF9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a77__head_88A7CC25__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a77__head_88A7CC25__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a37__head_141AFE65__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a37__head_141AFE65__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a74__head_90DAAD15__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a74__head_90DAAD15__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a32__head_B7957195__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a32__head_B7957195__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a45__head_CCCFB5D5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a45__head_CCCFB5D5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a24__head_3B937275__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a24__head_3B937275__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a26__head_2AB240F5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a26__head_2AB240F5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a89__head_8E387EF5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a89__head_8E387EF5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a80__head_6FEFE78D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a80__head_6FEFE78D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a51__head_0BCC72CD__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a51__head_0BCC72CD__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a71__head_88F4796D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a71__head_88F4796D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a88__head_B0A64FED__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a88__head_B0A64FED__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a8__head_F885EA9D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a8__head_F885EA9D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a83__head_1322679D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a83__head_1322679D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a76__head_B8285A7D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a76__head_B8285A7D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a94__head_D3BBB683__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a94__head_D3BBB683__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a46__head_E2C6C983__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a46__head_E2C6C983__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a56__head_A1E888C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a56__head_A1E888C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a99__head_DD3B45C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a99__head_DD3B45C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a79__head_AC19FC13__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a79__head_AC19FC13__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a81__head_BC0AFFF3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a81__head_BC0AFFF3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a64__head_C042B84B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a64__head_C042B84B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a97__head_29054B4B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a97__head_29054B4B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a96__head_BAAC0DCB__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a96__head_BAAC0DCB__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a62__head_84A40AAB__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a62__head_84A40AAB__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a98__head_C15FD53B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a98__head_C15FD53B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a87__head_12F9237B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a87__head_12F9237B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a2__head_E2983C17__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a2__head_E2983C17__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a20__head_7E477A77__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a20__head_7E477A77__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a49__head_3ADEC577__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a49__head_3ADEC577__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a61__head_C860ABF7__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a61__head_C860ABF7__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a68__head_BC5C8F8F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a68__head_BC5C8F8F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a38__head_78AE322F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a38__head_78AE322F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a65__head_7EE57AEF__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a65__head_7EE57AEF__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a47__head_B6C48D1F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a47__head_B6C48D1F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a86__head_7FB2C85F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a86__head_7FB2C85F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a40__head_5F0404DF__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a40__head_5F0404DF__1</span><br></pre></td></tr></table></figure></p>
<p>在给osd.0开启debug_osd=20后观测chunky相关的日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># cat ceph-osd.0.log |grep chunky:1|grep handle_replica_op</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.262448</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2696</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">00000000</span>::::head,end:<span class="number">1</span>:<span class="number">42307943</span>:::a100:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.294637</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2694</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">42307943</span>:::a100:<span class="number">0</span>,end:<span class="number">1</span>:<span class="number">80463</span>ac6:::a9:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.320986</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2690</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">80463</span>ac6:::a9:<span class="number">0</span>,end:<span class="number">1</span>:b7f2650d:::a88:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.337646</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2700</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:b7f2650d:::a88:<span class="number">0</span>,end:<span class="number">1</span>:fb2020fa:::a40:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.373227</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2636</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:fb2020fa:::a40:<span class="number">0</span>,end:MAX,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br></pre></td></tr></table></figure></p>
<p>截取关键部分看下，如图<br><img src="http://static.zybuluo.com/zphj1987/2zxne6hdwre1fnrqd5xiabzu/image.png" alt="a100"><br>我们看下上面的文件访问监控里面这些对象在什么位置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">25</span>:<span class="number">1.0</span>_head/ ACCESS a100__head_C29E0C42__1</span><br><span class="line"><span class="number">50</span>:<span class="number">1.0</span>_head/ ACCESS a9__head_635C6201__1</span><br><span class="line"><span class="number">75</span>:<span class="number">1.0</span>_head/ ACCESS a88__head_B0A64FED__1</span><br><span class="line"><span class="number">100</span>:<span class="number">1.0</span>_head/ ACCESS a40__head_5F0404DF__1</span><br></pre></td></tr></table></figure></p>
<p>看上去是不是很有规律，这个地方在ceph里面会有个chunk的概念，在做scrub的时候，ceph会对这个chunk进行加锁，这个可以在很多地方看到这个，这个也就是为什么有slow request，并不一定是你的磁盘慢了，而是加了锁，就没法读的</p>
<blockquote>
<p>osd scrub chunk min</p>
<p>Description:    The minimal number of object store chunks to scrub during single operation. Ceph blocks writes to single chunk during scrub.<br>Type:    32-bit Integer<br>Default:    5</p>
</blockquote>
<p>从配置文件上面看说是会锁住写，没有提及读的锁定的问题，那么我们下面验证下这个问题，到底deep-scrub，是不是会引起读的slow request</p>
<p>上面的环境100个对象，现在把100个对象的大小调整为100M一个，并且chunk设置为100个对象的，也就是我把我这个环境所有的对象认为是一个大的chunk，然后去用rados读取这个对象，来看下会发生什么</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">100</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<p>使用ceph -w监控<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">26.045032</span> mon.<span class="number">0</span> [INF] pgmap v377: <span class="number">1</span> pgs: <span class="number">1</span> active+clean+scrubbing+deep; <span class="number">10000</span> MB data, <span class="number">30103</span> MB used, <span class="number">793</span> GB / <span class="number">822</span> GB avail</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">17.540413</span> osd.<span class="number">0</span> [WRN] <span class="number">1</span> slow requests, <span class="number">1</span> included below; oldest blocked <span class="keyword">for</span> &gt; <span class="number">30.398705</span> secs</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">17.540456</span> osd.<span class="number">0</span> [WRN] slow request <span class="number">30.398705</span> seconds old, received at <span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">18</span>:<span class="number">47.141483</span>: replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:26'</span><span class="number">5200</span>,epoch:<span class="number">32</span>,start:<span class="number">1</span>:<span class="number">00000000</span>::::head,end:MAX,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) currently reached_pg</span><br></pre></td></tr></table></figure></p>
<p>我从deep scrub 一开始就进行a40对象的get rados -p rbd get a40 a40，直接就卡着不返回，在pg内对象不变的情况下，对pg做scrub的顺序是不变的，我专门挑了我这个scrub顺序下最后一个scrub的对象来做get，还是出现了slow request ，这个可以证明上面的推断，也就是在做scrub的时候，对scub的chunk的对象的读取请求也会卡死，现在我把我的scrub的chunk弄成1看下会发生什么</p>
<p>配置参数改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">1</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a1'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a2'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a3'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a4'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a5'</span></span><br></pre></td></tr></table></figure>
<p>使用五个请求同时去get a9,循环的去做</p>
<p>然后做deep scrub，这一次并没有出现slow  request 的情况</p>
<h3 id="另外一个重要参数">另外一个重要参数</h3><p>再看看这个参数osd_scrub_sleep = 0</p>
<blockquote>
<p>osd scrub sleep</p>
<p>Description:    Time to sleep before scrubbing next group of chunks. Increasing this value will slow down whole scrub operation while client operations will be less impacted.<br>Type:    Float<br>Default:    0</p>
</blockquote>
<p>可以看到还有scrub group这个概念，从数据上分析这个group 是3，也就是3个chunks<br>我们来设置下</p>
<blockquote>
<p>osd_scrub_sleep = 5</p>
</blockquote>
<p>然后再次做deep-scrub,然后看下日志的内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log |grep be_deep_scrub|awk <span class="string">'&#123;print $1,$2,$28&#125;'</span>|less</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">37.930455</span> <span class="number">1</span>:<span class="number">02</span>f625f1:::a16:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">38.477271</span> <span class="number">1</span>:<span class="number">02</span>f625f1:::a16:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">38.477367</span> <span class="number">1</span>:<span class="number">04</span>ebf846:::a39:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.023952</span> <span class="number">1</span>:<span class="number">04</span>ebf846:::a39:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.024084</span> <span class="number">1</span>:<span class="number">07</span>e14aa6:::a30:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.572683</span> <span class="number">1</span>:<span class="number">07</span>e14aa6:::a30:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">44.989551</span> <span class="number">1</span>:<span class="number">0</span>bc7740d:::a91:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">45.556758</span> <span class="number">1</span>:<span class="number">0</span>bc7740d:::a91:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">45.556857</span> <span class="number">1</span>:<span class="number">0</span>c7c7979:::a33:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.109657</span> <span class="number">1</span>:<span class="number">0</span>c7c7979:::a33:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.109768</span> <span class="number">1</span>:<span class="number">0</span><span class="built_in">cd</span>63f56:::a92:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.657849</span> <span class="number">1</span>:<span class="number">0</span><span class="built_in">cd</span>63f56:::a92:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.084712</span> <span class="number">1</span>:<span class="number">0</span>d551235:::a22:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.614345</span> <span class="number">1</span>:<span class="number">0</span>d551235:::a22:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.614458</span> <span class="number">1</span>:<span class="number">13509</span>d6e:::a42:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">53.158826</span> <span class="number">1</span>:<span class="number">13509</span>d6e:::a42:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">53.158916</span> <span class="number">1</span>:<span class="number">14</span>e585a7:::a5:head</span><br></pre></td></tr></table></figure></p>
<p>可以看到1s做一个对象的deep-scrub，然后在做了3个对象后就停止了5s</p>
<h3 id="默认情况下的scrub和修改后的对比">默认情况下的scrub和修改后的对比</h3><p>我们来计算下在修改前后的情况对比，我们来模拟pg里面有10000个对象的情况小文件 测试的文件都是1K的，这个可以根据自己的文件模型进行测试</p>
<p>假设是海量对象的场景，那么算下来单pg 1w左右对象左右也算比较多了，我们就模拟10000个对象的场景的deep-scrub<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log |grep be_deep_scrub|awk <span class="string">'&#123;print $1,$2,$28&#125;'</span>|awk <span class="string">'&#123;sub(/.*/,substr($2,1,8),$2); print $0&#125;'</span>|uniq|awk <span class="string">'&#123;a[$1," ",$2]++&#125;END&#123;for (j in a) print j,a[j]|"sort -k 1"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>使用上面的脚本统计每秒scrub的对象数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">33</span> <span class="number">184</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">34</span> <span class="number">236</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">35</span> <span class="number">261</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">36</span> <span class="number">263</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">37</span> <span class="number">229</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">38</span> <span class="number">289</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">39</span> <span class="number">236</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">40</span> <span class="number">258</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">41</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">42</span> <span class="number">238</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">43</span> <span class="number">224</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">44</span> <span class="number">282</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">45</span> <span class="number">254</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">46</span> <span class="number">258</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">47</span> <span class="number">261</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">48</span> <span class="number">233</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">49</span> <span class="number">300</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">50</span> <span class="number">243</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">51</span> <span class="number">257</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">52</span> <span class="number">252</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">53</span> <span class="number">246</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">54</span> <span class="number">313</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">55</span> <span class="number">252</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">56</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">57</span> <span class="number">245</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">58</span> <span class="number">256</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">59</span> <span class="number">307</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">00</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">01</span> <span class="number">310</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">02</span> <span class="number">220</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">03</span> <span class="number">250</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">04</span> <span class="number">313</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">05</span> <span class="number">265</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">06</span> <span class="number">304</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">07</span> <span class="number">262</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">08</span> <span class="number">308</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">09</span> <span class="number">263</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">10</span> <span class="number">293</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">11</span> <span class="number">42</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到1s 会扫300个对象左右，差不多40s钟就扫完了一个pg，默认25个对象一个trunk</p>
<p>这里可以打个比喻，在一条长为40m的马路上，一个汽车以1m/s速度前进，中间会有人来回穿，如果穿梭的人只有一两个可能没什么问题，但是一旦有40个人在这个区间进行穿梭的时候，可想而知碰撞的概率会有多大了</p>
<p>或者同一个文件被连续请求40次，那么对应到这里就是40个人在同一个位置不停的穿马路，这样撞上的概率是不是非常的大了？</p>
<p>上面说了这么多，那么我想如果整个看下来，应该知道怎么处理了<br>我们看下这样的全部为1的情况下，会出现什么情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">1</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">1</span></span><br><span class="line">osd_scrub_sleep = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>这里减少chunk大小，相当于减少上面例子当中汽车的长度，原来25米的大卡车，变成1米的自行车了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># cat /var/log/ceph/ceph-osd.0.log |grep be_deep_scrub|awk '&#123;print $1,$2,$28&#125;'</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.927440</span> <span class="number">1</span>:<span class="number">0000</span>b488:::a5471:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.931914</span> <span class="number">1</span>:<span class="number">0000</span>b488:::a5471:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.932039</span> <span class="number">1</span>:<span class="number">000</span>fbbcb:::a5667:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.933568</span> <span class="number">1</span>:<span class="number">000</span>fbbcb:::a5667:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.933646</span> <span class="number">1</span>:<span class="number">00134</span>ebd:::a1903:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.934972</span> <span class="number">1</span>:<span class="number">00134</span>ebd:::a1903:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.960697</span> <span class="number">1</span>:<span class="number">0018</span>f641:::a2028:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.966653</span> <span class="number">1</span>:<span class="number">0018</span>f641:::a2028:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.966733</span> <span class="number">1</span>:<span class="number">00197</span>a21:::a1463:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967085</span> <span class="number">1</span>:<span class="number">00197</span>a21:::a1463:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967162</span> <span class="number">1</span>:<span class="number">001</span>cb17d:::a1703:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967492</span> <span class="number">1</span>:<span class="number">001</span>cb17d:::a1703:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.972252</span> <span class="number">1</span>:<span class="number">002</span>d911c:::a1585:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.976621</span> <span class="number">1</span>:<span class="number">002</span>d911c:::a1585:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.976740</span> <span class="number">1</span>:<span class="number">00301</span>acf:::a6131:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.977097</span> <span class="number">1</span>:<span class="number">00301</span>acf:::a6131:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.977181</span> <span class="number">1</span>:<span class="number">0039</span>a0a8:::a1840:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.979053</span> <span class="number">1</span>:<span class="number">0039</span>a0a8:::a1840:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.983556</span> <span class="number">1</span>:<span class="number">00484881</span>:::a8781:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989098</span> <span class="number">1</span>:<span class="number">00484881</span>:::a8781:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989181</span> <span class="number">1</span>:<span class="number">004</span>f234f:::a4402:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989531</span> <span class="number">1</span>:<span class="number">004</span>f234f:::a4402:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989626</span> <span class="number">1</span>:<span class="number">00531</span>b36:::a5251:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989954</span> <span class="number">1</span>:<span class="number">00531</span>b36:::a5251:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">33.994419</span> <span class="number">1</span>:<span class="number">00584</span>c30:::a3374:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.001296</span> <span class="number">1</span>:<span class="number">00584</span>c30:::a3374:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.001378</span> <span class="number">1</span>:<span class="number">005</span>d6aa5:::a2115:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002174</span> <span class="number">1</span>:<span class="number">005</span>d6aa5:::a2115:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002287</span> <span class="number">1</span>:<span class="number">005</span>e0dfd:::a9945:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002686</span> <span class="number">1</span>:<span class="number">005</span>e0dfd:::a9945:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.005645</span> <span class="number">1</span>:<span class="number">006320</span>f9:::a5207:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011498</span> <span class="number">1</span>:<span class="number">006320</span>f9:::a5207:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011655</span> <span class="number">1</span>:<span class="number">006</span>d32b4:::a7517:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011998</span> <span class="number">1</span>:<span class="number">006</span>d32b4:::a7517:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.012111</span> <span class="number">1</span>:<span class="number">006</span>dae55:::a4702:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.012442</span> <span class="number">1</span>:<span class="number">006</span>dae55:::a4702:head</span><br></pre></td></tr></table></figure>
<p>上面从日志里面截取部分的日志，这个是什么意思呢，是每秒钟扫描3个对象，然后休息3s再进行下一个，这个是不是已经把速度压到非常低了？还有上面做测试scrub sleep例子里面好像是1s 会scrub 1个对象，这里怎么就成了1s会scrub 3 个对象了，这个跟scrub的对象大小有关，对象越大，scrub的时间就相对长一点，这个测试里面的对象是1K的，基本算非常小了，也就是1s会扫描3个对象，然后根据你的设置的sleep值等待进入下一组的scrub</p>
<p>在上面的环境下默认每秒钟会对300左右的对象进行scrub，以25个对象的锁定窗口移动，无法写入和读取，而参数修改后每秒有3个对象被scrub，以1个对象的锁定窗口移动，这个单位时间锁定的对象的数目已经降低到一个非常低的程度了，如果你有生产环境又想去开scrub，不妨尝试下降低chunk，增加sleep</p>
<p>这个的影响就是扫描的速度而已，而如果你想加快扫描速度，就去调整sleep参数来控制这个扫描的速度了，这个就不在这里赘述了</p>
<p>本篇讲述的是一个PG上开启deep-scrub以后的影响，默认的是到了最大的intelval以后就会开启自动开启scrub了，所以我建议的是不用系统自带的时间控制，而是自己去分析的scrub的时间戳和对象数目，然后计算好以后，可以是每天晚上，扫描指定个数的PG，然后等一轮全做完以后，中间就是自定义的一段时间的不扫描期，这个可以自己定义，是一个月或者两个月扫一轮都行，这个会在后面单独写一篇文章来讲述这个</p>
<h2 id="总结">总结</h2><p>关于scrub，你需要了解，scrub什么时候会发生，发生以后会对你的osd产生多少的负载，每秒钟会扫描多少对象，如何去降低这个影响，这些问题就是本篇的来源了，很多问题是能从参数上进行解决的，关键是你要知道它们到底在干嘛</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/scrub.png" alt="scrub"><br></center>

<h2 id="前言">前言</h2><p>关于scrub这块一直想写一篇文章的，这个在很久前，就做过一次测试，当时是看这个scrub到底有多大的影响，当时看到的是磁盘读占很高，启动deep-scrub后会有大量的读,前端可能会出现 slow request,这个是当时测试看到的现象，一个比较简单的处理办法就是直接给scrub关掉了，当然关掉了就无法检测底层到底有没有对象不一致的问题<br>关于这个scrub生产上是否开启，仁者见仁，智者见智，就是选择的问题了，这里不做讨论，个人觉得开和关都有各自的道理，本篇是讲述的如果想开启的情况下如何把scrub给控制住<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何测量Ceph OSD内存占用]]></title>
    <link href="http://www.zphj1987.com/2017/08/10/how-to-get-Ceph-OSD-mem-used/"/>
    <id>http://www.zphj1987.com/2017/08/10/how-to-get-Ceph-OSD-mem-used/</id>
    <published>2017-08-10T08:55:41.000Z</published>
    <updated>2017-08-10T09:06:53.554Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newmemory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个工具我第一次看到是在填坑群里面看到，是由研发-北京-蓝星同学分享的，看到比较有趣，就写一篇相关的记录下用法</p>
<p>火焰图里面也可以定位内存方面的问题，那个是通过一段时间的统计，以一个汇总的方式来查看内存在哪个地方可能出了问题<br><a id="more"></a><br>本篇是另外一个工具，这个工具的好处是有很清晰的图表操作，以及基于时间线的统计，下面来看下这个工具怎么使用的</p>
<p>本篇对具体的内存函数的调用占用不会做更具体的分析，这里是提供一个工具的使用方法供感兴趣的研发同学来使用</p>
<h2 id="环境准备">环境准备</h2><p>目前大多数的ceph运行在centos7系列上面，笔者的环境也是在centos7上面，所以以这个举例，其他平台同样可以</p>
<p>需要用到的工具</p>
<ul>
<li>valgrind</li>
<li>massif-visualizer</li>
</ul>
<p>安装valgrind<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install valgrind</span><br></pre></td></tr></table></figure></p>
<p>massif-visualizer是数据可视化的工具，由于并没有centos的发行版本，但是有fedora的版本，从网上看到资料说这个可以直接安装忽略掉需要的依赖即可，我自己跑了下，确实可行</p>
<p>下载massif-visualizer<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget ftp://ftp.pbone.net/mirror/download.fedora.redhat.com/pub/fedora/linux/releases/<span class="number">23</span>/Everything/x86_64/os/Packages/m/massif-visualizer-<span class="number">0.4</span>.<span class="number">0</span>-<span class="number">6</span>.fc23.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>安装massif-visualizer<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ivh massif-visualizer-<span class="number">0.4</span>.<span class="number">0</span>-<span class="number">6</span>.fc23.x86_64.rpm  --nodeps</span><br></pre></td></tr></table></figure></p>
<p>不要漏了后面的nodeps</p>
<h2 id="抓取ceph_osd运行时内存数据">抓取ceph osd运行时内存数据</h2><p>停掉需要监控的osd（例如我的是osd.4）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@4</span></span><br></pre></td></tr></table></figure></p>
<p>开始运行监控<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># valgrind --tool=massif /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span></span><br><span class="line">==<span class="number">21522</span>== Massif, a heap profiler</span><br><span class="line">==<span class="number">21522</span>== Copyright (C) <span class="number">2003</span>-<span class="number">2015</span>, and GNU GPL<span class="string">'d, by Nicholas Nethercote</span><br><span class="line">==21522== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info</span><br><span class="line">==21522== Command: /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span><br><span class="line">==21522== </span><br><span class="line">==21522== </span><br><span class="line">starting osd.4 at :/0 osd_data /var/lib/ceph/osd/ceph-4 /var/lib/ceph/osd/ceph-4/journal</span><br><span class="line">2017-08-10 16:36:42.395682 a14d680 -1 osd.4 522 log_to_monitors &#123;default=true&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>监控已经开始了,在top下可以看到有这个进程运行，占用cpu还是比较高的，可能是要抓取很多数据的原因<br><img src="http://static.zybuluo.com/zphj1987/yf0kp4qr32mtcmhtock1krnv/image.png" alt="valtop"></p>
<p>等待一段时间后，就可以把之前运行的命令ctrl+C掉</p>
<p>在当前目录下面就会生成一个【massif.out.进程号】的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll massif.out.21522 </span></span><br><span class="line">-rw------- <span class="number">1</span> root root <span class="number">142682</span> Aug <span class="number">10</span> <span class="number">16</span>:<span class="number">39</span> massif.out.<span class="number">21522</span></span><br></pre></td></tr></table></figure></p>
<h2 id="查看截取的数据">查看截取的数据</h2><h3 id="命令行下的查看">命令行下的查看</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ms_print massif.out.21522 |less</span></span><br></pre></td></tr></table></figure>
<p>这个方式是文本方式的查看，也比较方便，自带的文本分析工具，效果如下：<br><img src="http://static.zybuluo.com/zphj1987/6az5gderq4i4jdg0l98bnm8n/image.png" alt="image.png-38kB"><br><img src="http://static.zybuluo.com/zphj1987/hfc1nosugnkx9plc8p9iwvyn/image.png" alt="image.png-94.6kB"></p>
<h3 id="图形界面的查看">图形界面的查看</h3><p>首先在windows上面运行好xmanager-Passive，这个走的x11转发的（也可以用另外一个工具MobaXterm）<br><img src="http://static.zybuluo.com/zphj1987/jqt14e5gakmr9ftwuz3r8g5m/image.png" alt="image.png-4.4kB"><br>运行好了后，直接在xshell命令行运行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># massif-visualizer massif.out.21522 </span></span><br><span class="line">massif-visualizer(<span class="number">22494</span>)/kdeui (kdelibs): Attempt to use QAction <span class="string">"toggleDataTree"</span> with KXMLGUIFactory! </span><br><span class="line">massif-visualizer(<span class="number">22494</span>)/kdeui (kdelibs): Attempt to use QAction <span class="string">"toggleAllocators"</span> with KXMLGUIFactory! </span><br><span class="line">description: <span class="string">"(none)"</span> </span><br><span class="line"><span class="built_in">command</span>: <span class="string">"/usr/bin/ceph-osd -f --cluster ceph --id 4"</span> </span><br><span class="line">time unit: <span class="string">"i"</span> </span><br><span class="line">snapshots: <span class="number">56</span> </span><br><span class="line">peak: snapshot <span class="comment"># 52 after "2.3138e+09i" </span></span><br><span class="line">peak cost: <span class="string">"16.2 MiB"</span>  heap <span class="string">"749.0 KiB"</span>  heap extra <span class="string">"0 B"</span>  stacks</span><br></pre></td></tr></table></figure></p>
<p>然后在windows上面就会弹出下面的<br><img src="http://static.zybuluo.com/zphj1987/inkjtgxe6dw2k2qjjvr4rclx/osdmem.png" alt="osdmem.png-282kB"><br>就可以交互式的查看快照点的内存占用了，然后根据这个就可以进行内存分析了，剩下的工作就留给研发去做了</p>
<h2 id="相关链接">相关链接</h2><p><a href="https://codeday.me/bug/20170415/1699.html" target="_blank" rel="external">linux – 如何测量应用程序或进程的实际内存使用情况？</a></p>
<h2 id="总结">总结</h2><p>只有分析落地到数据层面，这样的分析才是比较精准的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newmemory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个工具我第一次看到是在填坑群里面看到，是由研发-北京-蓝星同学分享的，看到比较有趣，就写一篇相关的记录下用法</p>
<p>火焰图里面也可以定位内存方面的问题，那个是通过一段时间的统计，以一个汇总的方式来查看内存在哪个地方可能出了问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
