<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-09-12T05:54:56.885Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Centos7下Jewel版本radosgw服务启动]]></title>
    <link href="http://www.zphj1987.com/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/09/12/Centos7下Jewel版本radosgw服务启动/</id>
    <published>2016-09-12T05:47:47.000Z</published>
    <updated>2016-09-12T05:54:56.885Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>
<a id="more"></a>
<h2 id="二、预备环境">二、预备环境</h2><h3 id="一个完整的集群">一个完整的集群</h3><p>拥有一个正常的集群是需要提前准备好的，ceph -s检查正确的输出</p>
<h3 id="关闭各种auth">关闭各种auth</h3><p>这个地方也可以不关闭，注意配置好用户认证就可以了，这里关闭了，配置起来方便，我是从来不开的,也避免了新手不会配置用户造成认证的各种异常<br>关闭认证就是在ceph.conf里面添加下面字段<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br></pre></td></tr></table></figure></p>
<h3 id="安装ceph-radosgw的包">安装ceph-radosgw的包</h3><p>这个因为默认不会安装，所以要安装好<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<h2 id="三、默认启动过程">三、默认启动过程</h2><p>我们先什么都不配置，看下一般的会怎么处理</p>
<h3 id="3-1_启动服务">3.1 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-radosgw.target</span><br></pre></td></tr></table></figure>
<h3 id="3-2_检查服务的状态">3.2 检查服务的状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw.target </span></span><br><span class="line">● ceph-radosgw.target - ceph target allowing to start/stop all ceph-radosgw@.service instances at once</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw.target; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> CST; <span class="number">51</span>s ago</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Stopping ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Starting ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">51</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br></pre></td></tr></table></figure>
<p>可以看到进程是启动的，没有任何异常</p>
<h3 id="3-3_检查端口是否启动">3.3 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br></pre></td></tr></table></figure>
<p>但是并没有生成任何端口，这个是因为还没有配置实例,这个地方就是新手经常卡住的地方</p>
<h2 id="四、下面开始配置默认单实例">四、下面开始配置默认单实例</h2><h3 id="4-1_写配置文件">4.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写一个最简配置文件<br>注意下面的client.radosgw1这个包起来的，这个是固定写法，在 <code>systemctl</code> 启动服务的时候 <code>@</code> 取后面的radosgw1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4-2_启动服务">4.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3_检查服务状态">4.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> CST; <span class="number">17</span>s ago</span><br><span class="line"> Main PID: <span class="number">19996</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">19996</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="4-4_检查端口是否启动">4.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7480</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">19996</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到默认的端口是7480</p>
<h2 id="五、配置多个自定义端口实例">五、配置多个自定义端口实例</h2><h3 id="5-1_写配置文件">5.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写下配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7481</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[client.radosgw2]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7482</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方配置两个实例，用了不同的名称，用了不同的端口</p>
<h3 id="5-2_启动服务">5.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw2</span></span><br></pre></td></tr></table></figure>
<h3 id="5-3_检查服务状态">5.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> CST; <span class="number">1</span>min <span class="number">4</span>s ago</span><br><span class="line"> Main PID: <span class="number">20509</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">20509</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw2</span></span><br><span class="line">● ceph-radosgw@radosgw2.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> CST; <span class="number">1</span>min <span class="number">3</span>s ago</span><br><span class="line"> Main PID: <span class="number">20696</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw2.service</span><br><span class="line">           └─<span class="number">20696</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw2 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="5-4_检查端口是否启动">5.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7481</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20509</span>/radosgw       </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7482</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20696</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到服务和端口都能正常的启动了</p>
<p>好了，关于centos7下jewel版本的radosgw配置的启动已经介绍完了，这里不涉及更多深入的东西，其他的东西可以参照其他文档配置即可，这个地方只是对启动服务这里专门的介绍一下</p>
<h2 id="六、总结">六、总结</h2><p>从上面的过程可以看出大致的流程如下</p>
<ul>
<li>安装软件</li>
<li>启动服务</li>
<li>检查服务状态</li>
<li>检查服务端口</li>
</ul>
<p>这些很多都是基础的做法，在centos7下面虽然比6做了一些改变，但是掌握了一些通用的排查方法后，是很容易举一反三的，因为看到有新手不熟悉启动，所以写下这篇文章，自己因为也没经常用，所以也写下当个笔记了</p>
<h2 id="七、For_me">七、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何统计Ceph的RBD真实使用容量]]></title>
    <link href="http://www.zphj1987.com/2016/09/08/%E5%A6%82%E4%BD%95%E7%BB%9F%E8%AE%A1Ceph%E7%9A%84RBD%E7%9C%9F%E5%AE%9E%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F/"/>
    <id>http://www.zphj1987.com/2016/09/08/如何统计Ceph的RBD真实使用容量/</id>
    <published>2016-09-08T09:17:08.000Z</published>
    <updated>2016-09-12T05:54:30.809Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>
<a id="more"></a>
<p>详细介绍</p>
<h3 id="2-1_方法一：使用rbd_du查询">2.1 方法一：使用rbd du查询</h3><p>这个参考我之前的文章：<a href="http://www.zphj1987.com/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/" target="_blank" rel="external">查询rbd的使用容量</a></p>
<h3 id="2-2_方法二：使用rbd_diff">2.2 方法二：使用rbd diff</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd diff rbd/zp | awk '&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<h3 id="2-3_方法三：根据对象统计的方法进行统计">2.3 方法三：根据对象统计的方法进行统计</h3><p>这个是本篇着重介绍的一点，在集群非常大的时候，再去按上面的一个个的查询，需要花很长的时间，并且需要时不时的跟集群进行交互，这里采用的方法是把统计数据一次获取下来，然后进行数据的统计分析，从而获取结果，获取的粒度是以存储池为基准的</p>
<h4 id="拿到所有对象的信息">拿到所有对象的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<p>这个获取的时间长短是根据对象的多少来的，如果担心出问题，可以换个终端查看进度<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tail <span class="operator">-f</span>  obj.txt</span><br></pre></td></tr></table></figure></p>
<h4 id="获取RBD的镜像列表">获取RBD的镜像列表</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd ls</span></span><br><span class="line"><span class="built_in">test</span>1</span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<h3 id="获取RBD的镜像的prefix">获取RBD的镜像的prefix</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `rbd -p rbd ls`;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$a</span> ;rbd -p rbd info <span class="variable">$a</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span> ;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="获取指定RBD镜像的大小">获取指定RBD镜像的大小</h3><p>查询 test1 的镜像大小<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat obj.txt |grep rbd_data.3ac16b8b4567|awk  '&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">4014.27</span> MB</span><br></pre></td></tr></table></figure></p>
<h3 id="将上面的汇总，使用脚本一次查询出所有的">将上面的汇总，使用脚本一次查询出所有的</h3><h4 id="第一步获取：">第一步获取：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="第二步计算：">第二步计算：</h4><p>创建一个获取的脚本getused.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line"><span class="comment">##default pool name use rbd,you can change it </span></span><br><span class="line"><span class="comment">##default objfile is obj.txt,you can change it</span></span><br><span class="line">objfile=obj.txt</span><br><span class="line">Poolname=rbd</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> `rbd -p <span class="variable">$Poolname</span> ls`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">Imagename=<span class="variable">$image</span></span><br><span class="line">Prefix=`rbd  -p <span class="variable">$Poolname</span> info <span class="variable">$image</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line">Used=`cat <span class="variable">$objfile</span> |grep <span class="variable">$Prefix</span>|awk <span class="string">'&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$Imagename</span> <span class="variable">$Prefix</span></span><br><span class="line"><span class="built_in">echo</span> Used: <span class="variable">$Used</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h4 id="我的输出如下：">我的输出如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sh getused.sh </span></span><br><span class="line"><span class="built_in">test</span>1 rbd_data.<span class="number">3</span>ac16b8b4567</span><br><span class="line">Used: <span class="number">4014.27</span> MB</span><br><span class="line">zp rbd_data.<span class="number">11</span>f66b8b4567</span><br><span class="line">Used: <span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意这里只统计了image里面的真实容量，如果是用了快速clone的,存在容量复用的问题，需要自己看是否需要统计那一部分的对象，方法同上</p>
</blockquote>
<h2 id="三、总结">三、总结</h2><p>对于已存在的系统，并且数据量很大的系统，不要频繁的去做请求，最好把统计请求，集中起来，并且就单线程的处理，慢一点不要紧，然后拉取到数据后，慢慢处理，这样能把影响降低到最少，可以在最不忙的时候去进行相关的操作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-08</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中的Copyset概念和使用方法]]></title>
    <link href="http://www.zphj1987.com/2016/09/06/Ceph%E4%B8%AD%E7%9A%84Copyset%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/06/Ceph中的Copyset概念和使用方法/</id>
    <published>2016-09-06T09:39:15.000Z</published>
    <updated>2016-09-07T03:31:05.791Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>
<a id="more"></a>
<h2 id="二、copyset的概念">二、copyset的概念</h2><p>首先我们要理解copyset的概念，用通俗的话说就是，包含一个数据的所有副本的节点，也就是一个copyset损坏的情况下，数据就是全丢的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomcopy.png" alt="radomcopy"><br></center><br>如上图所示，这里的copyset就是：<br>{1,5,6}，{2,6,8} 两组</p>
<p>如果不做特殊的设置，那么基本上就是会随机的去分布</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomall.png" alt="allcopy"><br></center></p>
<h3 id="2-1_最大copyset">2.1 最大copyset</h3><p>如上图的所示，一般来说，最终组合将是一个最大的随机组合，比如这样的一个9个node随机组合3个的，这样的组合数有：<br>从 n个元素中取出  k个元素， k个元素的组合数量为：</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/gongshi.png" alt="计算公式"><br></center><br>9个随机3个的组合为84<br>如果3个节点down掉，那么有数据丢失概率就是100%</p>
<h3 id="2-2_最小copyset">2.2 最小copyset</h3><p>如果存在一种情况，分布是这样的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/mincopy.png" alt="mincopy"><br></center><br>那么copyset为<br>{1,5,7},{2,4,9},{3,6,8}<br>如果3个节点down掉,只有正好是上面的3种组合中的一种出现的时候，才会出现数据丢失<br>那么数据丢失的概率为 3/84</p>
<p>最小copyset可能带来的不好的地方</p>
<ul>
<li>真出现丢失的时候（概率极低），丢失的数据量将是最大化的，这个是因为出现丢的时候，那么三个上面的组合配对为100%，其他情况不是100%</li>
<li>失效恢复时间将会增大一些，根据facebook的报告100GB的39节点的HDFS随机分布恢复时间在60s,最小分布为700s，这个是因为可用于恢复的点相对减少了，恢复时间自然长了</li>
</ul>
<h3 id="2-3_比较好的处理方式">2.3 比较好的处理方式</h3><p>比较好的方式就是取copyset值为介于纯随机和最小之间的数，那么失效的概率计算方式就是：</p>
<blockquote>
<p>当前的copyset数目/最大copyset</p>
</blockquote>
<h2 id="三、这个概念在ceph当中的实现">三、这个概念在ceph当中的实现</h2><p>其实这个概念在ceph当中就是bucket的概念，PG为最小故障单元，PG就可以理解为上图当中的node上的元素，默认的分组方式为host，这个copyset就是全随机的在这些主机当中进行组合，我们在提升故障域为rack的时候，实际上就是将copyset进行了减少，一个rack之内的主机是形成不了copyset，这样down掉rack的时候，就不会数据丢失了，这个地方的实际可以做的控制方式有三种，下面将详细的介绍三种模式</p>
<h3 id="3-1、缩小最小主机单位">3.1、缩小最小主机单位</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hostzu.png" alt="最小主机组"><br></center><br>默认的为主机组，这样的主机间的copyset为<br>{1,2}，{1,3}，{1,4}，{2,3}，{2,4}，{3,4}<br>这样的有六组</p>
<p>现在我们对host进行一个合并看下</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hebing.png" alt="此处输入图片的描述"><br></center><br>注意这个地方并不是往上加了一层bucket，而是把最底层的host给拆掉了，加入一台机器有24个osd，那么这里的vhost1里面的osd个数实际是48个osd，那么当前的copyset为<br>{vhost1,vhost2}<br>copyset已经为上面默认情况的1/6<br>这样会带来两个好处</p>
<ul>
<li>减少了copyset，减少的好处就见上面的分析</li>
<li>增加可接收恢复的osd数目，之前坏了一个osd的时候，能接收数据的osd为n-1,那么现在坏一个osd，可接收的osd为2n-1(n为单node上的osd个数)</li>
</ul>
<h3 id="3-2、增加分组">3.2、增加分组</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/rackfenzu.png" alt="rack分组"><br></center><br>这个地方是增加了rack分组的，同一个rack里面不会出现copyset，那么当前的模式的copyset就是<br>{1,3}，{1,4}，{2,3}，{2,4}</p>
<p>同没有处理相比copyset为4/6</p>
<h3 id="3-3、增加分组的情况进行PG分流">3.3、增加分组的情况进行PG分流</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/zone.png" alt="zone"><br></center><br>这里看上去跟上面的分组很像，但是在做crush的时候是有区别的，上面的分组以后，会让PG分布在两个rack当中，这里的crush写的时候会让PG只在一个zone当中，在进入zone的下层再去进行分离主副PG，那么这种方式的copyset为<br>{1,2} {3,4}<br>为上面默认情况的2/6</p>
<h2 id="四、总结">四、总结</h2><p>关于ceph中的ceph的copyset的三种模式已经总结完了，需要补充的是，上面的node都是一个虚拟的概念，你可以扩充为row，或者rack都行，这里只是说明了不同的处理方式，针对每个集群都可以有很多种组合，这个关键看自己怎么处理，减少copyset会明显的减低机器上的线程数目和资源的占用，这一点可以自行研制，从原理上来说少了很多配对的通信，crush的是非常灵活的一个分布控制，可以做很精细的控制，当然也会增加了维护的难度</p>
<h2 id="五、参考资料：">五、参考资料：</h2><p><a href="https://www.ustack.com/blog/build-block-storage-service/" target="_blank" rel="external">打造高性能高可靠块存储系统</a><br><a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/cidon" target="_blank" rel="external">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
<h2 id="七、For_me">七、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Proftp最简匿名访问配置]]></title>
    <link href="http://www.zphj1987.com/2016/09/01/Proftp%E6%9C%80%E7%AE%80%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.zphj1987.com/2016/09/01/Proftp最简匿名访问配置/</id>
    <published>2016-09-01T06:04:24.000Z</published>
    <updated>2016-09-01T06:24:34.153Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>
<a id="more"></a>
<h2 id="配置proftp过程">配置proftp过程</h2><p>1、配置过程尽量少的动原配置文件，需要共享的为/share/a目录，首先修改默认的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DefaultRoot                     ~ !adm</span><br></pre></td></tr></table></figure></p>
<p>修改为:<br><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="title">DefaultRoot</span>                     /share</span><br></pre></td></tr></table></figure></p>
<p>让默认的根目录为 /share,默认的为用户的根目录，匿名用户对应的ftp用户的根目录</p>
<p>2、修改匿名用户的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous ~ftp&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous /share&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改原匿名用户ftp的用户目录为/share</p>
<p>3、修改默认屏蔽权限WRITE<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit WRITE SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>默认会屏蔽掉写的操作，就没法上传了</p>
<p>5、配置访问的目录<br>默认启用了vroot，所以写路径的时候写相对路径即可，添加如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br><span class="line">&lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure></p>
<p>/a就代表的是/share/a</p>
<p>6、开启匿名<br>修改配置vim /etc/sysconfig/proftpd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>改成:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">"-DANONYMOUS_FTP"</span></span><br></pre></td></tr></table></figure></p>
<p>7、给目录访问权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chown ftp:ftp /share/a</span><br><span class="line">chmod <span class="number">755</span>  /share/a</span><br></pre></td></tr></table></figure></p>
<p>8、启动proftp服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart proftpd</span><br></pre></td></tr></table></figure></p>
<h2 id="完整配置文件">完整配置文件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ServerName			<span class="string">"ProFTPD server"</span></span><br><span class="line">ServerIdent			on <span class="string">"FTP Server ready."</span></span><br><span class="line">ServerAdmin			root@localhost</span><br><span class="line">DefaultServer			on</span><br><span class="line">DefaultRoot			~ !adm</span><br><span class="line">AuthPAMConfig			proftpd</span><br><span class="line">AuthOrder			mod_auth_pam.c* mod_auth_unix.c</span><br><span class="line">UseReverseDNS			off</span><br><span class="line">User				nobody</span><br><span class="line">Group				nobody</span><br><span class="line">MaxInstances			<span class="number">20</span></span><br><span class="line">UseSendfile			off</span><br><span class="line">LogFormat			default	<span class="string">"%h %l %u %t \"%r\" %s %b"</span></span><br><span class="line">LogFormat			auth	<span class="string">"%v [%P] %h %t \"%r\" %s"</span></span><br><span class="line">LoadModule mod_ctrls_admin.c</span><br><span class="line">LoadModule mod_vroot.c</span><br><span class="line">ModuleControlsACLs		insmod,rmmod allow user root</span><br><span class="line">ModuleControlsACLs		lsmod allow user *</span><br><span class="line">ControlsEngine			on</span><br><span class="line">ControlsACLs			all allow user root</span><br><span class="line">ControlsSocketACL		allow user *</span><br><span class="line">ControlsLog			/var/<span class="built_in">log</span>/proftpd/controls.log</span><br><span class="line">&lt;IfModule mod_ctrls_admin.c&gt;</span><br><span class="line">  AdminControlsEngine		on</span><br><span class="line">  AdminControlsACLs		all allow user root</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfModule mod_vroot.c&gt;</span><br><span class="line">  VRootEngine			on</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfDefine TLS&gt;</span><br><span class="line">  TLSEngine			on</span><br><span class="line">  TLSRequired			on</span><br><span class="line">  TLSRSACertificateFile		/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSRSACertificateKeyFile	/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSCipherSuite		ALL:!ADH:!DES</span><br><span class="line">  TLSOptions			NoCertRequest</span><br><span class="line">  TLSVerifyClient		off</span><br><span class="line">  TLSLog			/var/<span class="built_in">log</span>/proftpd/tls.log</span><br><span class="line">  &lt;IfModule mod_tls_shmcache.c&gt;</span><br><span class="line">    TLSSessionCache		shm:/file=/var/run/proftpd/sesscache</span><br><span class="line">  &lt;/IfModule&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine DYNAMIC_BAN_LISTS&gt;</span><br><span class="line">  LoadModule			mod_ban.c</span><br><span class="line">  BanEngine			on</span><br><span class="line">  BanLog			/var/<span class="built_in">log</span>/proftpd/ban.log</span><br><span class="line">  BanTable			/var/run/proftpd/ban.tab</span><br><span class="line">  BanOnEvent			MaxLoginAttempts <span class="number">2</span>/<span class="number">00</span>:<span class="number">10</span>:<span class="number">00</span> <span class="number">01</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">  BanMessage			<span class="string">"Host %a has been banned"</span></span><br><span class="line">  BanControlsACLs		all allow user ftpadm</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine QOS&gt;</span><br><span class="line">  LoadModule			mod_qos.c</span><br><span class="line">  QoSOptions			dataqos throughput ctrlqos lowdelay</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;Global&gt;</span><br><span class="line">  Umask				<span class="number">022</span></span><br><span class="line">  AllowOverwrite		yes</span><br><span class="line">  &lt;Limit ALL SITE_CHMOD&gt;</span><br><span class="line">    AllowAll</span><br><span class="line">  &lt;/Limit&gt;</span><br><span class="line">&lt;/Global&gt;</span><br><span class="line">&lt;IfDefine ANONYMOUS_FTP&gt;</span><br><span class="line">  &lt;Anonymous /share/&gt;</span><br><span class="line">    User			ftp</span><br><span class="line">    Group			ftp</span><br><span class="line">    AccessGrantMsg		<span class="string">"Anonymous login ok, restrictions apply."</span></span><br><span class="line">    UserAlias			anonymous ftp</span><br><span class="line">    MaxClients			<span class="number">10</span> <span class="string">"Sorry, max %m users -- try again later"</span></span><br><span class="line">    DisplayLogin		/welcome.msg</span><br><span class="line">    DisplayChdir		.message</span><br><span class="line">    DisplayReadme		README*</span><br><span class="line">    DirFakeUser			on ftp</span><br><span class="line">    DirFakeGroup		on ftp</span><br><span class="line">    &lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">      DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;IfModule mod_vroot.c&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">	       AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">        &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">              AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">          AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line">    WtmpLog			off</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/access.log WRITE,READ default</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/auth.log AUTH auth</span><br><span class="line">  &lt;/Anonymous&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>最简配置就完成了，也可以根据需要再去做更复杂的配置，这里就不做过多的介绍，比较容易错误的点就是容易出现权限问题无法访问，或者是上下的设置关联错误，可以开启调试模式进行调试<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">proftpd   -n <span class="operator">-d</span> <span class="number">10</span> -c /etc/proftpd.conf -DANONYMOUS_FTP</span><br></pre></td></tr></table></figure></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-01</td>
</tr>
</tbody>
</table>
<h2 id="For_me">For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday北京总结-August 20, 2016（未完待续）]]></title>
    <link href="http://www.zphj1987.com/2016/08/29/Cephday%E5%8C%97%E4%BA%AC%E6%80%BB%E7%BB%93-August-20-2016%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/"/>
    <id>http://www.zphj1987.com/2016/08/29/Cephday北京总结-August-20-2016（未完待续）/</id>
    <published>2016-08-29T15:57:52.000Z</published>
    <updated>2016-09-06T04:19:47.616Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>
<a id="more"></a>
<h2 id="分享的PPT">分享的PPT</h2><h3 id="开幕致辞-张建">开幕致辞-张建</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/00-%E5%BC%80%E5%B9%95%E8%87%B4%E8%BE%9E-%E5%BC%A0%E5%BB%BA.pdf" width="850" height="700"></center>

<p>首先说下这位分享者，之前在2015的Ceph Hackathon上，就是他最先发现的老版本的ceph与 TCMalloc结合的一个bug，然后提出了用jemalloc获取了随机IO的提升，并且降低了资源占用， 这对于老版本的环境提升还是比较大的，在新的环境下，差别没有那么大了，不过分享者还是非常无私的分享了他们的发现<br>本篇主要讲了下面几点：ceph在中国很火，intel投入很多，并且参与了很多的功能的开发，这只是一个致辞，发出的信号就是Intel 很关注ceph</p>
<h3 id="Ceph社区进展-Patrick">Ceph社区进展-Patrick</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/01-Ceph%E7%A4%BE%E5%8C%BA%E8%BF%9B%E5%B1%95-Patrick.pdf" width="850" height="700"></center>

<p>Patrick是红帽的ceph社区的总监，负责推进ceph各方面的发展，<br>本篇主要讲了：ceph当前的发展情况，各大厂对ceph的关注，ceph的固定的活动，cephfs在jewel版本会稳定下来</p>
<h3 id="Ceph中国社区-孙琦">Ceph中国社区-孙琦</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/02-Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BA-%E5%AD%99%E7%90%A6.pdf" width="850" height="700"></center><br>这篇是由孙琦进行的演讲，他对推动ceph在中国的发展做了很多工作<br>本篇主要讲了：ceph中国社区在中国做了哪些推广方面的活动，主要是建立圈子，关注的人很多，发布了一本翻译的技术书籍，未来会做的事情，需要关注的是社区自己写的书会在10月份出来<br><br>###SSD-Ceph在360游戏云的应用-谷忠言<br><br><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/03-SSD%20Ceph%E5%9C%A8360%E6%B8%B8%E6%88%8F%E4%BA%91%E7%9A%84%E5%BA%94%E7%94%A8-%E8%B0%B7%E5%BF%A0%E8%A8%80.pdf" width="850" height="700"></center>

<p>本篇是由360游戏的谷忠言进行演讲的，主要讲述了ceph在360游戏中使用的经验<br>提出了IO容量计算模型；概括了ceph主要调优的方法;相同负载情况下分池对线程和资源的占用帮助很大;如果不限流很有可能因为过载造成心跳超时，进程自杀了;扩容采用扩池的方式避免数据大量变动；网络问题不好定位，根据osd的提交时间的异常来追踪问题（这个数据看下采集方法）；图形化监控采用的是grafana；纯ssd才能满足360游戏主机对性能的需求</p>
<h3 id="SPDK加速Ceph-XSKY_Bluestore案例分享-扬子夜-王豪迈">SPDK加速Ceph-XSKY Bluestore案例分享-扬子夜-王豪迈</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/04-SPDK%E5%8A%A0%E9%80%9FCeph-XSKY%20Bluestore%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB-%E6%89%AC%E5%AD%90%E5%A4%9C-%E7%8E%8B%E8%B1%AA%E8%BF%88.pdf" width="850" height="700"></center><br>本篇是由xsky的扬子夜-王豪迈进行演讲的，ceph设计是在低性能硬件基础上设计的，现在的网络磁盘都是高性能的，软件设计和实现是性能瓶颈，介绍了底层对象存储的写入模型的优点和缺点；设计了新的写入方式，解决这些问题，介绍了spdk；spdk的nvme 驱动比内核的nvme驱动带来了6倍随机读性能的提升；spdk对iscsi场景也能带来很大性能提升；替换内核驱动NVME SSD的OSD为spdk驱动，OSD网络用dpdk替换；介绍bluestore，性能全线提升，当前还在完善功能；总之这个地方会对性能提升很多，但是目前资料太少，目前还没普及，只能有一定功底研发实力的才能参与进来，目前主要是xsky和Intel还有Redhat等大厂在进行驱动在</p>
<h3 id="Ceph_Tiering高性能架构-Thor_Chin">Ceph Tiering高性能架构-Thor Chin</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/05-Ceph%20Tiering%E9%AB%98%E6%80%A7%E8%83%BD%E6%9E%B6%E6%9E%84-Thor%20Chin.pdf" width="850" height="700"></center><br>本篇是由 Thor Chin 进行的演讲，首先介绍了自己的使用场景，然后介绍了下crushmap文件里面各个字段的意思；介绍了各种测试的工具；然后给出了测试的情况，这里作者没有给出测试模型，并且没有说明是否在cache满载的情况下，这个方案是可以使用的，但是性能数据就不做过多的评价</p>
<h3 id="Ceph在视频应用上的性能优化-何营">Ceph在视频应用上的性能优化-何营</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/06-Ceph%E5%9C%A8%E8%A7%86%E9%A2%91%E5%BA%94%E7%94%A8%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%BD%95%E8%90%A5.pdf" width="850" height="700"></center><br>本篇来自浪潮的何营的演讲，主要介绍了ceph在视频行业的运用，并且提出了直接纠删码的实现方法，研发可以看看，实现起来代码量还是很高的</p>
<h3 id="借助当今的NVM_Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘">借助当今的NVM Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/07-%E5%80%9F%E5%8A%A9%E5%BD%93%E4%BB%8A%E7%9A%84NVM%20Express%E5%9B%BA%E6%80%81%E7%9B%98%E5%92%8C%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%8B%B1%E7%89%B9%E5%B0%94Optane%E6%8A%80%E6%9C%AF%E6%89%93%E9%80%A0%E7%BB%8F%E6%B5%8E%E9%AB%98%E6%95%88%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%AD%98%E5%82%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E5%91%A8%E6%B8%8A-%E5%BC%A0%E7%BC%98.pdf" width="850" height="700"></center><br>本篇来自Intel的周渊-张缘作的演讲，开始介绍了Intel在ceph上的贡献，有三大工具，CeTune性能调优工具，Vsm部署管理工具，COSbench压力测试工具，三大工具目前都是开源可部署的，然后介绍了基于Intel的硬件的Ceph方案，介绍了几个调优点，4K随机写提高了6倍，4K随机读提高了16倍，介绍了Bluestore和Filestore在Intel硬件上性能的差别，根据火焰图的输出提出rocksdb需要调优；介绍了Intel的3D Xpoint，最后给出了参考配置文件，这个是针对全闪存的调优</p>
<h3 id="基于ARM的Ceph可扩展高效解决方案-罗旭">基于ARM的Ceph可扩展高效解决方案-罗旭</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/08-%E5%9F%BA%E4%BA%8EARM%E7%9A%84Ceph%E5%8F%AF%E6%89%A9%E5%B1%95%E9%AB%98%E6%95%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E7%BD%97%E6%97%AD.pdf " width="850" height="700"></center><br>本篇来自罗旭的演讲，介绍了arm在存储方面的应用，社区也在发布arm版本的发行包，提供了ceph的arm基本解决方案，介绍了功耗的优势，西数之前做了一个504个osd的arm的测试，arm目前在国内还属于起步概念的东西，很多人想上，但是因为因为不是通用平台，目前的成本其实并没有太大的优势，未来还是值得期待，在冷数据存储的场景上，还是大有可为的，还有一个原因，国内在功耗这一块并没有特别的重视</p>
<h3 id="Ceph存储设备案例研究与S3对象存储性能优化-刘志刚">Ceph存储设备案例研究与S3对象存储性能优化-刘志刚</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/09-Ceph%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E4%B8%8ES3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%88%98%E5%BF%97%E5%88%9A.pdf " width="850" height="700"></center><br>本篇来自富士通的刘志刚演讲，这也是本次分享里面唯一的RGW方面的分享，开始介绍了富士通在ceph上的投入，介绍了cache tiering存在性能衰减的问题，介绍了一些方案上调优的点，介绍了ownCloud与对象存储对接的方案和性能，介绍了cosbench测试出来的性能的情况，介绍了rgw调优的参数</p>
<h3 id="Ceph全闪存存储-周皓">Ceph全闪存存储-周皓</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/10-Ceph%E5%85%A8%E9%97%AA%E5%AD%98%E5%AD%98%E5%82%A8-%E5%91%A8%E7%9A%93.pdf" width="850" height="700"></center><br>本篇来自SanDisk的周皓的演讲，介绍了Sandisk的全闪存ceph的方案InfiniFlash，性能确实非常的好，并且TCO非常的低；介绍了一些调优的点BlueStore，KV Store，Memory allocation等等</p>
<h3 id="将Ceph引入企业-在30分钟安装一个50T移动集群">将Ceph引入企业-在30分钟安装一个50T移动集群</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/11-%E5%B0%86Ceph%E5%BC%95%E5%85%A5%E4%BC%81%E4%B8%9A-%E5%9C%A830%E5%88%86%E9%92%9F%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AA50T%E7%A7%BB%E5%8A%A8%E9%9B%86%E7%BE%A4-Alex%20Lau.pdf" width="850" height="700"></center><br>本篇来自Suse的劉俊賢的演讲，主要讲的是快速部署一个50T的ceph集群，介绍了Suse的iscsi的方案，目前Suse这块做的不错，商业版本提供了基于rbd的iscsi方案的高可用，介绍了基于LIO的LRBD，介绍了openATTIC，这个是之前一家做存储管理平台的公司，后来和Suse合作比较紧密，这个在openATTIC更稳定一点我会写下部署的相关文档，介绍了基于salt的快速部署</p>
<h2 id="完结">完结</h2><p>现在一些大厂的分享都会带上一些优化的方法和优化的参数，这个比前几年已经好了很多，这些参数建议都自己在自己的环境上跑一跑，因为优化是基于当前环境的优化，如果有通用优化，那不用优化了，直接固定参数值就行了，举个简单的例子，在ssd场景上常用的一个优化需要调高IO的线程，如果直接参数硬套到sata的场景，性能不会提高，反而增大了延时，IO一般在增大到一个峰值后，就不会增大，延时反而会增大，所以调优就是找到自己环境的最适合的参数，上面只是简单的介绍了PPT里面的内容点，如果感兴趣可以深挖里面的东西</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-29</td>
</tr>
<tr>
<td style="text-align:center">完成</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重写ceph-lazy]]></title>
    <link href="http://www.zphj1987.com/2016/08/28/%E9%87%8D%E5%86%99ceph-lazy/"/>
    <id>http://www.zphj1987.com/2016/08/28/重写ceph-lazy/</id>
    <published>2016-08-28T15:58:04.000Z</published>
    <updated>2016-08-29T01:05:09.111Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/" target="_blank" rel="external">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>
<a id="more"></a>
<h2 id="项目地址">项目地址</h2><p>原作者项目地址：<a href="https://github.com/gcharot/ceph-lazy" target="_blank" rel="external">https://github.com/gcharot/ceph-lazy</a><br>我重写的地址：<a href="https://github.com/zphj1987/ceph-lazy/tree/lazy-python" target="_blank" rel="external">https://github.com/zphj1987/ceph-lazy/tree/lazy-python</a></p>
<h3 id="安装方法">安装方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -O /sbin/ceph-lazy https://raw.githubusercontent.com/zphj1987/ceph-lazy/lazy-python/ceph-lazy.py</span><br><span class="line">chmod <span class="number">777</span> /sbin/ceph-lazy</span><br></pre></td></tr></table></figure>
<h3 id="详细使用说明">详细使用说明</h3><h4 id="列出节点上的所有的OSD">列出节点上的所有的OSD</h4><p>命令：ceph-lazy host-get-osd {hostname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-osd lab8106</span></span><br><span class="line">osd.<span class="number">0</span> </span><br><span class="line">osd.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有的存储主机节点">列出所有的存储主机节点</h4><p>命令：ceph-lazy host-get-nodes<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-nodes </span></span><br><span class="line">lab8106</span><br><span class="line">lab8107</span><br></pre></td></tr></table></figure></p>
<h4 id="列出存储节点上的存储使用的情况(detail看详细信息)">列出存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-osd-usage {hostname} {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106</span></span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106 detail</span></span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有存储节点上的存储使用的情况(detail看详细信息)">列出所有存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-all-usage {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage detail</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">2</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">3.8</span>GB | Available:<span class="number">274.4</span>GB</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出PG所在的节点(first_is_primary)">列出PG所在的节点(first is primary)</h4><p>命令： ceph-lazy pg-get-host {pg_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-get-host   10.2</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host :lab8107</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(_operations_number)">列出写操作最多的PG ( operations number)</h4><p>命令：ceph-lazy pg-most-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-most-write</span></span><br><span class="line">PG:<span class="number">10.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(_operations_number)">列出写操作最少的PG ( operations number)</h4><p>命令：ceph-lazy pg-less-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-less-write</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(data_written)">列出写操作最多的PG (data written)</h4><p>命令：ceph-lazy pg-most-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-write-kb</span></span><br><span class="line">PG:<span class="number">10.0</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(data_written)">列出写操作最少的PG (data written)</h4><p>命令：ceph-lazy pg-less-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-write-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(operations_number)">列出读操作最多的PG (operations number)</h4><p>命令：ceph-lazy pg-most-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read</span></span><br><span class="line">PG:<span class="number">10.1</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(operations_number)">列出读操作最少的PG (operations number)</h4><p>命令：ceph-lazy pg-less-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(data_read)">列出读操作最多的PG (data read)</h4><p>命令：ceph-lazy pg-most-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read-kb</span></span><br><span class="line">PG:<span class="number">10.4</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(data_read)">列出读操作最少的PG (data read)</h4><p>命令：ceph-lazy pg-less-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出空的PG_(没有存储对象)">列出空的PG (没有存储对象)</h4><p>命令：ceph-lazy pg-empty<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-empty</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">11.2</span></span><br><span class="line"><span class="number">11.1</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">11.4</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的prefix">列出RBD的prefix</h4><p>命令：ceph-lazy rbd-prefix {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-prefix rbd zp</span></span><br><span class="line">rbd_data.<span class="number">1</span>b93a6b8b4567</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的对象数目">列出RBD的对象数目</h4><p>命令：ceph-lazy rbd-count {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-count rbd zp</span></span><br><span class="line"></span><br><span class="line">    RBD image rbd/zp has prefix rbd_data.<span class="number">1</span>b93a6b8b4567; now couning objects...</span><br><span class="line">    count: <span class="number">27</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的存储主机">列出RBD的Primary所在的存储主机</h4><p>命令：ceph-lazy rbd-host {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-host rbd zp</span></span><br><span class="line">Primary Host: lab8107</span><br><span class="line">Primary Host: lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的OSD节点">列出RBD的Primary所在的OSD节点</h4><p>命令：ceph-lazy rbd-osd {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-osd rbd zp</span></span><br><span class="line">Primary Osd: <span class="number">0</span></span><br><span class="line">Primary Osd: <span class="number">1</span></span><br><span class="line">Primary Osd: <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Image的真实大小">列出RBD的Image的真实大小</h4><p>命令：ceph-lazy rbd-size rbd zp<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-size rbd zp</span></span><br><span class="line">Pool: rbd | Image:zp | Real_size:<span class="number">71.5586</span> MB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最多的OSD">列出容量使用最多的OSD</h4><p>命令：ceph-lazy osd-most-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-most-used</span></span><br><span class="line">OSD:osd.<span class="number">1</span> | Host: lab8106 | Used: <span class="number">8</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最少的OSD">列出容量使用最少的OSD</h4><p>命令：ceph-lazy osd-less-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-less-used</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host: lab8107 | Used: <span class="number">3</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上所有的primary_PG">列出指定OSD上所有的primary PG</h4><p>命令： ceph-lazy osd-get-ppg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-ppg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上的所有PG">列出指定OSD上的所有PG</h4><p>命令：ceph-lazy osd-get-pg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-pg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.2</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">10.1</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">10.4</span></span><br><span class="line"><span class="number">10.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定对象所在的主机（第一个是主）">列出指定对象所在的主机（第一个是主）</h4><p>命令：ceph-lazy object-get-host   {poolname} {obj_name}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy object-get-host   rbd rbd_data.1b93a6b8b4567.00000000000000a0</span></span><br><span class="line">Pg: <span class="number">10.4</span></span><br><span class="line">OSD:osd.<span class="number">0</span> | Host :lab8106</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h3 id="总结">总结</h3><p>本篇只是暂时结束了，目前完成了原作者的一些想法，等有空再写点自己比较注重的数据</p>
<p>最近一直在关注冯大辉的事情，看完后还是原来的感觉，在利益面前，公司总是会追求最大化，当出现分离的时候，总会显得无情，还是自己让自己强大一点，拿到属于自己的那一部分就好</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
<h3 id="For_me">For me</h3><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux配置邮件发送信息]]></title>
    <link href="http://www.zphj1987.com/2016/08/19/Linux%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF/"/>
    <id>http://www.zphj1987.com/2016/08/19/Linux配置邮件发送信息/</id>
    <published>2016-08-18T16:48:17.000Z</published>
    <updated>2016-08-28T16:08:27.386Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="编写一个发送的脚本">编写一个发送的脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /root/sendmail.sh </span><br><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line">timeout <span class="number">20</span> date &gt; /tmp/mail</span><br><span class="line">timeout <span class="number">20</span> ceph <span class="operator">-s</span> &gt;&gt; /tmp/mail</span><br><span class="line">timeout <span class="number">600</span> mail <span class="operator">-s</span> <span class="string">"cephstatus-`date`"</span> zbkc2016@sina.com &lt; /tmp/mail</span><br></pre></td></tr></table></figure>
<h3 id="在crontab中添加定期执行">在crontab中添加定期执行</h3><p>修改crontab配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim crontab</span><br><span class="line">*/<span class="number">5</span> * * * *  root  sh /root/sendmail.sh  <span class="number">2</span>&gt;&amp;<span class="number">1</span>  &gt; /dev/null</span><br></pre></td></tr></table></figure></p>
<p>让crontab服务生效<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crontab crontab</span><br><span class="line">/etc/init.d/crontab restart</span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>这个东西很简单，不过自己真去配置的时候，还是找半天资料，还是自己写好文档，方便以后使用，最快最简单的实现需求</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph 状态报警告 pool rbd has many more objects per pg than average (too few pgs?)]]></title>
    <link href="http://www.zphj1987.com/2016/07/27/Ceph-%E7%8A%B6%E6%80%81%E6%8A%A5%E8%AD%A6%E5%91%8A-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/"/>
    <id>http://www.zphj1987.com/2016/07/27/Ceph-状态报警告-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/</id>
    <published>2016-07-27T13:42:05.000Z</published>
    <updated>2016-07-27T14:45:55.641Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>
<a id="more"></a>
<p>这个地方查看集群详细信息：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph health detail</span></span><br><span class="line">HEALTH_WARN pool rbd has many more objects per pg than average (too few pgs?); mon.lab8106 low disk space</span><br><span class="line">pool rbd objects per pg (<span class="number">1912</span>) is more than <span class="number">50.3158</span> <span class="built_in">times</span> cluster average (<span class="number">38</span>)</span><br></pre></td></tr></table></figure></p>
<p>看下集群的pool的对象状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    <span class="number">556</span>G      <span class="number">538</span>G       <span class="number">18668</span>M          <span class="number">3.28</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME       ID     USED       %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd        <span class="number">6</span>      <span class="number">16071</span>M      <span class="number">2.82</span>          <span class="number">536</span>G       <span class="number">15296</span> </span><br><span class="line">    pool1      <span class="number">7</span>        <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool2      <span class="number">8</span>        <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool3      <span class="number">9</span>        <span class="number">188</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">48</span> </span><br><span class="line">    pool4      <span class="number">10</span>       <span class="number">192</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">49</span> </span><br><span class="line">    pool5      <span class="number">11</span>       <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool6      <span class="number">12</span>       <span class="number">148</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">38</span> </span><br><span class="line">    pool7      <span class="number">13</span>       <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool8      <span class="number">14</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool9      <span class="number">15</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool10     <span class="number">16</span>       <span class="number">248</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">63</span> </span><br><span class="line">    pool11     <span class="number">17</span>       <span class="number">232</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">59</span> </span><br><span class="line">    pool12     <span class="number">18</span>       <span class="number">264</span>M      <span class="number">0.05</span>          <span class="number">536</span>G          <span class="number">67</span></span><br></pre></td></tr></table></figure></p>
<p>查看存储池的pg个数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">6</span> <span class="string">'rbd'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">8</span> pgp_num <span class="number">8</span> last_change <span class="number">132</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">7</span> <span class="string">'pool1'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">134</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">8</span> <span class="string">'pool2'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">136</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">9</span> <span class="string">'pool3'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">138</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">10</span> <span class="string">'pool4'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">140</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">11</span> <span class="string">'pool5'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">142</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">12</span> <span class="string">'pool6'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">144</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">13</span> <span class="string">'pool7'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">146</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">14</span> <span class="string">'pool8'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">148</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">15</span> <span class="string">'pool9'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">150</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">16</span> <span class="string">'pool10'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">152</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">17</span> <span class="string">'pool11'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">154</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">18</span> <span class="string">'pool12'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">200</span> pgp_num <span class="number">200</span> last_change <span class="number">156</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>我们看下这个是怎么得到的</p>
<blockquote>
<p>pool rbd objects per pg (1912) is more than 50.3158 times cluster average (38)</p>
</blockquote>
<p>rbd objects_per_pg = 15296 / 8 = 1912<br>objects_per_pg = 15920 /417  ≈ 38<br>50.3158 =  rbd objects_per_pg / objects_per_pg =  1912 / 38 </p>
<p>也就是出现其他pool的对象太少，而这个pg少，对象多，就会提示这个了，我们看下代码里面的判断</p>
<p><a href="https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">int average_objects_per_pg = pg_map.pg_sum.stats.sum.num_objects / pg_map.pg_stat.size();</span><br><span class="line">     <span class="keyword">if</span> (average_objects_per_pg &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">         pg_map.pg_sum.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_objects &amp;&amp;</span><br><span class="line">         p-&gt;second.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_pool_objects) &#123;</span><br><span class="line">int objects_per_pg = p-&gt;second.stats.sum.num_objects / pi-&gt;get_pg_num();</span><br><span class="line"><span class="built_in">float</span> ratio = (<span class="built_in">float</span>)objects_per_pg / (<span class="built_in">float</span>)average_objects_per_pg;</span><br><span class="line"><span class="keyword">if</span> (g_conf-&gt;mon_pg_warn_max_object_skew &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">    ratio &gt; g_conf-&gt;mon_pg_warn_max_object_skew) &#123;</span><br><span class="line">  ostringstream ss;</span><br><span class="line">  ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" has many more objects per pg than average (too few pgs?)"</span>;</span><br><span class="line">  summary.push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  <span class="keyword">if</span> (detail) &#123;</span><br><span class="line">    ostringstream ss;</span><br><span class="line">    ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" objects per pg ("</span></span><br><span class="line">       &lt;&lt; objects_per_pg &lt;&lt; <span class="string">") is more than "</span> &lt;&lt; ratio &lt;&lt; <span class="string">" times cluster average ("</span></span><br><span class="line">       &lt;&lt; average_objects_per_pg &lt;&lt; <span class="string">")"</span>;</span><br><span class="line">    detail-&gt;push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>主要下面的几个限制条件</p>
<blockquote>
<p>mon_pg_warn_min_objects = 10000   //总的对象超过10000<br>mon_pg_warn_min_pool_objects = 1000     //存储池对象超过1000<br>mon_pg_warn_max_object_skew = 10        //就是上面的存储池的平均对象与所有pg的平均值的倍数关系</p>
</blockquote>
<h2 id="解决问题">解决问题</h2><p>有三个方法解决这个警告的提示：</p>
<ul>
<li><p>删除无用的存储池<br>如果集群中有一些不用的存储池，并且相对的pg数目还比较高，那么可以删除一些这样的存储池，从而降低<code>mon_pg_warn_max_object_skew</code>这个值，警告就会没有了</p>
</li>
<li><p>增加提示的pool的pg数目<br>有可能的情况就是，这个存储池的pg数目从一开始就不够，增加pg和pgp数目，同样降低了<code>mon_pg_warn_max_object_skew</code>这个值了</p>
</li>
<li>增加<code>mon_pg_warn_max_object_skew</code>的参数值<br>如果集群里面已经有足够多的pg了，再增加pg会不稳定，如果想去掉这个警告，就可以增加这个参数值，默认为10</li>
</ul>
<h2 id="总结">总结</h2><p>这个警告是比较的是存储池中的对象数目与整个集群的pg的平均对象数目的偏差，如果偏差太大就会发出警告</p>
<p>检查的步骤：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">ceph df</span><br><span class="line">ceph osd dump | grep pool</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>mon_pg_warn_max_object_skew = 10.0</p>
</blockquote>
<p>((objects/pg_num) in the affected pool)/(objects/pg_num in the entire system) &gt;= 10.0 警告就会出现</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何替换Ceph的Journal]]></title>
    <link href="http://www.zphj1987.com/2016/07/26/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2Ceph%E7%9A%84Journal/"/>
    <id>http://www.zphj1987.com/2016/07/26/如何替换Ceph的Journal/</id>
    <published>2016-07-26T14:32:18.000Z</published>
    <updated>2016-07-26T17:31:44.315Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>
<a id="more"></a>
<h2 id="第一种方法">第一种方法</h2><p>在开始处理前，最开始要设置OSD状态为<code>noout</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set noout</span></span><br><span class="line"><span class="built_in">set</span> noout</span><br></pre></td></tr></table></figure>
<p>停止需要替换journal的osd(这里是osd.1)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>我的版本是jewel的，如果是hammer版本，就使用 /etc/init.d/ceph stop osd.1</p>
</blockquote>
<p>下刷journal到osd，使用 -i 指定需要替换journal的 osd的编号</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --flush-journal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">26</span> <span class="number">22</span>:<span class="number">47</span>:<span class="number">20.185292</span> <span class="number">7</span><span class="built_in">fc</span>54a6c3800 -<span class="number">1</span> flushed journal /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="创建一个新的journal">创建一个新的journal</h3><p>删除原来的journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">58</span> Jul <span class="number">25</span> <span class="number">09</span>:<span class="number">25</span> /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal -&gt; /dev/disk/by-partuuid/<span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7</span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-1/journal</span></span><br></pre></td></tr></table></figure></p>
<p>准备一个新的分区</p>
<p>我的环境准备使用/dev/sdd1,分区大小为10G，这个注意磁盘大小比参数设置的要大一点即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ls -l /dev/disk/by-partuuid/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">25</span> <span class="number">14</span>:<span class="number">25</span> <span class="number">4766</span>ce93<span class="operator">-a</span>476-<span class="number">4</span>e97-<span class="number">9</span>aac-<span class="number">894</span>d461b367e -&gt; ../../sdb2</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">51</span> <span class="number">5</span>bb48687-<span class="number">6</span>be6-<span class="number">4</span>aef-<span class="number">82</span>f6-<span class="number">5</span>af822c3fad8 -&gt; ../../sdd1</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">47</span> <span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7 -&gt; ../../sdc2</span><br></pre></td></tr></table></figure>
<p>我的新的journal的uuid的路径为<code>/dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8</code></p>
<p>将这个磁盘的分区链接到原始路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ln -s /dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8 /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo 5bb48687-6be6-4aef-82f6-5af822c3fad8 &gt; /var/lib/ceph/osd/ceph-1/journal_uuid</span></span><br></pre></td></tr></table></figure></p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<p>启动后检查集群的状态</p>
<hr>
<h2 id="第二种方法">第二种方法</h2><p>这个属于备份和转移分区表的方法<br>首先进行上面方法的停进程，下刷journal</p>
<p>备份需要替换journal的分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --backup=/tmp/backup_journal_sdd /dev/sdd</span></span><br></pre></td></tr></table></figure></p>
<p>还原分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --load-backup=/tmp/backup_journal_sde /dev/sde</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># parted -s /dev/sde print</span></span><br></pre></td></tr></table></figure></p>
<p>新的journal磁盘现在跟老的journal的磁盘的分区表一样的了。这意味着新的分区的UUID和老的相同的。如果选择的是这种备份还原分布的方法，那么journal的那个软连接是不需要进行修改的，因为两个磁盘的uuid是一样的，所以需要注意将老的磁盘拔掉或者清理掉分区，以免冲突</p>
<p>在做完这个以后同样跟上面的方法一样需要重建journal</p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<h2 id="第一种方法的实践记录">第一种方法的实践记录</h2><p>这样你可以看到完整的操作过程，而不是枯燥的文档了，虽然命令行看上去也是那么的枯燥</p>
<iframe src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2journal.html" height="530px" width="90%" align="center"></iframe>

<p>支持暂停复制，是不是很屌？</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[modprobe: FATAL: Module ceph not found解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found解决办法/</id>
    <published>2016-07-24T14:48:23.000Z</published>
    <updated>2016-07-24T15:10:25.730Z</updated>
    <content type="html"><![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>
<a id="more"></a>
<pre><code class="bash">[root@lab8106 ~]<span class="comment"># uname -a</span>
Linux ciserver <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 <span class="comment">#1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux</span>
[root@lab8106 ~]<span class="comment"># modinfo ceph</span>
filename:       /lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64/kernel/fs/ceph/ceph.ko
license:        GPL
description:    Ceph filesystem <span class="keyword">for</span> Linux
author:         Patience Warnick &lt;patience@newdream.net&gt;
author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
author:         Sage Weil &lt;sage@newdream.net&gt;
<span class="built_in">alias</span>:          fs-ceph
rhelversion:    <span class="number">7.1</span>
srcversion:     <span class="number">2086</span>D500AFAF47B7260E08A
depends:        libceph
intree:         Y
vermagic:       <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 SMP mod_unload modversions 
signer:         CentOS Linux kernel signing key
sig_key:        A6:<span class="number">2</span>A:<span class="number">0</span>E:<span class="number">1</span>D:<span class="number">6</span>A:<span class="number">6</span>E:<span class="number">48</span>:<span class="number">4</span>E:<span class="number">9</span>B:FD:<span class="number">73</span>:<span class="number">68</span>:AF:<span class="number">34</span>:<span class="number">08</span>:<span class="number">10</span>:<span class="number">48</span>:E5:<span class="number">35</span>:E5
sig_hashalgo:   sha256
</code></pre>
<p>可以从上面的输出可以看到有个路径为 <code>/lib/modules/3.10.0-229.el7.x86_64/kernel/fs/ceph/ceph.ko</code> 的内核模块，这个就是 cephfs 客户端需要使用到的模块</p>
<h3 id="二、解决办法">二、解决办法</h3><p>解决这个缺失的模块的办法就是升级内核，并且在编译内核的时候需要选上这个模块，在某些商用的 Ceph 里面都是默认把这个模块给屏蔽了，这是因为 Cephfs 并没有达到稳定的标准，而这个在后端版本升级到 10.2 版本（jewel）版本，才正式宣布为第一个稳定版本，当然这个还是慎用为好，除非有比较强大的技术力量支撑，否则也不会出现那么多的大的商用厂家也不开放 Cephfs。</p>
<h3 id="三、总结">三、总结</h3><p>Cephfs这块是比rbd和radosgw这两个部分都复杂的部分，而真正能控制住这个开发的目前主要是 Intel 的<code>zhengyan</code>，从邮件列表里面可以看到主要都是他在修bug，这一块未知的可能性太多，任何小的故障抖动都可能是致命的</p>
<p>Bug不会自己消失，都是在那里的，只是看你有没有碰到</p>
<h3 id="四、变更记录">四、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-24</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[解决自动安装Freebsd系统盘符无法确定问题]]></title>
    <link href="http://www.zphj1987.com/2016/07/19/%E8%A7%A3%E5%86%B3%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85Freebsd%E7%B3%BB%E7%BB%9F%E7%9B%98%E7%AC%A6%E6%97%A0%E6%B3%95%E7%A1%AE%E5%AE%9A%E9%97%AE%E9%A2%98/"/>
    <id>http://www.zphj1987.com/2016/07/19/解决自动安装Freebsd系统盘符无法确定问题/</id>
    <published>2016-07-18T16:35:37.000Z</published>
    <updated>2016-07-18T17:06:52.181Z</updated>
    <content type="html"><![CDATA[<p>最近因为需要用到Freebsd，所以研究了打包的一些方法，这个没什么太大问题，通过网上的一些资料可以解决，但是由于确实不太熟悉这套系统，还是碰上了一些比较麻烦的地方，目前也没看到有人写如何处理，那就自己总结一下，以免以后再用忘记如何处理</p>
<h3 id="一、问题来源">一、问题来源</h3><p>在linux下的iso自动安装的时候，在无法确定盘符的情况下，可以不写盘符，从而在遇到任何奇怪的磁盘的时候也是能安装的，比如 sda,xvda，vda,这些都可以通过不精确盘符的方式解决</p>
<p>而在freebsd当中处理就不一样了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ./etc/installerconfig</span><br><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">DISTRIBUTIONS=<span class="string">"custom_kernel.txz base.txz lib32.txz custom_files.txz"</span></span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>这个地方写配置文件的第一句就要告诉安装环境需要安装到哪里，这个地方是写死的一个数据，而碰上ada为系统盘就没法解决了，得不断的适配这个盘符</p>
<a id="more"></a>
<h3 id="二、解决问题">二、解决问题</h3><p>最开始的时候写 etc/installerconfig这个配置文件我也不知道为什么要写这里就可以，根据网上的资料是写这个就可以了，在查阅更多的资料后，可以发现是在光盘的etc/rc.local里面会去调用这个脚本，然后去安装</p>
<p>最开始的思路是直接修改这个脚本，后来发现在安装过程中，这个文件实际是只读的，无法去修改的，所以这个地方需要做一个折中的修改</p>
<p>先准备好etc/installerconfig，写死几个值<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">···</span><br><span class="line"><span class="comment">#changge fstab to gpt id</span></span><br><span class="line">systemuuid=`gpart list | grep -A <span class="number">11</span> <span class="string">'da0p2'</span> | grep <span class="string">'rawuuid'</span> | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line">sed -i <span class="operator">-e</span> <span class="string">"s/da0p2/gptid\/<span class="variable">$systemuuid</span>/g"</span> /etc/fstab</span><br></pre></td></tr></table></figure></p>
<p>下面的那个部分是解决盘符变动，在安装过程中就处理好盘符的uuid挂载，这个在linux下面，是操作系统默认就处理好了，这个地方写定一个da0,等下后面处理的时候可以去匹配这个da0</p>
<p>处理默认的./etc/rc.local<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> TERM</span><br><span class="line"></span><br><span class="line">cp /etc/installerconfig /tmp/installerconfig</span><br><span class="line">sh -c <span class="string">'. /usr/share/bsdconfig/device.subr;f_device_menu "" "" "" DISK'</span></span><br><span class="line"><span class="built_in">echo</span> -n  <span class="string">"Which disk your what install :"</span></span><br><span class="line"><span class="built_in">read</span> mydisk</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$mydisk</span>"</span></span><br><span class="line">sed -i <span class="operator">-e</span> <span class="string">"s/da0/<span class="variable">$mydisk</span>/g"</span> /tmp/installerconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="operator">-f</span> /tmp/installerconfig ]; <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">if</span> bsdinstall script /tmp/installerconfig; <span class="keyword">then</span></span><br><span class="line">                dialog --backtitle <span class="string">"FreeBSD Installer"</span> --title <span class="string">"Complete"</span> --no-cancel --ok-label <span class="string">"Reboot"</span> --pause <span class="string">"Inst</span><br><span class="line">allation of FreeBSD complete! Rebooting in 10 seconds"</span> <span class="number">10</span> <span class="number">30</span> <span class="number">10</span></span><br><span class="line">                reboot</span><br></pre></td></tr></table></figure></p>
<p>处理思路就是先拷贝到一个临时的环境下面，然后去修改它，利用系统接口去获取可以安装的磁盘，这个地方只是起一个告诉有哪些盘可以安装的作用，然后根据提示输入想安装的磁盘的盘符名称，这个地方是什么名称就输入什么名称就可以安装了，然后系统就会根据改好的脚本去安装操作系统了</p>
<h3 id="三、总结">三、总结</h3><p>这是一个遗留问题，之前一直没解决，造成了越来越多的问题，在花了一个晚上的时间后，终于能够解决了，对系统越熟悉越能够知道怎么去处理问题，未知的东西太多，只能一点点花时间解决</p>
<h3 id="四、变更记录">四、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近因为需要用到Freebsd，所以研究了打包的一些方法，这个没什么太大问题，通过网上的一些资料可以解决，但是由于确实不太熟悉这套系统，还是碰上了一些比较麻烦的地方，目前也没看到有人写如何处理，那就自己总结一下，以免以后再用忘记如何处理</p>
<h3 id="一、问题来源">一、问题来源</h3><p>在linux下的iso自动安装的时候，在无法确定盘符的情况下，可以不写盘符，从而在遇到任何奇怪的磁盘的时候也是能安装的，比如 sda,xvda，vda,这些都可以通过不精确盘符的方式解决</p>
<p>而在freebsd当中处理就不一样了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ./etc/installerconfig</span><br><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">DISTRIBUTIONS=<span class="string">"custom_kernel.txz base.txz lib32.txz custom_files.txz"</span></span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>这个地方写配置文件的第一句就要告诉安装环境需要安装到哪里，这个地方是写死的一个数据，而碰上ada为系统盘就没法解决了，得不断的适配这个盘符</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[通过ceph-deploy安装不同版本ceph]]></title>
    <link href="http://www.zphj1987.com/2016/07/14/%E9%80%9A%E8%BF%87ceph-deploy%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%ACceph/"/>
    <id>http://www.zphj1987.com/2016/07/14/通过ceph-deploy安装不同版本ceph/</id>
    <published>2016-07-14T15:28:33.000Z</published>
    <updated>2016-07-15T10:28:06.373Z</updated>
    <content type="html"><![CDATA[<p>之前有在论坛写了怎么用 yum 安装 ceph，但是看到ceph社区的群里还是有人经常用 ceph-deploy 进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49" target="_blank" rel="external">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装 ceph-deploy 这个工具，机器上应该安装好 epel 源和 base 源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从 epel 的 repo 里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>
<a id="more"></a>
<p>现在什么都不修改，看下默认的安装会什么样的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">25</span>): /usr/bin/ceph-deploy install lab8106</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version hammer on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">[lab8106][WARNIN] http://ceph.com/rpm-hammer/rhel7/x86_64/repodata/repomd.xml: [Errno <span class="number">14</span>] HTTP Error <span class="number">404</span> - Not Found</span><br></pre></td></tr></table></figure>
<p>这个默认的版本没安装成功<br>这个地方的原因是默认会去下载<a href="http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm" target="_blank" rel="external">http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm</a> 这个包，而这个包是有问题的，安装以后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># cat /etc/yum.repos.d/ceph.repo |grep baseurl</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/<span class="variable">$basearch</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/noarch</span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/SRPMS</span><br></pre></td></tr></table></figure></p>
<p>这路径rhel7是根本就没有的，所以这个地方所以会出错，可以去修改repo的方式解决，这里先忽略这个问题，我们换一个ceph-deploy看看会怎样</p>
<h3 id="二、安装另外版本的ceph-deploy">二、安装另外版本的ceph-deploy</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum remove ceph-deploy</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rpm -ivh http://download.ceph.com/rpm/el7/noarch/ceph-deploy-1.5.34-0.noarch.rpm</span></span><br></pre></td></tr></table></figure>
<p>安装好了后，再次执行安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">···</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://download.ceph.com/keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[lab8106][DEBUG ] ---&gt; Package ceph.x86_64 <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">2</span>-<span class="number">0</span>.el7 will be installed</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>如果网络好的话，那么可以看到，执行这个命令后会在ceph.com的官网上去下载安装包了，如果网络不好的话，就会卡住了，这里是要说明的是</p>
<blockquote>
<p>不同的 ceph-deploy 去 install 的时候会安装不同的版本，这个因为代码里面会写上当时的版本，这样默认安装的就是当时的版本了</p>
</blockquote>
<p>到了这里要开始本篇的主题了，主要的目的有两个:</p>
<ul>
<li>自己选择想安装的 ceph 版本</li>
<li>自己选择通过什么地址安装</li>
</ul>
<p>第一个是解决了安装自己的版本，第二个是避免ceph.com无法访问的时候无法安装，通过国内的源进行加速</p>
<h3 id="三、自定义安装ceph">三、自定义安装ceph</h3><h4 id="通过阿里云安装ceph-hammer">通过阿里云安装ceph-hammer</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<p>通过这个命令，就通过阿里云的源安装了ceph的hammer版本的ceph</p>
<h4 id="通过阿里云安装ceph-jewel">通过阿里云安装ceph-jewel</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum clean all</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<h3 id="四、总结">四、总结</h3><p>安装的方式有很多，对于新手来说如果想用 ceph-deploy 去安装的话，可以根据上面的很简单的命令就解决了，这里没有写本地做源的相关的知识，安装这一块怎么顺手怎么来，不要在安装上面耗费太多的时间</p>
<h3 id="五、变更记录">五、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-14</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>私人朋友群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜（收费入群）</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前有在论坛写了怎么用 yum 安装 ceph，但是看到ceph社区的群里还是有人经常用 ceph-deploy 进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装 ceph-deploy 这个工具，机器上应该安装好 epel 源和 base 源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从 epel 的 repo 里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重构rbd镜像的元数据]]></title>
    <link href="http://www.zphj1987.com/2016/07/02/%E9%87%8D%E6%9E%84rbd%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/"/>
    <id>http://www.zphj1987.com/2016/07/02/重构rbd镜像的元数据/</id>
    <published>2016-07-01T18:52:33.000Z</published>
    <updated>2016-07-01T19:01:08.994Z</updated>
    <content type="html"><![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>将这几个镜像下载下来<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_header.60276b8b4567 rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_id.zp rbd_id.zp</span></span><br></pre></td></tr></table></figure></p>
<p>查看下载下来的几个镜像的元数据的文件信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># ll</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_directory</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">16</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<p>有没有发现有两个镜像的文件大小是0，这个是因为rbd format 2 格式下（默认格式），这两个对象的元数据信息是存储在扩展属性里面的，所以下载下来的对象是没有内容，那我们怎么查看这个属性，看下面讲述的查询相关的操作</p>
<h4 id="2、查询这个image的信息">2、查询这个image的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure>
<p>这里可以看到这个image文件的大小，对象大小，前缀信息，属性相关信息，这是用我们比较常规的方式来查询到的信息，现在用另外一种方式来查询信息，查到的会是另外一种方式，也就是上面一节提到的空对象的扩展属性的查询</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure>
<p>先来查询 rbd_directory 这个的元数据信息，这个里面的信息可以看到两组对应关系<br>id_60276b8b4567,就是这个image的id，也是前缀信息，后面对应的是一个名称zp<br>第二组name_zp,对应的就是后面的60276b8b4567，也就是名称对应到id<br>，那个value值就是后面的字符串对应的16进制的一种方式，这个地方就是需要备份的元数据信息，现在准备做第一次重构，重构rbd_directory这个的元数据信息，这个rbd_directory记录所属存储池有哪些镜像</p>
<h3 id="三、恢复rbd_directory的元数据信息">三、恢复rbd_directory的元数据信息</h3><p>先来破坏这个元数据信息，破坏的方式很简单，就是做删除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到删除了元数据信息以后，再进行镜像的ls，是查询不到信息的</p>
<p>开始做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># touch rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br></pre></td></tr></table></figure></p>
<p>上面做的三步是创建一个空文件，然后上传，然后列属性，可以看到，都是空的（这个地方也可以不创建空对象，直接做后面的给属性的时候，集群会自动创建相关的对象）<br>现在给这个对象写入属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x02\\x00\\x00\\x00\\x7a\\x70|rados -p rbd setomapval rbd_directory id_60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37|rados -p rbd setomapval rbd_directory name_zp</span></span><br></pre></td></tr></table></figure></p>
<p>写入的值就是上面让记录下来的信息，这个地方就用这个格式就行了，为什么要这么写，因为16进制的字符是需要转义的，之前不清楚怎么写，在邮件列表中提问后，有一个人低调的给回复了怎么写入这种进制数据，现在就这么固定写法就行了，现在再查询写入以后的属性情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<p>到这里 rbd_directory这个的信息就恢复了，下面再进行image的元数据的信息的恢复</p>
<h3 id="四、恢复image的元数据信息">四、恢复image的元数据信息</h3><p>先查询下这个对象包含的元数据信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>记录下这个信息，然后进行破坏，跟上面一样的删除掉对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd info zp</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">00</span>:<span class="number">57</span>:<span class="number">50.150559</span> <span class="number">7</span>ff4b56b3700 -<span class="number">1</span> librbd::image::OpenRequest: failed to retreive immutable metadata: (<span class="number">2</span>) No such file or directory</span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在删除了这个对象以后，已经无法查询到镜像信息了，当然也就无法使用了，下面开始进行image的元数据信息的重构<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00|rados -p rbd setomapval rbd_header.60276b8b4567 features</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x15\\x00\\x00\\x00\\x72\\x62\\x64\\x5f\\x64\\x61\\x74\\x61\</span></span><br><span class="line">\x2e\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37    |rados -p rbd setomapval rbd_header.<span class="number">60276</span>b8b4567  object_prefix</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x16|rados -p rbd setomapval rbd_header.60276b8b4567 order</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\xc4\\x09\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 size</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 snap_seq</span><br></pre></td></tr></table></figure></p>
<p>设置完了所有属性后查询，验证是否恢复了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>元数据完整的回来了<br>上面已经将两个导出的空对象元数据信息恢复好了，再看最后一个有文件大小的对象怎么做恢复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># cat rbd_id.zp </span></span><br><span class="line"></span><br><span class="line"><span class="number">60276</span>b8b4567[root@lab8106 zp]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>这个第一种方式是直接备份好,然后倒入的方式<br>跟上面的方法一样，开始通过删除对象来破坏<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到破坏了就无法访问镜像了，下面直接利用备份对象倒入的方式进行恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_id.zp rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到，倒入后即可，也可以用另外一种方式，记录字符串的方式进行备份<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># xxd rbd_id.zp</span></span><br><span class="line"><span class="number">0000000</span>: <span class="number">0</span>c00 <span class="number">0000</span> <span class="number">3630</span> <span class="number">3237</span> <span class="number">3662</span> <span class="number">3862</span> <span class="number">3435</span> <span class="number">3637</span>  ....<span class="number">60276</span>b8b4567</span><br></pre></td></tr></table></figure></p>
<p>我们可以查看这个文件的16进制的信息输出，这个信息就是要保留的字符串信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zp</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>需要保留的就是这个信息,我们根据这个信息来重新创建一个文件，然后检查文件内容是不是能跟下载下来的对象一样<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37   &gt;rbd_id.zpre</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zpre</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，可以用字符串完整恢复这个对象了，然后put进集群即可恢复了</p>
<h3 id="五、总结">五、总结</h3><p>可以看到，所有的元数据信息都可以以字符串的形式保留下来，然后进行元数据重构，其中的rbd_id.zp这个可以保存对象方式，也可以是获取对象后，然后保存16进制字符串信息，然后再进行本地创建对象,然后put的方式，其它的两个空对象可以用设置属性的方式进行恢复，在openstack场景下，这些元数据信息最好都保留下来，一旦有问题的时候，可以很方便的进行数据的重构，备份并不是说所有数据都需要备份，对于这种数据量很小，而且很重要的信息，定期备份一下，也许哪天就用上了</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-02</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

<h3 id="打个广告">打个广告</h3><p>私人朋友ceph技术讨论收费群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[parted分区对齐]]></title>
    <link href="http://www.zphj1987.com/2016/06/24/parted%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90/"/>
    <id>http://www.zphj1987.com/2016/06/24/parted分区对齐/</id>
    <published>2016-06-24T08:32:43.000Z</published>
    <updated>2016-06-24T08:35:39.321Z</updated>
    <content type="html"><![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>
<a id="more"></a>
<h3 id="二、通过计算分区">二、通过计算分区</h3><h4 id="获取磁盘的几个参数（这里是软raid）">获取磁盘的几个参数（这里是软raid）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /sys/block/md127/queue/optimal_io_size</span></span><br><span class="line"><span class="number">3670016</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/minimum_io_size</span></span><br><span class="line"><span class="number">524288</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/alignment_offset</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/physical_block_size</span></span><br><span class="line"><span class="number">512</span></span><br></pre></td></tr></table></figure>
<p>optimal_io_size 加上 alignment_offset 的和 然后除以  physical_block_size<br>在这个环境下是：<br>(3670016 + 0) / 512 = 7168</p>
<p>那么分区的时候命令就应该是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">7168</span>s <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>如果上面的顺利的完成检查一下  (‘1’是分区的编号):</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 aligned</p>
</blockquote>
<p>这个是正常的结果，如果没对齐就会是</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 not aligned</p>
</blockquote>
<h3 id="三、其他情况">三、其他情况</h3><p>默认情况下直接用下列的分区参数就可以，出现提示再用上面的计算，总之最后align-check 验证下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<h3 id="四、相关文章">四、相关文章</h3><p><a href="http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/" target="_blank" rel="external">How to align partitions for best performance using parted</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-24</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd的增量备份和恢复]]></title>
    <link href="http://www.zphj1987.com/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/06/22/rbd的增量备份和恢复/</id>
    <published>2016-06-21T17:17:24.000Z</published>
    <updated>2016-06-22T01:40:00.378Z</updated>
    <content type="html"><![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br><a id="more"></a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v1 testimage_v1</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v1那个时间点的差异数据导出来了testimage_v1，导出成本地文件testimage_v1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 testimage_v2</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v2那个时间点的差异数据导出来了，导出成本地文件testimage_v2<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 --from-snap v1 testimage_v1_v2</span><br></pre></td></tr></table></figure></p>
<p>这个命令是导出了从v1快照时间点到v2快照时间点的差异数据，导出成本地文件testimage_v1_v2</p>
<p>这个地方上面的导出的数据：</p>
<blockquote>
<p>v1时间点数据 + v1_v2之间数据 = v2 时间点数据</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage testimage_now</span><br></pre></td></tr></table></figure>
<p>这个就是导出了从image创建到当前的时间点的差异数据</p>
<h3 id="三、快照的数据恢复">三、快照的数据恢复</h3><p><img src="http://static.zybuluo.com/zphj1987/ue1feys17yiya6doa2audkbo/image_1alpuprird31dltilpro7kf52a.png" alt=""></p>
<p>快照的恢复过程使用的是刚刚上面提到的备份到本地的那些文件<br>首先随便创建一个image,名称大小都不限制，因为后面恢复的时候会覆盖掉大小的信息<br><figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">create</span> testbacknew <span class="comment">--size 1</span></span><br></pre></td></tr></table></figure></p>
<p>现在假如想恢复到v2那个快照的时间点，那么可以用两个方法</p>
<p>1、直接基于v2的时间点的快照做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd import-diff testimage_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>2、直接基于v1的时间点的数据，和后面的增量的v1_v2数据(要按顺序导入)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd impot-diff testimage_v1 rbd/testbacknew</span><br><span class="line">rbd impot-diff testimage_v1_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>到这里数据就已经恢复了</p>
<h3 id="四、如何利用这个">四、如何利用这个</h3><p>实际项目当中就是，定期做快照，然后导出某个时间点快照的数据，然后导出增量的快照的数据，就可以了，例如：<br>今天对所有的rbd的image做一个基础快照，然后导出这个快照的数据，然后从今天开始，每天晚上做一个快照，然后导出快照时间点之间的数据，这样每天导出来的就是一个增量的数据了，在做恢复的时候，就从第一个快照导入，然后按顺序导入增量的快照即可，也可以定期做一个快照，导出完整的快照数据，以防中间的增量快照漏了，然后就是要注意可以定期清理快照，如果是做备份的模式，在导入了快照数据后，也可以清理一些本地的数据，本地数据做异地机房复制的时候也可以做一下数据的压缩，来减少数据量的传输</p>
<h3 id="五、相关文章">五、相关文章</h3><p><a href="https://github.com/skuicloud/openstack-hacker/tree/master/tsinghua-cluster/script/ceph/volume_backup" target="_blank" rel="external">rbd备份还原的脚本</a><br><a href="http://ceph.com/dev-notes/incremental-snapshots-with-rbd/" target="_blank" rel="external">INCREMENTAL SNAPSHOTS WITH RBD</a><br><a href="http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication" target="_blank" rel="external">RBD Replication</a><br><a href="http://www.evil0x.com/posts/14638.html" target="_blank" rel="external">云杉网络：基于Ceph RBD的快照技术实现异地灾备</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-22</td>
</tr>
<tr>
<td style="text-align:center">修改错别字</td>
<td style="text-align:center">武汉-运维-磨渣 -from- 运维-北京-小白</td>
<td style="text-align:center">2016-06-22</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rgw实现nfs的首测]]></title>
    <link href="http://www.zphj1987.com/2016/06/19/rgw%E5%AE%9E%E7%8E%B0nfs%E7%9A%84%E9%A6%96%E6%B5%8B/"/>
    <id>http://www.zphj1987.com/2016/06/19/rgw实现nfs的首测/</id>
    <published>2016-06-18T18:47:56.000Z</published>
    <updated>2016-09-08T15:04:06.109Z</updated>
    <content type="html"><![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS" target="_blank" rel="external">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718" target="_blank" rel="external">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>
<a id="more"></a>
<h3 id="二、功能架构图">二、功能架构图</h3><p><img src="http://static.zybuluo.com/zphj1987/o5ruvtr9f7nyegbv0ly7ekv5/image_1alibfc78g96dsa1c26crkgis1e.png" alt="image_1alibfc78g96dsa1c26crkgis1e.png-78.3kB"><br>简单说明一下：<br>集群配置s3接口，nfs-genesha将s3接口转换成nfs，然后nfs客户端挂载后访问的就是s3的bucket里面的数据了</p>
<h3 id="三、准备工作">三、准备工作</h3><p>准备代码，这个是需要从源码编译的，并且需要将模块编译进去才可以的，源码分支地址：</p>
<blockquote>
<p><a href="https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable" target="_blank" rel="external">https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable</a></p>
</blockquote>
<p>这个地方要注意下，需要使用next分支(此分支开发中有编译BUG)，换分支V2.3-stable<br>使用git 进行clone分支到本地<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b V2.<span class="number">3</span>-stable https://github.com/nfs-ganesha/nfs-ganesha.git</span><br></pre></td></tr></table></figure></p>
<p>检查是否有这个RGW模块目录</p>
<blockquote>
<p>nfs-ganesha/src/FSAL/FSAL_RGW/</p>
</blockquote>
<p>默认clone下来后  <code>nfs-ganesha/src/libntirpc/</code> 这个目录是空的，而这个是因为如果在git里面某个目录嵌套的用了其他项目的代码，并且也是有git的分支的话，clone下来就会是空的，这个在ceph的源码里面也会这样，具体的看看下图：<br><img src="http://static.zybuluo.com/zphj1987/00dbog7s6nzbze55qyjilzt9/libntir.png" alt="libntir.png-38.4kB"><br>下载下面的链接的这个版本，然后把代码解压到nfs-ganesha/src/libntirpc/这个目录当中去<br><a href="https://github.com/nfs-ganesha/ntirpc/archive/e9cefd2ebfa0a5fc25932cc1088663d39c27e549.zip" target="_blank" rel="external">https://github.com/nfs-ganesha/ntirpc/archive/e9cefd2ebfa0a5fc25932cc1088663d39c27e549.zip</a></p>
<p>代码的编译采用的是cmake的模式(cmake目录后面接的是nfs-ganesha代码的src目录)</p>
<blockquote>
<p>注意在执行cmake之前编译环境需要安装librgw2-devel这个包，才能编译成功，执行cmake的时候检查下是否真的开启了</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/u3xku4jf3swljl0bub9zkwv0/image_1alian0db17e91gg1mhg866i1q11.png" alt="image_1alian0db17e91gg1mhg866i1q11.png-11.1kB"></p>
<p>开始编译安装过程，创建一个用于编译的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 newbian]<span class="comment">#mkdir mybuild</span></span><br><span class="line">[root@lab8106 newbian]<span class="comment">#cd mybuild</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#cmake -DUSE_FSAL_RGW=ON ../nfs-ganesha/src/</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment"># ll FSAL/FSAL_RGW/</span></span><br><span class="line">total <span class="number">16</span></span><br><span class="line">drwxr-xr-x <span class="number">3</span> root root    <span class="number">83</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> CMakeFiles</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">2979</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> cmake_install.cmake</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">10164</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> Makefile</span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make install</span></span><br></pre></td></tr></table></figure></p>
<p>编译安装工作就到此完成了，还是比较简单的</p>
<h3 id="四、配置服务">四、配置服务</h3><h4 id="1、准备一个s3的环境，我的如下：">1、准备一个s3的环境，我的如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin user create --uid=admin --display-name=<span class="string">"admin"</span>   --access-key=admin  --secret=admin</span><br></pre></td></tr></table></figure>
<p>用户信息如下：</p>
<ul>
<li>s3的User_Id：admin </li>
<li>s3的Access_Key:admin </li>
<li>s3的Secret_Access_Key:admin</li>
</ul>
<p>注意，配置ganesha-nfs服务的机器需要安装librgw</p>
<h4 id="2、修改ganesha-nfs的配置文件">2、修改ganesha-nfs的配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/ganesha/ganesha.conf</span><br></pre></td></tr></table></figure>
<p>修改如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">EXPORT</span><br><span class="line">&#123;</span><br><span class="line">        Export_ID=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        Path = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Pseudo = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Access_Type = RW;</span><br><span class="line"></span><br><span class="line">        NFS_Protocols = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">        Transport_Protocols = TCP;</span><br><span class="line"></span><br><span class="line">        FSAL &#123;</span><br><span class="line">                Name = RGW;</span><br><span class="line">                User_Id = <span class="string">"admin"</span>;</span><br><span class="line">                Access_Key_Id =<span class="string">"admin"</span>;</span><br><span class="line">                Secret_Access_Key = <span class="string">"admin"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RGW &#123;</span><br><span class="line">    ceph_conf = <span class="string">"/etc/ceph/ceph.conf"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>RGW-NFS配置文件的模板路径在：</p>
<blockquote>
<p>/usr/share/doc/ganesha/config_samples/rgw.conf</p>
</blockquote>
<h4 id="4、启动ganesha-nfs服务">4、启动ganesha-nfs服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart  nfs-ganesha.service</span><br></pre></td></tr></table></figure>
<h4 id="5、NFS客户端挂载ganesha-nfs服务">5、NFS客户端挂载ganesha-nfs服务</h4><p>找一台其它的客户端机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t nfs <span class="number">192.168</span>.<span class="number">8.106</span>:/ /mnt</span><br></pre></td></tr></table></figure></p>
<p>直接挂载即可，这里注意因为rgw是没有文件系统的容量概念的，这里df是看不到的，所以用mount命令检测<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># mount|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/ on /mnt <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/testnfsrgw on /mnt/testnfsrgw <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,port=<span class="number">0</span>,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以查看挂载的目录里面的子目录对应的就是bucket<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># ll /mnt/</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> ./</span><br><span class="line">drwxr-xr-x <span class="number">25</span> root root <span class="number">4096</span> Apr <span class="number">13</span> <span class="number">03</span>:<span class="number">04</span> ../</span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> testnfsrgw/</span><br></pre></td></tr></table></figure></p>
<h3 id="五、总结">五、总结</h3><p>在实现这个功能以后，实际上为文件接口和对象接口打通了一个通道，能够方便的实现传统的文件接口的数据到对象接口的转移，在性能方面，本篇并没有做测试，这个交给实际项目中去检测了，如果有问题欢迎探讨</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-19</td>
</tr>
<tr>
<td style="text-align:center">修改无法编译的BUG</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-08</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的分答-付费语音回答问题-￥1]]></title>
    <link href="http://www.zphj1987.com/2016/06/15/%E6%88%91%E7%9A%84%E5%88%86%E7%AD%94-%E4%BB%98%E8%B4%B9%E8%AF%AD%E9%9F%B3%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98-%EF%BF%A51/"/>
    <id>http://www.zphj1987.com/2016/06/15/我的分答-付费语音回答问题-￥1/</id>
    <published>2016-06-14T17:34:06.000Z</published>
    <updated>2016-06-15T14:31:23.666Z</updated>
    <content type="html"><![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>
<a id="more"></a>
<center><br><img src="http://static.zybuluo.com/zphj1987/qxfdnyf1c6o1hd4irdaz7xp4/liantu.png" alt="微信扫一扫"><br></center>




]]></content>
    <summary type="html">
    <![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CPU相关的学习]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/CPU%E7%9B%B8%E5%85%B3%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.zphj1987.com/2016/06/13/CPU相关的学习/</id>
    <published>2016-06-13T10:13:49.000Z</published>
    <updated>2016-06-13T10:14:21.018Z</updated>
    <content type="html"><![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br><a id="more"></a></p>
<h3 id="单核和多核">单核和多核</h3><p>在英文里面，单核（single-core）和多核（multi-core）多称作uniprocessor和multiprocessor，这里先对这些概念做一个说明：</p>
<blockquote>
<p>这里所说的core（或processor），是一个泛指，是从使用者（或消费者）的角度看计算机系统。因此，core，或者processor，或者处理器（CPU），都是逻辑概念，指的是一个可以独立运算、处理的核心。<br>而这个核心，可以以任何形式存在，例如：单独的一个chip（如通常意义上的单核处理器）；一个chip上集成多个核心（如SMP，symmetric multiprocessing）；一个核心上实现多个hardware context，以支持多线程（如SMT，Simultaneous multithreading）；等等。这是从硬件实现的角度看的。<br>最后，从操作系统进程调度的角度，又会统一看待这些不同硬件实现的核心，例如上面开始所提及的CPU（24个CPUs，从0编号开始），因为它们都有一个共同的特点：执行进程（或线程）。</p>
</blockquote>
<h3 id="NUNA与SMP的概念">NUNA与SMP的概念</h3><p>NUMA(Non-Uniform Memory Access，非一致性内存访问)和SMP(Symmetric Multi-Processor，对称多处理器系统)是两种不同的CPU硬件体系架构</p>
<p>SMP（Symmetric Multi-Processing）的主要特征是共享，所有的CPU共享使用全部资源，例如内存、总线和I/O，多个CPU对称工作，彼此之间没有主次之分，平等地访问共享的资源，这样势必引入资源的竞争问题，从而导致它的扩展内力非常有限。特别是在现在一台机器CPU核心比较多，内存比较大的情况</p>
<p>NUMA技术将CPU划分成不同的组（Node)，每个Node由多个CPU组成，并且有独立的本地内存、I/O等资源。Node之间通过互联模块连接和沟通，因此除了本地内存外，每个CPU仍可以访问远端Node的内存，只不过效率会比访问本地内存差一些，我们用Node之间的距离（Distance，抽象的概念）来定义各个Node之间互访资源的开销。</p>
<p>本章主要是去做NUMA的相关探索，下图是一个多核系统简单的topology</p>
<center><img src="http://static.zybuluo.com/zphj1987/vg7eprp72ibucwyq0fljvfl2/coremuti.gif" alt="coremuti.gif-23.7kB"></center>

<h3 id="Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)">Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)</h3><p>如果你只知道CPU这么一个概念，那么是无法理解CPU的拓扑的。事实上，在NUMA架构下，CPU的概念从大到小依次是：Node、Socket、Core、Processor</p>
<ul>
<li>Sockets 可以理解成主板上cpu的插槽数，物理cpu的颗数，一般同一socket上的core共享三级缓存</li>
<li>Cores 而Socket中的每个核心被称为Core,常说的核,核有独立的物理资源.比如单独的一级二级缓存什么的</li>
<li>Threads 为了进一步提升CPU的处理能力，Intel又引入了HT（Hyper-Threading，超线程)的技术，一个Core打开HT之后，在OS看来就是两个核，当然这个核是逻辑上的概念，所以也被称为Logical Processor,如果不开超线程,threads应该与cores相等,如果开了超线程,threads应该是cores的倍数.相互之间共享物理资源</li>
<li>Nodes 上图的多核图中没有涉及， Node是NUMA体系中的概念．由于SMP体系中各个CPU访问内存只能通过单一的通道．导致内存访问成为瓶颈,cpu再多也无用．后来引入了NUMA．通过划分node,每个node有本地RAM,这样node内访问RAM速度会非常快．但跨Node的RAM访问代价会相对高一点，下面看一下两种架构的明显区别</li>
</ul>
<center><img src="http://static.zybuluo.com/zphj1987/y0thygclxkbl9y8e1f6rl9yj/smpnuma.png" alt="smpnuma.png-67.5kB"></center>


<p>由此可以总结这样的逻辑关系(包含):Node &gt; Socket &gt; Core &gt; Thread 区分这几个概念为了了解cache的分布,因为cpu绑定的目的就是提高cache的命中率,降低cpu颠簸.所以了解cache与cpu之间的mapping关系是非常重要的.通常来讲:</p>
<ul>
<li>同Socket内的cpu共享三级级缓存</li>
<li>每个Core有自己独立的二级缓存</li>
<li>一个Core上超线程出来的Threads,避免绑定，看似可能会提高L2 cache命中率,但也可能有严重的cpu争抢，导致性能非常差.</li>
</ul>
<h3 id="查看CPU信息">查看CPU信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lscpu </span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        <span class="number">32</span>-bit, <span class="number">64</span>-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                <span class="number">24</span></span><br><span class="line">On-line CPU(s) list:   <span class="number">0</span>-<span class="number">23</span></span><br><span class="line">Thread(s) per core:    <span class="number">2</span></span><br><span class="line">Core(s) per socket:    <span class="number">6</span></span><br><span class="line">Socket(s):             <span class="number">2</span></span><br><span class="line">NUMA node(s):          <span class="number">2</span></span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            <span class="number">6</span></span><br><span class="line">Model:                 <span class="number">62</span></span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br><span class="line">Stepping:              <span class="number">4</span></span><br><span class="line">CPU MHz:               <span class="number">1607.894</span></span><br><span class="line">BogoMIPS:              <span class="number">4205.65</span></span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             <span class="number">32</span>K</span><br><span class="line">L1i cache:             <span class="number">32</span>K</span><br><span class="line">L2 cache:              <span class="number">256</span>K</span><br><span class="line">L3 cache:              <span class="number">15360</span>K</span><br><span class="line">NUMA node0 CPU(s):     <span class="number">0</span>-<span class="number">5</span>,<span class="number">12</span>-<span class="number">17</span></span><br><span class="line">NUMA node1 CPU(s):     <span class="number">6</span>-<span class="number">11</span>,<span class="number">18</span>-<span class="number">23</span></span><br></pre></td></tr></table></figure>
<p>2颗6核双线程，一共是24 processors,也可以看到是NUMA体系，可以使用以下命令详细查看numa信息.非NUMA体系时,所有cpu都划分为一个Node<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># numactl --hardware</span></span><br><span class="line">available: <span class="number">2</span> nodes (<span class="number">0</span>-<span class="number">1</span>)</span><br><span class="line">node <span class="number">0</span> cpus: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span></span><br><span class="line">node <span class="number">0</span> size: <span class="number">31880</span> MB</span><br><span class="line">node <span class="number">0</span> free: <span class="number">19634</span> MB</span><br><span class="line">node <span class="number">1</span> cpus: <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span></span><br><span class="line">node <span class="number">1</span> size: <span class="number">32253</span> MB</span><br><span class="line">node <span class="number">1</span> free: <span class="number">29315</span> MB</span><br><span class="line">node distances:</span><br><span class="line">node   <span class="number">0</span>   <span class="number">1</span> </span><br><span class="line">  <span class="number">0</span>:  <span class="number">10</span>  <span class="number">21</span> </span><br><span class="line">  <span class="number">1</span>:  <span class="number">21</span>  <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>cpu的id不连续的原因是开启了超线程，超线程的cpuid是从新的ID开始计数的，也就是从12开始计数的</p>
</blockquote>
<p>两个node，每个node32G内存左右，这台机器我的物理内存是64G</p>
<h3 id="通过命令行查看cpu信息">通过命令行查看cpu信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取cpu名称与主频</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | cut <span class="operator">-f</span>2 <span class="operator">-d</span>: | head -n1 | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取逻辑核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取物理核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'physical id'</span> | sort | uniq | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cpu的flags</span></span><br><span class="line">cat /proc/cpuinfo | grep flags | uniq | cut <span class="operator">-f</span>2 <span class="operator">-d</span> : | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否打开超线程（检查 physical id * cpu cores 与 processor的比例 1:1为未开启）</span></span><br><span class="line">cat /proc/cpuinfo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cache大小,X自省替换</span></span><br><span class="line">sudo cat /sys/devices/system/cpu/cpuX/cache/indexX/size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看各个cpu之间与cache的mapping</span></span><br><span class="line">cat /sys/devices/system/cpu/cpuX/cache/indexX/shared_cpu_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取CPU分布的信息（id-&gt; core信息）（这一个可以看出来CPU0和CPU12在同一个core）</span></span><br><span class="line">egrep <span class="string">'processor|core id|physical id'</span> /proc/cpuinfo | cut <span class="operator">-d</span> : <span class="operator">-f</span> <span class="number">2</span> | paste - - -  | awk <span class="string">'&#123;print "CPU"$1"\tsocket "$2" core "$3&#125;'</span></span><br><span class="line">CPU0	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU1	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU2	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU3	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU4	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU5	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU6	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU7	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU8	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU9	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU10	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU11	socket <span class="number">1</span> core <span class="number">5</span></span><br><span class="line">CPU12	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU13	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU14	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU15	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU16	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU17	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU18	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU19	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU20	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU21	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU22	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU23	socket <span class="number">1</span> core <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>lscpu,numactl都是读取proc,sys文件系统信息并进行格式化，输出人性化的内容．当没有网络,而lscpu,numactl都没有安装时，只能使用这种命令行方式了</p>
<p>能用工具还是用工具，工具就是解放双手的</p>
<h3 id="Cpu_Topology可视化">Cpu Topology可视化</h3><p>lstopo 指令由 hwloc 数据包提供，创建了用户的系统示意图。lstopo-no-graphics 指令提供详尽的文本输出<br>通过lscpu与numactl获取的信息，必要的时候查询了/sys/devices/system/cpu/cpuX/*的数据将正在使用的 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz的topology进行可视化<br>详细的cache信息可以通过sysfs查看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">index0 index1 index2 index3</span><br></pre></td></tr></table></figure></p>
<p>包含以下4个目录：</p>
<ul>
<li>index0:1级数据cache </li>
<li>index1:1级指令cache </li>
<li>index2:2级cache </li>
<li>index3:3级cache,对应cpuinfo里的cache</li>
</ul>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">文件</th>
<th style="text-align:center">内容</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">type</td>
<td style="text-align:center">Data</td>
<td style="text-align:center">数据cache，如果查看index1就是Instruction</td>
</tr>
<tr>
<td style="text-align:center">Level</td>
<td style="text-align:center">1</td>
<td style="text-align:center">L1</td>
</tr>
<tr>
<td style="text-align:center">Size</td>
<td style="text-align:center">32K</td>
<td style="text-align:center">大小为32K</td>
</tr>
<tr>
<td style="text-align:center">coherency_line_size</td>
<td style="text-align:center">64</td>
<td style="text-align:center">64<em>4</em>128=32K</td>
</tr>
<tr>
<td style="text-align:center">physical_line_partition</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">ways_of_associativity</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">number_of_sets</td>
<td style="text-align:center">128</td>
</tr>
<tr>
<td style="text-align:center">shared_cpu_map</td>
<td style="text-align:center">00000101</td>
<td style="text-align:center">表示这个cache被CPU0和CPU8 share</td>
</tr>
</tbody>
</table>
<p>解释一下shared_cpu_map内容的格式：<br>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu 截取00000101的后4位，转换为2进制表示</p>
<table>
<thead>
<tr>
<th style="text-align:center">CPU id</th>
<th style="text-align:center">15</th>
<th style="text-align:center">14</th>
<th style="text-align:center">13</th>
<th style="text-align:center">12</th>
<th style="text-align:center">11</th>
<th style="text-align:center">10</th>
<th style="text-align:center">9</th>
<th style="text-align:center">8</th>
<th style="text-align:center">7</th>
<th style="text-align:center">6</th>
<th style="text-align:center">5</th>
<th style="text-align:center">4</th>
<th style="text-align:center">3</th>
<th style="text-align:center">2</th>
<th style="text-align:center">1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0×0101的2进制表示</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。<br>也可以使用上面提到的lstopo-no-graphics命令进行查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lstopo-no-graphics </span></span><br><span class="line">Machine (<span class="number">63</span>GB)</span><br><span class="line">  NUMANode L<span class="comment">#0 (P#0 31GB)</span></span><br><span class="line">    Socket L<span class="comment">#0 + L3 L#0 (15MB)</span></span><br><span class="line">      L2 L<span class="comment">#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0</span></span><br><span class="line">        PU L<span class="comment">#0 (P#0)</span></span><br><span class="line">        PU L<span class="comment">#1 (P#12)</span></span><br><span class="line">      L2 L<span class="comment">#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1</span></span><br><span class="line">        PU L<span class="comment">#2 (P#1)</span></span><br><span class="line">        PU L<span class="comment">#3 (P#13)</span></span><br><span class="line">      L2 L<span class="comment">#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2</span></span><br><span class="line">        PU L<span class="comment">#4 (P#2)</span></span><br><span class="line">        PU L<span class="comment">#5 (P#14)</span></span><br><span class="line">      L2 L<span class="comment">#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3</span></span><br><span class="line">        PU L<span class="comment">#6 (P#3)</span></span><br><span class="line">        PU L<span class="comment">#7 (P#15)</span></span><br><span class="line">      L2 L<span class="comment">#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4</span></span><br><span class="line">        PU L<span class="comment">#8 (P#4)</span></span><br><span class="line">        PU L<span class="comment">#9 (P#16)</span></span><br><span class="line">      L2 L<span class="comment">#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5</span></span><br><span class="line">        PU L<span class="comment">#10 (P#5)</span></span><br><span class="line">        PU L<span class="comment">#11 (P#17)</span></span><br><span class="line">    HostBridge L<span class="comment">#0</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">1000</span>:<span class="number">0086</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#0 "enp4s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#1 "enp4s0f1"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#2 "enp4s0f2"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#3 "enp4s0f3"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#4 "enp6s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#5 "enp6s0f1"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1</span>d6b</span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">102</span>b:<span class="number">0532</span></span><br><span class="line">          GPU L<span class="comment">#6 "card0"</span></span><br><span class="line">          GPU L<span class="comment">#7 "controlD64"</span></span><br><span class="line">      PCI <span class="number">8086</span>:<span class="number">1</span>d02</span><br><span class="line">        Block L<span class="comment">#8 "sda"</span></span><br><span class="line">  NUMANode L<span class="comment">#1 (P#1 31GB) + Socket L#1 + L3 L#1 (15MB)</span></span><br><span class="line">    L2 L<span class="comment">#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6</span></span><br><span class="line">      PU L<span class="comment">#12 (P#6)</span></span><br><span class="line">      PU L<span class="comment">#13 (P#18)</span></span><br><span class="line">    L2 L<span class="comment">#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7</span></span><br><span class="line">      PU L<span class="comment">#14 (P#7)</span></span><br><span class="line">      PU L<span class="comment">#15 (P#19)</span></span><br><span class="line">    L2 L<span class="comment">#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8</span></span><br><span class="line">      PU L<span class="comment">#16 (P#8)</span></span><br><span class="line">      PU L<span class="comment">#17 (P#20)</span></span><br><span class="line">    L2 L<span class="comment">#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9</span></span><br><span class="line">      PU L<span class="comment">#18 (P#9)</span></span><br><span class="line">      PU L<span class="comment">#19 (P#21)</span></span><br><span class="line">    L2 L<span class="comment">#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10</span></span><br><span class="line">      PU L<span class="comment">#20 (P#10)</span></span><br><span class="line">      PU L<span class="comment">#21 (P#22)</span></span><br><span class="line">    L2 L<span class="comment">#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11</span></span><br><span class="line">      PU L<span class="comment">#22 (P#11)</span></span><br><span class="line">      PU L<span class="comment">#23 (P#23)</span></span><br></pre></td></tr></table></figure></p>
<p>这个得到的是文本的拓扑，这个转换成一个图看的要清楚一些<br><img src="http://static.zybuluo.com/zphj1987/u4t0qbmeh2i4nemf007vqepr/nodesock.png" alt="nodesock.png-45.9kB"></p>
<h4 id="NUMA分组信息">NUMA分组信息</h4><ul>
<li>通过图可以看到cpu为numa架构,且有两个node</li>
<li>将同一socket内的cpu(threads)都划分在一个node中.通过上图也解释了node中cpu序列不连续的问题.因为同一个Core上的两个Threads是超线程出来的.超线程Thread的cpu id在原有的core id基础上增长的</li>
<li>每个node中有32G左右的本地RAM可用</li>
</ul>
<h4 id="cache信息">cache信息</h4><ul>
<li>每个core都有独立的二级缓存,而不是socket中所有的core共享二级缓存</li>
<li>同node中的cpu共享三级缓存</li>
<li>跨node的内存访问的花费要大些</li>
</ul>
<h3 id="cpu绑定注意的几点">cpu绑定注意的几点</h3><ul>
<li>Numa体系中,如果夸node绑定,性能会下降.因为L3 cache命中率低,跨node内存访问代价高.</li>
<li>绑定同Node,同一个Core中的两个超线程出来的cpu,性能会急剧下降.cpu密集型的线程硬件争用严重.”玩转CPU Topology”中也提到了.</li>
<li>Numa架构可能引起swap insanity.需要注意</li>
</ul>
<h3 id="测试CPU绑定性能">测试CPU绑定性能</h3><p>这个部分就不在这里赘述了，上面是把cpu比较清晰的剥离出来，至于效果，需要在实际环境当中去验证了，有可能变坏，也有可能变好</p>
<p>本篇参考了很多网络上的很多其他资料</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[推荐一本性能相关的书]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%9C%AC%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B9%A6/"/>
    <id>http://www.zphj1987.com/2016/06/13/推荐一本性能相关的书/</id>
    <published>2016-06-13T08:57:38.000Z</published>
    <updated>2016-06-13T09:19:01.017Z</updated>
    <content type="html"><![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>

<a id="more"></a>
<p>京东纸质书购买链接：<a href="http://item.jd.com/11755695.html" target="_blank" rel="external">性能之巅</a></p>
<h3 id="英文版本下载地址：">英文版本下载地址：</h3><p>地址：<a href="http://pan.baidu.com/s/1i5LJTxN" target="_blank" rel="external">http://pan.baidu.com/s/1i5LJTxN</a><br>密码：nam5</p>
<h3 id="中文电子版">中文电子版</h3><p>联系我<br>QQ:199383004</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[加速OSD的启动]]></title>
    <link href="http://www.zphj1987.com/2016/06/07/%E5%8A%A0%E9%80%9FOSD%E7%9A%84%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/06/07/加速OSD的启动/</id>
    <published>2016-06-07T14:50:02.000Z</published>
    <updated>2016-06-07T15:03:50.697Z</updated>
    <content type="html"><![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br><a id="more"></a></p>
<h3 id="一、问题分析">一、问题分析</h3><p>OSD重启是需要重视的，这个地方是ceph的一个设计的弱点。ceph集群有很多的OSD进程，OSD管理对磁盘上的对象的访问，磁盘的对象被分布到PG组当中，对象有相同的分布，副本会在相同的PG当中存在，如果不理解可以看看（<a href="http://docs.ceph.com/docs/master/architecture/" target="_blank" rel="external">ceph概览</a>）</p>
<p>当集群OSD进程出现down的情况，会被mon认为 “OUT” 了，这个 “OUT” 不是触发迁移的那个 “OUT”，是不服务的 “OUT” ,这个OSD上受影响的PG的I/O请求会被其他拥有这个PG的OSD接管，当OSD重新启动的时候,OSD会被加入进来，将会检查PG，看是否有在down的期间错过东西，然后进行更新，这里问题就来了，启动之后会访问磁盘检查PG是否有缺失的东西进行更新，会进行一定量的数据恢复，同时会开始接受新的IO的请求，如果本来磁盘就只剩很少的余量，那么一旦请求发送到这个OSD上，那么性能将会开始下降</p>
<p>如果去看ceph的邮件列表，在极端情况下，这种效应会让整个集群停机，这发生在OSD太忙了，连心跳都无法回复，然后mon就会把它标记为down，这个时候OSD的进程都还在的，这个时候客户端的请求会导入到其他的OSD上，然后负载小了，OSD又会自己进来，然后又开始响应请求了，然后之前没有受影响的OSD节点，需要把新写入的数据同步过来，这个又增加了其他的OSD的负载了，一旦集群接近I/O的限制，也会让其他的OSD无法响应了，结果就是整个集群的OSD在反复的”in”和”out”状态之间变化了，集群在这种情况下，就无法接收客户端的请求了，如果不人工干预甚至无法恢复正常，这个在高负载下是很好复现出来的;另外一种较轻的情况，在OSD重启过程，I/O可能会hung住，影响性能.如果不能避免，至少能想办法去降低这个影响</p>
<p>我们能做些什么？在ceph开发者列表当中有开发者提出了这个<a href="http://thread.gmane.org/gmane.comp.file-systems.ceph.user/25881/focus=25890" target="_blank" rel="external">设计上需要修复</a>，这个估计需要等很久以后的事情了，我们能做什么来降低这个的影响？最明显的一点是要保证集群有足够的I/O的余量，另一种思路就是减少关键过程启动检查和接收I/O的竞争</p>
<h3 id="二、减少OSD启动过程当中的IO">二、减少OSD启动过程当中的IO</h3><p>OSD在启动的时候可以预测到磁盘的访问的模式。我们可以了解这个访问模式，然后提前将文件读取到内核的缓存当中。这样这些文件在启动的时候就不需要再次访问磁盘了，意味着更少的磁盘消耗和更好的性能</p>
<p>现在来定位下OSD启动过程中做了哪些事情，使用性能大师 Brendan Gregg 的 <a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a> 工具，一个OSD启动的过程如下：</p>
<h4 id="2-1_OSD启动过程">2.1 OSD启动过程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># opensnoop ceph-7</span></span><br><span class="line">Tracing open()s <span class="keyword">for</span> filenames containing <span class="string">"ceph-7"</span>. Ctrl-C to end.</span><br><span class="line">COMM             PID      FD FILE</span><br><span class="line">ceph             osd     <span class="number">0</span>x3 /var/lib/ceph/osd/ceph-<span class="number">7</span>/</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/<span class="built_in">type</span></span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/magic</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/whoami</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/ceph_fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/superblock</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/fiemap_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/xattr_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/current</span><br><span class="line">ceph             osd     <span class="number">0</span>xe /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/commit_op_seq</span><br><span class="line">ceph             osd     <span class="number">0</span>xf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/LOCK</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/CURRENT</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000135</span></span><br></pre></td></tr></table></figure>
<p>开始的时候，OSD读取了很多元数据文件，没有什么特别的<br>下面读取omap的数据库文件，读取了一部分的osdmap文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000137</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000144</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000142</span></span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000142</span>.dbtmp</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000138</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">298</span>__0_AC96EE75__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.3</span>b_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">297</span>__0_AC96EEA5__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.34</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.20</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.22</span>_head</span><br></pre></td></tr></table></figure></p>
<p>可以看到读取一个sst后，就会继续读取pg的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000139</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.ec_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>e_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>b_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000141</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>fb_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.cf_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>f_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000140</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.8</span>f_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>c_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>e_head</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会读取每个pg里面的<em>head</em>文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x17 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.23</span>a_head/__head_0000023A__0</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x18 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.1</span>a2_head/DIR_2/DIR_A/DIR_1/__head_000001A2__0</span><br><span class="line">&lt;...&gt;            <span class="number">23689</span>  <span class="number">0</span>x19 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>ea_head/__head_000002EA__0</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会进行osdmap文件的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x3a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">299</span>__0_C67CF872__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">299</span>__0_AC96EF05__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">300</span>__0_C67CF142__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4f /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">300</span>__0_AC96E415__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x6e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">301</span>__0_C67CF612__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x55 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">301</span>__0_AC96E5A5__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>xbf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">302</span>__0_C67CF7A2__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x60 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">302</span>__0_AC96E575__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x86 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">303</span>__0_C67CF772__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x6a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">303</span>__0_AC96FA05__none</span><br><span class="line">s</span><br></pre></td></tr></table></figure></p>
<p>我们无法确定哪些对象将需要读取，单我们知道，所有的OMAP和元数据文件将会打开，<em>head</em>文件将会打开</p>
<h4 id="2-2_使用vmtouch进行预读取">2.2 使用vmtouch进行预读取</h4><p>下面将进入 <a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> ,这个小工具能够读取文件并锁定到内存当中，这样后续的I/O请求能够从缓存当中读取它们，这样就减少了对磁盘的访问请求<br>在这里我们的访问模式是这样的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">7</span>]<span class="comment"># vmtouch -t /var/lib/ceph/osd/ceph-7/current/meta/ /var/lib/ceph/osd/ceph-7/current/omap/</span></span><br><span class="line">          Files: <span class="number">618</span></span><br><span class="line">     Directories: <span class="number">6</span></span><br><span class="line">   Touched Pages: <span class="number">3972</span> (<span class="number">15</span>M)</span><br><span class="line">         Elapsed: <span class="number">0.009621</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>关于这个vmtouch很好使用也很强大，可以使用  <code>vmtouch -L</code> 将数据锁定到内存当中去，这里用 <code>-t</code> 也可以，使用 <code>-v</code>  参数能打印更多详细的信息,这个效果有多大？这个原作者的效果很好，我的环境太小，看不出太多的效果，但是从原理上看，应该是会有用的，我的读取过程跟原作者的读取过程有一定的差别，作者的数据库文件是 <code>ldb</code> ，我的环境是 <code>sst</code>，并且作者的压力应该是很大的情况下的，我的环境较小</p>
<h3 id="三、判断是否有作用">三、判断是否有作用</h3><p>一个很好的衡量的方法就是看启动过程当中的 <code>peering</code> 的阶段的长度,<code>peering</code>状态是osd做相互的协调的，PG的请求在这个时候是无法响应的，理想状况下这个过程会很快，无法察觉，如果集群集群处于高负载或者过载状态，这个持续的时间就会很久，然后关闭一个OSD，然后等待一分钟，以便让一部分写入只写到了其他OSD，在down掉的OSD启动后，需要从其他OSD恢复一些数据，然后重新打开，从日志当中，绘制一段时间的<code>peering</code>状态PG的数目，score是统计的所有时间线上<code>peering</code>状态的计数的总和</p>
<p>为了验证这个vmtouch将会减少 <code>peering</code> 的状态,将负载压到略小于集群满载情况</p>
<h4 id="第一个实验是OSD重启（无vmtouch）">第一个实验是OSD重启（无vmtouch）</h4><p><img src="http://static.zybuluo.com/zphj1987/xisprl91q5ljn99rknb1ku8g/vmtouchno.png" alt="vmtouchno.png-22.3kB"><br>可以看到超过30s时，大量的pg是peering状态，导致集群出现缓慢</p>
<h4 id="第二个实验中使用vmtouch预读取OMAP的数据库文件">第二个实验中使用vmtouch预读取OMAP的数据库文件</h4><p><img src="http://static.zybuluo.com/zphj1987/d64ak6lbnzywokpl6j8zkune/vmtouch.png" alt="vmtouch.png-17.9kB"><br>这些 <code>peering</code> 状态并没有消失，但是可以看到有很大的改善 <code>peering</code> 会更早的开始（OMAP已经加载），总体的score也要小很多，这个是一个很不错的结果</p>
<h3 id="四、结论">四、结论</h3><p>根据之前监测到的读取数据的情况，预读取文件，能够有不错的改善，虽然不是完整的解决方案，但是能够帮助改善一个痛点，从长远来看，希望ceph能改进设计，是这个情况消失</p>
<h3 id="五、总结">五、总结</h3><p>本章节里面介绍了两个工具<br><strong>opensnoop</strong><br>这个工具已经存在了很久很久了，也是到现在才看到的，一个用于监控文件的操作，是Gregg 大师的作品，仅仅是一个shell脚本就能实现监控，关键还在于其对操作系统的了解<br><strong>vmtouch</strong><br>这个是将数据加载到内存的，以前关注的是清理内存，其实在某些场景下，能够预加载到内存将会解决很多问题，关键看怎么去用了</p>
<h3 id="六、参考文章">六、参考文章</h3><p><a href="https://blog.flyingcircus.io/2016/03/11/improving-ceph-osd-start-up-behaviour-with-vmtouch/" target="_blank" rel="external">Improving Ceph OSD start-up behaviour with vmtouch</a><br><a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a><br><a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a></p>
<h3 id="七、变更记录">七、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-07</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
