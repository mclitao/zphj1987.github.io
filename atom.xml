<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[止于至善]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2017-09-05T02:42:23.055Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[osd磁盘空间足够无法写入数据的分析与解决]]></title>
    <link href="http://www.zphj1987.com/2017/09/04/osd-has-inode-cannot-write/"/>
    <id>http://www.zphj1987.com/2017/09/04/osd-has-inode-cannot-write/</id>
    <published>2017-09-04T15:06:17.000Z</published>
    <updated>2017-09-05T02:42:23.055Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/full.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个问题的来源是ceph社区里面一个群友的环境出现在85%左右的时候，启动osd报错，然后在本地文件系统当中进行touch文件的时候也是报错，df -i查询inode也是没用多少，使用的也是inode64挂载的，开始的时候排除了配置原因引起的，在ceph的邮件列表里面有一个相同<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2016-October/013929.html" target="_blank" rel="external">问题</a>，也是没有得到解决</p>
<p>看到这个问题比较感兴趣，就花了点时间来解决来定位和解决这个问题，现在分享出来，如果有类似的生产环境，可以提前做好检查预防工作</p>
<h2 id="现象描述">现象描述</h2><p>ceph版本</p>
<blockquote>
<p>[root@lab8107 mnt]# ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br>我复现的环境为这个版本<br><a id="more"></a></p>
</blockquote>
<p>查询使用空间</p>
<p><img src="http://static.zybuluo.com/zphj1987/mqyxyf1tthpo3596f0gt6ujj/image.png" alt="image.png-19.8kB"><br>可以看到空间才使用了54%<br><img src="http://static.zybuluo.com/zphj1987/434y23gzh7mhy3sjzmv9f8wg/image.png" alt="image.png-28kB"><br>可以看到，inode剩余比例很多，而文件确实无法创建</p>
<p>这个时候把一个文件mv出来，然后又可以创建了，并且可以写入比mv出来的文件更大的文件，写完一个无法再写入更多文件了</p>
<p>这里有个初步判断，不是容量写完了，而是文件的个数限制住了</p>
<p>那么来查询下文件系统的inode还剩余多少，xfs文件系统的inode是动态分配的，我们先检查无法写入的文件系统的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs_db -r -c <span class="string">"sb 0"</span> -c <span class="string">"p"</span> -c <span class="string">"freesp -s"</span> /dev/sdb1|grep ifree</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/eqn4pcu0bj7rz5bn2rfum7pj/image.png" alt="image.png-5.1kB"><br>可以看到剩余的inode确实为0，这里确实是没有剩余inode了，所以通过df -i来判断inode是否用完并不准确，那个是已经使用值与理论值的相除的结果</p>
<p>查询xfs碎片，也是比例很低</p>
<h2 id="定位问题">定位问题</h2><p>首先查看xfs上面的数据结构<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs_db -r -c <span class="string">"sb 0"</span> -c <span class="string">"p"</span> -c <span class="string">"freesp -s "</span> /dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/2a48824pncg1g9up7n1npud3/image.png" alt="image.png-13.7kB"></p>
<p>上面的输出结果这里简单解释一下，这里我也是反复比对和查看资料才理解这里的意思，这里有篇<a href="https://www.novell.com/support/kb/doc.php?id=7014320" target="_blank" rel="external">novell</a>的资料有提到这个，这里我再拿一个刚刚格式化完的分区结果来看下<br><img src="http://static.zybuluo.com/zphj1987/p1c3mb2e5kfypt4kw2ow9tm6/image.png" alt="image.png-14.3kB"></p>
<p>这里用我自己的理解来描述下，这个extents的剩余数目是动态变化的，刚分完区的那个，有4个1048576-1220608左右的逻辑区间，而上面的无法写入数据的数据结构，剩下的extent的平均大小为22个block，而这样的blocks总数有1138886个，占总体的99.85，也就是剩余的空间的的extents所覆盖的区域全部是16个block到31个block的这种空洞，相当于蛋糕被切成很多小块了，大的都拿走了，剩下的总量还很多，但是都是很小的碎蛋糕，所以也没法取了</p>
<p>默认来说inode chunk 为64 ，也就是需要64*inodesize的存储空间来存储inode，这个剩下的空间已经不够分配了</p>
<h2 id="解决办法">解决办法</h2><p>下个段落会讲下为什么会出现上面的情况，现在先说解决办法，把文件mv出来，然后mv进去，这个是在其他场景下的一个解决方法，这个操作要小心，因为有扩展属性，操作不小心会弄掉了，这里建议用另外一个办法xfs_dump的方法</p>
<p>我的环境比较小，20G的盘，如果盘大就准备大盘,这里是验证是否可行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfsdump -L osd0 -M osd0 <span class="operator">-f</span> /mnt/osd0 /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>还原回去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ceph-<span class="number">0</span>]<span class="comment"># xfsrestore -f /mnt/osd0 /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">xfsrestore: using file dump (drive_simple) strategy</span><br><span class="line">xfsrestore: version <span class="number">3.1</span>.<span class="number">4</span> (dump format <span class="number">3.0</span>) - <span class="built_in">type</span> ^C <span class="keyword">for</span> status and control</span><br><span class="line">xfsrestore: ERROR: unable to create /var/lib/ceph/osd/ceph-<span class="number">0</span>/xfsrestorehousekeepingdir: No space left on device</span><br><span class="line">xfsrestore: Restore Status: ERROR</span><br></pre></td></tr></table></figure></p>
<p>直接还原还是会有问题,没有可以写的地方了，这里因为已经dump了一份，这里就mv pg的数据目录出去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mv /var/lib/ceph/osd/ceph-<span class="number">0</span>/current/ /mnt</span><br></pre></td></tr></table></figure></p>
<p>开始还原<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfsrestore  -o <span class="operator">-f</span> /mnt/osd0 /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>还原以后如果有权限需要处理的就处理下权限，先检查下文件系统的数据结构<br><img src="http://static.zybuluo.com/zphj1987/r8fmdzz923pju8p48gqq0ma3/image.png" alt="image.png-19.6kB"><br>可以看到数据结构已经很理想了<br>然后启动osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-osd@<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>然后检查下数据是不是都可以正常写进去了</p>
<ul>
<li>如果出现了上面的空间已经满了的情况，处理的时候需要注意</li>
<li>备份好数据</li>
<li>单个盘进行处理</li>
<li>备份的数据先保留好以防万一</li>
<li>启动好了后，验证下集群的状态后再继续，可以尝试get下数据检查数据</li>
</ul>
<h2 id="为什么会出现这样">为什么会出现这样</h2><p>我们在本地文件系统里面连续写100个文件<br>准备一个a文件里面有每行150个a字符，700行，这个文件大小就是100K<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 <span class="built_in">test</span>]<span class="comment"># seq 100|xargs -i dd if=a of=a&#123;&#125; bs=100K count=1</span></span><br></pre></td></tr></table></figure></p>
<p>检查文件的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 <span class="built_in">test</span>]<span class="comment"># seq 100|xargs -i xfs_bmap -v a&#123;&#125; |less</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/cnx9c7dwm2vm1njym9c2ogc2/image.png" alt="image.png-47.1kB"></p>
<p>大部分情况下这个block的分配是连续的</p>
<p>先检查下当前的数据结构<br><img src="http://static.zybuluo.com/zphj1987/9065j88etksn793ewezr6fh3/image.png" alt="image.png-30.8kB"></p>
<p>我们把刚刚的100个对象put到集群里面去，监控下集群的数据目录的写入情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">inotifywait -m --timefmt <span class="string">'%Y %B %d %H:%M:%S'</span> --format <span class="string">'%T %w %e %f'</span> -r -m /var/lib/ceph/osd/ceph-<span class="number">0</span>/</span><br></pre></td></tr></table></figure></p>
<p>put数据进去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `ls ./`;<span class="keyword">do</span> rados -p rbd put <span class="variable">$a</span> <span class="variable">$a</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/l2bfn1hg02ot8oiz1bkpj2n4/image.png" alt="image.png-53.7kB"><br><img src="http://static.zybuluo.com/zphj1987/7myxv8qwe9cjrybah3gnoj5v/image.png" alt="image.png-64.2kB"><br>查看对象的数据，里面并没有连续起来，并且写入的数据的方式是:<br>打开文件，设置扩展属性，填充内容，设置属性，关闭，很多并发在一起做</p>
<p>写完的数据结构<br><img src="http://static.zybuluo.com/zphj1987/9njqf9rlqfd8mfefub9sqp39/image.png" alt="image.png-30.9kB"></p>
<p>结果就是在100K这个数据模型下，会产生很多小的block空隙，最后就是无法写完文件的情况，这里产生空隙并不是很大的问题，问题是这里剩下的空隙无法完成inode的动态分配的工作，这里跟一个格式化选项的变化有关</p>
<p>准备一个集群<br>然后写入(一直写)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd bench -b <span class="number">100</span>K <span class="number">6000</span> write --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>就可以必现这个问题，可以看到上面的从16-31 block的区间从 12 extents涨到了111 extents</p>
<h2 id="解决办法-1">解决办法</h2><p>用deploy在部署的时候默认的格式化参数为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">command</span>_check_call: Running <span class="built_in">command</span>: /usr/sbin/mkfs -t xfs <span class="operator">-f</span> -i size=<span class="number">2048</span> -- /dev/sdb1</span><br></pre></td></tr></table></figure></p>
<p>这个isize设置的是2048，这个在后面剩余的空洞比较小的时候就无法写入新的数据了，所以在ceph里面存储100K这种小文件的场景的时候，把mkfs.xfs的isize改成默认的256就可以提前避免这个问题<br>修改 /usr/lib/python2.7/site-packages/ceph_disk/main.py的256行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs=[</span><br><span class="line">    <span class="comment"># xfs insists on not overwriting previous fs; even if we wipe</span></span><br><span class="line">    <span class="comment"># partition table, we often recreate it exactly the same way,</span></span><br><span class="line">    <span class="comment"># so we'll see ghosts of filesystems past</span></span><br><span class="line">    <span class="string">'-f'</span>,</span><br><span class="line">    <span class="string">'-i'</span>, <span class="string">'size=2048'</span>,</span><br><span class="line">],</span><br></pre></td></tr></table></figure></p>
<p>改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="string">'-i'</span>, <span class="string">'size=256'</span>,</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/crdjd24yzed9s0tdh8og3d1e/image.png" alt="image.png-24.4kB"><br>这个地方检查下是不是对的，然后就可以避免这个问题了，可以测试下是不是一直可以写到很多，我的这个测试环境写到91%还没问题</p>
<h2 id="总结">总结</h2><p>在特定的数据写入模型下，可能出现一些可能无法预料的问题，而参数的改变可能也没法覆盖所有场景，本篇就是其中的一个比较特殊的问题，定位好问题，在遇到的时候能够解决，或者提前避免掉</p>
<h2 id="后续">后续</h2><p>在升级了内核到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># uname  -a</span></span><br><span class="line">Linux lab8107 <span class="number">4.13</span>.<span class="number">0</span>-<span class="number">1</span>.el7.elrepo.x86_64 <span class="comment">#1 SMP Sun Sep 3 19:07:24 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux</span></span><br></pre></td></tr></table></figure></p>
<p>升级xfsprogs到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># rpm -qa|grep xfsprogs</span></span><br><span class="line">xfsprogs-<span class="number">4.12</span>.<span class="number">0</span>-<span class="number">4</span>.el7.centos.x86_64</span><br></pre></td></tr></table></figure></p>
<p>重新部署osd，还是一样的isize=2048，一样的写入模型<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># df -h /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sdb1       <span class="number">9.4</span>G  <span class="number">9.0</span>G  <span class="number">395</span>M  <span class="number">96</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">meta_uuid = <span class="number">00000000</span>-<span class="number">0000</span>-<span class="number">0000</span>-<span class="number">0000</span>-<span class="number">000000000000</span></span><br><span class="line">   from      to extents  blocks    pct</span><br><span class="line">      <span class="number">1</span>       <span class="number">1</span>     <span class="number">545</span>     <span class="number">545</span>   <span class="number">0.50</span></span><br><span class="line">      <span class="number">2</span>       <span class="number">3</span>     <span class="number">665</span>    <span class="number">1666</span>   <span class="number">1.52</span></span><br><span class="line">      <span class="number">4</span>       <span class="number">7</span>    <span class="number">1624</span>    <span class="number">8927</span>   <span class="number">8.12</span></span><br><span class="line">      <span class="number">8</span>      <span class="number">15</span>    <span class="number">1853</span>   <span class="number">19063</span>  <span class="number">17.34</span></span><br><span class="line">     <span class="number">16</span>      <span class="number">31</span>      <span class="number">19</span>     <span class="number">352</span>   <span class="number">0.32</span></span><br><span class="line">   <span class="number">4096</span>    <span class="number">8191</span>       <span class="number">1</span>    <span class="number">7694</span>   <span class="number">7.00</span></span><br><span class="line">  <span class="number">16384</span>   <span class="number">32767</span>       <span class="number">3</span>   <span class="number">71659</span>  <span class="number">65.20</span></span><br><span class="line">total free extents <span class="number">4710</span></span><br><span class="line">total free blocks <span class="number">109906</span></span><br><span class="line">average free extent size <span class="number">23.3346</span></span><br><span class="line">[root@lab8107 ~]<span class="comment"># xfs_db -r -c "sb 0" -c "p" -c "freesp -s " /dev/sdb1</span></span><br></pre></td></tr></table></figure>
<p>可以看到已经很少的稀疏空间了，留下比较大的空间，这个地方应该是优化了底层数据存储的算法</p>
<p>另外，xfs的inode是动态分配的,xfs官方也考虑到了这个可能空洞太多无法分配inode问题，这个是最新的mkfs.xfs的man page<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sparse[=value]</span><br><span class="line">  Enable sparse inode chunk allocation. The value is either <span class="number">0</span> or <span class="number">1</span>, with <span class="number">1</span> signifying that sparse allocation is enabled.  If  the value  is omitted, <span class="number">1</span> is assumed. Sparse inode allocation is disabled by default. This feature is only available <span class="keyword">for</span> filesystems formatted with -m crc=<span class="number">1</span>.</span><br><span class="line">  </span><br><span class="line">   When enabled, sparse inode allocation allows the filesystem to allocate smaller than the  standard  <span class="number">64</span>-inode  chunk  when  free space  is  severely  limited. This feature is useful <span class="keyword">for</span> filesystems that might fragment free space over time such that no free extents are large enough to accommodate a chunk of <span class="number">64</span> inodes. Without this feature enabled, inode allocations can fail with out of space errors under severe fragmented free space conditions.</span><br></pre></td></tr></table></figure></p>
<p>是以64个inode为chunk来进行动态分配的，应该是有两个chunk，也就是动态查询看到的是128个inode以下，在更新到最新的版本以后，因为已经没有那么多空洞了，所以即使在没开这个稀疏inode的情况下，ceph的小文件也能够把磁盘写满</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-04</td>
</tr>
<tr>
<td style="text-align:center">增加更新内核和xfsprogs的验证</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-05</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/full.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个问题的来源是ceph社区里面一个群友的环境出现在85%左右的时候，启动osd报错，然后在本地文件系统当中进行touch文件的时候也是报错，df -i查询inode也是没用多少，使用的也是inode64挂载的，开始的时候排除了配置原因引起的，在ceph的邮件列表里面有一个相同<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2016-October/013929.html">问题</a>，也是没有得到解决</p>
<p>看到这个问题比较感兴趣，就花了点时间来解决来定位和解决这个问题，现在分享出来，如果有类似的生产环境，可以提前做好检查预防工作</p>
<h2 id="现象描述">现象描述</h2><p>ceph版本</p>
<blockquote>
<p>[root@lab8107 mnt]# ceph -v<br>ceph version 10.2.9 (2ee413f77150c0f375ff6f10edd6c8f9c7d060d0)<br>我复现的环境为这个版本<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[为什么关不掉所有的OSD]]></title>
    <link href="http://www.zphj1987.com/2017/08/21/why-can-not-stop-allosd/"/>
    <id>http://www.zphj1987.com/2017/08/21/why-can-not-stop-allosd/</id>
    <published>2017-08-21T05:39:09.000Z</published>
    <updated>2017-08-21T06:03:03.343Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>碰到一个cepher问了一个问题：</p>
<blockquote>
<p>为什么我的OSD关闭到最后有92个OSD无法关闭,总共的OSD有300个左右</p>
</blockquote>
<p>想起来在很久以前帮人处理过一次问题，当时环境是遇上了一个BUG，需要升级到新版本进行解决，然后当时我来做操作，升级以后，发现osd无法启动，进程在，状态无法更新，当时又回滚回去，就可以了，当时好像是K版本升级到J版本，想起来之前看过这个版本里面有数据结构的变化，需要把osd全部停掉以后才能升级，然后就stop掉所有osd，当时发现有的osd还是无法stop，然后就手动去标记了，然后顺利升级<br><a id="more"></a><br>今天这个现象应该跟当时是一个问题，然后搜索了一番参数以后，最后定位在确实是参数进行了控制</p>
<h2 id="实践">实践</h2><p>我的一个8个osd的单机环境，对所有OSD进行stop以后就是这个状态，还有2个状态无法改变<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">49</span>ee8a7f-fb7c-<span class="number">4239</span><span class="operator">-a</span>4b7-acf0bc37430d</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">295</span> pgs are stuck inactive <span class="keyword">for</span> more than <span class="number">300</span> seconds</span><br><span class="line">            <span class="number">295</span> pgs stale</span><br><span class="line">            <span class="number">295</span> pgs stuck stale</span><br><span class="line">            too many PGs per OSD (<span class="number">400</span> &gt; max <span class="number">300</span>)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e77: <span class="number">8</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span>; <span class="number">178</span> remapped pgs</span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v296: <span class="number">400</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">76440</span> kB used, <span class="number">548</span> GB / <span class="number">548</span> GB avail</span><br><span class="line">                 <span class="number">295</span> stale+active+clean</span><br><span class="line">                 <span class="number">105</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>看下这组参数：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_osd_min_up_ratio = <span class="number">0.3</span></span><br><span class="line">mon_osd_min_<span class="keyword">in</span>_ratio = <span class="number">0.3</span></span><br></pre></td></tr></table></figure></p>
<p>我们修改成0 后再测试</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mon_osd_min_up_ratio = <span class="number">0</span></span><br><span class="line">mon_osd_min_<span class="keyword">in</span>_ratio = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>停止进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<p>查看状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">49</span>ee8a7f-fb7c-<span class="number">4239</span><span class="operator">-a</span>4b7-acf0bc37430d</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">48</span> pgs are stuck inactive <span class="keyword">for</span> more than <span class="number">300</span> seconds</span><br><span class="line">            <span class="number">85</span> pgs degraded</span><br><span class="line">            <span class="number">15</span> pgs peering</span><br><span class="line">            <span class="number">400</span> pgs stale</span><br><span class="line">            <span class="number">48</span> pgs stuck inactive</span><br><span class="line">            <span class="number">48</span> pgs stuck unclean</span><br><span class="line">            <span class="number">85</span> pgs undersized</span><br><span class="line">            <span class="number">8</span>/<span class="number">8</span> <span class="keyword">in</span> osds are down</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e86: <span class="number">8</span> osds: <span class="number">0</span> up, <span class="number">8</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v310: <span class="number">400</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">286</span> MB used, <span class="number">2193</span> GB / <span class="number">2194</span> GB avail</span><br><span class="line">                 <span class="number">300</span> stale+active+clean</span><br><span class="line">                  <span class="number">85</span> stale+undersized+degraded+peered</span><br><span class="line">                  <span class="number">15</span> stale+peering</span><br></pre></td></tr></table></figure></p>
<p>可以看到状态已经可以正常全部关闭了</p>
<h2 id="分析">分析</h2><p>这里不清楚官方做这个的理由，个人推断是这样的，默认的副本为3，那么在集群有三分之二的OSD都挂掉了以后，再出现OSD挂掉的情况下，这个集群其实就是一个废掉的状态的集群，而这个时候，还去触发down和out，对于环境来说已经是无效的操作了，触发的迁移也属于无效的迁移了，这个时候保持一个最终的可用的osdmap状态，对于整个环境的恢复也有一个基准点</p>
<p>在Luminous版本中已经把这个参数改成</p>
<blockquote>
<p>mon_osd min_up_ratio = 0.3<br>mon_osd_min_in_ratio = 0.75</p>
</blockquote>
<p>来降低其他异常情况引起的down，来避免过量的迁移</p>
<h2 id="总结">总结</h2><p>本篇就是一个参数的实践</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-21</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>碰到一个cepher问了一个问题：</p>
<blockquote>
<p>为什么我的OSD关闭到最后有92个OSD无法关闭,总共的OSD有300个左右</p>
</blockquote>
<p>想起来在很久以前帮人处理过一次问题，当时环境是遇上了一个BUG，需要升级到新版本进行解决，然后当时我来做操作，升级以后，发现osd无法启动，进程在，状态无法更新，当时又回滚回去，就可以了，当时好像是K版本升级到J版本，想起来之前看过这个版本里面有数据结构的变化，需要把osd全部停掉以后才能升级，然后就stop掉所有osd，当时发现有的osd还是无法stop，然后就手动去标记了，然后顺利升级<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于scrub的详细分析和建议]]></title>
    <link href="http://www.zphj1987.com/2017/08/19/about-scrub-suggestion/"/>
    <id>http://www.zphj1987.com/2017/08/19/about-scrub-suggestion/</id>
    <published>2017-08-19T15:08:56.000Z</published>
    <updated>2017-08-21T06:02:44.778Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/scrub.png" alt="scrub"><br></center>

<h2 id="前言">前言</h2><p>关于scrub这块一直想写一篇文章的，这个在很久前，就做过一次测试，当时是看这个scrub到底有多大的影响，当时看到的是磁盘读占很高，启动deep-scrub后会有大量的读,前端可能会出现 slow request,这个是当时测试看到的现象，一个比较简单的处理办法就是直接给scrub关掉了，当然关掉了就无法检测底层到底有没有对象不一致的问题<br>关于这个scrub生产上是否开启，仁者见仁，智者见智，就是选择的问题了，这里不做讨论，个人觉得开和关都有各自的道理，本篇是讲述的如果想开启的情况下如何把scrub给控制住<br><a id="more"></a><br>最近在ceph群里看到一段大致这样的讨论：</p>
<blockquote>
<p>scrub是个坑<br>小文件多的场景一定要把scrub关掉<br>单pg的文件量达到一定规模，scrub一开就会有slow request<br>这个问题解决不了</p>
</blockquote>
<p>上面的说法有没有问题呢？在一般情况下来看，确实如此，但是我们是否能尝试去解决下这个问题，或者缓解下呢？那么我们就来尝试下</p>
<h2 id="scrub的一些追踪">scrub的一些追踪</h2><p>下面的一些追踪并不涉及代码，仅仅从配置和日志的观测来看看scrub到底干了什么</p>
<h3 id="环境准备">环境准备</h3><p>我的环境为了便于观测，配置的是一个pg的存储池，然后往这个pg里面put了100个对象，然后对这个pg做deep-scrub，deep-scrub比scrub对磁盘的压力要大些，所以本篇主要是去观测的deep-scrub</p>
<h4 id="开启对pg目录的访问的监控">开启对pg目录的访问的监控</h4><p>使用的是inotifywait，我想看下deep-scrub的时候，pg里面的对象到底接收了哪些请求</p>
<p>inotifywait -m 1.0_head<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a16__head_8FA46F40__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a16__head_8FA46F40__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a39__head_621FD720__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a39__head_621FD720__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a30__head_655287E0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a30__head_655287E0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a91__head_B02EE3D0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a91__head_B02EE3D0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a33__head_9E9E3E30__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a33__head_9E9E3E30__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a92__head_6AFC6B30__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a92__head_6AFC6B30__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a22__head_AC48AAB0__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a22__head_AC48AAB0__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a42__head_76B90AC8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a42__head_76B90AC8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a5__head_E5A1A728__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a5__head_E5A1A728__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a34__head_4D9ABA68__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a34__head_4D9ABA68__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a69__head_7AF2B6E8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a69__head_7AF2B6E8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a95__head_BD3695B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a95__head_BD3695B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a67__head_6BCD37B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a67__head_6BCD37B8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a10__head_F0F08AF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a10__head_F0F08AF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a3__head_88EF0BF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a3__head_88EF0BF8__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a82__head_721BC094__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a82__head_721BC094__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a48__head_27A729D4__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a48__head_27A729D4__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a36__head_F63E6AF4__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a36__head_F63E6AF4__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a29__head_F06D540C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a29__head_F06D540C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a31__head_AC83164C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a31__head_AC83164C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a59__head_884F9B6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a59__head_884F9B6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a58__head_06954F6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a58__head_06954F6C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a55__head_2A42E61C__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a55__head_2A42E61C__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a90__head_1B88FEDC__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a90__head_1B88FEDC__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a100__head_C29E0C42__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a100__head_C29E0C42__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a15__head_87123BE2__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a15__head_87123BE2__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a23__head_AABFFB92__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a23__head_AABFFB92__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a41__head_4EA9A5D2__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a41__head_4EA9A5D2__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a85__head_83760D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a85__head_83760D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a72__head_8A105D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a72__head_8A105D72__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a60__head_5536480A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a60__head_5536480A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a73__head_F1819D0A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a73__head_F1819D0A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a78__head_6929D12A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a78__head_6929D12A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a57__head_2C43153A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a57__head_2C43153A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a1__head_51903B7A__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a1__head_51903B7A__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a53__head_DF95B716__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a53__head_DF95B716__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a13__head_E09E0896__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a13__head_E09E0896__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a27__head_7ED31896__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a27__head_7ED31896__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a43__head_7052A656__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a43__head_7052A656__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a28__head_E6257CD6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a28__head_E6257CD6__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a35__head_ACABD736__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a35__head_ACABD736__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a54__head_B9482876__1</span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_WRITE,CLOSE a12__head_14D7ABC6__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a54__head_B9482876__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a4__head_F12ACA76__1</span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_WRITE,CLOSE a63__head_9490B166__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a4__head_F12ACA76__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a84__head_B033038E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a84__head_B033038E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a19__head_D6A64F9E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a19__head_D6A64F9E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a93__head_F54E757E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a93__head_F54E757E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a7__head_1F08F77E__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a7__head_1F08F77E__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a9__head_635C6201__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a9__head_635C6201__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a11__head_12780121__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a11__head_12780121__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a50__head_5E524321__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a50__head_5E524321__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a75__head_27E1CB21__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a75__head_27E1CB21__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a21__head_69ACD1A1__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a21__head_69ACD1A1__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a25__head_698E7751__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a25__head_698E7751__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a44__head_57E29949__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a44__head_57E29949__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a66__head_944E79C9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a66__head_944E79C9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a52__head_DAC6BF29__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a52__head_DAC6BF29__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a14__head_295EA1A9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a14__head_295EA1A9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a70__head_62941259__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a70__head_62941259__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a18__head_53B48959__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a18__head_53B48959__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a17__head_7D103759__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a17__head_7D103759__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a6__head_9505BEF9__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a6__head_9505BEF9__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a77__head_88A7CC25__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a77__head_88A7CC25__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a37__head_141AFE65__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a37__head_141AFE65__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a74__head_90DAAD15__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a74__head_90DAAD15__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a32__head_B7957195__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a32__head_B7957195__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a45__head_CCCFB5D5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a45__head_CCCFB5D5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a24__head_3B937275__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a24__head_3B937275__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a26__head_2AB240F5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a26__head_2AB240F5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a89__head_8E387EF5__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a89__head_8E387EF5__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a80__head_6FEFE78D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a80__head_6FEFE78D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a51__head_0BCC72CD__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a51__head_0BCC72CD__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a71__head_88F4796D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a71__head_88F4796D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a88__head_B0A64FED__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a88__head_B0A64FED__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a8__head_F885EA9D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a8__head_F885EA9D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a83__head_1322679D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a83__head_1322679D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a76__head_B8285A7D__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a76__head_B8285A7D__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a94__head_D3BBB683__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a94__head_D3BBB683__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a46__head_E2C6C983__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a46__head_E2C6C983__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a56__head_A1E888C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a56__head_A1E888C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a99__head_DD3B45C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a99__head_DD3B45C3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a79__head_AC19FC13__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a79__head_AC19FC13__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a81__head_BC0AFFF3__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a81__head_BC0AFFF3__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a64__head_C042B84B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a64__head_C042B84B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a97__head_29054B4B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a97__head_29054B4B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a96__head_BAAC0DCB__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a96__head_BAAC0DCB__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a62__head_84A40AAB__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a62__head_84A40AAB__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a98__head_C15FD53B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a98__head_C15FD53B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a87__head_12F9237B__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a87__head_12F9237B__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a2__head_E2983C17__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a2__head_E2983C17__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a20__head_7E477A77__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a20__head_7E477A77__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a49__head_3ADEC577__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a49__head_3ADEC577__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a61__head_C860ABF7__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a61__head_C860ABF7__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a68__head_BC5C8F8F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a68__head_BC5C8F8F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a38__head_78AE322F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a38__head_78AE322F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a65__head_7EE57AEF__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a65__head_7EE57AEF__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a47__head_B6C48D1F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a47__head_B6C48D1F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a86__head_7FB2C85F__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a86__head_7FB2C85F__1</span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ CLOSE_NOWRITE,CLOSE,ISDIR </span><br><span class="line"><span class="number">1.0</span>_head/ OPEN a40__head_5F0404DF__1</span><br><span class="line"><span class="number">1.0</span>_head/ ACCESS a40__head_5F0404DF__1</span><br></pre></td></tr></table></figure></p>
<p>在给osd.0开启debug_osd=20后观测chunky相关的日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># cat ceph-osd.0.log |grep chunky:1|grep handle_replica_op</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.262448</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2696</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">00000000</span>::::head,end:<span class="number">1</span>:<span class="number">42307943</span>:::a100:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.294637</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2694</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">42307943</span>:::a100:<span class="number">0</span>,end:<span class="number">1</span>:<span class="number">80463</span>ac6:::a9:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.320986</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2690</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:<span class="number">80463</span>ac6:::a9:<span class="number">0</span>,end:<span class="number">1</span>:b7f2650d:::a88:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.337646</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2700</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:b7f2650d:::a88:<span class="number">0</span>,end:<span class="number">1</span>:fb2020fa:::a40:<span class="number">0</span>,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">18</span> <span class="number">23</span>:<span class="number">50</span>:<span class="number">40.373227</span> <span class="number">7</span>f2ac583c700 <span class="number">10</span> osd.<span class="number">0</span> <span class="number">26</span> handle_replica_op replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:22'</span><span class="number">2636</span>,epoch:<span class="number">26</span>,start:<span class="number">1</span>:fb2020fa:::a40:<span class="number">0</span>,end:MAX,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) v6 epoch <span class="number">26</span></span><br></pre></td></tr></table></figure></p>
<p>截取关键部分看下，如图<br><img src="http://static.zybuluo.com/zphj1987/2zxne6hdwre1fnrqd5xiabzu/image.png" alt="a100"><br>我们看下上面的文件访问监控里面这些对象在什么位置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">25</span>:<span class="number">1.0</span>_head/ ACCESS a100__head_C29E0C42__1</span><br><span class="line"><span class="number">50</span>:<span class="number">1.0</span>_head/ ACCESS a9__head_635C6201__1</span><br><span class="line"><span class="number">75</span>:<span class="number">1.0</span>_head/ ACCESS a88__head_B0A64FED__1</span><br><span class="line"><span class="number">100</span>:<span class="number">1.0</span>_head/ ACCESS a40__head_5F0404DF__1</span><br></pre></td></tr></table></figure></p>
<p>看上去是不是很有规律，这个地方在ceph里面会有个chunk的概念，在做scrub的时候，ceph会对这个chunk进行加锁，这个可以在很多地方看到这个，这个也就是为什么有slow request，并不一定是你的磁盘慢了，而是加了锁，就没法读的</p>
<blockquote>
<p>osd scrub chunk min</p>
<p>Description:    The minimal number of object store chunks to scrub during single operation. Ceph blocks writes to single chunk during scrub.<br>Type:    32-bit Integer<br>Default:    5</p>
</blockquote>
<p>从配置文件上面看说是会锁住写，没有提及读的锁定的问题，那么我们下面验证下这个问题，到底deep-scrub，是不是会引起读的slow request</p>
<p>上面的环境100个对象，现在把100个对象的大小调整为100M一个，并且chunk设置为100个对象的，也就是我把我这个环境所有的对象认为是一个大的chunk，然后去用rados读取这个对象，来看下会发生什么</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">100</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">100</span></span><br></pre></td></tr></table></figure>
<p>使用ceph -w监控<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">26.045032</span> mon.<span class="number">0</span> [INF] pgmap v377: <span class="number">1</span> pgs: <span class="number">1</span> active+clean+scrubbing+deep; <span class="number">10000</span> MB data, <span class="number">30103</span> MB used, <span class="number">793</span> GB / <span class="number">822</span> GB avail</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">17.540413</span> osd.<span class="number">0</span> [WRN] <span class="number">1</span> slow requests, <span class="number">1</span> included below; oldest blocked <span class="keyword">for</span> &gt; <span class="number">30.398705</span> secs</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">19</span>:<span class="number">17.540456</span> osd.<span class="number">0</span> [WRN] slow request <span class="number">30.398705</span> seconds old, received at <span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">18</span>:<span class="number">47.141483</span>: replica scrub(pg: <span class="number">1.0</span>,from:<span class="number">0</span><span class="string">'0,to:26'</span><span class="number">5200</span>,epoch:<span class="number">32</span>,start:<span class="number">1</span>:<span class="number">00000000</span>::::head,end:MAX,chunky:<span class="number">1</span>,deep:<span class="number">1</span>,seed:<span class="number">4294967295</span>,version:<span class="number">6</span>) currently reached_pg</span><br></pre></td></tr></table></figure></p>
<p>我从deep scrub 一开始就进行a40对象的get rados -p rbd get a40 a40，直接就卡着不返回，在pg内对象不变的情况下，对pg做scrub的顺序是不变的，我专门挑了我这个scrub顺序下最后一个scrub的对象来做get，还是出现了slow request ，这个可以证明上面的推断，也就是在做scrub的时候，对scub的chunk的对象的读取请求也会卡死，现在我把我的scrub的chunk弄成1看下会发生什么</p>
<p>配置参数改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">1</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a1'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a2'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a3'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a4'</span></span><br><span class="line">watch -n <span class="number">1</span> <span class="string">'rados -p rbd get a9 a5'</span></span><br></pre></td></tr></table></figure>
<p>使用五个请求同时去get a9,循环的去做</p>
<p>然后做deep scrub，这一次并没有出现slow  request 的情况</p>
<h3 id="另外一个重要参数">另外一个重要参数</h3><p>再看看这个参数osd_scrub_sleep = 0</p>
<blockquote>
<p>osd scrub sleep</p>
<p>Description:    Time to sleep before scrubbing next group of chunks. Increasing this value will slow down whole scrub operation while client operations will be less impacted.<br>Type:    Float<br>Default:    0</p>
</blockquote>
<p>可以看到还有scrub group这个概念，从数据上分析这个group 是3，也就是3个chunks<br>我们来设置下</p>
<blockquote>
<p>osd_scrub_sleep = 5</p>
</blockquote>
<p>然后再次做deep-scrub,然后看下日志的内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log |grep be_deep_scrub|awk <span class="string">'&#123;print $1,$2,$28&#125;'</span>|less</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">37.930455</span> <span class="number">1</span>:<span class="number">02</span>f625f1:::a16:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">38.477271</span> <span class="number">1</span>:<span class="number">02</span>f625f1:::a16:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">38.477367</span> <span class="number">1</span>:<span class="number">04</span>ebf846:::a39:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.023952</span> <span class="number">1</span>:<span class="number">04</span>ebf846:::a39:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.024084</span> <span class="number">1</span>:<span class="number">07</span>e14aa6:::a30:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">39.572683</span> <span class="number">1</span>:<span class="number">07</span>e14aa6:::a30:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">44.989551</span> <span class="number">1</span>:<span class="number">0</span>bc7740d:::a91:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">45.556758</span> <span class="number">1</span>:<span class="number">0</span>bc7740d:::a91:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">45.556857</span> <span class="number">1</span>:<span class="number">0</span>c7c7979:::a33:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.109657</span> <span class="number">1</span>:<span class="number">0</span>c7c7979:::a33:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.109768</span> <span class="number">1</span>:<span class="number">0</span><span class="built_in">cd</span>63f56:::a92:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">46.657849</span> <span class="number">1</span>:<span class="number">0</span><span class="built_in">cd</span>63f56:::a92:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.084712</span> <span class="number">1</span>:<span class="number">0</span>d551235:::a22:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.614345</span> <span class="number">1</span>:<span class="number">0</span>d551235:::a22:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">52.614458</span> <span class="number">1</span>:<span class="number">13509</span>d6e:::a42:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">53.158826</span> <span class="number">1</span>:<span class="number">13509</span>d6e:::a42:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">00</span>:<span class="number">48</span>:<span class="number">53.158916</span> <span class="number">1</span>:<span class="number">14</span>e585a7:::a5:head</span><br></pre></td></tr></table></figure></p>
<p>可以看到1s做一个对象的deep-scrub，然后在做了3个对象后就停止了5s</p>
<h3 id="默认情况下的scrub和修改后的对比">默认情况下的scrub和修改后的对比</h3><p>我们来计算下在修改前后的情况对比，我们来模拟pg里面有10000个对象的情况小文件 测试的文件都是1K的，这个可以根据自己的文件模型进行测试</p>
<p>假设是海量对象的场景，那么算下来单pg 1w左右对象左右也算比较多了，我们就模拟10000个对象的场景的deep-scrub<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log |grep be_deep_scrub|awk <span class="string">'&#123;print $1,$2,$28&#125;'</span>|awk <span class="string">'&#123;sub(/.*/,substr($2,1,8),$2); print $0&#125;'</span>|uniq|awk <span class="string">'&#123;a[$1," ",$2]++&#125;END&#123;for (j in a) print j,a[j]|"sort -k 1"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>使用上面的脚本统计每秒scrub的对象数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">33</span> <span class="number">184</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">34</span> <span class="number">236</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">35</span> <span class="number">261</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">36</span> <span class="number">263</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">37</span> <span class="number">229</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">38</span> <span class="number">289</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">39</span> <span class="number">236</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">40</span> <span class="number">258</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">41</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">42</span> <span class="number">238</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">43</span> <span class="number">224</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">44</span> <span class="number">282</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">45</span> <span class="number">254</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">46</span> <span class="number">258</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">47</span> <span class="number">261</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">48</span> <span class="number">233</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">49</span> <span class="number">300</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">50</span> <span class="number">243</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">51</span> <span class="number">257</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">52</span> <span class="number">252</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">53</span> <span class="number">246</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">54</span> <span class="number">313</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">55</span> <span class="number">252</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">56</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">57</span> <span class="number">245</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">58</span> <span class="number">256</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">23</span>:<span class="number">59</span> <span class="number">307</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">00</span> <span class="number">276</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">01</span> <span class="number">310</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">02</span> <span class="number">220</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">03</span> <span class="number">250</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">04</span> <span class="number">313</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">05</span> <span class="number">265</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">06</span> <span class="number">304</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">07</span> <span class="number">262</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">08</span> <span class="number">308</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">09</span> <span class="number">263</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">10</span> <span class="number">293</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">01</span>:<span class="number">24</span>:<span class="number">11</span> <span class="number">42</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到1s 会扫300个对象左右，差不多40s钟就扫完了一个pg，默认25个对象一个trunk</p>
<p>这里可以打个比喻，在一条长为40m的马路上，一个汽车以1m/s速度前进，中间会有人来回穿，如果穿梭的人只有一两个可能没什么问题，但是一旦有40个人在这个区间进行穿梭的时候，可想而知碰撞的概率会有多大了</p>
<p>或者同一个文件被连续请求40次，那么对应到这里就是40个人在同一个位置不停的穿马路，这样撞上的概率是不是非常的大了？</p>
<p>上面说了这么多，那么我想如果整个看下来，应该知道怎么处理了<br>我们看下这样的全部为1的情况下，会出现什么情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_scrub_chunk_min = <span class="number">1</span></span><br><span class="line">osd_scrub_chunk_max = <span class="number">1</span></span><br><span class="line">osd_scrub_sleep = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>这里减少chunk大小，相当于减少上面例子当中汽车的长度，原来25米的大卡车，变成1米的自行车了</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># cat /var/log/ceph/ceph-osd.0.log |grep be_deep_scrub|awk '&#123;print $1,$2,$28&#125;'</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.927440</span> <span class="number">1</span>:<span class="number">0000</span>b488:::a5471:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.931914</span> <span class="number">1</span>:<span class="number">0000</span>b488:::a5471:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.932039</span> <span class="number">1</span>:<span class="number">000</span>fbbcb:::a5667:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.933568</span> <span class="number">1</span>:<span class="number">000</span>fbbcb:::a5667:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.933646</span> <span class="number">1</span>:<span class="number">00134</span>ebd:::a1903:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">21.934972</span> <span class="number">1</span>:<span class="number">00134</span>ebd:::a1903:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.960697</span> <span class="number">1</span>:<span class="number">0018</span>f641:::a2028:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.966653</span> <span class="number">1</span>:<span class="number">0018</span>f641:::a2028:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.966733</span> <span class="number">1</span>:<span class="number">00197</span>a21:::a1463:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967085</span> <span class="number">1</span>:<span class="number">00197</span>a21:::a1463:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967162</span> <span class="number">1</span>:<span class="number">001</span>cb17d:::a1703:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">24.967492</span> <span class="number">1</span>:<span class="number">001</span>cb17d:::a1703:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.972252</span> <span class="number">1</span>:<span class="number">002</span>d911c:::a1585:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.976621</span> <span class="number">1</span>:<span class="number">002</span>d911c:::a1585:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.976740</span> <span class="number">1</span>:<span class="number">00301</span>acf:::a6131:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.977097</span> <span class="number">1</span>:<span class="number">00301</span>acf:::a6131:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.977181</span> <span class="number">1</span>:<span class="number">0039</span>a0a8:::a1840:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">27.979053</span> <span class="number">1</span>:<span class="number">0039</span>a0a8:::a1840:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.983556</span> <span class="number">1</span>:<span class="number">00484881</span>:::a8781:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989098</span> <span class="number">1</span>:<span class="number">00484881</span>:::a8781:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989181</span> <span class="number">1</span>:<span class="number">004</span>f234f:::a4402:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989531</span> <span class="number">1</span>:<span class="number">004</span>f234f:::a4402:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989626</span> <span class="number">1</span>:<span class="number">00531</span>b36:::a5251:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">30.989954</span> <span class="number">1</span>:<span class="number">00531</span>b36:::a5251:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">33.994419</span> <span class="number">1</span>:<span class="number">00584</span>c30:::a3374:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.001296</span> <span class="number">1</span>:<span class="number">00584</span>c30:::a3374:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.001378</span> <span class="number">1</span>:<span class="number">005</span>d6aa5:::a2115:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002174</span> <span class="number">1</span>:<span class="number">005</span>d6aa5:::a2115:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002287</span> <span class="number">1</span>:<span class="number">005</span>e0dfd:::a9945:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">34.002686</span> <span class="number">1</span>:<span class="number">005</span>e0dfd:::a9945:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.005645</span> <span class="number">1</span>:<span class="number">006320</span>f9:::a5207:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011498</span> <span class="number">1</span>:<span class="number">006320</span>f9:::a5207:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011655</span> <span class="number">1</span>:<span class="number">006</span>d32b4:::a7517:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.011998</span> <span class="number">1</span>:<span class="number">006</span>d32b4:::a7517:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.012111</span> <span class="number">1</span>:<span class="number">006</span>dae55:::a4702:head</span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">19</span> <span class="number">16</span>:<span class="number">12</span>:<span class="number">37.012442</span> <span class="number">1</span>:<span class="number">006</span>dae55:::a4702:head</span><br></pre></td></tr></table></figure>
<p>上面从日志里面截取部分的日志，这个是什么意思呢，是每秒钟扫描3个对象，然后休息3s再进行下一个，这个是不是已经把速度压到非常低了？还有上面做测试scrub sleep例子里面好像是1s 会scrub 1个对象，这里怎么就成了1s会scrub 3 个对象了，这个跟scrub的对象大小有关，对象越大，scrub的时间就相对长一点，这个测试里面的对象是1K的，基本算非常小了，也就是1s会扫描3个对象，然后根据你的设置的sleep值等待进入下一组的scrub</p>
<p>在上面的环境下默认每秒钟会对300左右的对象进行scrub，以25个对象的锁定窗口移动，无法写入和读取，而参数修改后每秒有3个对象被scrub，以1个对象的锁定窗口移动，这个单位时间锁定的对象的数目已经降低到一个非常低的程度了，如果你有生产环境又想去开scrub，不妨尝试下降低chunk，增加sleep</p>
<p>这个的影响就是扫描的速度而已，而如果你想加快扫描速度，就去调整sleep参数来控制这个扫描的速度了，这个就不在这里赘述了</p>
<p>本篇讲述的是一个PG上开启deep-scrub以后的影响，默认的是到了最大的intelval以后就会开启自动开启scrub了，所以我建议的是不用系统自带的时间控制，而是自己去分析的scrub的时间戳和对象数目，然后计算好以后，可以是每天晚上，扫描指定个数的PG，然后等一轮全做完以后，中间就是自定义的一段时间的不扫描期，这个可以自己定义，是一个月或者两个月扫一轮都行，这个会在后面单独写一篇文章来讲述这个</p>
<h2 id="总结">总结</h2><p>关于scrub，你需要了解，scrub什么时候会发生，发生以后会对你的osd产生多少的负载，每秒钟会扫描多少对象，如何去降低这个影响，这些问题就是本篇的来源了，很多问题是能从参数上进行解决的，关键是你要知道它们到底在干嘛</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/scrub.png" alt="scrub"><br></center>

<h2 id="前言">前言</h2><p>关于scrub这块一直想写一篇文章的，这个在很久前，就做过一次测试，当时是看这个scrub到底有多大的影响，当时看到的是磁盘读占很高，启动deep-scrub后会有大量的读,前端可能会出现 slow request,这个是当时测试看到的现象，一个比较简单的处理办法就是直接给scrub关掉了，当然关掉了就无法检测底层到底有没有对象不一致的问题<br>关于这个scrub生产上是否开启，仁者见仁，智者见智，就是选择的问题了，这里不做讨论，个人觉得开和关都有各自的道理，本篇是讲述的如果想开启的情况下如何把scrub给控制住<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何测量Ceph OSD内存占用]]></title>
    <link href="http://www.zphj1987.com/2017/08/10/how-to-get-Ceph-OSD-mem-used/"/>
    <id>http://www.zphj1987.com/2017/08/10/how-to-get-Ceph-OSD-mem-used/</id>
    <published>2017-08-10T08:55:41.000Z</published>
    <updated>2017-08-10T09:06:53.554Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newmemory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个工具我第一次看到是在填坑群里面看到，是由研发-北京-蓝星同学分享的，看到比较有趣，就写一篇相关的记录下用法</p>
<p>火焰图里面也可以定位内存方面的问题，那个是通过一段时间的统计，以一个汇总的方式来查看内存在哪个地方可能出了问题<br><a id="more"></a><br>本篇是另外一个工具，这个工具的好处是有很清晰的图表操作，以及基于时间线的统计，下面来看下这个工具怎么使用的</p>
<p>本篇对具体的内存函数的调用占用不会做更具体的分析，这里是提供一个工具的使用方法供感兴趣的研发同学来使用</p>
<h2 id="环境准备">环境准备</h2><p>目前大多数的ceph运行在centos7系列上面，笔者的环境也是在centos7上面，所以以这个举例，其他平台同样可以</p>
<p>需要用到的工具</p>
<ul>
<li>valgrind</li>
<li>massif-visualizer</li>
</ul>
<p>安装valgrind<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install valgrind</span><br></pre></td></tr></table></figure></p>
<p>massif-visualizer是数据可视化的工具，由于并没有centos的发行版本，但是有fedora的版本，从网上看到资料说这个可以直接安装忽略掉需要的依赖即可，我自己跑了下，确实可行</p>
<p>下载massif-visualizer<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget ftp://ftp.pbone.net/mirror/download.fedora.redhat.com/pub/fedora/linux/releases/<span class="number">23</span>/Everything/x86_64/os/Packages/m/massif-visualizer-<span class="number">0.4</span>.<span class="number">0</span>-<span class="number">6</span>.fc23.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>安装massif-visualizer<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ivh massif-visualizer-<span class="number">0.4</span>.<span class="number">0</span>-<span class="number">6</span>.fc23.x86_64.rpm  --nodeps</span><br></pre></td></tr></table></figure></p>
<p>不要漏了后面的nodeps</p>
<h2 id="抓取ceph_osd运行时内存数据">抓取ceph osd运行时内存数据</h2><p>停掉需要监控的osd（例如我的是osd.4）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@4</span></span><br></pre></td></tr></table></figure></p>
<p>开始运行监控<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># valgrind --tool=massif /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span></span><br><span class="line">==<span class="number">21522</span>== Massif, a heap profiler</span><br><span class="line">==<span class="number">21522</span>== Copyright (C) <span class="number">2003</span>-<span class="number">2015</span>, and GNU GPL<span class="string">'d, by Nicholas Nethercote</span><br><span class="line">==21522== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info</span><br><span class="line">==21522== Command: /usr/bin/ceph-osd -f --cluster ceph --id 4 --setuser ceph --setgroup ceph</span><br><span class="line">==21522== </span><br><span class="line">==21522== </span><br><span class="line">starting osd.4 at :/0 osd_data /var/lib/ceph/osd/ceph-4 /var/lib/ceph/osd/ceph-4/journal</span><br><span class="line">2017-08-10 16:36:42.395682 a14d680 -1 osd.4 522 log_to_monitors &#123;default=true&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>监控已经开始了,在top下可以看到有这个进程运行，占用cpu还是比较高的，可能是要抓取很多数据的原因<br><img src="http://static.zybuluo.com/zphj1987/yf0kp4qr32mtcmhtock1krnv/image.png" alt="valtop"></p>
<p>等待一段时间后，就可以把之前运行的命令ctrl+C掉</p>
<p>在当前目录下面就会生成一个【massif.out.进程号】的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll massif.out.21522 </span></span><br><span class="line">-rw------- <span class="number">1</span> root root <span class="number">142682</span> Aug <span class="number">10</span> <span class="number">16</span>:<span class="number">39</span> massif.out.<span class="number">21522</span></span><br></pre></td></tr></table></figure></p>
<h2 id="查看截取的数据">查看截取的数据</h2><h3 id="命令行下的查看">命令行下的查看</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ms_print massif.out.21522 |less</span></span><br></pre></td></tr></table></figure>
<p>这个方式是文本方式的查看，也比较方便，自带的文本分析工具，效果如下：<br><img src="http://static.zybuluo.com/zphj1987/6az5gderq4i4jdg0l98bnm8n/image.png" alt="image.png-38kB"><br><img src="http://static.zybuluo.com/zphj1987/hfc1nosugnkx9plc8p9iwvyn/image.png" alt="image.png-94.6kB"></p>
<h3 id="图形界面的查看">图形界面的查看</h3><p>首先在windows上面运行好xmanager-Passive，这个走的x11转发的（也可以用另外一个工具MobaXterm）<br><img src="http://static.zybuluo.com/zphj1987/jqt14e5gakmr9ftwuz3r8g5m/image.png" alt="image.png-4.4kB"><br>运行好了后，直接在xshell命令行运行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># massif-visualizer massif.out.21522 </span></span><br><span class="line">massif-visualizer(<span class="number">22494</span>)/kdeui (kdelibs): Attempt to use QAction <span class="string">"toggleDataTree"</span> with KXMLGUIFactory! </span><br><span class="line">massif-visualizer(<span class="number">22494</span>)/kdeui (kdelibs): Attempt to use QAction <span class="string">"toggleAllocators"</span> with KXMLGUIFactory! </span><br><span class="line">description: <span class="string">"(none)"</span> </span><br><span class="line"><span class="built_in">command</span>: <span class="string">"/usr/bin/ceph-osd -f --cluster ceph --id 4"</span> </span><br><span class="line">time unit: <span class="string">"i"</span> </span><br><span class="line">snapshots: <span class="number">56</span> </span><br><span class="line">peak: snapshot <span class="comment"># 52 after "2.3138e+09i" </span></span><br><span class="line">peak cost: <span class="string">"16.2 MiB"</span>  heap <span class="string">"749.0 KiB"</span>  heap extra <span class="string">"0 B"</span>  stacks</span><br></pre></td></tr></table></figure></p>
<p>然后在windows上面就会弹出下面的<br><img src="http://static.zybuluo.com/zphj1987/inkjtgxe6dw2k2qjjvr4rclx/osdmem.png" alt="osdmem.png-282kB"><br>就可以交互式的查看快照点的内存占用了，然后根据这个就可以进行内存分析了，剩下的工作就留给研发去做了</p>
<h2 id="相关链接">相关链接</h2><p><a href="https://codeday.me/bug/20170415/1699.html" target="_blank" rel="external">linux – 如何测量应用程序或进程的实际内存使用情况？</a></p>
<h2 id="总结">总结</h2><p>只有分析落地到数据层面，这样的分析才是比较精准的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newmemory.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>这个工具我第一次看到是在填坑群里面看到，是由研发-北京-蓝星同学分享的，看到比较有趣，就写一篇相关的记录下用法</p>
<p>火焰图里面也可以定位内存方面的问题，那个是通过一段时间的统计，以一个汇总的方式来查看内存在哪个地方可能出了问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph recover的速度控制]]></title>
    <link href="http://www.zphj1987.com/2017/08/10/Ceph-recover-speed-control/"/>
    <id>http://www.zphj1987.com/2017/08/10/Ceph-recover-speed-control/</id>
    <published>2017-08-10T06:53:12.000Z</published>
    <updated>2017-08-10T09:13:20.965Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>磁盘损坏对于一个大集群来说，可以说是必然发生的事情，即使再小的概率，磁盘量上去，总会坏那么几块盘，这个时候就会触发内部的修复过程，修复就是让不满足副本要求的PG，恢复到满足的情况<br><a id="more"></a><br>一般是踢掉坏盘和增加新盘会触发这个修复过程，或者对磁盘的权重做了修改，也会触发这个迁移的过程，本篇是用剔除OSD的方式来对这个修复的控制做一个探索</p>
<p>大部分场景下要求的是不能影响前端的业务，而加速迁移，忽略迁移影响不在本篇的讨论范围内，本篇将用数据来说明迁移的控制</p>
<p>本次测试在无读写情况下进程的</p>
<h2 id="几个需要用到脚本和命令">几个需要用到脚本和命令</h2><h3 id="磁盘本身的大概速度">磁盘本身的大概速度</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph tell osd.0 bench</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"bytes_written"</span>: <span class="number">1073741824</span>,</span><br><span class="line">    <span class="string">"blocksize"</span>: <span class="number">4194304</span>,</span><br><span class="line">    <span class="string">"bytes_per_sec"</span>: <span class="number">102781897</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>得到的结果为102MB/s</p>
<h3 id="获取osd上pg迁移的对象的脚本">获取osd上pg迁移的对象的脚本</h3><p>OSD的日志需要开启到10，这里采取动态开启的方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph daemon osd.<span class="number">0</span> config <span class="built_in">set</span> debug_osd <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>日志解析的脚本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat  /var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log | awk  <span class="string">'$7=="finish_recovery_op"&amp;&amp;$8=="pg[0.15(" &#123;sub(/.*/,substr($2,1,8),$2); print $0&#125;'</span>|awk <span class="string">'&#123;a[$1," ",$2]++&#125;END&#123;for (j in a) print j,a[j]|"sort -k 1"&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>获取osd.0上的pg0.15的迁移速度<br>运行后的效果如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">33</span> <span class="number">1</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">34</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">35</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">36</span> <span class="number">1</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">37</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">38</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">39</span> <span class="number">1</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">40</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">41</span> <span class="number">1</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">42</span> <span class="number">2</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">08</span>-<span class="number">08</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">43</span> <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h3 id="设置不迁移和恢复迁移">设置不迁移和恢复迁移</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd <span class="built_in">set</span> nobackfill;ceph osd <span class="built_in">set</span> norecover</span><br><span class="line">ceph osd <span class="built_in">unset</span> nobackfill;ceph osd <span class="built_in">unset</span> norecover</span><br></pre></td></tr></table></figure>
<p>获取当前的正在迁移的PG<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump|grep recovering</span></span><br><span class="line">dumped all</span><br><span class="line"><span class="number">3</span>.e         <span class="number">513</span>                  <span class="number">0</span>      <span class="number">978</span>         <span class="number">0</span>       <span class="number">0</span> <span class="number">2151677952</span> <span class="number">513</span>      <span class="number">513</span>    active+recovering+degraded <span class="number">2017</span>-<span class="number">08</span>-<span class="number">07</span> <span class="number">16</span>:<span class="number">40</span>:<span class="number">44.840780</span> <span class="number">118</span><span class="string">'513  332:7367 [2,3]          2  [2,3]              2        0'</span><span class="number">0</span> <span class="number">2017</span>-<span class="number">07</span>-<span class="number">28</span> <span class="number">14</span>:<span class="number">28</span>:<span class="number">53.351664</span>             <span class="number">0</span><span class="string">'0 2017-07-28 14:28:53.351664 </span><br><span class="line">3.2c        522                  0      996         0       0 2189426688 522      522    active+recovering+degraded 2017-08-07 16:40:44.882450 118'</span><span class="number">522</span>  <span class="number">332</span>:<span class="number">1177</span> [<span class="number">3</span>,<span class="number">2</span>]          <span class="number">3</span>  [<span class="number">3</span>,<span class="number">2</span>]              <span class="number">3</span>    <span class="number">118</span><span class="string">'522 2017-07-29 16:21:56.398682             0'</span><span class="number">0</span> <span class="number">2017</span>-<span class="number">07</span>-<span class="number">28</span> <span class="number">14</span>:<span class="number">28</span>:<span class="number">53.351664</span></span><br></pre></td></tr></table></figure></p>
<p>过滤下输出结果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump|grep recovering|awk '&#123;print $1,$2,$4,$10,$15,$16,$17,$18&#125;'</span></span><br><span class="line">dumped all <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">0.1</span>d <span class="number">636</span> <span class="number">1272</span> active+recovering+degraded [<span class="number">5</span>,<span class="number">3</span>] <span class="number">5</span> [<span class="number">5</span>,<span class="number">3</span>] <span class="number">5</span></span><br><span class="line"><span class="number">0.14</span> <span class="number">618</span> <span class="number">1236</span> active+recovering+degraded [<span class="number">1</span>,<span class="number">0</span>] <span class="number">1</span> [<span class="number">1</span>,<span class="number">0</span>] <span class="number">1</span></span><br><span class="line"><span class="number">0.15</span> <span class="number">682</span> <span class="number">1364</span> active+recovering+degraded [<span class="number">0</span>,<span class="number">5</span>] <span class="number">0</span> [<span class="number">0</span>,<span class="number">5</span>] <span class="number">0</span></span><br><span class="line"><span class="number">0.35</span> <span class="number">661</span> <span class="number">1322</span> active+recovering+degraded [<span class="number">2</span>,<span class="number">1</span>] <span class="number">2</span> [<span class="number">2</span>,<span class="number">1</span>] <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>动态监控PG的迁移<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">watch -n <span class="number">1</span> <span class="operator">-d</span> <span class="string">"ceph pg dump|grep recovering|awk '&#123;print \$1,\$2,\$4,\$10,\$15,\$16,\$17,\$18&#125;'"</span></span><br></pre></td></tr></table></figure></p>
<p>我们要看PG 0.15的</p>
<h3 id="防止缓存影响">防止缓存影响</h3><p>同步数据然后清空缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sync</span><br><span class="line"><span class="built_in">echo</span> <span class="number">3</span> &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure></p>
<p>重启OSD进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-osd.target</span><br></pre></td></tr></table></figure></p>
<h3 id="磁盘的读写速度">磁盘的读写速度</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dstat -td -D /dev/sdb -o disk.csv</span><br></pre></td></tr></table></figure>
<p>sdb为需要监控的盘</p>
<h2 id="测试的步骤与流程">测试的步骤与流程</h2><p>整个测试需要保证每一次获取数据的过程都近似，这样才能最大程度减少环境对数据的影响</p>
<p>开始需要写入一些测试数据，这个可以用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd bench <span class="number">3600</span> --no-cleanup</span><br></pre></td></tr></table></figure></p>
<p>这个让每个PG上面大概有600-700个object，写入这个数据后就不再写入数据了</p>
<p>每一轮测试步骤如下：</p>
<ol>
<li>恢复集群状态为active+clean</li>
<li>设置nobackfill，norecover</li>
<li>清空缓存</li>
<li>设置需要调整的参数</li>
<li>重启osd进程</li>
<li>停止osd，out osd</li>
<li>观察需要迁移的数据（尽量每次监测同一个PG）</li>
<li>清空日志，设置OSD debug 10</li>
<li>开启监控磁盘脚本</li>
<li>解除设置nobackfill，norecover</li>
<li>动态监控迁移状态，等待指定PG迁移完毕</li>
<li>停止磁盘监控脚本</li>
<li>获取PG迁移的情况，获取磁盘的读写情况</li>
<li>数据处理</li>
</ol>
<p>每一轮测试需要按上面的步骤进行处理</p>
<h2 id="测试分析">测试分析</h2><p>我的测试选择的是osd.4,按上面的步骤进行处理后，到了观察PG的步骤，此时因为做了不迁移的标记，只会状态改变，不会真正的迁移 我们来观察下需要迁移的pg<br>默认情况下的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump|grep recovering|awk '&#123;print $1,$2,$10,$15,$16,$17,$18&#125;'</span></span><br><span class="line">dumped all <span class="keyword">in</span> format plain</span><br><span class="line"><span class="number">0.15</span> <span class="number">682</span> active+recovering+degraded [<span class="number">0</span>,<span class="number">5</span>] <span class="number">0</span> [<span class="number">0</span>,<span class="number">5</span>] <span class="number">0</span></span><br><span class="line"><span class="number">0.24</span> <span class="number">674</span> active+recovering+degraded [<span class="number">5</span>,<span class="number">2</span>] <span class="number">5</span> [<span class="number">5</span>,<span class="number">2</span>] <span class="number">5</span></span><br><span class="line"><span class="number">0.35</span> <span class="number">661</span> active+recovering+degraded [<span class="number">2</span>,<span class="number">1</span>] <span class="number">2</span> [<span class="number">2</span>,<span class="number">1</span>] <span class="number">2</span></span><br><span class="line"><span class="number">0.37</span> <span class="number">654</span> active+recovering+degraded [<span class="number">1</span>,<span class="number">0</span>] <span class="number">1</span> [<span class="number">1</span>,<span class="number">0</span>] <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到这个环境下，每个OSD上面基本上是一个PG的写入，和一个PG的读取，实际上是读写同时在进行的</p>
<p>默认的</p>
<blockquote>
<p>osd_max_backfills = 1<br>osd_recovery_max_active = 3</p>
</blockquote>
<p>两个参数是一个是每个OSD上面启动的恢复的PG数目，下面一个是控制同时恢复的请求数目</p>
<p>默认的参数的情况<br><img src="http://static.zybuluo.com/zphj1987/2f1dubw81g0r99beds6u374d/pg.png" alt="pg.png-37.1kB"><br>上图为迁移的对象数目<br><img src="http://static.zybuluo.com/zphj1987/jvnhe3t93yqqrhaqvf6prkct/diskspeed.png" alt="diskspeed.png-63.7kB"><br>上图为OSD的磁盘读取写入的情况</p>
<p>可以看到迁移的对象每秒在6-15之间<br>磁盘上的读取为20-60MB/s，写入为80MB左右</p>
<p>这个只是默认的情况下的,占用了磁盘带宽的80%左右，在真正有写入的时候，因为有优先级的控制，占的带宽可能没那么多，本篇目的是在静态的时候就把磁盘占用给控制下来，那么即使有读写，恢复的磁盘占用只会更低</p>
<h3 id="调整一个参数">调整一个参数</h3><blockquote>
<p>osd_recovery_max_active = 3<br>调整如下<br>osd_recovery_max_active = 1</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/geq2b8eng7vckbfybnvnf5oa/pgactive1.png" alt="pgactive1.png-30.9kB"></p>
<p><img src="http://static.zybuluo.com/zphj1987/ll20uhofrbz65b6agsduv59t/diskactive1.png" alt="diskactive1.png-66.4kB"></p>
<p>从磁盘占用上和迁移上面可以看到，磁盘的负载确实降低了一些，峰值从16降低到了11左右</p>
<h2 id="sleep_参数的控制">sleep 参数的控制</h2><p>下面是一个关键的参数了</p>
<blockquote>
<p>osd_recovery_sleep = 0</p>
</blockquote>
<p>这个在jewel最新版本下还是0，在luminous版本已经设置成ssd是0，sata变成0.1，相当于增加了一个延时的过程，本篇主要就是对这个参数进行研究，看下能控制最低到一个什么程度</p>
<p>下面的测试的数据就统计到一个图当中去了，这样也便于对比的</p>
<p><img src="http://static.zybuluo.com/zphj1987/axkyzcmwwsgcio65kaqq5n5v/sleeppg.png" alt="sleeppg.png-76.6kB"></p>
<p><img src="http://static.zybuluo.com/zphj1987/fv366hsqetuaht64i5p6wcej/sleepdiskread.png" alt="sleepdiskread.png-86.7kB"></p>
<p><img src="http://static.zybuluo.com/zphj1987/paya55b97o1sfdtmm68pku65/sleepdiskwrite.png" alt="sleepdiskwrite.png-130.8kB"></p>
<p>上面测试了几组参数:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sleep=<span class="number">0</span>;sleep=<span class="number">0.1</span>;sleep=<span class="number">0.2</span>;sleep=<span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p>从上面的图中可以看到：<br>迁移速度从12降低到1-2个<br>磁盘读取占用从40Mb/s降到 8Mb/s左右<br>磁盘写入的占用从60MB/s-80MB/s降低到8MB/s-40MB/s</p>
<h2 id="结论">结论</h2><p>通过sleep的控制可以大大的降低迁移磁盘的占用，对于本身磁盘性能不太好的硬件环境下，可以用这个参数进行一下控制，能够缓解磁盘压力过大引起的osd崩溃的情况</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recovery.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>磁盘损坏对于一个大集群来说，可以说是必然发生的事情，即使再小的概率，磁盘量上去，总会坏那么几块盘，这个时候就会触发内部的修复过程，修复就是让不满足副本要求的PG，恢复到满足的情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph S3 基于NGINX的集群复制方案]]></title>
    <link href="http://www.zphj1987.com/2017/08/10/Ceph-S3-nginx-mirror/"/>
    <id>http://www.zphj1987.com/2017/08/10/Ceph-S3-nginx-mirror/</id>
    <published>2017-08-10T01:37:59.000Z</published>
    <updated>2017-08-10T09:10:00.800Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/nginx.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>ceph的s3数据的同步可以通过radosgw-agent进行同步，同region可以同步data和metadata，不同region只能同步metadata，这个地方可以参考下秦牧羊梳理的 <a href="https://my.oschina.net/diluga/blog/391928" target="_blank" rel="external">ceph radosgw 多集群同步部署流程</a>，本篇讲述的方案与radosgw-agent的复制方案不同在于,这个属于前端复制，后端相当于透明的两个相同集群，在入口层面就将数据进行了复制分流<br><a id="more"></a><br>在某些场景下，需求可能比较简单：</p>
<ul>
<li>需要数据能同时存储在两个集群当中</li>
<li>数据写一次，读多次</li>
<li>两个集群都能写</li>
</ul>
<p>一方面两个集群可以增加数据的可靠性，另一方面可以提高读带宽，两个集群同时可以提供读的服务</p>
<p>radosgw-agent是从底层做的同步，正好看到秦牧羊有提到nginx新加入了ngx_http_mirror_module 这个模块，那么本篇就尝试用这个模块来做几个简单的配置来实现上面的需求，这里纯架构的尝试，真正上生产还需要做大量的验证和修改的测试的</p>
<h2 id="结构设想">结构设想</h2><p><img src="http://static.zybuluo.com/zphj1987/y9uxssbwqydkworh347czxer/nginxs3.png" alt="nginxs3.png-30.8kB"></p>
<p>当数据传到nginx的server的时候，nginx本地进行负载均衡到两个本地端口上面，本地的两个端口对应到两个集群上面,一个主写集群1，一个主写集群2，这个是最简结构，集群的civetweb可以是很多机器，nginx这个也可以是多台的机器，在一台上面之所以做个均衡是可以让两个集群是对等关系，而不是一个只用nginx写，另一个只mirror写</p>
<h2 id="环境准备">环境准备</h2><p>准备两个完全独立的集群，分别配置一个s3的网关，我的环境为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">192.168</span>.<span class="number">19.101</span>:<span class="number">8080</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<p>在每个机器上都创建一个管理员的账号，这个用于后面的通过restapi来进行管理的,其他的后面的操作都通过http来做能保证两个集群的数据是一致的</p>
<blockquote>
<p>nginx的机器在192.168.19.104</p>
</blockquote>
<p>在两个集群当中都创建相同的管理用户<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin user create --uid=admin --display-name=admin --access_key=admin --secret=<span class="number">123456</span></span><br></pre></td></tr></table></figure></p>
<p>这里为了测试方便使用了简单密码</p>
<p>此时admin还仅仅是普通的权限，需要通过—cap添加user的capabilities，例如：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin caps add --uid=admin --caps=<span class="string">"users=read, write"</span></span><br><span class="line">radosgw-admin caps add --uid=admin --caps=<span class="string">"usage=read, write"</span></span><br></pre></td></tr></table></figure></p>
<p>下面就用到了nginx的最新的模块了<br>Nginx 1.13.4 发布，新增 ngx_http_mirror_module 模块</p>
<p>软件下载：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://nginx.org/packages/mainline/centos/<span class="number">7</span>/x86_64/RPMS/nginx-<span class="number">1.13</span>.<span class="number">4</span>-<span class="number">1</span>.el7.ngx.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>下载rpm包然后安装<br>安装：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ivh nginx-<span class="number">1.13</span>.<span class="number">4</span>-<span class="number">1</span>.el7.ngx.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>修改nginx配置文件：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">upstream s3 &#123;</span><br><span class="line">      server <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">81</span>;</span><br><span class="line">      server <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">82</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       <span class="number">81</span>;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">    mirror /mirror;</span><br><span class="line">    proxy_pass http://<span class="number">192.168</span>.<span class="number">19.101</span>:<span class="number">8080</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /mirror &#123;</span><br><span class="line">    internal;</span><br><span class="line">    proxy_pass http://<span class="number">192.168</span>.<span class="number">19.102</span>:<span class="number">8080</span><span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       <span class="number">82</span>;</span><br><span class="line">    server_name  localhost;</span><br><span class="line">    </span><br><span class="line">    location / &#123;</span><br><span class="line">    mirror /mirror;</span><br><span class="line">    proxy_pass http://<span class="number">192.168</span>.<span class="number">19.102</span>:<span class="number">8080</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location /mirror &#123;</span><br><span class="line">    internal;</span><br><span class="line">    proxy_pass http://<span class="number">192.168</span>.<span class="number">19.101</span>:<span class="number">8080</span><span class="variable">$request_uri</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">server&#123;</span><br><span class="line">    listen <span class="number">80</span>;</span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass         http://s3;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>负载均衡的设置有很多种，这里用最简单的轮训的模式，想配置其他负载均衡模式可以参考我的<a href="http://www.zphj1987.com/2015/03/22/%E5%85%B3%E4%BA%8Enginx-upstream%E7%9A%84%E5%87%A0%E7%A7%8D%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F/" target="_blank" rel="external">这篇文章</a></p>
<p>重启进程并检查服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node04 ~]<span class="comment"># systemctl restart nginx</span></span><br><span class="line">[root@node04 ~]<span class="comment"># netstat -tunlp|grep nginx</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">80</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1582973</span>/nginx: mast </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">81</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1582973</span>/nginx: mast </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">82</span>              <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">1582973</span>/nginx: mast</span><br></pre></td></tr></table></figure></p>
<p>整个环境就配置完成了，下面我们就来验证下这个配置的效果是什么样的，下面会提供几个s3用户的相关的脚本</p>
<h2 id="s3用户相关脚本">s3用户相关脚本</h2><h3 id="创建用户的脚本">创建用户的脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">#S3 USER ADMIN </span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###==============WRITE BEGIN=============###</span></span><br><span class="line">ACCESS_KEY=admin <span class="comment">## ADMIN_USER_TOKEN</span></span><br><span class="line">SECRET_KEY=<span class="number">123456</span> <span class="comment">## ADMIN_USER_SECRET</span></span><br><span class="line">HOST=<span class="number">192.168</span>.<span class="number">19.104</span>:<span class="number">80</span></span><br><span class="line">USER_ACCESS_KEY=<span class="string">"&amp;access-key=user1"</span></span><br><span class="line">USER_SECRET_KEY=<span class="string">"&amp;secret-key=123456"</span></span><br><span class="line"><span class="comment">###==============WRITE  FINAL=======FINAL=====###</span></span><br><span class="line"></span><br><span class="line">query2=admin/user</span><br><span class="line">userid=<span class="variable">$1</span></span><br><span class="line">name=<span class="variable">$2</span></span><br><span class="line">uid=<span class="string">"&amp;uid="</span></span><br><span class="line">date=`TZ=GMT LANG=en_US date <span class="string">"+%a, %d %b %Y %H:%M:%S GMT"</span>`</span><br><span class="line">header=<span class="string">"PUT\n\n\n<span class="variable">$&#123;date&#125;</span>\n/<span class="variable">$&#123;query2&#125;</span>"</span></span><br><span class="line">sig=$(<span class="built_in">echo</span> -en <span class="variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="variable">$&#123;SECRET_KEY&#125;</span> -binary | base64)</span><br><span class="line">curl -v -H <span class="string">"Date: <span class="variable">$&#123;date&#125;</span>"</span> -H <span class="string">"Authorization: AWS <span class="variable">$&#123;ACCESS_KEY&#125;</span>:<span class="variable">$&#123;sig&#125;</span>"</span> -L -X PUT <span class="string">"http://<span class="variable">$&#123;HOST&#125;</span>/<span class="variable">$&#123;query2&#125;</span>?format=json<span class="variable">$&#123;uid&#125;</span><span class="variable">$&#123;userid&#125;</span>&amp;display-name=<span class="variable">$&#123;name&#125;</span><span class="variable">$&#123;USER_ACCESS_KEY&#125;</span><span class="variable">$&#123;USER_SECRET_KEY&#125;</span>"</span> -H <span class="string">"Host: <span class="variable">$&#123;HOST&#125;</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">""</span></span><br></pre></td></tr></table></figure>
<p>运行脚本：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]<span class="comment"># sh  addusernew.sh user1 USER1</span></span><br><span class="line">* About to connect() to <span class="number">192.168</span>.<span class="number">19.104</span> port <span class="number">80</span> (<span class="comment">#0)</span></span><br><span class="line">*   Trying <span class="number">192.168</span>.<span class="number">19.104</span>...</span><br><span class="line">* Connected to <span class="number">192.168</span>.<span class="number">19.104</span> (<span class="number">192.168</span>.<span class="number">19.104</span>) port <span class="number">80</span> (<span class="comment">#0)</span></span><br><span class="line">&gt; PUT /admin/user?format=json&amp;uid=user1&amp;display-name=USER1&amp;access-key=user1&amp;secret-key=<span class="number">123456</span> HTTP/<span class="number">1.1</span></span><br><span class="line">&gt; User-Agent: curl/<span class="number">7.29</span>.<span class="number">0</span></span><br><span class="line">&gt; Accept: */*</span><br><span class="line">&gt; Date: Wed, <span class="number">09</span> Aug <span class="number">2017</span> <span class="number">07</span>:<span class="number">51</span>:<span class="number">58</span> GMT</span><br><span class="line">&gt; Authorization: AWS admin:wuqQUUXhhar5nQS5D5B14Dpx+Rw=</span><br><span class="line">&gt; Host: <span class="number">192.168</span>.<span class="number">19.104</span>:<span class="number">80</span></span><br><span class="line">&gt; </span><br><span class="line">&lt; HTTP/<span class="number">1.1</span> <span class="number">200</span> OK</span><br><span class="line">&lt; Server: nginx/<span class="number">1.13</span>.<span class="number">4</span></span><br><span class="line">&lt; Date: Wed, <span class="number">09</span> Aug <span class="number">2017</span> <span class="number">07</span>:<span class="number">51</span>:<span class="number">58</span> GMT</span><br><span class="line">&lt; Content-Type: application/json</span><br><span class="line">&lt; Content-Length: <span class="number">195</span></span><br><span class="line">&lt; Connection: keep-alive</span><br><span class="line">&lt; </span><br><span class="line">* Connection <span class="comment">#0 to host 192.168.19.104 left intact</span></span><br><span class="line">&#123;<span class="string">"user_id"</span>:<span class="string">"user1"</span>,<span class="string">"display_name"</span>:<span class="string">"USER1"</span>,<span class="string">"email"</span>:<span class="string">""</span>,<span class="string">"suspended"</span>:<span class="number">0</span>,<span class="string">"max_buckets"</span>:<span class="number">1000</span>,<span class="string">"subusers"</span>:[],<span class="string">"keys"</span>:[&#123;<span class="string">"user"</span>:<span class="string">"user1"</span>,<span class="string">"access_key"</span>:<span class="string">"user1"</span>,<span class="string">"secret_key"</span>:<span class="string">"123456"</span>&#125;],<span class="string">"swift_keys"</span>:[],<span class="string">"caps"</span>:[]&#125;</span><br></pre></td></tr></table></figure></p>
<p>在两个集群中检查：<br><img src="http://static.zybuluo.com/zphj1987/aflko6hufp7ls0r27resxqn9/usercreate.png" alt="usercreate.png-36.5kB"></p>
<p>可以看到两个集群当中都产生了相同的用户信息</p>
<h3 id="修改用户">修改用户</h3><p>直接把上面的创建脚本里面的PUT改成POST就是修改用户的脚本</p>
<h3 id="删除用户脚本">删除用户脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">#S3 USER ADMIN</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###==============WRITE BEGIN=============###</span></span><br><span class="line">ACCESS_KEY=admin <span class="comment">## ADMIN_USER_TOKEN</span></span><br><span class="line">SECRET_KEY=<span class="number">123456</span> <span class="comment">## ADMIN_USER_SECRET</span></span><br><span class="line">HOST=<span class="number">192.168</span>.<span class="number">19.104</span>:<span class="number">80</span></span><br><span class="line"><span class="comment">###==============WRITE  FINAL=======FINAL=====###</span></span><br><span class="line"></span><br><span class="line">query2=admin/user</span><br><span class="line">userid=<span class="variable">$1</span></span><br><span class="line">uid=<span class="string">"&amp;uid="</span></span><br><span class="line">date=`TZ=GMT LANG=en_US date <span class="string">"+%a, %d %b %Y %H:%M:%S GMT"</span>`</span><br><span class="line">header=<span class="string">"DELETE\n\n\n<span class="variable">$&#123;date&#125;</span>\n/<span class="variable">$&#123;query2&#125;</span>"</span></span><br><span class="line">sig=$(<span class="built_in">echo</span> -en <span class="variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="variable">$&#123;SECRET_KEY&#125;</span> -binary | base64)</span><br><span class="line">curl -v -H <span class="string">"Date: <span class="variable">$&#123;date&#125;</span>"</span> -H <span class="string">"Authorization: AWS <span class="variable">$&#123;ACCESS_KEY&#125;</span>:<span class="variable">$&#123;sig&#125;</span>"</span> -L -X DELETE <span class="string">"http://<span class="variable">$&#123;HOST&#125;</span>/<span class="variable">$&#123;query2&#125;</span>?format=json<span class="variable">$&#123;uid&#125;</span><span class="variable">$&#123;userid&#125;</span>"</span> -H <span class="string">"Host: <span class="variable">$&#123;HOST&#125;</span>"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">""</span></span><br></pre></td></tr></table></figure>
<p>执行删除用户：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@node01 ~]<span class="comment"># sh deluser.sh user1</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/tzozt0tpmq6p4c3tssuy6xvo/deluser.png" alt="deluser.png-6.3kB"></p>
<p>可以看到两边都删除了</p>
<h3 id="获取用户的信息脚本">获取用户的信息脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">#S3 USER ADMIN </span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###==============WRITE BEGIN=============###</span></span><br><span class="line">ACCESS_KEY=admin <span class="comment">## ADMIN_USER_TOKEN</span></span><br><span class="line">SECRET_KEY=<span class="number">123456</span> <span class="comment">## ADMIN_USER_SECRET</span></span><br><span class="line">HOST=<span class="number">192.168</span>.<span class="number">19.101</span>:<span class="number">8080</span></span><br><span class="line"><span class="comment">###==============WRITE  FINAL=======FINAL=====###</span></span><br><span class="line"></span><br><span class="line">query2=admin/user</span><br><span class="line">userid=<span class="variable">$1</span></span><br><span class="line">uid=<span class="string">"&amp;uid="</span></span><br><span class="line">date=`TZ=GMT LANG=en_US date <span class="string">"+%a, %d %b %Y %H:%M:%S GMT"</span>`</span><br><span class="line">header=<span class="string">"GET\n\n\n<span class="variable">$&#123;date&#125;</span>\n/<span class="variable">$&#123;query2&#125;</span>"</span></span><br><span class="line">sig=$(<span class="built_in">echo</span> -en <span class="variable">$&#123;header&#125;</span> | openssl sha1 -hmac <span class="variable">$&#123;SECRET_KEY&#125;</span> -binary | base64)</span><br><span class="line">curl -v -H <span class="string">"Date: <span class="variable">$&#123;date&#125;</span>"</span> -H <span class="string">"Authorization: AWS <span class="variable">$&#123;ACCESS_KEY&#125;</span>:<span class="variable">$&#123;sig&#125;</span>"</span> -L -X GET <span class="string">"http://<span class="variable">$&#123;HOST&#125;</span>/<span class="variable">$&#123;query2&#125;</span>?format=json<span class="variable">$&#123;uid&#125;</span><span class="variable">$&#123;userid&#125;</span>&amp;display-name=<span class="variable">$&#123;name&#125;</span>"</span>  -H <span class="string">"Host: <span class="variable">$&#123;HOST&#125;</span>"</span></span><br></pre></td></tr></table></figure>
<h3 id="测试上传一个文件">测试上传一个文件</h3><p>通过192.168.19.104:80端口上传一个文件，然后通过nginx的端口，以及两个集群的端口进行查看</p>
<p><img src="http://static.zybuluo.com/zphj1987/46aq5ifckuvj3v36i1wokdyg/same.png" alt="same.png-24.6kB"></p>
<p>可以看到在上传一次的情况下，两个集群里面同时拥有了这个文件</p>
<h2 id="总结">总结</h2><p>真正将方案运用到生产还需要做大量的验证测试，中间的失效处理，以及是否可以将写镜像，读取的时候不镜像，这些都需要进一步做相关的验证工作</p>
<p>本篇中的S3用户的管理接口操作参考了网上的其他资料</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/nginx.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>ceph的s3数据的同步可以通过radosgw-agent进行同步，同region可以同步data和metadata，不同region只能同步metadata，这个地方可以参考下秦牧羊梳理的 <a href="https://my.oschina.net/diluga/blog/391928">ceph radosgw 多集群同步部署流程</a>，本篇讲述的方案与radosgw-agent的复制方案不同在于,这个属于前端复制，后端相当于透明的两个相同集群，在入口层面就将数据进行了复制分流<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RBD快速删除的方法分析与改进]]></title>
    <link href="http://www.zphj1987.com/2017/07/27/RBD-fast-remove/"/>
    <id>http://www.zphj1987.com/2017/07/27/RBD-fast-remove/</id>
    <published>2017-07-27T14:20:37.000Z</published>
    <updated>2017-07-28T06:16:57.059Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/deleting.gif?imageMogr2/thumbnail/!75p" alt="delete"><br></center>

<h2 id="前言">前言</h2><p>这个问题在很久以前就有一篇文章进行过讨论 <a href="http://cephnotes.ksperis.com/blog/2014/07/04/remove-big-rbd-image" target="_blank" rel="external">remove-big-rbd</a>,这个文章写的比较清楚了，并且对不同的方法做了分析，这里先把结论说下</p>
<table>
<thead>
<tr>
<th style="text-align:center">rbd类型</th>
<th style="text-align:center">rbd rm 方法</th>
<th style="text-align:center">rados -p rm方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">未填充很多</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">快</td>
</tr>
<tr>
<td style="text-align:center">已填充很多</td>
<td style="text-align:center">快</td>
<td style="text-align:center">慢</td>
</tr>
</tbody>
</table>
<a id="more"></a>
<p>在rbd进行删除的时候，即使内部没有对象数据，也一样需要一个个对象去发请求，即使对象不存在，这个可以开日志看到</p>
<h2 id="实验过程">实验过程</h2><h3 id="开启日志的方法">开启日志的方法</h3><p>在/etc/ceph/ceph.conf中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">debug_ms=<span class="number">1</span></span><br><span class="line"><span class="built_in">log</span>_file=/var/<span class="built_in">log</span>/ceph/rados.log</span><br></pre></td></tr></table></figure></p>
<p>这个默认也会在执行命令的时候打印到前台，所以处理下比较好，最简单的办法就是做alias<br>添加下面内容到 /etc/bashrc<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">alias</span> ceph=<span class="string">'ceph  --debug-ms=0'</span></span><br><span class="line"><span class="built_in">alias</span> rados=<span class="string">'rados  --debug-ms=0'</span></span><br></pre></td></tr></table></figure></p>
<p>然后命令行执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/bashrc</span><br></pre></td></tr></table></figure></p>
<p>在做操作的时候就只会记录日志，前台不会打印调试信息了,但是这个会影响到ceph daemon的命令，这个可以用这种方式在线屏蔽即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph --debug_ms=<span class="number">0</span>  <span class="operator">-s</span></span><br></pre></td></tr></table></figure></p>
<p>然后执行操作后，去分析每秒钟的操作数目即可,类似下面的这个，也可以用日志系统进行分析，这里不赘述<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat  /var/<span class="built_in">log</span>/ceph/rados.log|grep delete|grep -v <span class="string">"&gt;"</span>|grep <span class="number">13</span>:<span class="number">29</span>:<span class="number">46</span>|wc <span class="operator">-l</span></span><br></pre></td></tr></table></figure></p>
<p>原始的快速删除方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd ls | grep <span class="string">'^rbd_data.25ae86b8b4567'</span> | xargs -n <span class="number">200</span>  rados -p rbd rm</span><br></pre></td></tr></table></figure></p>
<h2 id="开启多进程删除的方法">开启多进程删除的方法</h2><p>这个比上面那种方法好的是：</p>
<ul>
<li>可以显示当前删除的进度</li>
<li>可以指定删除的进程并发数</li>
<li>可以显示当时正在删除的对象</li>
<li>可以增加一个中断时间降低负载</li>
</ul>
<p>首先获取一个需要快速删除的rbd的列表<br>获取prifix<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 put]<span class="comment"># rbd info testrbd|grep prefix</span></span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">32</span>c0f6b8b4567</span><br></pre></td></tr></table></figure></p>
<p>获取列表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 put]<span class="comment"># rados -p rbd ls |grep rbd_data.32c0f6b8b4567 &gt; delobject</span></span><br></pre></td></tr></table></figure></p>
<p>这里可以看下内容有没有问题，检查确认下</p>
<p>删除的fastremove.sh脚本如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line"><span class="comment">#####config</span></span><br><span class="line">process=<span class="number">5</span></span><br><span class="line">objectlistfile=<span class="string">"./delobject"</span></span><br><span class="line">deletepool=rbd</span><br><span class="line"><span class="comment">#####</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="title">delete_fun</span></span>()</span><br><span class="line">  &#123;</span><br><span class="line">      date <span class="string">"+%Y-%m-%d %H:%M:%S"</span></span><br><span class="line">      rados -p <span class="variable">$deletepool</span> rm <span class="variable">$1</span></span><br><span class="line">	  <span class="comment">#sleep 1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="title">concurrent</span></span>()</span><br><span class="line"> &#123;</span><br><span class="line">     start=<span class="variable">$1</span> &amp;&amp; end=<span class="variable">$2</span> &amp;&amp; cur_num=<span class="variable">$3</span></span><br><span class="line">     mkfifo   ./fifo.$$ &amp;&amp;  <span class="built_in">exec</span> <span class="number">4</span>&lt;&gt; ./fifo.$$ &amp;&amp; rm <span class="operator">-f</span> ./fifo.$$</span><br><span class="line">     <span class="keyword">for</span> ((i=<span class="variable">$start</span>; i&lt;<span class="variable">$cur_num</span>+<span class="variable">$start</span>; i++)); <span class="keyword">do</span></span><br><span class="line">         <span class="built_in">echo</span> <span class="string">"init  start delete process <span class="variable">$i</span>"</span> &gt;&amp;<span class="number">4</span></span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span>((i=<span class="variable">$start</span>; i&lt;=<span class="variable">$end</span>; i++)); <span class="keyword">do</span></span><br><span class="line">         <span class="built_in">read</span> -u <span class="number">4</span></span><br><span class="line">         &#123;</span><br><span class="line">             <span class="built_in">echo</span> <span class="operator">-e</span> <span class="string">"-- current delete: [:delete <span class="variable">$i</span>/<span class="variable">$objectnum</span>  <span class="variable">$REPLY</span>]"</span></span><br><span class="line">             delob=`sed -n <span class="string">"<span class="variable">$&#123;i&#125;</span>p"</span> <span class="variable">$objectlistfile</span>`</span><br><span class="line">             delete_fun <span class="variable">$delob</span></span><br><span class="line">             <span class="built_in">echo</span> <span class="string">"delete <span class="variable">$delob</span> done"</span>  <span class="number">1</span>&gt;&amp;<span class="number">4</span> <span class="comment"># write to $ff_file</span></span><br><span class="line">         &#125; &amp;</span><br><span class="line">     <span class="keyword">done</span></span><br><span class="line">     <span class="built_in">wait</span></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">objectnum=`cat <span class="variable">$objectlistfile</span>|wc <span class="operator">-l</span>`</span><br><span class="line">concurrent <span class="number">1</span> <span class="variable">$objectnum</span> <span class="variable">$process</span></span><br></pre></td></tr></table></figure></p>
<p>上面直接把配置写到脚本里面了，根据需要进行修改<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#####config</span></span><br><span class="line">process=<span class="number">10</span></span><br><span class="line">objectlistfile=<span class="string">"./delobject"</span></span><br><span class="line">deletepool=rbd</span><br><span class="line"><span class="comment">#####</span></span><br></pre></td></tr></table></figure></p>
<p>指定并发数目，指定准备删除的对象的list文件，指定对象所在的存储池</p>
<p>然后执行即可</p>
<h2 id="本次测试删除的性能差别">本次测试删除的性能差别</h2><p>准备对象数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd map testrbd</span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/dev/rbd2 bs=<span class="number">4</span>M count=<span class="number">1200</span></span><br></pre></td></tr></table></figure></p>
<p>获取列表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 put]<span class="comment"># rados -p rbd ls |grep rbd_data.32c0f6b8b4567 &gt; delobject</span></span><br></pre></td></tr></table></figure></p>
<p>执行删除脚本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 put]<span class="comment"># sh fastremove.sh</span></span><br></pre></td></tr></table></figure></p>
<p>测试结果如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">并发数</th>
<th style="text-align:center">删除时间</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">71s</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">35s</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">5s</td>
</tr>
<tr>
<td style="text-align:center">25</td>
<td style="text-align:center">6s</td>
</tr>
<tr>
<td style="text-align:center">50</td>
<td style="text-align:center">5s</td>
</tr>
<tr>
<td style="text-align:center">100</td>
<td style="text-align:center">5s</td>
</tr>
</tbody>
</table>
<p>从测试结果来看在并发数为5的时候就能达到每秒删除200个对象了，根据自己的需要进行增减，也可以增减删除的间隔加上sleep</p>
<p>下面看下这个过程：</p>
<iframe src="http://7xweck.com1.z0.glb.clouddn.com/fastremove.html" height="530px" width="90%" align="center"></iframe>

<h2 id="总结">总结</h2><p>在ceph里面一些系统的操作默认是单进程去处理的，一般情况下都没什么问题，在数据量超大，追求效率的时候，我们可以通过加上一些并发加速这个过程，本篇脚本当中的并发同样适用于其他需要并发的场景</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-07-27</td>
</tr>
<tr>
<td style="text-align:center">增加前台调试信息的屏蔽</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-07-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/deleting.gif?imageMogr2/thumbnail/!75p" alt="delete"><br></center>

<h2 id="前言">前言</h2><p>这个问题在很久以前就有一篇文章进行过讨论 <a href="http://cephnotes.ksperis.com/blog/2014/07/04/remove-big-rbd-image">remove-big-rbd</a>,这个文章写的比较清楚了，并且对不同的方法做了分析，这里先把结论说下</p>
<table>
<thead>
<tr>
<th style="text-align:center">rbd类型</th>
<th style="text-align:center">rbd rm 方法</th>
<th style="text-align:center">rados -p rm方法</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">未填充很多</td>
<td style="text-align:center">慢</td>
<td style="text-align:center">快</td>
</tr>
<tr>
<td style="text-align:center">已填充很多</td>
<td style="text-align:center">快</td>
<td style="text-align:center">慢</td>
</tr>
</tbody>
</table>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[从ceph对象中提取RBD中的指定文件]]></title>
    <link href="http://www.zphj1987.com/2017/07/22/from-ceph-object-get-rbd-file/"/>
    <id>http://www.zphj1987.com/2017/07/22/from-ceph-object-get-rbd-file/</id>
    <published>2017-07-22T15:25:35.000Z</published>
    <updated>2017-07-22T15:35:41.820Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/BLDG.png" alt=""><br></center>


<h2 id="前言">前言</h2><p>之前有个想法，是不是有办法找到rbd中的文件与对象的关系，想了很久但是一直觉得文件系统比较复杂，在fs 层的东西对ceph来说是透明的，并且对象大小是4M，而文件很小，可能在fs层进行了合并，应该很难找到对应关系，最近看到小胖有提出这个问题，那么就再次尝试了，现在就是把这个实现方法记录下来<br><a id="more"></a><br>这个提取的作用个人觉得最大的好处就是一个rbd设备，在文件系统层被破坏以后，还能够从rbd提取出文件，我们知道很多情况下设备的文件系统一旦破坏，无法挂载，数据也就无法读取，而如果能从rbd中提取出文件，这就是保证了即使文件系统损坏的情况下，数据至少不丢失</p>
<p>本篇是基于xfs文件系统情况下的提取，其他文件系统有时间再看看，因为目前使用的比较多的就是xfs文件系统</p>
<p>本篇也回答了一个可能会经常被问起的问题，能告诉我虚拟机里面的文件在后台存储在哪里么，看完本篇就知道存储在哪里了</p>
<h2 id="XFS文件系统介绍">XFS文件系统介绍</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkfs.xfs -f /dev/rbd0p1 </span></span><br><span class="line">warning: device is not properly aligned /dev/rbd0p1</span><br><span class="line">meta-data=/dev/rbd0p1            isize=<span class="number">256</span>    agcount=<span class="number">9</span>, agsize=<span class="number">162816</span> blks</span><br><span class="line">         =                       sectsz=<span class="number">512</span>   attr=<span class="number">2</span>, projid32bit=<span class="number">1</span></span><br><span class="line">         =                       crc=<span class="number">0</span>        finobt=<span class="number">0</span></span><br><span class="line">data     =                       bsize=<span class="number">4096</span>   blocks=<span class="number">1310475</span>, imaxpct=<span class="number">25</span></span><br><span class="line">         =                       sunit=<span class="number">1024</span>   swidth=<span class="number">1024</span> blks</span><br><span class="line">naming   =version <span class="number">2</span>              bsize=<span class="number">4096</span>   ascii-ci=<span class="number">0</span> ftype=<span class="number">0</span></span><br><span class="line"><span class="built_in">log</span>      =internal <span class="built_in">log</span>           bsize=<span class="number">4096</span>   blocks=<span class="number">2560</span>, version=<span class="number">2</span></span><br><span class="line">         =                       sectsz=<span class="number">512</span>   sunit=<span class="number">8</span> blks, lazy-count=<span class="number">1</span></span><br><span class="line">realtime =none                   extsz=<span class="number">4096</span>   blocks=<span class="number">0</span>, rtextents=<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>XFS文件系统采取是AG管理的，每个AG维护自己的inode和数据，所以XFS文件系统是一种很容易扩展的文件系统，本篇里面主要用到的命令是xfs_bmap这个命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># xfs_bmap -lvp /etc/fstab</span></span><br><span class="line">/etc/fstab:</span><br><span class="line"> EXT: FILE-OFFSET      BLOCK-RANGE        AG AG-OFFSET        TOTAL FLAGS</span><br><span class="line">   <span class="number">0</span>: [<span class="number">0</span>..<span class="number">7</span>]:          <span class="number">26645424</span>..<span class="number">26645431</span>  <span class="number">1</span> (<span class="number">431024</span>..<span class="number">431031</span>)     <span class="number">8</span> <span class="number">00000</span></span><br></pre></td></tr></table></figure></p>
<p>一个文件最小就是8个block（512b），也就是4k,这个因为上面默认的xfs的格式化就是data bsize=4K,这个值可以自行调整的，本篇尽量用默认常规的参数来讲例子</p>
<p>查看man xfs_bmap这个命令可以看到：</p>
<blockquote>
<p>Holes are marked by replacing the startblock..endblock with hole.  All the file offsets and disk blocks are in units of 512-byte blocks, no matter what the filesystem’s block size is.</p>
</blockquote>
<p>意思是这个查询到的里面的计数单位都是512-byte，不管上层设置的block大小是多少，我们知道文件系统底层的sector就是512-byte，所以这个查询到的结果就可以跟当前的文件系统的sector的偏移量联系起来，这里强调一下，这个偏移量的起始位子为当前文件系统所在分区的偏移量，如果是多分区的情况，在计算整个偏移量的时候就要考虑分区的偏移量了，这个会在后面用实例进行讲解的</p>
<p>rbd的对象是不清楚内部分区的偏移量，所以在rbd层进行提取的时候是需要得到的是分区当中的文件相对整个磁盘的一个sector的偏移量</p>
<h2 id="rbd的对象结构">rbd的对象结构</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls|grep data</span></span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">00000000000009</span>ff</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">00000000000001</span>dd</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">0000000000000000</span></span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000009</span>f</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">0000000000000459</span></span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000027</span>e</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">00000000000004</span>ff</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000027</span>c</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000027</span>d</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">0000000000000001</span></span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000013</span>e</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">00000000000003</span>ba</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">000000000000031</span>b</span><br><span class="line">rbd_data.<span class="number">25</span>a636b8b4567.<span class="number">00000000000004</span>f8</span><br></pre></td></tr></table></figure>
<p>rbd被xfs格式化以后会产生一些对象，这些对象是以16进制名称的方式存储在后台的，也就是rbd大小一定的情况下对象数目是一定的，也就是名称也是一定的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># parted -s /dev/rbd0 unit s print</span></span><br><span class="line">Model: Unknown (unknown)</span><br><span class="line">Disk /dev/rbd0: <span class="number">20971520</span>s</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start      End        Size       File system  Name     Flags</span><br><span class="line"> <span class="number">1</span>      <span class="number">1953</span>s      <span class="number">10485759</span>s  <span class="number">10483807</span>s  xfs          primari</span><br><span class="line"> <span class="number">2</span>      <span class="number">10485760</span>s  <span class="number">20963327</span>s  <span class="number">10477568</span>s               primari</span><br></pre></td></tr></table></figure></p>
<p>上面可以看到rbd0的sector个数为20971520s<br>20971520s*512byte=10737418240byte=10485760KB=10240MB<br>sector的大小一定，总rbd大小一定的情况下sector的数目也是一定的，本篇实例的rbd大小<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">10000</span> MB <span class="keyword">in</span> <span class="number">2500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">25</span>a776b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Sat Jul <span class="number">22</span> <span class="number">18</span>:<span class="number">04</span>:<span class="number">12</span> <span class="number">2017</span></span><br></pre></td></tr></table></figure></p>
<h2 id="sector和ceph_object的对应关系的查询">sector和ceph object的对应关系的查询</h2><p>这个就像个map一样，需要把这个关系给找到，一个sector的区间对应到object的map，这里我用python写个简单的方法来做查询，也可以自己用其他语言来实现</p>
<p>首先查询到rbd的对象数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">10000</span> MB <span class="keyword">in</span> <span class="number">2500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">25</span>a776b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags: </span><br><span class="line">	create_timestamp: Sat Jul <span class="number">22</span> <span class="number">18</span>:<span class="number">04</span>:<span class="number">12</span> <span class="number">2017</span></span><br></pre></td></tr></table></figure></p>
<p>处理脚本如下:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim getsecob.py</span><br></pre></td></tr></table></figure></p>
<p>添加下面内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#! /bin/python</span></span><br><span class="line"><span class="comment"># *-* conding=UTF-8 *-*</span></span><br><span class="line"></span><br><span class="line">import commands</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    getmap(<span class="number">2500</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def getmap(object):</span><br><span class="line">    sector=int(object)*<span class="number">4096</span>*<span class="number">1024</span>/<span class="number">512</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">"object:"</span>+str(object)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"sector:"</span>+str(sector)</span><br><span class="line">    incre=sector/object</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> range(int(object)):</span><br><span class="line">        a=int(item*<span class="number">8192</span>)</span><br><span class="line">        b=int((item+<span class="number">1</span>)*<span class="number">8192</span>-<span class="number">1</span>)</span><br><span class="line">        <span class="built_in">print</span> str([a,b])+<span class="string">"  --&gt;  "</span>+<span class="string">"%016x"</span> %item</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<p>其中getmap后面为对象数目<br>输出是这个形式的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># python getsecob.py</span></span><br><span class="line">object:<span class="number">2500</span></span><br><span class="line">sector:<span class="number">20480000</span></span><br><span class="line">[<span class="number">0</span>, <span class="number">8191</span>]  --&gt;  <span class="number">0000000000000000</span></span><br><span class="line">[<span class="number">8192</span>, <span class="number">16383</span>]  --&gt;  <span class="number">0000000000000001</span></span><br><span class="line">[<span class="number">16384</span>, <span class="number">24575</span>]  --&gt;  <span class="number">0000000000000002</span></span><br><span class="line">[<span class="number">24576</span>, <span class="number">32767</span>]  --&gt;  <span class="number">0000000000000003</span></span><br><span class="line">[<span class="number">32768</span>, <span class="number">40959</span>]  --&gt;  <span class="number">0000000000000004</span></span><br><span class="line">[<span class="number">40960</span>, <span class="number">49151</span>]  --&gt;  <span class="number">0000000000000005</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>对rbd0进行分区，分区后的结果如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># parted -s /dev/rbd0 unit s print</span></span><br><span class="line">Model: Unknown (unknown)</span><br><span class="line">Disk /dev/rbd0: <span class="number">20480000</span>s</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start      End        Size       File system  Name     Flags</span><br><span class="line"> <span class="number">1</span>      <span class="number">1953</span>s      <span class="number">10240000</span>s  <span class="number">10238048</span>s               primari</span><br><span class="line"> <span class="number">2</span>      <span class="number">10248192</span>s  <span class="number">20471807</span>s  <span class="number">10223616</span>s               primari</span><br></pre></td></tr></table></figure></p>
<p>这个是个测试用的image，大小为10G分成两个5G的分区，现在我们在两个分区里面分别写入两个测试文件，然后经过计算后，从后台的对象中把文件读出<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount /dev/rbd0p1 /mnt1</span><br><span class="line">mount /dev/rbd0p2 /mnt2</span><br><span class="line">cp /etc/fstab /mnt1</span><br><span class="line">cp /etc/hostname /mnt2</span><br></pre></td></tr></table></figure></p>
<p>首先获取文件在分区上的sector的偏移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># xfs_bmap -lvp /mnt1/fstab </span></span><br><span class="line">/mnt1/fstab:</span><br><span class="line"> EXT: FILE-OFFSET      BLOCK-RANGE      AG AG-OFFSET        TOTAL FLAGS</span><br><span class="line">   <span class="number">0</span>: [<span class="number">0</span>..<span class="number">7</span>]:          <span class="number">8224</span>..<span class="number">8231</span>        <span class="number">0</span> (<span class="number">8224</span>..<span class="number">8231</span>)         <span class="number">8</span> <span class="number">01111</span></span><br></pre></td></tr></table></figure></p>
<p>可以得到是(8224..8231)共8个sector<br>从上面的分区1的start的sector可以知道起始位置是1953，那么相对于磁盘的偏移量就变成了</p>
<blockquote>
<p>(8224+1953..8231+1953) = (10177..10184)</p>
</blockquote>
<p>这里说下，这个地方拿到偏移量后，直接通过对rbd设备进行dd读取也可以把这个文件读取出来，这个顺带讲下，本文主要是从对象提取：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/rbd0 of=a bs=<span class="number">512</span> count=<span class="number">8</span> skip=<span class="number">10177</span></span><br></pre></td></tr></table></figure></p>
<p>bs取512是因为sector的单位就是512b<br>这样就把刚刚的fstab文件读取出来了，skip就是文件的sector相对磁盘的起始位置，count就是文件所占的block数目</p>
<p>继续我们的对象提取方式，上面的（10177..10184）这个我们根据上面那个脚本输出的对象列表来找到对象</p>
<blockquote>
<p>[8192, 16383]  —&gt;  0000000000000001<br>获取名称，这个因为我的是测试环境，就只有一个匹配，多个image的时候要过滤出对用的rbd的对象，用prifix过滤即可</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls|grep 0000000000000001</span></span><br><span class="line">rbd_data.<span class="number">25</span>a776b8b4567.<span class="number">0000000000000001</span></span><br></pre></td></tr></table></figure>
<p>下载对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd get rbd_data.25a776b8b4567.0000000000000001 rbd_data.25a776b8b4567.0000000000000001</span></span><br></pre></td></tr></table></figure></p>
<p>根据偏移量计算对象中的偏移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">（<span class="number">10177</span>..<span class="number">10184</span>）</span><br><span class="line">[<span class="number">8192</span>, <span class="number">16383</span>]  --&gt;  <span class="number">0000000000000001</span></span><br></pre></td></tr></table></figure></p>
<p>得到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">10177</span>-<span class="number">8192</span>=<span class="number">1985</span></span><br><span class="line"></span><br><span class="line">dd <span class="keyword">if</span>=rbd_data.<span class="number">25</span>a776b8b4567.<span class="number">0000000000000001</span> of=a bs=<span class="number">512</span> count=<span class="number">8</span> skip=<span class="number">1985</span></span><br></pre></td></tr></table></figure></p>
<p>得到的文件a的内容即为之前文件的内容</p>
<p>准备取第二个分区的文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># xfs_bmap -lvp /mnt2/hostname </span></span><br><span class="line">/mnt2/hostname:</span><br><span class="line"> EXT: FILE-OFFSET      BLOCK-RANGE      AG AG-OFFSET        TOTAL FLAGS</span><br><span class="line">   <span class="number">0</span>: [<span class="number">0</span>..<span class="number">7</span>]:          <span class="number">8224</span>..<span class="number">8231</span>        <span class="number">0</span> (<span class="number">8224</span>..<span class="number">8231</span>)         <span class="number">8</span> <span class="number">01111</span></span><br></pre></td></tr></table></figure></p>
<p>8224+10248192..8231+10248192=10256416..10256423</p>
<p>从磁盘方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=/dev/rbd0 of=a bs=512 count=8 skip=10256416</span></span><br></pre></td></tr></table></figure></p>
<p>从对象方式<br>10256416..10256423 对应<br>[10256384, 10264575]  —&gt;  00000000000004e4<br>对象偏移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">10256416</span>-<span class="number">10256384</span>=<span class="number">32</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd get </span><br><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd get rbd_data.25a776b8b4567.00000000000004e4 rbd_data.25a776b8b4567.00000000000004e4</span></span><br></pre></td></tr></table></figure>
<p>获取文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dd if=rbd_data.25a776b8b4567.00000000000004e4 of=a bs=512 count=8 skip=32</span></span><br></pre></td></tr></table></figure></p>
<p>如果文件比较大的情况，可能出现就是文件是跨对象的，那么还是跟上面的提取方法一样，然后进行提取后的文件进行合并即可</p>
<h2 id="总结">总结</h2><p>在存储系统上面存储的文件必然会对应到底层磁盘的sector，而sector也是会一一对应到后台的对象的，这个在本文当中得到了验证，所以整个逻辑就是，在文件系统层找到文件对应的sector位置，然后再在底层把sector和对象关系找好，就能从找到文件在对象当中的具体的位置，也就能定位并且能提取了，本篇是基于xfs的，其他文件系统只要能定位文件的sector，就可以在底层找到文件，这个以后会补充其他文件系统进来</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-07-22</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/BLDG.png" alt=""><br></center>


<h2 id="前言">前言</h2><p>之前有个想法，是不是有办法找到rbd中的文件与对象的关系，想了很久但是一直觉得文件系统比较复杂，在fs 层的东西对ceph来说是透明的，并且对象大小是4M，而文件很小，可能在fs层进行了合并，应该很难找到对应关系，最近看到小胖有提出这个问题，那么就再次尝试了，现在就是把这个实现方法记录下来<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用火焰图分析ceph pg分布]]></title>
    <link href="http://www.zphj1987.com/2017/07/18/use-flame-show-ceph-pg/"/>
    <id>http://www.zphj1987.com/2017/07/18/use-flame-show-ceph-pg/</id>
    <published>2017-07-18T05:35:07.000Z</published>
    <updated>2017-07-18T14:09:45.747Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/flame.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>性能优化大神Brendan Gregg发明了火焰图来定位性能问题，通过图表就可以发现问题出在哪里，通过svg矢量图来查看性能卡在哪个点，哪个操作占用的资源最多<br><a id="more"></a><br>在查看了原始数据后，这个分析的原理是按层级来对调用进行一个计数，然后以层级去做比对，来看横向的占用的比例情况</p>
<p>基于这个原理，把osd tree的数据和pg数据可以做一个层级的组合，从而可以很方便的看出pg的分布情况，主机的分布情况，还可以进行搜索，在一个简单的图表内汇聚了大量的信息</p>
<h2 id="实践">实践</h2><p>获取需要的数据，这个获取数据是我用一个脚本解析的osd tree 和pg dump，然后按照需要的格式进行输出</p>
<blockquote>
<p>default;lab8106;osd.2;0.0 6<br>default;lab8106;osd.3;0.0 6<br>default;rack1;lab8107;osd.0;0.0 6</p>
</blockquote>
<p>需要的格式是这个样的，最后一个为权重，使用的是对象数，因为对象数可能为0，所以默认在每个数值进行了加一的操作，前面就是osd的分布的位置</p>
<p>脚本/sbin/stackcollapse-crush内容如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#! /bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line">import os</span><br><span class="line">import commands</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    global list_all_host</span><br><span class="line">    list_all_host = commands.getoutput(<span class="string">'ceph osd tree -f json-pretty  2&gt;/dev/null'</span>)</span><br><span class="line">    getpgmap()</span><br><span class="line">def getosd(osd):</span><br><span class="line">    mylist=[]</span><br><span class="line">    crushid=&#123;&#125;</span><br><span class="line">    json_str = json.loads(list_all_host)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> json_str[<span class="string">'nodes'</span>]:</span><br><span class="line">        <span class="keyword">if</span> item.has_key(<span class="string">'children'</span>):</span><br><span class="line">            crushid[str(item[<span class="string">'id'</span>])]=str(item[<span class="string">'name'</span>])</span><br><span class="line">            <span class="keyword">for</span> child <span class="keyword">in</span> item[<span class="string">'children'</span>]:</span><br><span class="line">                tmplist=[item[<span class="string">'id'</span>],child]</span><br><span class="line">                mylist.append(tmplist)</span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'type'</span>] == <span class="string">"osd"</span>:</span><br><span class="line">            crushid[str(item[<span class="string">'id'</span>])]=str(item[<span class="string">'name'</span>])</span><br><span class="line">    listnum=len(mylist)</span><br><span class="line">    compareindex=<span class="number">0</span></span><br><span class="line"><span class="comment">###从数组开始跟后面的数组进行比较，如果有就改变后面的数组，然后删除当前比较的list(index),进行list更新</span></span><br><span class="line"><span class="comment">###如果没有改变，就把索引往后推即可</span></span><br><span class="line">    <span class="keyword">while</span> compareindex &lt; len(mylist):</span><br><span class="line">        change = False</span><br><span class="line">        <span class="keyword">for</span> index,num <span class="keyword">in</span> enumerate(mylist):</span><br><span class="line">            <span class="keyword">if</span> compareindex != index and compareindex &lt; index:</span><br><span class="line">                <span class="keyword">if</span> str(mylist[compareindex][-<span class="number">1</span>]) == str(num[<span class="number">0</span>]):</span><br><span class="line">                    del mylist[index][<span class="number">0</span>]</span><br><span class="line">                    mylist[index]=mylist[compareindex]+mylist[index]</span><br><span class="line">                    change=True</span><br><span class="line">                <span class="keyword">if</span> str(mylist[compareindex][<span class="number">0</span>]) == str(num[-<span class="number">1</span>]):</span><br><span class="line">                    del mylist[index][-<span class="number">1</span>]</span><br><span class="line">                    mylist[index]=mylist[index]+mylist[compareindex]</span><br><span class="line">                    change=True</span><br><span class="line">        <span class="keyword">if</span> change == True:</span><br><span class="line">            del mylist[compareindex]</span><br><span class="line">        <span class="keyword">if</span> change == False:</span><br><span class="line">            compareindex = compareindex + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index,crushlist <span class="keyword">in</span> enumerate(mylist):</span><br><span class="line">        osdcrushlist=[]</span><br><span class="line">        <span class="keyword">for</span> osdlocaltion <span class="keyword">in</span> crushlist:</span><br><span class="line">            <span class="built_in">local</span>=str(crushid[<span class="string">'%s'</span> %osdlocaltion])</span><br><span class="line">            osdcrushlist.append(<span class="built_in">local</span>)</span><br><span class="line">        <span class="keyword">if</span> osdcrushlist[-<span class="number">1</span>] == osd:</span><br><span class="line">            <span class="built_in">return</span> osdcrushlist</span><br><span class="line"></span><br><span class="line">def getpgmap():</span><br><span class="line">    list_all_host = commands.getoutput(<span class="string">'ceph pg  ls --format json-pretty  2&gt;/dev/null'</span>)</span><br><span class="line">    json_str = json.loads(list_all_host)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> json_str:</span><br><span class="line">        <span class="keyword">for</span> osdid <span class="keyword">in</span> item[<span class="string">'up'</span>]:</span><br><span class="line">            osd=<span class="string">"osd."</span>+str(osdid)</span><br><span class="line">            b=<span class="string">""</span></span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> getosd(osd):</span><br><span class="line">                b=b+str(a)+<span class="string">";"</span></span><br><span class="line">            <span class="built_in">print</span> b+item[<span class="string">'pgid'</span>]+<span class="string">" "</span>+str(item[<span class="string">'stat_sum'</span>][<span class="string">'num_objects'</span>]+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></p>
<h3 id="获取数据">获取数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/sbin/stackcollapse-crush &gt; /tmp/mydata</span><br></pre></td></tr></table></figure>
<h3 id="解析数据">解析数据</h3><p>获取解析脚本，这个脚本是Brendan Gregg写好的，这地方托管到我的github里面了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -O /sbin/flamegraph https://raw.githubusercontent.com/zphj1987/cephcrushflam/master/flamegraph.pl</span><br></pre></td></tr></table></figure></p>
<p>对数据进行解析<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/sbin/flamegraph  --title  <span class="string">"Ceph crush flame graph"</span> --width <span class="string">"1800"</span> --countname <span class="string">"num"</span> /tmp/mydata &gt; /tmp/mycrush.svg</span><br></pre></td></tr></table></figure></p>
<p>将/tmp/mycrush.svg拷贝到windows机器，然后用浏览器打开即可，推荐chrome</p>
<h3 id="效果图如下">效果图如下</h3><p>Example (右键在新窗口中打开):<br><a href="http://7xweck.com1.z0.glb.clouddn.com/mycrush.svg" target="_blank" rel="external"><img src="http://7xweck.com1.z0.glb.clouddn.com/mycrush.svg" alt="Example"></a></p>
<ul>
<li>通过颜色来区分比例占用的区别</li>
<li>支持搜索</li>
<li>tree方式，可以清楚看到分布</li>
<li>可以查看pg对象数目</li>
<li>可以查看osd上面有哪些pg，主机上有哪些osd</li>
</ul>
<h2 id="总结">总结</h2><p>通过ceph osd tree可以查到整个的信息，但是一个屏幕的信息量有限，而通过滚屏或者过滤进行查询的信息，需要做一下关联，而这种可以缩放的svg位图的方式，可以包含大量的信息，如果是做分析的时候还是能比较直观的看到，上面的难点在于获取数据部分，而绘图的部分是直接用的现有的处理，比自己重新开发一个要简单的多，类似的工具还有个桑基图方式，这个在inkscope这个管理平台里面有用到</p>
<p>本篇就是在最小的视野里容纳尽量多的信息量一个实例，其他的数据有类似模型的也可以做相似的处理</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-07-18</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/flame.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>性能优化大神Brendan Gregg发明了火焰图来定位性能问题，通过图表就可以发现问题出在哪里，通过svg矢量图来查看性能卡在哪个点，哪个操作占用的资源最多<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephfs 操作输出到日志查询系统]]></title>
    <link href="http://www.zphj1987.com/2017/07/13/CEPHFS-op-to-graylog/"/>
    <id>http://www.zphj1987.com/2017/07/13/CEPHFS-op-to-graylog/</id>
    <published>2017-07-13T03:32:04.000Z</published>
    <updated>2017-07-13T04:02:53.320Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/flhdkmrs0d5dqt02ymi30phy/log.png" alt="log.png-116.6kB"><br></center>

<h2 id="前言">前言</h2><p>文件系统当中如果某些文件不见了，有什么办法判断是删除了还是自己不见了，这个就需要去日志里面定位了，通常情况下是去翻日志，而日志是会进行压缩的，并且查找起来非常的不方便,还有可能并没有开启<br><a id="more"></a><br>这个时候就需要日志系统了，最近正好看到一篇<a href="https://zhuanlan.zhihu.com/p/27363484" target="_blank" rel="external">最佳日志实践（v2.0）</a>，一篇非常好的文章，本篇日志属于文章里面所提到的统计日志，统计客户端做了什么操作</p>
<p>对于日志系统来说，很重要的一点，能够很方便的进行查询，这就需要对日志信息进行一些处理了，怎么处理就是设计问题，要求就是不多不少</p>
<h2 id="结构">结构</h2><center><br><img src="http://static.zybuluo.com/zphj1987/22grh70azoj3owga4lwmnpmk/mdslogsystem.png" alt="mdslogsystem.png-32.4kB"><br></center>

<p>其中graylog配置部分在这篇<a href="http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/" target="_blank" rel="external">使用日志系统graylog获取Ceph集群状态</a>，根据这篇的操作，配置出12201的udp监听端口即可，剩余部分就是本篇中的配置</p>
<h2 id="配置">配置</h2><h3 id="集群的配置">集群的配置</h3><p>需要对MDS的配置进行debug_ms=1,在/etc/ceph/ceph.conf当中添加下面配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mds.lab8106]</span><br><span class="line">debug_ms=<span class="number">1</span></span><br><span class="line">hostname=lab8106</span><br></pre></td></tr></table></figure></p>
<p>这个地方集群的文件操作日志是记录在message里面的1级别的，所以把mds的debug_ms开到1<br>日志长这个样子：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2017</span>-<span class="number">07</span>-<span class="number">13</span> <span class="number">11</span>:<span class="number">26</span>:<span class="number">23.703624</span> <span class="number">7</span><span class="built_in">fc</span>3128c3700  <span class="number">1</span> -- <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6804</span>/<span class="number">3280969928</span> &lt;== client.<span class="number">14180</span> <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">0</span>/<span class="number">1092795882</span> <span class="number">2384</span> ==== client_request(client.<span class="number">14180</span>:<span class="number">2346</span> mkdir <span class="comment">#1/ppop 2017-07-13 11:26:23.702532 caller_uid=0, caller_gid=0&#123;&#125;) v2 ==== 170+0+0 (843685338 0 0) 0x5645ec243600 con 0x5645ec247000</span></span><br></pre></td></tr></table></figure></p>
<p>下面会对这个日志进行提取</p>
<h3 id="下载logstash">下载logstash</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">https://artifacts.elastic.co/downloads/logstash/logstash-<span class="number">5.5</span>.<span class="number">0</span>.rpm</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rpm -ivh logstash-5.5.0.rpm</span></span><br></pre></td></tr></table></figure>
<p>修改启动进程为root权限<br>修改/etc/systemd/system/logstash.service文件当中的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">User=root</span><br><span class="line">Group=root</span><br></pre></td></tr></table></figure></p>
<p>因为logstash需要本地文件的读取权限，这里是为了方便直接给的root权限，方便使用，如果对权限要求比较严的环境，就给文件</p>
<h3 id="创建一个配置文件">创建一个配置文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/logstash/conf.d/logstash.conf</span><br></pre></td></tr></table></figure>
<p>添加下面的配置文件，这个配置文件包含的内容比较多，会在后面详细的介绍下处理过程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">    path =&gt; <span class="string">"/var/log/ceph/ceph-mds.*.log"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">    grok &#123;</span><br><span class="line">     match =&gt;&#123;<span class="string">"message"</span> =&gt; <span class="string">"%&#123;TIMESTAMP_ISO8601:logtime&#125; %&#123;BASE16FLOAT&#125;  %&#123;BASE10NUM&#125; -- %&#123;HOSTPORT:mdsip&#125;%&#123;NOTSPACE&#125; &lt;== %&#123;NOTSPACE:clientid&#125; %&#123;IP:clientip&#125;%&#123;NOTSPACE&#125; %&#123;INT&#125; ==== client_request\(%&#123;NOTSPACE&#125; %&#123;WORD:do&#125; %&#123;NOTSPACE:where&#125; %&#123;TIMES</span><br><span class="line">TAMP_ISO8601:dotime&#125;%&#123;GREEDYDATA&#125;"</span>&#125;</span><br><span class="line">    overwrite =&gt; [<span class="string">"message"</span>]</span><br><span class="line">    remove_field =&gt;[<span class="string">"logtime"</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ![dotime] &#123;</span><br><span class="line">        drop &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"mkdir"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"创建目录"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"create"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"创建文件"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"unlink"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"删除文件"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"rmdir"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"删除目录"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"rename"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"重命名"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"symlink"</span> &#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"链接"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> [<span class="keyword">do</span>] == <span class="string">"unlink"</span>&#123;</span><br><span class="line">        mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"do"</span> =&gt; <span class="string">"删除链接"</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    mutate &#123;</span><br><span class="line">        replace =&gt; &#123; <span class="string">"message"</span> =&gt; <span class="string">"time:%&#123;dotime&#125; ClientIp:%&#123;clientip&#125;  ClintId:%&#123;clientid&#125;  MdsIp: %&#123;mdsip&#125; do:%&#123;do&#125; where:%&#123;where&#125;"</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    date &#123;</span><br><span class="line">        match =&gt; [ <span class="string">"dotime"</span>, <span class="string">"yyyy-MM-dd HH:mm:ss.SSSSSS"</span>]</span><br><span class="line">        target =&gt; <span class="string">"@timestamp"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    gelf &#123;</span><br><span class="line">        host =&gt; <span class="string">"192.168.8.106"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stdout &#123; codec =&gt; json_lines&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>处理后的日志是这个样子：</p>
<blockquote>
<p>{“path”:”/var/log/ceph/ceph-mds.lab8106.log”,”@timestamp”:”2017-07-13T04:01:01.251Z”,”clientid”:”client.14180”,”clientip”:”192.168.8.106”,”@version”:”1”,”host”:”lab8106”,”where”:”#1/test1”,”do”:”创建目录”,”message”:”time:2017-07-13 12:01:01.251358 ClientIp:192.168.8.106  ClintId:client.14180  MdsIp: 192.168.8.106:6804 do:创建目录 where:#1/test1”,”mdsip”:”192.168.8.106:6804”,”dotime”:”2017-07-13 12:01:01.251358”}</p>
</blockquote>
<p>是一个json形式的，可以根据自己的需要增加减少字段，这些信息都会传递到graylog当中</p>
<h3 id="解析配置文件">解析配置文件</h3><p>logstash配置文件的结构包括三个大模块：</p>
<ul>
<li>input</li>
<li>filter</li>
<li>output</li>
</ul>
<p>input是文件的来源，也就是我们需要解析的日志，filter是处理日志的模块，output是输出的模块，这里我们需要使用的是gelf的输出模式，在本地进行调试的时候，可以开启stdout来进行调试</p>
<p>采用grok进行正则匹配，这个里面的匹配正则可以用 <a href="http://grokconstructor.appspot.com/do/construction" target="_blank" rel="external">http://grokconstructor.appspot.com/do/construction</a> 这个进行正则表达式的验证和书写，可以一步步的进行匹配，还是很方便的工具</p>
<p>output输出gelf的信息流</p>
<h3 id="grok内部解析">grok内部解析</h3><ul>
<li>remove_field可以用来删除无用的字段</li>
<li>if ![dotime] 这个是用来过滤消息的，如果没拿到这个值，也就是没匹配上的时候，就把消息丢弃</li>
<li>使用mutate replace模块来进行字段的替换，将固定操作转换为中文</li>
<li>使用mutate replace模块来重写message，根据自己定义的格式进行输出</li>
<li>使用date 模块进行@timestamp的重写，将日志内的时间写入到这个里面</li>
</ul>
<p>查询插件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/share/logstash/bin/logstash-plugin list</span><br></pre></td></tr></table></figure></p>
<p>logstash-output-gelf默认没有安装的,需要安装一下</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/share/logstash/bin/logstash-plugin install logstash-output-gelf</span><br></pre></td></tr></table></figure>
<p>这个安装可能有点慢，稍微多等一下</p>
<p>调试方式的启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/share/logstash/bin/logstash <span class="operator">-f</span> /etc/logstash/conf.d/logstash.conf</span><br></pre></td></tr></table></figure></p>
<p>如果调试完毕了后就用系统命令启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart logstash</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/95e2t54owr6ket33b9t00fa8/graylogfs.png" alt="graylogfs.png-188.3kB"></p>
<p>通过graylog系统就可以很方便的看到日志里面节获取的内容了</p>
<h3 id="总结">总结</h3><p>对于一套系统来说，日志系统是一个很重要的组成部分，可以更好的掌握系统内部的运行情况，并不是说出了问题再去找日志，这个日志的需求来源其实很简单</p>
<blockquote>
<p>哪个客户端对着哪个MDS做了一个什么操作</p>
</blockquote>
<p>然后就可以用这个搜索引擎去进行相关的搜索了，可以查询一段时间创建了多少文件，是不是删除了哪个文件</p>
<p>本次实践的难点在于logstash对日志的相关解析的操作，掌握了方法以后，对于其他日志的提取也可以用类似的方法，提取自己需要的信息，然后进行整合，输出到一个系统当中，剩下的就是在界面上获取信息</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-07-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/flhdkmrs0d5dqt02ymi30phy/log.png" alt="log.png-116.6kB"><br></center>

<h2 id="前言">前言</h2><p>文件系统当中如果某些文件不见了，有什么办法判断是删除了还是自己不见了，这个就需要去日志里面定位了，通常情况下是去翻日志，而日志是会进行压缩的，并且查找起来非常的不方便,还有可能并没有开启<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph luminous 新功能之磁盘智能分组]]></title>
    <link href="http://www.zphj1987.com/2017/06/28/ceph-luminous-new-osd-class/"/>
    <id>http://www.zphj1987.com/2017/06/28/ceph-luminous-new-osd-class/</id>
    <published>2017-06-28T07:51:07.000Z</published>
    <updated>2017-06-28T08:50:13.483Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ssd.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇是luminous一个新功能介绍，关于磁盘智能分组的，这个在ceph里面叫crush class，这个我自己起名叫磁盘智能分组，因为这个实现的功能就是根据磁盘类型进行属性关联，然后进行分类，减少了很多的人为操作</p>
<p>以前我们需要对ssd和hdd进行分组的时候，需要大量的修改crush map，然后绑定不同的存储池到不同的 crush 树上面，现在这个逻辑简化了很多</p>
<a id="more"></a>
<blockquote>
<p>ceph crush class {create,rm,ls} manage the new CRUSH device<br>class feature. ceph crush set-device-class <osd> <class><br>will set the clas for a particular device.</class></osd></p>
<p>Each OSD can now have a device class associated with it (e.g., hdd or<br>ssd), allowing CRUSH rules to trivially map data to a subset of devices<br>in the system. Manually writing CRUSH rules or manual editing of the CRUSH is normally not required.</p>
</blockquote>
<p>这个是发布的公告里面关于这两个功能的说明的，本篇就来看看这个功能怎么用</p>
<h2 id="实践">实践</h2><h3 id="首先创建分类的规则">首先创建分类的规则</h3><p>创建一个ssd的分组<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush class create  ssd</span></span><br><span class="line">created class ssd with id <span class="number">0</span> to crush map</span><br></pre></td></tr></table></figure></p>
<p>也就是一个名称，这里我认为是ssd的分组就创建名词为ssd</p>
<p>再创建一个hdd的分组<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush class create  hdd</span></span><br><span class="line">created class hdd with id <span class="number">1</span> to crush map</span><br></pre></td></tr></table></figure></p>
<h3 id="查询分组规则">查询分组规则</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush class ls</span></span><br><span class="line">[</span><br><span class="line">    <span class="string">"ssd"</span>,</span><br><span class="line">    <span class="string">"hdd"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3 id="把osd绑定不同的属性(属性名称就是上面的分类)">把osd绑定不同的属性(属性名称就是上面的分类)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush set-device-class osd.0  ssd</span></span><br><span class="line"><span class="built_in">set</span>-device-class item id <span class="number">0</span> name <span class="string">'osd.0'</span> device_class ssd</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush set-device-class osd.2  ssd</span></span><br><span class="line"><span class="built_in">set</span>-device-class item id <span class="number">2</span> name <span class="string">'osd.2'</span> device_class ssd</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush set-device-class osd.1  hdd</span></span><br><span class="line"><span class="built_in">set</span>-device-class item id <span class="number">1</span> name <span class="string">'osd.1'</span> device_class hdd</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush set-device-class osd.3  hdd</span></span><br><span class="line"><span class="built_in">set</span>-device-class item id <span class="number">3</span> name <span class="string">'osd.3'</span> device_class hdd</span><br></pre></td></tr></table></figure>
<p>查询设置以后的效果<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME            UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">6</span> <span class="number">0.54559</span> root default~hdd                                       </span><br><span class="line">-<span class="number">5</span> <span class="number">0.54559</span>     host lab8106~hdd                                   </span><br><span class="line"> <span class="number">1</span> <span class="number">0.27280</span>         osd.<span class="number">1</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">3</span> <span class="number">0.27280</span>         osd.<span class="number">3</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">4</span> <span class="number">0.54559</span> root default~ssd                                       </span><br><span class="line">-<span class="number">3</span> <span class="number">0.54559</span>     host lab8106~ssd                                   </span><br><span class="line"> <span class="number">0</span> <span class="number">0.27280</span>         osd.<span class="number">0</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.27280</span>         osd.<span class="number">2</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">1</span> <span class="number">1.09119</span> root default                                           </span><br><span class="line">-<span class="number">2</span> <span class="number">1.09119</span>     host lab8106                                       </span><br><span class="line"> <span class="number">0</span> <span class="number">0.27280</span>         osd.<span class="number">0</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.27280</span>         osd.<span class="number">1</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.27280</span>         osd.<span class="number">2</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">3</span> <span class="number">0.27280</span>         osd.<span class="number">3</span>             up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure></p>
<p>这个就是这个功能比较核心的地方，会根据磁盘类型不同，自动的创建了不同的树，并且把磁盘放入到了树里面去了</p>
<h3 id="根据根创建规则（这个地方有bug，下面会提及）">根据根创建规则（这个地方有bug，下面会提及）</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd crush rule create-simple ssd default~ssd host firstn</span></span><br></pre></td></tr></table></figure>
<p>检查创建的rule规则：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 build]<span class="comment"># ceph   osd  crush rule  dump ssd</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"rule_id"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"rule_name"</span>: <span class="string">"ssd"</span>,</span><br><span class="line">    <span class="string">"ruleset"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"type"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"min_size"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"max_size"</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">"steps"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"op"</span>: <span class="string">"take"</span>,</span><br><span class="line">            <span class="string">"item"</span>: -<span class="number">4</span>,</span><br><span class="line">            <span class="string">"item_name"</span>: <span class="string">"default~ssd"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"op"</span>: <span class="string">"chooseleaf_firstn"</span>,</span><br><span class="line">            <span class="string">"num"</span>: <span class="number">0</span>,</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"host"</span></span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"op"</span>: <span class="string">"emit"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="根据rule创建存储池">根据rule创建存储池</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph  osd pool create testpool <span class="number">64</span> <span class="number">64</span> ssd</span><br><span class="line">ceph   osd dump|grep pool</span><br><span class="line">pool <span class="number">3</span> <span class="string">'testpool'</span> replicated size <span class="number">3</span> min_size <span class="number">1</span> crush_rule <span class="number">1</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">27</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>这里有个验证规则的小bug  代码在src/mon/MonCommands.h<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> COMMAND(<span class="string">"osd crush rule create-simple "</span> \</span><br><span class="line"><span class="string">"name=name,type=CephString,goodchars=[A-Za-z0-9-_.] "</span> \</span><br><span class="line"><span class="string">"name=root,type=CephString,goodchars=[A-Za-z0-9-_.] "</span> \</span><br><span class="line"><span class="string">"name=type,type=CephString,goodchars=[A-Za-z0-9-_.] "</span> \</span><br></pre></td></tr></table></figure></p>
<p>默认的goodchars不包含’~’,这里不清楚社区是准备去改创建的逻辑去掉这个特殊符号，还是去改创建rule相关的规则，我已经提交了<a href="http://tracker.ceph.com/issues/20446" target="_blank" rel="external">issue#20446</a>，等待社区的修改方案</p>
<h2 id="功能逻辑">功能逻辑</h2><h3 id="现在方法">现在方法</h3><p>创建一个磁盘类型的class，给磁盘标记class的统一标签，自动会根据class的类型创建一个树，根据树创建rule，根据rule创建存储池，整个操作没有动crushmap的操作</p>
<p>增加或修改盘的时候，设置下属性即可</p>
<h3 id="以前的方法">以前的方法</h3><p>先添加盘，手动创建树，新加的osd要找下原来的树的名称，然后把osd放到这个树里面去，然后创建规则，根据rule创建存储池</p>
<p>增加盘或修改盘的时候，需要查找下，然后根据查找的规则进行相关操作</p>
<h2 id="总结">总结</h2><p>现在方法对用户操作来说更透明，直接对磁盘进行分类打标签即可，减少了一些复杂的操作逻辑，是一个很不错的功能</p>
<h2 id="更新">更新</h2><p>后面会在crush rule创建的时候指定一个class的选项，就可以不改规则，也不改命令了<br><a href="https://www.spinics.net/lists/ceph-devel/msg37343.html，下个版本的rc应该会解决" target="_blank" rel="external">https://www.spinics.net/lists/ceph-devel/msg37343.html，下个版本的rc应该会解决</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-28</td>
</tr>
<tr>
<td style="text-align:center">更新进度</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ssd.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇是luminous一个新功能介绍，关于磁盘智能分组的，这个在ceph里面叫crush class，这个我自己起名叫磁盘智能分组，因为这个实现的功能就是根据磁盘类型进行属性关联，然后进行分类，减少了很多的人为操作</p>
<p>以前我们需要对ssd和hdd进行分组的时候，需要大量的修改crush map，然后绑定不同的存储池到不同的 crush 树上面，现在这个逻辑简化了很多</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph luminous 新功能之内置dashboard]]></title>
    <link href="http://www.zphj1987.com/2017/06/25/ceph-luminous-new-dashboard/"/>
    <id>http://www.zphj1987.com/2017/06/25/ceph-luminous-new-dashboard/</id>
    <published>2017-06-25T15:59:44.000Z</published>
    <updated>2017-08-29T10:20:17.350Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dashboard.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>ceph luminous版本新增加了很多有意思的功能，这个也是一个长期支持版本，所以这些新功能的特性还是很值得期待的，从底层的存储改造，消息方式的改变，以及一些之前未实现的功能的完成，都让ceph变得更强，这里面有很多核心模块来自中国的开发者，在这里准备用一系列的文章对这些新功能进行一个简单的介绍，也是自己的一个学习的过程</p>
<a id="more"></a>
<h2 id="相关配置">相关配置</h2><h3 id="配置ceph国内源">配置ceph国内源</h3><p>修改 /etc/yum.repos.d/ceph.repo文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">name=ceph</span><br><span class="line">baseurl=http://mirrors.<span class="number">163</span>.com/ceph/rpm-luminous/el7/x86_64/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=cephnoarch</span><br><span class="line">baseurl=http://mirrors.<span class="number">163</span>.com/ceph/rpm-luminous/el7/noarch/</span><br><span class="line">gpgcheck=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>添加完更新下缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum makecache</span><br></pre></td></tr></table></figure></p>
<p>前一段时间163源上的ceph没有了，可能是误操作的，现在的163源已经恢复，上面添加的是最新的luminous版本源，本篇实践的功能是在这个版本才加入的</p>
<h3 id="安装ceph相关软件包">安装ceph相关软件包</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum install ceph-deploy ceph</span></span><br></pre></td></tr></table></figure>
<p>检查版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">12.1</span>.<span class="number">0</span> (<span class="number">262617</span>c9f16c55e863693258061c5b25dea5b086) luminous (dev)</span><br></pre></td></tr></table></figure></p>
<h3 id="搭建一个集群">搭建一个集群</h3><p>这个就不描述配置集群的步骤，这个网上很多资料，也是很基础的操作<br>这里提几个luminous重要的变化</p>
<ul>
<li>默认的消息处理从simple变成了async了（ms_type = async+posix）</li>
<li>默认的后端存储从filestore变成了bluestore了</li>
<li>ceph-s的命令的输出发生了改变(显示如下)</li>
</ul>
<p>添加mgr<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy mgr create lab8106</span><br><span class="line">ceph mgr module <span class="built_in">enable</span> dashboard</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     <span class="number">49</span>ee8a7f-fb7c-<span class="number">4239</span><span class="operator">-a</span>4b7-acf0bc37430d</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: <span class="number">1</span> daemons, quorum lab8106</span><br><span class="line">    mgr: lab8106(active)</span><br><span class="line">    osd: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   <span class="number">1</span> pools, <span class="number">64</span> pgs</span><br><span class="line">    objects: <span class="number">0</span> objects, <span class="number">0</span> bytes</span><br><span class="line">    usage:   <span class="number">2110</span> MB used, <span class="number">556</span> GB / <span class="number">558</span> GB avail</span><br><span class="line">    pgs:     <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure>
<h3 id="开启监控模块">开启监控模块</h3><p>在/etc/ceph/ceph.conf中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br></pre></td></tr></table></figure></p>
<p>设置dashboard的ip和端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph config-key put mgr/dashboard/server_addr <span class="number">192.168</span>.<span class="number">8.106</span></span><br><span class="line">ceph config-key put mgr/dashboard/server_port <span class="number">7000</span></span><br></pre></td></tr></table></figure></p>
<p>这个从代码上看应该是可以支持配置文件方式的设置，目前还没看到具体的文档，先按这个设置即可，默认的端口是7000</p>
<p>重启mgr服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl restart ceph-mgr@lab8106</span></span><br></pre></td></tr></table></figure></p>
<p>检查端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># netstat -tunlp|grep 7000</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">7000</span>      <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">31485</span>/ceph-mgr</span><br></pre></td></tr></table></figure></p>
<h3 id="访问界面">访问界面</h3><p><img src="http://7xweck.com1.z0.glb.clouddn.com/health.png" alt="dashboard"><br>这个是首页的信息</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/server.png" alt="image.png-137.3kB"><br>这个是主机的相关信息</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/osdstat.png" alt="servers"><br>这个界面是显示的osd相关的信息的</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/rbd.png" alt="rbd"></p>
<p>rbd相关的信息</p>
<p><img src="http://7xweck.com1.z0.glb.clouddn.com/filesystem.png" alt="filesystem"><br>文件系统相关的信息</p>
<h2 id="总结">总结</h2><p>从部署方便性来说，这个部署还是非常的方便的，而且走的是ceph原生接口，ceph通过增加一个mgr模块，可以把一些管理的功能独立出来，从而让mon自己做最重要的一些事情</p>
<p>目前的监控功能还比较少，主要是监控功能，未来应该会慢慢增加更多的功能，从产品角度来看，一个原生的UI监控使得ceph整个模块更加的完整了</p>
<p>有的时候也许 simple is the best</p>
<h2 id="参考资料">参考资料</h2><p>/usr/lib64/ceph/mgr/dashboard/README.rst</p>
<h2 id="补充">补充</h2><p>目前还缺iscsi部分的，这个需要看下底层iscsi的实现方法</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-26</td>
</tr>
<tr>
<td style="text-align:center">更新最新版的</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-08-29</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dashboard.jpg" alt=""><br></center>

<h2 id="前言">前言</h2><p>ceph luminous版本新增加了很多有意思的功能，这个也是一个长期支持版本，所以这些新功能的特性还是很值得期待的，从底层的存储改造，消息方式的改变，以及一些之前未实现的功能的完成，都让ceph变得更强，这里面有很多核心模块来自中国的开发者，在这里准备用一系列的文章对这些新功能进行一个简单的介绍，也是自己的一个学习的过程</p>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[调整PG分多次调整和一次到位的迁移差别分析]]></title>
    <link href="http://www.zphj1987.com/2017/06/14/different-change-pg/"/>
    <id>http://www.zphj1987.com/2017/06/14/different-change-pg/</id>
    <published>2017-06-14T05:39:26.000Z</published>
    <updated>2017-06-14T05:41:18.539Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/diff.png" alt="different"><br></center>

<h2 id="前言">前言</h2><p>这个问题来源于我们研发的一个问题，在进行pg调整的时候，是一次调整到位好，还是分多次调整比较好，分多次调整的时候会不会出现某个pg反复挪动的问题，造成整体迁移量大于一次调整的</p>
<p>最近自己的项目上也有pg调整的需求，这个需求一般来源于pg规划好了，后期出现节点扩容的情况，需要对pg进行增加的调整</p>
<p>本篇用具体的数据来分析两种方式的差别</p>
<p>因为本篇的篇幅较长，直接先把结论拿出来<br><a id="more"></a></p>
<h2 id="数据结论">数据结论</h2><table>
<thead>
<tr>
<th style="text-align:center">调整pg</th>
<th style="text-align:center">迁移pg</th>
<th style="text-align:center">迁移对象</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1200-&gt;1440</td>
<td style="text-align:center">460</td>
<td style="text-align:center">27933</td>
</tr>
<tr>
<td style="text-align:center">1440-&gt;1680</td>
<td style="text-align:center">458</td>
<td style="text-align:center">27730</td>
</tr>
<tr>
<td style="text-align:center">1680-&gt;1920</td>
<td style="text-align:center">465</td>
<td style="text-align:center">27946</td>
</tr>
<tr>
<td style="text-align:center">1920-&gt;2160</td>
<td style="text-align:center">457</td>
<td style="text-align:center">21141</td>
</tr>
<tr>
<td style="text-align:center">2160-&gt;2400</td>
<td style="text-align:center">458</td>
<td style="text-align:center">13938</td>
</tr>
<tr>
<td style="text-align:center">总和</td>
<td style="text-align:center">2305</td>
<td style="text-align:center">132696</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align:center">调整pg</th>
<th style="text-align:center">迁移pg</th>
<th style="text-align:center">迁移对象</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1200-&gt;2400</td>
<td style="text-align:center">2299</td>
<td style="text-align:center">115361</td>
</tr>
</tbody>
</table>
<p>结论：<br>分多次调整的时候，PG迁移量比一次调整多了6个，多了0.2%，对象的迁移量多了17335，多了15%</p>
<p>从数据上看pg迁移的数目基本一样，但是数据量是多了15%，这个是因为分多次迁移的时候，在pg基数比较小的时候，迁移一个pg里面的对象要比后期分裂以后的对象要多，就产生了这个数据量的差别</p>
<p>从整体上来看二者需要迁移的pg基本差不多，数据量上面会增加15%，分多次的时候是可以进行周期性调整的，拆分到不同的时间段来做，所以各有好处</p>
<h2 id="实践">实践</h2><h3 id="环境准备">环境准备</h3><p>本次测试采用的是开发环境，使用开发环境可以很快的部署一个需要的环境，本次分析采用的就是一台机器模拟的4台机器48个 4T osd的环境</p>
<h4 id="环境搭建">环境搭建</h4><p>生成集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./vstart.sh -n  --mon_num <span class="number">1</span> --osd_num <span class="number">48</span> --mds_num <span class="number">1</span> --short  <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>后续操作都在源码的src目录下面执行</p>
<p>设置存储池副本为2</p>
<p>修改crush weight 为3.7模拟4T盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">seq <span class="number">0</span> <span class="number">47</span>| xargs -i ./ceph -c ceph.conf osd crush reweight osd.&#123;&#125; <span class="number">3.8</span></span><br></pre></td></tr></table></figure></p>
<p>模拟主机分组<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">seq <span class="number">0</span> <span class="number">11</span> |xargs -i ./ceph -c ceph.conf osd crush <span class="built_in">set</span> osd.&#123;&#125; <span class="number">3.8</span> host=lab8106 root=default</span><br><span class="line">seq <span class="number">12</span> <span class="number">23</span> |xargs -i ./ceph -c ceph.conf osd crush <span class="built_in">set</span> osd.&#123;&#125; <span class="number">3.8</span> host=lab8107 root=default</span><br><span class="line">seq <span class="number">24</span> <span class="number">35</span> |xargs -i ./ceph -c ceph.conf osd crush <span class="built_in">set</span> osd.&#123;&#125; <span class="number">3.8</span> host=lab8108 root=default</span><br><span class="line">seq <span class="number">36</span> <span class="number">47</span> |xargs -i ./ceph -c ceph.conf osd crush <span class="built_in">set</span> osd.&#123;&#125; <span class="number">3.8</span> host=lab8109 root=default</span><br></pre></td></tr></table></figure></p>
<p>48个osd设置初始pg为1200，让每个osd上面差不多50个pg左右，做一下均衡操作，后续目标调整为pg为2400</p>
<p>准备120000个小文件准备put进去集群，使每个pg上面对象100个左右<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./rados -c ceph.conf -p rbd bench -b <span class="number">1</span>K <span class="number">600</span> write --no-cleanup</span><br></pre></td></tr></table></figure></p>
<h3 id="一次调整pg到2400">一次调整pg到2400</h3><p>统计一次调整到位的情况下的数据迁移情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph  -c ceph.conf  osd pool <span class="built_in">set</span> rbd pg_num <span class="number">2400</span></span><br></pre></td></tr></table></figure></p>
<p>记录当前的pg分布的情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappg_1200_pgp_2400</span><br></pre></td></tr></table></figure></p>
<p>调整存储池的pgp为2400<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd  pgp_num <span class="number">2400</span></span><br></pre></td></tr></table></figure></p>
<p>等迁移完成以后，统计最终的pg分布情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappg2400_pgp2400</span><br></pre></td></tr></table></figure></p>
<p>这里说明一下，调整pg的时候只会触发pg的分裂，并不会影响集群的分布，也就是不会出现pg迁移的情况，调整pgp以后才会去改变pg的分布，所以本次数据分析统计的是pgp变动后的迁移的数据量，这个量才是集群的真正的迁移量</p>
<p>用比较的脚本来进行统计（脚本会在本文文末提供）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment">#python compair.py pgmappg_1200_pgp_2400 pgmappg2400_pgp2400</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">2299</span>, <span class="number">115361</span>]</span><br></pre></td></tr></table></figure></p>
<p>也就是整个环境有2299次pg的变动，总共迁移的对象数目为115361个</p>
<h3 id="分五次调整到2400PG">分五次调整到2400PG</h3><h4 id="初始pg为1200个第一次调整，1200PG调整到1440PG">初始pg为1200个第一次调整，1200PG调整到1440PG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">1440</span></span><br></pre></td></tr></table></figure>
<p>调整pg为1440,当前pgp为1200<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaira1</span><br></pre></td></tr></table></figure></p>
<p>调整pgp为1440,当前pg为1440<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaira2</span><br></pre></td></tr></table></figure></p>
<p>统计第一次调整后的迁移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgdata]<span class="comment"># python compair.py pgmappaira1 pgmappaira2</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">460</span>, <span class="number">27933</span>]</span><br></pre></td></tr></table></figure></p>
<h4 id="第二次调整，1440PG调整到1680PG">第二次调整，1440PG调整到1680PG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">1680</span></span><br></pre></td></tr></table></figure>
<p>调整pg为1680,当前pgp为1440<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappairb1</span><br></pre></td></tr></table></figure></p>
<p>调整pgp为1680,当前pg为1680<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappairb2</span><br></pre></td></tr></table></figure></p>
<p>统计第二次调整后的迁移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgdata]<span class="comment"># python compair.py pgmappairb1 pgmappairb2</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">458</span>, <span class="number">27730</span>]</span><br></pre></td></tr></table></figure></p>
<h4 id="第三次调整，1680PG调整到1920PG">第三次调整，1680PG调整到1920PG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">1920</span></span><br></pre></td></tr></table></figure>
<p>调整pg为1920,当前pgp为1680<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappairc1</span><br></pre></td></tr></table></figure></p>
<p>调整pgp为1920,当前pg为1920<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappairc2</span><br></pre></td></tr></table></figure></p>
<p>统计第三次调整后的迁移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgdata]<span class="comment"># python compair.py  pgmappairc1 pgmappairc2</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">465</span>, <span class="number">27946</span>]</span><br></pre></td></tr></table></figure></p>
<h4 id="第四次调整，1920PG调整到2160PG">第四次调整，1920PG调整到2160PG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">2160</span></span><br></pre></td></tr></table></figure>
<p>调整pg为2160,当前pgp为1920<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaird1</span><br></pre></td></tr></table></figure></p>
<p>调整pgp为2160,当前pg为2160<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaird2</span><br></pre></td></tr></table></figure></p>
<p>统计第四次调整后的迁移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgdata]<span class="comment"># python compair.py pgmappaird1 pgmappaird2</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">457</span>, <span class="number">21141</span>]</span><br></pre></td></tr></table></figure></p>
<h4 id="第五次调整，2160PG调整到2400PG">第五次调整，2160PG调整到2400PG</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">2400</span></span><br></pre></td></tr></table></figure>
<p>调整pg为2400,当前pgp为2160<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaire1</span><br></pre></td></tr></table></figure></p>
<p>调整pgp为2400,当前pg为2400<br>记录当前的pg分布数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf pg dump pgs|awk <span class="string">'&#123;print $1,$2,$15,$17&#125;'</span> &gt; pgmappaire2</span><br></pre></td></tr></table></figure></p>
<p>统计第五次调整后的迁移量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 pgdata]<span class="comment"># python compair.py pgmappaire1 pgmappaire2</span></span><br><span class="line">| pgs | objects |</span><br><span class="line">-----------------</span><br><span class="line">[<span class="number">458</span>, <span class="number">13938</span>]</span><br></pre></td></tr></table></figure></p>
<p>上面五次加起来的总量为<br>2305 PGS    132696 objects</p>
<h2 id="统计的脚本">统计的脚本</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"> <span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">__author__ =<span class="string">"zp"</span></span><br><span class="line">import os,sys</span><br><span class="line"></span><br><span class="line">class filetojson(object):</span><br><span class="line">    def __init__(self,orin,new):</span><br><span class="line">        self.origin=orin</span><br><span class="line">        self.new=new</span><br><span class="line"></span><br><span class="line">    def tojson(self,filename):</span><br><span class="line">        data=&#123;&#125;</span><br><span class="line">        pginfo=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(filename):</span><br><span class="line">            <span class="keyword">if</span> <span class="string">"pg_stat"</span> <span class="keyword">in</span> line:</span><br><span class="line">                <span class="built_in">continue</span></span><br><span class="line">            lines=line.split()</span><br><span class="line">            pg=lines[<span class="number">0</span>]</span><br><span class="line">            objects=lines[<span class="number">1</span>]</span><br><span class="line">            withosd=lines[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">            data[pg]=&#123;<span class="string">'objects'</span>:objects,<span class="string">'osd'</span>:list(<span class="built_in">eval</span>(withosd))&#125;</span><br><span class="line">        <span class="built_in">return</span> data</span><br><span class="line"></span><br><span class="line">    def compare(self):</span><br><span class="line">        movepg=<span class="number">0</span></span><br><span class="line">        allmovepg=<span class="number">0</span></span><br><span class="line">        allmoveobject=<span class="number">0</span></span><br><span class="line">        moveobject=<span class="number">0</span></span><br><span class="line">        oringinmap=self.tojson(self.origin)</span><br><span class="line">        newmap=self.tojson(self.new)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> oringinmap:</span><br><span class="line">            amapn=<span class="built_in">set</span>(oringinmap[key][<span class="string">'osd'</span>])</span><br><span class="line">            bmapn=<span class="built_in">set</span>(newmap[key][<span class="string">'osd'</span>])</span><br><span class="line">            movepg=len(list(amapn.difference(bmapn)))</span><br><span class="line">            <span class="keyword">if</span> movepg != <span class="number">0</span>:</span><br><span class="line">                moveobject=int(oringinmap[key][<span class="string">'objects'</span>]) * int(movepg)</span><br><span class="line">                allmovepg=allmovepg+movepg</span><br><span class="line">                allmoveobject=allmoveobject+moveobject</span><br><span class="line">        <span class="built_in">return</span> [allmovepg,allmoveobject]</span><br><span class="line"></span><br><span class="line">mycom=filetojson(sys.argv[<span class="number">1</span>],sys.argv[<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span> <span class="string">"| pgs | objects |"</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">"-----------------"</span></span><br><span class="line"><span class="built_in">print</span> mycom.compare()</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>本篇是对集群进行pg调整的这个场景下迁移的数据进行分析的，对于一个集群来说，还是要用数据来进行问题的说明会比较有说服力，凭感觉还是没有那么强的说服力，本篇因为环境所限，所以在模拟的时候采用的是单个pg100个对象的样本，如果需要更精确的数据可以采用多次测试，并且加大这个单个pg的对象数目</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-14</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/diff.png" alt="different"><br></center>

<h2 id="前言">前言</h2><p>这个问题来源于我们研发的一个问题，在进行pg调整的时候，是一次调整到位好，还是分多次调整比较好，分多次调整的时候会不会出现某个pg反复挪动的问题，造成整体迁移量大于一次调整的</p>
<p>最近自己的项目上也有pg调整的需求，这个需求一般来源于pg规划好了，后期出现节点扩容的情况，需要对pg进行增加的调整</p>
<p>本篇用具体的数据来分析两种方式的差别</p>
<p>因为本篇的篇幅较长，直接先把结论拿出来<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[使用日志系统graylog获取Ceph集群状态]]></title>
    <link href="http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/"/>
    <id>http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/</id>
    <published>2017-06-09T08:30:56.000Z</published>
    <updated>2017-07-13T03:22:31.448Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/graylog.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在看集群的配置文件的时候看到ceph里面有一个graylog的输出选择，目前看到的是可以收集mon日志和clog，osd单个的日志没有看到，Elasticsearch有整套的日志收集系统，可以很方便的将所有日志汇总到一起，这个graylog的收集采用的是自有的udp协议，从配置上来说可以很快的完成，这里只做一个最基本的实践<br><a id="more"></a></p>
<h2 id="系统实践">系统实践</h2><p>graylog日志系统主要由三个组件组成的</p>
<ul>
<li>MongoDB – 存储配置信息和一些元数据信息的，MongoDB (&gt;= 2.4)</li>
<li>Elasticsearch – 用来存储Graylog server收取的log messages的，Elasticsearch (&gt;= 2.x)</li>
<li>Graylog server – 用来解析日志的并且提供内置的web的访问接口</li>
</ul>
<p>配置好基础源文件</p>
<blockquote>
<p>CentOS-Base.repo<br>epel.repo</p>
</blockquote>
<h3 id="安装java">安装java</h3><p>要求版本Java (&gt;= 8)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install java-<span class="number">1.8</span>.<span class="number">0</span>-openjdk</span><br></pre></td></tr></table></figure></p>
<h3 id="安装MongoDB">安装MongoDB</h3><p>安装软件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install mongodb mongodb-server</span><br></pre></td></tr></table></figure></p>
<p>启动服务并且加入自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart mongod</span><br><span class="line">systemctl <span class="built_in">enable</span> mongod</span><br></pre></td></tr></table></figure></p>
<p>安装完成检查服务启动端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># netstat -tunlp|grep 27017</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">27017</span>         <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">151840</span>/mongod</span><br></pre></td></tr></table></figure></p>
<h3 id="安装Elasticsearch">安装Elasticsearch</h3><p>倒入认证文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch</span><br></pre></td></tr></table></figure></p>
<p>添加源文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/yum.repos.d/elasticsearch.repo</span><br><span class="line">添加</span><br><span class="line">[elasticsearch-<span class="number">2</span>.x]</span><br><span class="line">name=Elasticsearch repository <span class="keyword">for</span> <span class="number">2</span>.x packages</span><br><span class="line">baseurl=https://packages.elastic.co/elasticsearch/<span class="number">2</span>.x/centos</span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line">gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch</span><br><span class="line">enabled=<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>安装elasticsearch包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install elasticsearch</span><br></pre></td></tr></table></figure></p>
<p>配置自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> elasticsearch</span><br></pre></td></tr></table></figure></p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># vim /etc/elasticsearch/elasticsearch.yml</span></span><br><span class="line"></span><br><span class="line">cluster.name: graylog</span><br></pre></td></tr></table></figure></p>
<p>重启服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart  elasticsearch</span><br></pre></td></tr></table></figure></p>
<p>检查运行服务端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># netstat -tunlp|grep java</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span>          <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">154116</span>/java </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9300</span>          <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">154116</span>/java</span><br></pre></td></tr></table></figure></p>
<p>检查elasticsearch状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#  curl -X GET http://localhost:9200</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"name"</span> : <span class="string">"Vibro"</span>,</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog"</span>,</span><br><span class="line">  <span class="string">"cluster_uuid"</span> : <span class="string">"11Y2GOTmQ9ynNbTlruFcyA"</span>,</span><br><span class="line">  <span class="string">"version"</span> : &#123;</span><br><span class="line">    <span class="string">"number"</span> : <span class="string">"2.4.5"</span>,</span><br><span class="line">    <span class="string">"build_hash"</span> : <span class="string">"c849dd13904f53e63e88efc33b2ceeda0b6a1276"</span>,</span><br><span class="line">    <span class="string">"build_timestamp"</span> : <span class="string">"2017-04-24T16:18:17Z"</span>,</span><br><span class="line">    <span class="string">"build_snapshot"</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"lucene_version"</span> : <span class="string">"5.5.4"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"tagline"</span> : <span class="string">"You Know, for Search"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># curl -XGET 'http://localhost:9200/_cluster/health?pretty=true'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"cluster_name"</span> : <span class="string">"graylog"</span>,</span><br><span class="line">  <span class="string">"status"</span> : <span class="string">"green"</span>,</span><br></pre></td></tr></table></figure>
<p>状态应该是green</p>
<h3 id="安装graylog">安装graylog</h3><p>安装源<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -Uvh https://packages.graylog2.org/repo/packages/graylog-<span class="number">2.2</span>-repository_latest.rpm</span><br></pre></td></tr></table></figure></p>
<p>安装软件包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install graylog-server pwgen</span><br></pre></td></tr></table></figure></p>
<p>生成password_secret<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># pwgen -N 1 -s 96</span></span><br><span class="line">DoqTYuvQPHaNW6XGFj5jru3FH8qxMjehj7Xk9OaVxhxaLYphF871CyiCMOKuAsHsJc0DtUUkK3ioFeqYo73mkMDUN7YklqgS</span><br></pre></td></tr></table></figure></p>
<p>在配置文件/etc/graylog/server/server.conf中password_secret填上上面的输出</p>
<p>生成root_password_sha2（后面生成的-不需要）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># echo -n 123456 |shasum -a 256</span></span><br><span class="line"><span class="number">8</span>d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92  -</span><br></pre></td></tr></table></figure></p>
<p>123456是我设置的密码<br>在配置文件/etc/graylog/server/server.conf中root_password_sha2填上上面的输出</p>
<p>设置时区<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root_timezone = Asia/Shanghai</span><br></pre></td></tr></table></figure></p>
<p>配置web监听端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rest_listen_uri = http://<span class="number">192.168</span>.<span class="number">10.2</span>:<span class="number">9000</span>/api/</span><br><span class="line">web_listen_uri = http://<span class="number">192.168</span>.<span class="number">10.2</span>:<span class="number">9000</span>/</span><br></pre></td></tr></table></figure></p>
<p>这里注意写上你的web准备使用的那个网卡的IP地址，不要全局监听</p>
<p>启动服务并配置自启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># systemctl restart graylog-server</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># systemctl enable graylog-server</span></span><br></pre></td></tr></table></figure></p>
<p>检查服务端口<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># netstat -tunlp|grep 9000</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">192.168</span>.<span class="number">10.2</span>:<span class="number">9000</span>       <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">160129</span>/java</span><br></pre></td></tr></table></figure></p>
<h3 id="使用web进行访问">使用web进行访问</h3><p>使用地址<a href="http://192.168.10.2:9000进行访问" target="_blank" rel="external">http://192.168.10.2:9000进行访问</a><br><img src="http://static.zybuluo.com/zphj1987/c07fkvdhinqn3ewzg0slxc44/image.png" alt="image.png-312kB"><br>用户名admin<br>密码123456</p>
<p><img src="http://static.zybuluo.com/zphj1987/6nvvez8axru9djfrn4iyx4hj/image.png" alt="image.png-69.9kB"><br>进来就是引导界面，这个地方是<br><figure class="highlight"><table><tr><td class="code"><pre><span class="line">1、把日志发送到graylog</span><br><span class="line">2、对收集到的数据做点搜索</span><br><span class="line">3、创建一个图表</span><br><span class="line">4、创建告警</span><br></pre></td></tr></table></figure></p>
<p>到这里配置graylog平台的基础工作就完成了，现在看下怎么跟ceph对接</p>
<p><img src="http://static.zybuluo.com/zphj1987/d2rbv3q5mfwvhkrqrf0bfyvc/image.png" alt="image.png-38.6kB"></p>
<h2 id="配置ceph的支持">配置ceph的支持</h2><p>日志从ceph里面输出是采用的GELF UDP方式的</p>
<p>GELF is Graylog2 的json格式的数据，内部采用键值对的方式，ceoh内部传输出来的数据不光有message还有下面的</p>
<ul>
<li>hostname</li>
<li>thread id</li>
<li>priority</li>
<li>subsystem name and id</li>
<li>fsid</li>
</ul>
<p><img src="http://static.zybuluo.com/zphj1987/0p2lgnam8tq0mn9ebcturx6i/image.png" alt="image.png-68.5kB"></p>
<p>选择GELF UDP协议 </p>
<p><img src="http://static.zybuluo.com/zphj1987/r5yibecnxsp04vo5hw8iboj2/image.png" alt="image.png-77.1kB"></p>
<p>选择节点，配置监听端口为12201，保存</p>
<p>在lab102上检查端口的监听情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># netstat -tunlp|grep 12201</span></span><br><span class="line">udp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">12201</span>           <span class="number">0.0</span>.<span class="number">0.0</span>:*                           <span class="number">160129</span>/java</span><br></pre></td></tr></table></figure></p>
<p>可以看到已经监听好了</p>
<p>修改ceph的配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#log_to_graylog = true</span></span><br><span class="line"><span class="comment">#err_to_graylog = true</span></span><br><span class="line"><span class="comment">#log_graylog_host = 192.168.10.2</span></span><br><span class="line"><span class="comment">#log_graylog_port = 12201</span></span><br><span class="line">clog_to_graylog = <span class="literal">true</span></span><br><span class="line">clog_to_graylog_host = <span class="number">192.168</span>.<span class="number">10.2</span></span><br><span class="line">clog_to_graylog_port = <span class="number">12201</span></span><br><span class="line">mon_cluster_<span class="built_in">log</span>_to_graylog = <span class="literal">true</span></span><br><span class="line">mon_cluster_<span class="built_in">log</span>_to_graylog_host = <span class="number">192.168</span>.<span class="number">10.2</span></span><br><span class="line">mon_cluster_<span class="built_in">log</span>_to_graylog_port = <span class="number">12201</span></span><br></pre></td></tr></table></figure></p>
<p>ceph.conf当中跟graylog有关的就是这些配置文件了，配置好端口是刚刚监听的那个udp端口，然后重启ceph服务，这里我只需要mon_cluster日志和clog，这个根据自己的需要选择</p>
<p><img src="http://static.zybuluo.com/zphj1987/haiphagvwq5fwgpe8cls2bm0/image.png" alt="image.png-199.9kB"></p>
<p>可以看到ceph -w的输出都可以在这个里面查询了</p>
<h3 id="配置告警">配置告警</h3><p><img src="http://static.zybuluo.com/zphj1987/niv3p6tq44rv7xif0ao62ykg/image.png" alt="image.png-128.7kB"><br>出现异常的时候<br><img src="http://static.zybuluo.com/zphj1987/cz7qz8qdi0if4ddvtb60nike/image.png" alt="image.png-62.2kB"></p>
<h2 id="总结">总结</h2><p>这个系统支持原生的接口接入，未来应该可以支持更多类型的日志倒入，这样相当于很容易就部署了一个日志搜索系统了，当然还有很多其他的方案，从功能完整性来说Elasticsearch要比这个强大，这套系统目前来看配置是非常的简单，也是一个优势</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-09</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/graylog.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>在看集群的配置文件的时候看到ceph里面有一个graylog的输出选择，目前看到的是可以收集mon日志和clog，osd单个的日志没有看到，Elasticsearch有整套的日志收集系统，可以很方便的将所有日志汇总到一起，这个graylog的收集采用的是自有的udp协议，从配置上来说可以很快的完成，这里只做一个最基本的实践<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph部署mon出现0.0.0.0地址]]></title>
    <link href="http://www.zphj1987.com/2017/06/06/Ceph-deploymon-with-error-address/"/>
    <id>http://www.zphj1987.com/2017/06/06/Ceph-deploymon-with-error-address/</id>
    <published>2017-06-06T09:42:22.000Z</published>
    <updated>2017-06-06T09:48:16.851Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/monitor.png" alt="monitor"><br></center>

<h2 id="前言">前言</h2><p>最近在群里两次看到出现mon地址不对的问题，都是显示0.0.0.0:0地址，如下所示：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">3137</span>d009<span class="operator">-e</span>41e-<span class="number">41</span>f0-b8f8-<span class="number">5</span>cb574502572</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8106,node8107,lab104</span><br><span class="line">     monmap e2: <span class="number">4</span> mons at &#123;lab104=<span class="number">192.168</span>.<span class="number">10.4</span>:<span class="number">6789</span>/<span class="number">0</span>,lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>,lab8107=<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">0</span>/<span class="number">2</span>,node8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>这个之前偶尔会看到有出现这个问题，但是自己一直没碰到过，想看下是什么情况下触发的，在征得这个cepher的同意后，登录上他的环境检查了一下，发现是主机名引起的这个问题</p>
<h2 id="问题复现">问题复现</h2><p>在部署的过程中，已经规划好了主机名，而又去修改了这个机器的主机名的情况下就会出现这个问题<br>比如我的这个机器，开始规划好lab8107主机名是这个，然后再lab8107上执行hostname node8107，就会触发这个问题</p>
<p>这个在deploy的部署输出日志中可以看得到<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[lab8107][WARNIN] ********************************************************************************</span><br><span class="line">[lab8107][WARNIN] provided hostname must match remote hostname</span><br><span class="line">[lab8107][WARNIN] provided hostname: lab8107</span><br><span class="line">[lab8107][WARNIN] remote hostname: node8107</span><br><span class="line">[lab8107][WARNIN] monitors may not reach quorum and create-keys will not complete</span><br><span class="line">[lab8107][WARNIN] ********************************************************************************</span><br></pre></td></tr></table></figure></p>
<p>可以看到 provided hostname: lab8107 而remote hostname: node8107，就会出现这个问题了</p>
<p>如果下次出现这个问题，首先就检查下规划的mon的主机名与真实的主机名是否一致</p>
<h2 id="总结">总结</h2><p>新手在部署环境的时候，经常会犯一些比较基础的错误，这个是一个经验积累的过程，当然对于已经比较熟悉的cepher来说，也去尝试多看下各种异常问题，这个对于以后定位异常还是很有帮助的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/monitor.png" alt="monitor"><br></center>

<h2 id="前言">前言</h2><p>最近在群里两次看到出现mon地址不对的问题，都是显示0.0.0.0:0地址，如下所示：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">3137</span>d009<span class="operator">-e</span>41e-<span class="number">41</span>f0-b8f8-<span class="number">5</span>cb574502572</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8106,node8107,lab104</span><br><span class="line">     monmap e2: <span class="number">4</span> mons at &#123;lab104=<span class="number">192.168</span>.<span class="number">10.4</span>:<span class="number">6789</span>/<span class="number">0</span>,lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>,lab8107=<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">0</span>/<span class="number">2</span>,node8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos7升级内核后无法启动解决办法]]></title>
    <link href="http://www.zphj1987.com/2017/06/01/centos7-update-kernel-can-not-boot/"/>
    <id>http://www.zphj1987.com/2017/06/01/centos7-update-kernel-can-not-boot/</id>
    <published>2017-06-01T06:09:39.000Z</published>
    <updated>2017-06-01T07:03:17.481Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newkernel.png?imageMogr2/thumbnail/!75p" alt="kernel"><br></center>

<h2 id="前言">前言</h2><p>这个问题存在有一段时间了，之前做的centos7的ISO，在进行内核的升级以后就存在这个问题：</p>
<ul>
<li>系统盘在板载sata口上是可以正常启动新内核并且能识别面板硬盘</li>
<li>系统盘插在面板口上新内核无法启动，调试发现无法找到系统盘</li>
<li>系统盘插在面板上默认的3.10内核可以正常启动<a id="more"></a>
暂时的解决办法就是让系统插在板载的sata口上，因为当时没找到具体的解决办法，在这个问题持续了一段时间后，最近再次搜索资料的时候，把问题定位在了initramfs内的驱动的问题，并且对问题进行了解决</li>
</ul>
<h2 id="解决过程">解决过程</h2><p>查询initramfs的驱动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 lab103]<span class="comment"># lsinitrd -k 3.10.0-327.el7.x86_64|grep mpt[23]sas</span></span><br><span class="line">drwxr-xr-x   <span class="number">2</span> root     root            <span class="number">0</span> Apr <span class="number">17</span> <span class="number">12</span>:<span class="number">05</span> usr/lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">327</span>.el7.x86_64/kernel/drivers/scsi/mpt2sas</span><br><span class="line">-rw-r--r--   <span class="number">1</span> root     root       <span class="number">337793</span> Nov <span class="number">20</span>  <span class="number">2015</span> usr/lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">327</span>.el7.x86_64/kernel/drivers/scsi/mpt2sas/mpt2sas.ko</span><br></pre></td></tr></table></figure></p>
<p>可以看到在3.10内核的时候是mpt2sas驱动</p>
<p>可以在4.x内核中看到<br>新版的内核已经把mpt2sas升级为mpt3sas<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/lib/modules/<span class="number">4.4</span>.<span class="number">46</span>/kernel/drivers/scsi/mpt3sas/mpt3sas.ko</span><br></pre></td></tr></table></figure></p>
<p>查询initramfs内的模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsinitrd -k  <span class="number">4.4</span>.<span class="number">46</span>|grep mpt[<span class="number">23</span>]sas</span><br></pre></td></tr></table></figure></p>
<p>可以看到并没有输出，说明initramfs并没有把这个驱动打进去</p>
<p>这个地方有两种方式来解决</p>
<h3 id="方法一：">方法一：</h3><p>修改 /etc/dracut.conf文件，增加字段</p>
<blockquote>
<p>add_drivers+=”mpt3sas”</p>
</blockquote>
<p>重新生成initramfs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dracut <span class="operator">-f</span> /boot/initramfs-<span class="number">4.4</span>.<span class="number">46</span>.img <span class="number">4.4</span>.<span class="number">46</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方法二：">方法二：</h3><p>强制加载驱动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dracut --force --add-drivers mpt3sas --kver=<span class="number">4.4</span>.<span class="number">46</span></span><br></pre></td></tr></table></figure></p>
<p>以上方法二选一做下驱动的集成，然后做下面的检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsinitrd -k  <span class="number">4.4</span>.<span class="number">46</span>|grep mpt[<span class="number">23</span>]sas</span><br></pre></td></tr></table></figure></p>
<p>如果有输出就是正常了的</p>
<p>然后重启操作系统即可</p>
<h2 id="总结">总结</h2><p>目前出现这个问题的原因不清楚来自内核还是dracut生成的地方，如果遇到这个问题就按照上面的方法进行处理下即可，问题能找到解决办法后就会发现只是小问题，没找到的时候，完全不知道问题在哪里</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-06-01</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/newkernel.png?imageMogr2/thumbnail/!75p" alt="kernel"><br></center>

<h2 id="前言">前言</h2><p>这个问题存在有一段时间了，之前做的centos7的ISO，在进行内核的升级以后就存在这个问题：</p>
<ul>
<li>系统盘在板载sata口上是可以正常启动新内核并且能识别面板硬盘</li>
<li>系统盘插在面板口上新内核无法启动，调试发现无法找到系统盘</li>
<li>系统盘插在面板上默认的3.10内核可以正常启动]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Freebsd10.2安装包升级pkg引起环境破坏的解决]]></title>
    <link href="http://www.zphj1987.com/2017/05/24/Freebsd-pkg-destroy/"/>
    <id>http://www.zphj1987.com/2017/05/24/Freebsd-pkg-destroy/</id>
    <published>2017-05-24T02:40:34.000Z</published>
    <updated>2017-06-01T06:46:51.484Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/brock.png?imageMogr2/thumbnail/!75p" alt=""><br></center>


<h2 id="前言">前言</h2><p>freebsd10.2环境在安装一个新软件包的时候提示升级pkg到1.10.1，然后点击了升级，然后整个pkg环境就无法使用了</p>
<h2 id="记录">记录</h2><p>升级完了软件包以后第一个错误提示</p>
<blockquote>
<p>FreeBSD: /usr/local/lib/libpkg.so.3: Undefined symbol “utimensat” </p>
</blockquote>
<p>这个是因为这个库是在freebsd的10.3当中才有的库，而我的环境是10.2的环境<br><a id="more"></a></p>
<h3 id="网上有一个解决办法">网上有一个解决办法</h3><p>更新源<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /usr/local/etc/pkg/repos/FreeBSD.conf</span></span><br><span class="line">FreeBSD: &#123;</span><br><span class="line">  url: <span class="string">"pkg+http://pkg.FreeBSD.org/<span class="variable">$&#123;ABI&#125;</span>/release_2"</span>,</span><br><span class="line">  enabled: yes</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>检查当前版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg --version</span></span><br><span class="line"><span class="number">1.10</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>更新缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg update</span></span><br></pre></td></tr></table></figure></p>
<p>卸载<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg delete -f pkg</span></span><br></pre></td></tr></table></figure></p>
<p>重新安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg install -y pkg</span></span><br><span class="line"><span class="comment"># pkg2ng</span></span><br></pre></td></tr></table></figure></p>
<p>检查版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg --version</span></span><br><span class="line"><span class="number">1.5</span>.<span class="number">4</span></span><br></pre></td></tr></table></figure></p>
<p>这个在我的环境下没有生效</p>
<h3 id="还有一个办法">还有一个办法</h3><p>有个pkg-static命令可以使用，，然后/var/cache/pkg里边缓存的包。执行命令： </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pkg-static install -f /var/cache/pkg/pkg-1.5.4.txz</span></span><br></pre></td></tr></table></figure>
<p>这个在我的环境下报错</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@mkiso:/usr/ports/ports-mgmt/pkg <span class="comment"># pkg info sqlite3</span></span><br><span class="line">pkg: warning: database version <span class="number">34</span> is newer than libpkg(<span class="number">3</span>) version <span class="number">33</span>, but still compatible</span><br><span class="line">pkg: sqlite error <span class="keyword">while</span> executing INSERT OR ROLLBACK INTO pkg_search(id, name, origin) VALUES (?<span class="number">1</span>, ?<span class="number">2</span> || <span class="string">'-'</span> || ?<span class="number">3</span>, ?<span class="number">4</span>); <span class="keyword">in</span> file pkgdb.c:<span class="number">1544</span>: no such table: pkg_search</span><br></pre></td></tr></table></figure>
<p>这个在网上看到有很多人出现了</p>
<h3 id="最终解决的办法">最终解决的办法</h3><p>在邮件列表里面看到一个解决办法，我是用的这个办法解决了的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#pkg shell</span></span><br></pre></td></tr></table></figure></p>
<p>进入交互模式,执行下面的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CREATE VIRTUAL TABLE pkg_search USING fts4(id, name, origin);</span><br><span class="line">pragma user_version=<span class="number">33</span>;</span><br></pre></td></tr></table></figure></p>
<p>执行完了以后pkg 环境可用了</p>
<h2 id="避免这个问题">避免这个问题</h2><p>锁定本机的pkg版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pkg lock pkg</span><br></pre></td></tr></table></figure></p>
<p>如果需要手动找包就是这个路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://pkg.freebsd.org/FreeBSD:<span class="number">10</span>:amd64/</span><br></pre></td></tr></table></figure></p>
<p>我的机器最终版本是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#pkg -v</span></span><br><span class="line"><span class="number">1.8</span>.<span class="number">7</span></span><br></pre></td></tr></table></figure></p>
<h2 id="参考资料">参考资料</h2><p><a href="http://www.07net01.com/2017/02/1816847.html" target="_blank" rel="external">freebsd pkg升级问题报错</a><br><a href="http://glasz.org/sheeplog/2017/02/freebsd-usrlocalliblibpkgso3-undefined-symbol-utimensat.html" target="_blank" rel="external">FreeBSD: /usr/local/lib/libpkg.so.3: Undefined symbol “utimensat” </a><br><a href="http://bbs.chinaunix.net/thread-4260263-1-1.html" target="_blank" rel="external">升级pkg失败, 安装低版本pkg失败</a><br><a href="https://lists.freebsd.org/pipermail/freebsd-ports/2017-January/106799.html" target="_blank" rel="external">pkg database issue: database version 34 is newer than libpkg(3) version 33 ?</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-24</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/brock.png?imageMogr2/thumbnail/!75p" alt=""><br></center>


<h2 id="前言">前言</h2><p>freebsd10.2环境在安装一个新软件包的时候提示升级pkg到1.10.1，然后点击了升级，然后整个pkg环境就无法使用了</p>
<h2 id="记录">记录</h2><p>升级完了软件包以后第一个错误提示</p>
<blockquote>
<p>FreeBSD: /usr/local/lib/libpkg.so.3: Undefined symbol “utimensat” </p>
</blockquote>
<p>这个是因为这个库是在freebsd的10.3当中才有的库，而我的环境是10.2的环境<br>]]>
    
    </summary>
    
      <category term="freebsd" scheme="http://www.zphj1987.com/tags/freebsd/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph OSD从filestore 转换到 bluestore的方法]]></title>
    <link href="http://www.zphj1987.com/2017/05/03/Ceph-filestore-to-bluestore/"/>
    <id>http://www.zphj1987.com/2017/05/03/Ceph-filestore-to-bluestore/</id>
    <published>2017-05-03T09:57:27.000Z</published>
    <updated>2017-05-03T10:13:00.113Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/blueprint.png" alt="myceph"><br></center>

<h2 id="前言">前言</h2><p>前段时间看到<a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&amp;mid=2651256389&amp;idx=1&amp;sn=e11edcce5722853f442b9a7b8211787e&amp;chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&amp;mpshare=1&amp;scene=23&amp;srcid=0502SazrSPsWnszP3xfdEId4#rd" target="_blank" rel="external">豪迈的公众号</a>上提到了这个离线转换工具，最近看到群里有人问，找了下没什么相关文档，就自己写了一个，供参考<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><h3 id="获取代码并安装">获取代码并安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ceph/ceph.git</span><br><span class="line"><span class="built_in">cd</span> ceph</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line">./make-dist</span><br><span class="line">rpm -bb ceph.spec</span><br></pre></td></tr></table></figure>
<p>生成rpm安装包后进行安装,这个过程就不讲太多，根据各种文档安装上最新的版本即可，这个代码合进去时间并不久，大概是上个月才合进去的</p>
<h3 id="配置集群">配置集群</h3><p>首先配置一个filestore的集群，这个也是很简单的，我的环境配置一个单主机三个OSD的集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">3</span>daaf51a-eeba-<span class="number">43</span>a6-<span class="number">9</span>f58-c26c5796f928</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr active: lab8106 </span><br><span class="line">     osdmap e16: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">      pgmap v34: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">323</span> MB used, <span class="number">822</span> GB / <span class="number">822</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">0.80338</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.80338</span>     host lab8106                                   </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26779</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br></pre></td></tr></table></figure></p>
<h3 id="写入少量数据">写入少量数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd bench 10 write --no-cleanup</span></span><br></pre></td></tr></table></figure>
<h3 id="设置noout">设置noout</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set noout</span></span><br><span class="line">noout is <span class="built_in">set</span></span><br></pre></td></tr></table></figure>
<h3 id="停止OSD-0">停止OSD.0</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd down 0</span></span><br><span class="line">osd.<span class="number">0</span> is already down.</span><br></pre></td></tr></table></figure>
<p>将数据换个目录挂载，换个新盘挂载到原路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /var/lib/ceph/osd/ceph-0.old/</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># umount /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># mount /dev/sdb1 /var/lib/ceph/osd/ceph-0.old/</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># mount /dev/sde1 /var/lib/ceph/osd/ceph-0/</span></span><br><span class="line"></span><br><span class="line">[root@lab8106 ~]<span class="comment"># df -h|grep osd</span></span><br><span class="line">/dev/sdc1       <span class="number">275</span>G  <span class="number">833</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">/dev/sdd1       <span class="number">275</span>G  <span class="number">833</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">2</span></span><br><span class="line">/dev/sdb1       <span class="number">275</span>G  <span class="number">759</span>M  <span class="number">274</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span>.old</span><br><span class="line">/dev/sde1       <span class="number">280</span>G   <span class="number">33</span>M  <span class="number">280</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>在配置文件/etc/ceph/ceph.conf中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">enable</span>_experimental_unrecoverable_data_corrupting_features = bluestore</span><br></pre></td></tr></table></figure></p>
<p>如果需要指定osd的block的路径需要写配置文件<br>在做<code>ceph-objectstore-tool --type bluestore --data-path  --op mkfs</code>这个操作之前，在配置文件的全局里面添加上</p>
<blockquote>
<p>bluestore_block_path = /dev/sde2</p>
</blockquote>
<p>然后再创建的时候就可以是链接到设备了，这个地方写全局变量，然后创建完了后就删除掉这项配置文件，写单独的配置文件的时候发现没读取成功,生成后应该是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0</span></span><br><span class="line">total <span class="number">20</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root  <span class="number">9</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> block -&gt; /dev/sde2</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">2</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> bluefs</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">37</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> fsid</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">8</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> kv_backend</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">4</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> mkfs_<span class="keyword">done</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">10</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">40</span> <span class="built_in">type</span></span><br></pre></td></tr></table></figure></p>
<p>如果不增加这个就是以文件形式的存在</p>
<h3 id="获取osd-0的fsid">获取osd.0的fsid</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat /var/lib/ceph/osd/ceph-0.old/fsid </span></span><br><span class="line">b2f73450-<span class="number">5</span>c4a-<span class="number">45</span>fb-<span class="number">9</span>c24-<span class="number">8218</span>a5803434</span><br></pre></td></tr></table></figure>
<h3 id="创建一个bluestore的osd-0">创建一个bluestore的osd.0</h3><figure class="highlight brainfuck"><table><tr><td class="code"><pre><span class="line"><span class="title">[</span><span class="comment">root@lab8106</span> <span class="comment">~</span><span class="title">]</span><span class="comment">#</span> <span class="comment">ceph</span><span class="literal">-</span><span class="comment">objectstore</span><span class="literal">-</span><span class="comment">tool</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">type</span> <span class="comment">bluestore</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">data</span><span class="literal">-</span><span class="comment">path</span> <span class="comment">/var/lib/ceph/osd/ceph</span><span class="literal">-</span><span class="comment">0</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">fsid</span> <span class="comment">b2f73450</span><span class="literal">-</span><span class="comment">5c4a</span><span class="literal">-</span><span class="comment">45fb</span><span class="literal">-</span><span class="comment">9c24</span><span class="literal">-</span><span class="comment">8218a5803434</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">op</span> <span class="comment">mkfs</span></span><br></pre></td></tr></table></figure>
<h3 id="转移数据">转移数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0.old --target-data-path /var/lib/ceph/osd/ceph-0 --op dup</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown -R ceph:ceph /var/lib/ceph/osd/ceph-0</span></span><br></pre></td></tr></table></figure>
<p>这个操作是将之前的filestore的数据转移到新的bluestore上了</p>
<h3 id="启动OSD-0">启动OSD.0</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 osd]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure>
<p>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 osd]<span class="comment"># ceph -s</span></span><br><span class="line"><span class="number">2017</span>-<span class="number">05</span>-<span class="number">03</span> <span class="number">17</span>:<span class="number">05</span>:<span class="number">13.119492</span> <span class="number">7</span>f20a501b700 -<span class="number">1</span> WARNING: the following dangerous and experimental features are enabled: bluestore</span><br><span class="line"><span class="number">2017</span>-<span class="number">05</span>-<span class="number">03</span> <span class="number">17</span>:<span class="number">05</span>:<span class="number">13.150181</span> <span class="number">7</span>f20a501b700 -<span class="number">1</span> WARNING: the following dangerous and experimental features are enabled: bluestore</span><br><span class="line">    cluster <span class="number">3</span>daaf51a-eeba-<span class="number">43</span>a6-<span class="number">9</span>f58-c26c5796f928</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            noout flag(s) <span class="built_in">set</span></span><br><span class="line">            mon.lab8106 low disk space</span><br><span class="line">     monmap e2: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">        mgr active: lab8106 </span><br><span class="line">     osdmap e25: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags noout</span><br><span class="line">      pgmap v80: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">724</span> MB data, <span class="number">182</span> objects</span><br><span class="line">            <span class="number">3431</span> MB used, <span class="number">555</span> GB / <span class="number">558</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>成功转移</p>
<h3 id="不同的block方式">不同的block方式</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-0/ -al|grep block</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> ceph ceph <span class="number">10737418240</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">32</span> block</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ll /var/lib/ceph/osd/ceph-4/ -al|grep block</span></span><br><span class="line">lrwxrwxrwx  <span class="number">1</span> ceph ceph  <span class="number">58</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">16</span> block -&gt; /dev/disk/by-partuuid/<span class="number">846</span>e93a2-<span class="number">0</span>f6d-<span class="number">47</span>d4-<span class="number">8</span>a90-<span class="number">85</span>ab3cf4ec4e</span><br><span class="line">-rw-r--r--  <span class="number">1</span> ceph ceph  <span class="number">37</span> May  <span class="number">3</span> <span class="number">17</span>:<span class="number">16</span> block_uuid</span><br></pre></td></tr></table></figure>
<p>可以看到直接创建的时候的block是以链接的方式链接到一个分区的，而不改配置文件的转移的方式里面是一个文件的形式，根据需要进行选择</p>
<h2 id="总结">总结</h2><p>转移工具的出现方便了以后从filestore到bluestore的转移，可以采取一个个osd的转移方式将整个集群进行转移，而免去了剔除osd，再添加的方式，减少了迁移量，可以一个个的离线进行操作</p>
<p>ceph的工具集越来越完整了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-03</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/blueprint.png" alt="myceph"><br></center>

<h2 id="前言">前言</h2><p>前段时间看到<a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&amp;mid=2651256389&amp;idx=1&amp;sn=e11edcce5722853f442b9a7b8211787e&amp;chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&amp;mpshare=1&amp;scene=23&amp;srcid=0502SazrSPsWnszP3xfdEId4#rd">豪迈的公众号</a>上提到了这个离线转换工具，最近看到群里有人问，找了下没什么相关文档，就自己写了一个，供参考<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[多MDS变成单MDS的方法]]></title>
    <link href="http://www.zphj1987.com/2017/05/03/mutimds-to-single-mds/"/>
    <id>http://www.zphj1987.com/2017/05/03/mutimds-to-single-mds/</id>
    <published>2017-05-03T07:53:10.000Z</published>
    <updated>2017-05-03T07:54:24.241Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>之前有个cepher的环境上是双活MDS的，需要变成MDS，目前最新版本是支持这个操作的</p>
<a id="more"></a>
<h2 id="方法">方法</h2><h3 id="设置最大mds">设置最大mds</h3><p>多活的mds的max_mds会超过1，这里需要先将max_mds设置为1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mds <span class="built_in">set</span> max_mds <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h3 id="deactive_mds">deactive mds</h3><p>看下需要停掉的mds是rank 0 还是rank1,然后执行下面的命令即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server8 ~]<span class="comment"># zbkc -s|grep mdsmap</span></span><br><span class="line">     mdsmap e13: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab8106=up:clientreplay&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个输出的lab8106前面的0，就是这个mds的rank，根据需要停止对应的rank<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph mds deactivate <span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>不建议用多活mds</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-05-03</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>之前有个cepher的环境上是双活MDS的，需要变成MDS，目前最新版本是支持这个操作的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph根据Crush位置读取数据]]></title>
    <link href="http://www.zphj1987.com/2017/04/27/Ceph-depend-Crush-read-data/"/>
    <id>http://www.zphj1987.com/2017/04/27/Ceph-depend-Crush-read-data/</id>
    <published>2017-04-27T08:47:04.000Z</published>
    <updated>2017-04-27T09:10:58.547Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/read.gif?imageMogr2/thumbnail/!75p" alt=""><br></center>

<h2 id="前言">前言</h2><p>在ceph研发群里面看到一个cepher在问关于怎么读取ceph的副本的问题，这个功能应该在2012年的时候,我们公司的研发就修改了代码去实现这个功能，只是当时的硬件条件所限，以及本身的稳定性问题，后来没有在生产当中使用<br><a id="more"></a><br>我们都知道ceph在写数据的时候，是先写主本，然后去写副本，而读取的时候，实际上只有主本能够提供服务，这对于磁盘的整体带宽来说，并没有充分的发挥其性能，所以能够读取副本当然是会有很大好处的，特别是对于读场景比较多的情况</p>
<p>那么在ceph当中是不是有这个功能呢？其实是有的，这个地方ceph更往上走了一层，是基于crush定义的地址去进行文件的读取，这样在读取的客户端眼里，就没有什么主副之分，他会按自己想要的区域去尽量读取，当然这个区域没有的时候就按正常读取就可以了<br><!--more--></p>
<h2 id="实践">实践</h2><p>如果你看过关于ceph hadoop的相关配置文档，应该会看到这么一个配置</p>
<blockquote>
<p>ceph.localize.reads<br>Allow reading from file replica objects<br>Default value: true</p>
</blockquote>
<p>显示的是可以从非主本去读取对象，这个对于hadoop场景肯定是越近越好的，可以在ceph的代码里面搜索下 localize-reads<br><a href="https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/ceph_fuse.cc</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (std::vector&lt;const char*&gt;::iterator i = args.begin(); i != args.end(); ) &#123;</span><br><span class="line">  <span class="keyword">if</span> (ceph_argparse_double_dash(args, i)) &#123;</span><br><span class="line">    <span class="built_in">break</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ceph_argparse_flag(args, i, <span class="string">"--localize-reads"</span>, (char*)NULL)) &#123;</span><br><span class="line">    cerr &lt;&lt; <span class="string">"setting CEPH_OSD_FLAG_LOCALIZE_READS"</span> &lt;&lt; std::endl;</span><br><span class="line">    filer_flags |= CEPH_OSD_FLAG_LOCALIZE_READS;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ceph_argparse_flag(args, i, <span class="string">"-h"</span>, <span class="string">"--help"</span>, (char*)NULL)) &#123;</span><br><span class="line">    usage();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    ++i;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到在ceph-fuse的情况下，是有这个隐藏的一个参数的，本篇就是用这个隐藏的参数来进行实践</p>
<h3 id="配置一个两节点集群">配置一个两节点集群</h3><p>配置完成了以后ceph的目录树如下,mon部署在lab8106上面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">1.07336</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.53778</span>     host lab8106                                   </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26999</span>         osd.<span class="number">0</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line">-<span class="number">3</span> <span class="number">0.53558</span>     host lab8107                                   </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">3</span> <span class="number">0.26779</span>         osd.<span class="number">3</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span></span><br><span class="line">[root@lab8107 ~]<span class="comment"># ceph -s|grep mon</span></span><br><span class="line">monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="在lab8107上挂载客户端">在lab8107上挂载客户端</h3><p>在/etc/ceph/ceph.conf中增加一个配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">crush_location = <span class="string">"host=lab8107 root=default"</span></span><br></pre></td></tr></table></figure></p>
<p>这个配置的作用是告诉这个客户端尽量去读取lab8107上面的对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph-fuse -m lab8106:6789 /mnt  --localize-reads</span></span><br></pre></td></tr></table></figure></p>
<p>写入一个大文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># dd if=/dev/zero of=a bs=4M count=4000</span></span><br></pre></td></tr></table></figure></p>
<p>在lab8106和lab8107上监控磁盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># iostat -dm 1</span></span><br></pre></td></tr></table></figure></p>
<p>读取数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># dd if=a of=/dev/null</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到只有lab8107上有磁盘的读取，也就是读取的数据里面肯定也有副本，都是从lab8107上面读取了</p>
<p>如果需要多次测试，需要清除下缓存<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sync; <span class="built_in">echo</span> <span class="number">3</span> &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></table></figure></p>
<p>并且重新挂载客户端，这个读取crush的位置的操作是在mount的时候读取的</p>
<h2 id="使用场景">使用场景</h2><p>上面的配置是可以指定多个平级的位置的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">crush_location = <span class="string">"host=lab8106 host=lab8107 root=default"</span></span><br></pre></td></tr></table></figure></p>
<p>这样，在一些读请求很多的场景下，可以把整个后端按逻辑上划分为一个个的区域，然后前面的客户端就可以平级分配到这些区域当中，这样就可以比较大的限度去把副本的读取也调动起来的</p>
<p>目前在ceph-fuse上已经实现，rbd里面也有类似的一些处理，这个是一个很不错的功能</p>
<h2 id="总结">总结</h2><p>ceph里面有很多可配置的东西，怎么用好它，最大限度的去适配使用场景，还是有很大的可调的空间的，所谓学无止境，我也在学习python coding了，有很多想法等着去实现</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-04-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/read.gif?imageMogr2/thumbnail/!75p" alt=""><br></center>

<h2 id="前言">前言</h2><p>在ceph研发群里面看到一个cepher在问关于怎么读取ceph的副本的问题，这个功能应该在2012年的时候,我们公司的研发就修改了代码去实现这个功能，只是当时的硬件条件所限，以及本身的稳定性问题，后来没有在生产当中使用<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
