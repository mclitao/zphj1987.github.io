<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[止于至善]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2018-08-20T03:40:45.253Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[cephfs根据存储池显示df容量]]></title>
    <link href="http://www.zphj1987.com/2018/08/19/show-useage-for-cephfs-pool/"/>
    <id>http://www.zphj1987.com/2018/08/19/show-useage-for-cephfs-pool/</id>
    <published>2018-08-19T03:29:16.000Z</published>
    <updated>2018-08-20T03:40:45.253Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/x7qsgok0tlsl6cttnul0em0z/pool1.png" alt="pool.png-115.2kB"><br></center>

<h2 id="前言">前言</h2><p>如果用cephfs比较多，应该都知道，在cephfs的客户端进行mount以后，看到的容量显示的是集群的总的容量，也就是你的总的磁盘空间是多少这个地方显示的就是多少</p>
<p>这个一直都是这样显示的，我们之前在hammer版本的时候，阿茂和大黄一起在公司内部实现了这个功能，社区会慢慢的集成一些类似的面向面向商业用户的需求</p>
<p>社区已经开发了一个版本，接口都做的差不多了，那么稍微改改，就能实现想要的需求的</p>
<p>本篇内的改动是基于内核客户端代码的改动，改动很小，应该能够看的懂</p>
<a id="more"></a>
<h2 id="改动过程">改动过程</h2><p>首先找到这个补丁</p>
<blockquote>
<p>Improve accuracy of statfs reporting for Ceph filesystems comprising exactly one data pool. In this case, the Ceph monitor can now report the space usage for the single data pool instead of the global data for the entire Ceph cluster. Include support for this message in mon_client and leverage it in ceph/super.</p>
</blockquote>
<p>地址：<a href="https://www.spinics.net/lists/ceph-devel/msg37937.html" target="_blank" rel="external">https://www.spinics.net/lists/ceph-devel/msg37937.html</a></p>
<p>这个说的是改善了statfs的显示，这个statfs就是在linux下面的mount的输出的显示的，说是改善了在单存储池下的显示效果，也就是在单存储池下能够显示存储池的容量空间，而不是全局的空间</p>
<p>这里就有个疑问了，单存储池？那么多存储池呢？我们测试下看看</p>
<p>这里这个补丁已经打到了centos7.5的默认内核里面去了，也就是内核版本</p>
<blockquote>
<p>Linux lab103 3.10.0-862.el7.x86_64</p>
</blockquote>
<p>对应的rpm包的版本是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># rpm -qa|grep  3.10.0-862</span></span><br><span class="line">kernel-devel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64</span><br><span class="line">kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64</span><br></pre></td></tr></table></figure></p>
<p>下载的地址为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://mirrors.<span class="number">163</span>.com/centos/<span class="number">7.5</span>.<span class="number">1804</span>/os/x86_64/Packages/kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>或者直接安装centos7.5也行，这里只要求是这个内核就可以了</p>
<p>我们看下默认情况下是怎样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -s</span></span><br><span class="line">  data:</span><br><span class="line">    pools:   <span class="number">3</span> pools, <span class="number">72</span> pgs</span><br><span class="line">    objects: <span class="number">22</span> objects, <span class="number">36179</span> bytes</span><br><span class="line">    usage:   <span class="number">5209</span> MB used, <span class="number">11645</span> GB / <span class="number">11650</span> GB avail</span><br><span class="line">    pgs:     <span class="number">72</span> active+clean</span><br><span class="line"> </span><br><span class="line">[root@lab102 ~]<span class="comment"># ceph fs ls</span></span><br><span class="line">name: ceph, metadata pool: metadata, data pools: [data ]</span><br><span class="line">[root@lab102 ~]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">11650</span>G     <span class="number">11645</span>G        <span class="number">5209</span>M          <span class="number">0.04</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    data         <span class="number">9</span>          <span class="number">0</span>         <span class="number">0</span>         <span class="number">3671</span>G           <span class="number">0</span> </span><br><span class="line">    metadata     <span class="number">10</span>     <span class="number">36179</span>         <span class="number">0</span>        <span class="number">11014</span>G          <span class="number">22</span> </span><br><span class="line">    newdata      <span class="number">11</span>         <span class="number">0</span>         <span class="number">0</span>         <span class="number">5507</span>G           <span class="number">0</span> </span><br><span class="line">[root@lab102 ~]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">9</span> <span class="string">'data'</span> replicated size <span class="number">3</span> min_size <span class="number">1</span> crush_rule <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">32</span> pgp_num <span class="number">32</span> last_change <span class="number">136</span> flags hashpspool stripe_width <span class="number">0</span> application cephfs</span><br><span class="line">pool <span class="number">10</span> <span class="string">'metadata'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_rule <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">32</span> pgp_num <span class="number">32</span> last_change <span class="number">112</span> flags hashpspool stripe_width <span class="number">0</span> application cephfs</span><br><span class="line">pool <span class="number">11</span> <span class="string">'newdata'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_rule <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">8</span> pgp_num <span class="number">8</span> last_change <span class="number">134</span> flags hashpspool  stripe_width <span class="number">0</span> application cephfs</span><br></pre></td></tr></table></figure></p>
<p>从上面可以看到我的硬盘裸空间为12T左右，data存储池副本3那么可用空间为4T左右，文件系统里面只有一个data存储池，看下挂载的情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># uname -a</span></span><br><span class="line">Linux lab101 <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64 <span class="comment">#1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># df -Th|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/        ceph      <span class="number">3.6</span>T     <span class="number">0</span>  <span class="number">3.6</span>T   <span class="number">0</span>% /mnt</span><br></pre></td></tr></table></figure></p>
<p>可以看到显示的容量就是存储池的可用容量为总空间的，现在我们加入一个数据池</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph mds add_data_pool newdata</span></span><br><span class="line">added data pool <span class="number">11</span> to fsmap</span><br></pre></td></tr></table></figure>
<p>再次查看df的显示<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -Th|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/        ceph       <span class="number">12</span>T  <span class="number">5.1</span>G   <span class="number">12</span>T   <span class="number">1</span>% /mnt</span><br></pre></td></tr></table></figure></p>
<p>容量回到了原始的显示的方式，这个跟上面的补丁的预期是一样的，我们看下代码这里怎么控制的</p>
<h2 id="获取当前内核版本的代码">获取当前内核版本的代码</h2><p>首先要找到当前的内核的src.rpm包，这样可以拿到当前内核版本的源码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://vault.centos.org/<span class="number">7.5</span>.<span class="number">1804</span>/os/Source/SPackages/kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>解压源码包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 origin]<span class="comment"># rpm2cpio kernel-3.10.0-862.el7.src.rpm |cpio -div</span></span><br><span class="line">[root@lab103 origin]<span class="comment"># tar -xvf linux-3.10.0-862.el7.tar.xz</span></span><br><span class="line">[root@lab103 origin]<span class="comment"># cd linux-3.10.0-862.el7/fs/ceph/</span></span><br></pre></td></tr></table></figure></p>
<p>上面的操作后我们已经进入了我们想要看的源码目录了<br>我们看下super.c这个文件，这个df的显示的控制是在这个文件里面的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># cat super.c |less</span></span><br></pre></td></tr></table></figure></p>
<p>看下这段代码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">static int ceph_statfs(struct dentry *dentry, struct kstatfs *buf)</span><br><span class="line">&#123;</span><br><span class="line">        struct ceph_fs_client *fsc = ceph_inode_to_client(dentry-&gt;d_inode);</span><br><span class="line">        struct ceph_monmap *monmap = fsc-&gt;client-&gt;monc.monmap;</span><br><span class="line">        struct ceph_statfs st;</span><br><span class="line">        u64 fsid;</span><br><span class="line">        int err;</span><br><span class="line">        u64 data_pool;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools == <span class="number">1</span>) &#123;</span><br><span class="line">                data_pool = fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[<span class="number">0</span>];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                data_pool = CEPH_NOPOOL;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        dout(<span class="string">"statfs\n"</span>);</span><br><span class="line">        err = ceph_monc_<span class="keyword">do</span>_statfs(&amp;fsc-&gt;client-&gt;monc, data_pool, &amp;st);</span><br><span class="line">        <span class="keyword">if</span> (err &lt; <span class="number">0</span>)</span><br><span class="line">                <span class="built_in">return</span> err;</span><br></pre></td></tr></table></figure></p>
<p>其中的fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools == 1和data_pool = fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[0];这个地方的意思是如果fs里面包含的存储池的存储池个数为1那么data_pool就取这个存储池的信息，所以上面的我们的实践过程中的就是单个存储池的时候显示存储池的容量，超过一个的时候就显示的全局的容量，这个是跟代码对应的上的</p>
<p>我们基于上面的已经做好的功能改变一下需求</p>
<blockquote>
<p>需要可以根据自己的需要指定存储池的容量来显示，通过挂载内核客户端的时候传递一个参数进去来进行显示</p>
</blockquote>
<h2 id="代码改动">代码改动</h2><p>[root@lab103 ceph]# vim super.h<br>在super.h内定义一个默认值<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#define ZP_POOL_DEFAULT      0  /* pool id */</span></span><br><span class="line"><span class="comment">#define CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT     60  /* cap release delay */</span></span><br><span class="line">struct ceph_mount_options &#123;</span><br><span class="line">        int flags;</span><br><span class="line">        int sb_flags;</span><br><span class="line"></span><br><span class="line">        int wsize;            /* max write size */</span><br><span class="line">        int rsize;            /* max <span class="built_in">read</span> size */</span><br><span class="line">        int zp_pool;            /* pool id */</span><br><span class="line">        int rasize;           /* max readahead */</span><br></pre></td></tr></table></figure></p>
<p>这里增加了两个一个zp_pool和ZP_POOL_DEFAULT<br>这个文件的改动就只有这么多了</p>
<p>改动super.c的代码<br>在enum里面加上Opt_zp_pool<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">enum &#123;</span><br><span class="line">        Opt_wsize,</span><br><span class="line">        Opt_rsize,</span><br><span class="line">        Opt_rasize,</span><br><span class="line">        Opt_caps_wanted_delay_min,</span><br><span class="line">        Opt_zp_pool,</span><br></pre></td></tr></table></figure></p>
<p>在match_table_t fsopt_tokens里面添加Opt_zp_pool相关的判断，我们自己注意传的是pool在fs里面的id即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">static match_table_t fsopt_tokens = &#123;</span><br><span class="line">        &#123;Opt_wsize, <span class="string">"wsize=%d"</span>&#125;,</span><br><span class="line">        &#123;Opt_rsize, <span class="string">"rsize=%d"</span>&#125;,</span><br><span class="line">        &#123;Opt_rasize, <span class="string">"rasize=%d"</span>&#125;,</span><br><span class="line">        &#123;Opt_caps_wanted_delay_min, <span class="string">"caps_wanted_delay_min=%d"</span>&#125;,</span><br><span class="line">        &#123;Opt_zp_pool, <span class="string">"zp_pool=%d"</span>&#125;,</span><br></pre></td></tr></table></figure></p>
<p>在static int parse_fsopt_token中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> Opt_caps_wanted_delay_max:</span><br><span class="line">                <span class="keyword">if</span> (intval &lt; <span class="number">1</span>)</span><br><span class="line">                        <span class="built_in">return</span> -EINVAL;</span><br><span class="line">                fsopt-&gt;caps_wanted_delay_max = intval;</span><br><span class="line">                <span class="built_in">break</span>;</span><br><span class="line">        <span class="keyword">case</span> Opt_zp_pool:</span><br><span class="line">                <span class="keyword">if</span> (intval &lt; <span class="number">0</span>)</span><br><span class="line">                        <span class="built_in">return</span> -EINVAL;</span><br><span class="line">                fsopt-&gt;zp_pool = intval;</span><br><span class="line">                <span class="built_in">break</span>;</span><br><span class="line">        <span class="keyword">case</span> Opt_readdir_max_entries:</span><br><span class="line">                <span class="keyword">if</span> (intval &lt; <span class="number">1</span>)</span><br><span class="line">                        <span class="built_in">return</span> -EINVAL;</span><br><span class="line">                fsopt-&gt;max_readdir = intval;</span><br><span class="line">                <span class="built_in">break</span>;</span><br></pre></td></tr></table></figure></p>
<p>判断如果小于0就抛错，这个id从0开始上升的，所以也不允许小于0</p>
<p>在static int parse_mount_options中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fsopt-&gt;caps_wanted_delay_min = CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT;</span><br><span class="line">fsopt-&gt;zp_pool = ZP_POOL_DEFAULT;</span><br><span class="line">fsopt-&gt;caps_wanted_delay_max = CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT;</span><br></pre></td></tr></table></figure></p>
<p>在static int ceph_show_options中添加<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (fsopt-&gt;caps_wanted_delay_min != CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT)</span><br><span class="line">        seq_<span class="built_in">printf</span>(m, <span class="string">",caps_wanted_delay_min=%d"</span>,</span><br><span class="line">                 fsopt-&gt;caps_wanted_delay_min);</span><br><span class="line"><span class="keyword">if</span> (fsopt-&gt;zp_pool)</span><br><span class="line">        seq_<span class="built_in">printf</span>(m, <span class="string">",zp_pool=%d"</span>,</span><br><span class="line">                 fsopt-&gt;zp_pool);</span><br><span class="line"><span class="keyword">if</span> (fsopt-&gt;caps_wanted_delay_max != CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT)</span><br><span class="line">        seq_<span class="built_in">printf</span>(m, <span class="string">",caps_wanted_delay_max=%d"</span>,</span><br><span class="line">                   fsopt-&gt;caps_wanted_delay_max);</span><br></pre></td></tr></table></figure></p>
<p>这个是用来在执行mount命令的时候显示选项的数值的<br>改动到这里我们检查下我们对super.c做过的的改动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># cat super.c |grep zp_pool</span></span><br><span class="line">	Opt_zp_pool,</span><br><span class="line">	&#123;Opt_zp_pool, <span class="string">"zp_pool=%d"</span>&#125;,</span><br><span class="line">    <span class="keyword">case</span> Opt_zp_pool:</span><br><span class="line">        fsopt-&gt;zp_pool = intval;</span><br><span class="line">	fsopt-&gt;zp_pool = ZP_POOL_DEFAULT;</span><br><span class="line">        <span class="keyword">if</span> (fsopt-&gt;zp_pool)</span><br><span class="line">                seq_<span class="built_in">printf</span>(m, <span class="string">",zp_pool=%d"</span>,</span><br><span class="line">                         fsopt-&gt;zp_pool);</span><br></pre></td></tr></table></figure></p>
<p>做了以上的改动后我们就可以把参数给传进来了，现在我们需要把参数传递到需要用的地方<br>也就是static int ceph_statfs内需要调用这个参数</p>
<p>在static int ceph_statfs中添加上struct ceph_mount_options *fsopt = fsc-&gt;mount_options;<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">static int ceph_statfs(struct dentry *dentry, struct kstatfs *buf)</span><br><span class="line">&#123;</span><br><span class="line">        struct ceph_fs_client *fsc = ceph_inode_to_client(dentry-&gt;d_inode);</span><br><span class="line">        struct ceph_monmap *monmap = fsc-&gt;client-&gt;monc.monmap;</span><br><span class="line">        struct ceph_statfs st;</span><br><span class="line">        struct ceph_mount_options *fsopt = fsc-&gt;mount_options;</span><br><span class="line">        u64 fsid;</span><br></pre></td></tr></table></figure></p>
<p>然后改掉这个fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools == 1的判断，我们判断大于0即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (fsc-&gt;mdsc-&gt;mdsmap-&gt;m_num_data_pg_pools &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        data_pool = fsc-&gt;mdsc-&gt;mdsmap-&gt;m_data_pg_pools[fsopt-&gt;zp_pool];</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        data_pool = CEPH_NOPOOL;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>并且把写死的0改成我们的变量fsopt-&gt;zp_pool</p>
<p>到这里改动就完成了，这里还没有完，我们需要编译成我们的需要的模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># modinfo ceph</span></span><br><span class="line">filename:       /lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64/kernel/fs/ceph/ceph.ko.xz</span><br></pre></td></tr></table></figure></p>
<p>可以看到内核在高版本的时候已经改成了xz压缩的模块了,这里等会需要多处理一步<br>我们只需要这一个模块就编译这一个ceph.ko模块就好<br>编译需要装好kernel-devel包kernel-devel-3.10.0-862.el7.x86_64</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># pwd</span></span><br><span class="line">/home/origin/linux-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7/fs/ceph</span><br><span class="line">[root@lab103 ceph]<span class="comment"># make CONFIG_CEPH_FS=m -C /lib/modules/3.10.0-862.el7.x86_64/build/ M=`pwd` modules</span></span><br><span class="line">make: Entering directory `/usr/src/kernels/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">862</span>.el7.x86_64<span class="string">'</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/super.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/inode.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/dir.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/file.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/locks.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/addr.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ioctl.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/export.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/caps.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/snap.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/xattr.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/mds_client.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/mdsmap.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/strings.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph_frag.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/debugfs.o</span><br><span class="line">  CC [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/acl.o</span><br><span class="line">  LD [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.o</span><br><span class="line">  Building modules, stage 2.</span><br><span class="line">  MODPOST 1 modules</span><br><span class="line">  CC      /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.mod.o</span><br><span class="line">  LD [M]  /home/origin/linux-3.10.0-862.el7/fs/ceph/ceph.ko</span><br><span class="line">make: Leaving directory `/usr/src/kernels/3.10.0-862.el7.x86_64'</span></span><br></pre></td></tr></table></figure>
<p>正常应该就是上面的没有报错的输出了<br>压缩ko模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># find * -name '*.ko' | xargs -n 1 xz</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># rmmod ceph</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># rm -rf  /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/ceph.ko.xz</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># cp -ra ceph.ko.xz /lib/modules/3.10.0-862.el7.x86_64/kernel/fs/ceph/</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># lsmod |grep ceph</span></span><br><span class="line">ceph                  <span class="number">345111</span>  <span class="number">0</span> </span><br><span class="line">libceph               <span class="number">301687</span>  <span class="number">1</span> ceph</span><br><span class="line">dns_resolver           <span class="number">13140</span>  <span class="number">1</span> libceph</span><br><span class="line">libcrc32c              <span class="number">12644</span>  <span class="number">2</span> xfs,libceph</span><br></pre></td></tr></table></figure></p>
<p>现在已经加载好模块了，我们试验下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">11650</span>G     <span class="number">11645</span>G        <span class="number">5210</span>M          <span class="number">0.04</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    data         <span class="number">9</span>          <span class="number">0</span>         <span class="number">0</span>         <span class="number">3671</span>G           <span class="number">0</span> </span><br><span class="line">    metadata     <span class="number">10</span>     <span class="number">36391</span>         <span class="number">0</span>        <span class="number">11014</span>G          <span class="number">22</span> </span><br><span class="line">    newdata      <span class="number">11</span>         <span class="number">0</span>         <span class="number">0</span>         <span class="number">5507</span>G           <span class="number">0</span> </span><br><span class="line"></span><br><span class="line">[root@lab103 ceph]<span class="comment"># mount -t ceph 192.168.19.102:/ /mnt</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># df -h|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/         <span class="number">3.6</span>T     <span class="number">0</span>  <span class="number">3.6</span>T   <span class="number">0</span>% /mnt</span><br><span class="line">[root@lab103 ceph]<span class="comment"># ceph fs ls</span></span><br><span class="line">name: ceph, metadata pool: metadata, data pools: [data newdata ]</span><br></pre></td></tr></table></figure></p>
<p>我们给了一个默认存储池的值为0的编号的，现在显示的是data的容量，没有问题，我们想显示newdata存储池的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># mount -t ceph 192.168.19.102:/ /mnt -o zp_pool=1</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># df -h|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/         <span class="number">5.4</span>T     <span class="number">0</span>  <span class="number">5.4</span>T   <span class="number">0</span>% /mnt</span><br></pre></td></tr></table></figure></p>
<p>这里我们显示的要么0，要么1的存储池的那么我如果想显示全局的怎么处理？那就是给个不存在的编号就行了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ceph]<span class="comment"># mount -t ceph 192.168.19.102:/ /mnt -o zp_pool=1000</span></span><br><span class="line">[root@lab103 ceph]<span class="comment"># mount|grep ceph|grep zp_pool</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/ on /mnt <span class="built_in">type</span> ceph (rw,relatime,acl,wsize=<span class="number">16777216</span>,zp_pool=<span class="number">1000</span>)</span><br><span class="line">[root@lab103 ceph]<span class="comment"># df -h|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">19.102</span>:/          <span class="number">12</span>T  <span class="number">5.1</span>G   <span class="number">12</span>T   <span class="number">1</span>% /mnt</span><br></pre></td></tr></table></figure></p>
<p>也可以自己去改成读取all字段的时候取全局变量，这个是直接用一个不存在的编号去走到全局的容量的逻辑里面去了,这样比较简单</p>
<p>通过mount命令可以查询到挂载的选项</p>
<p>到这里就根据需求改完了</p>
<h2 id="总结">总结</h2><p>本篇里面涉及的知识点包括了rpm包的源码的获取，解压，以及内核模块的单独编译，改动单个模块进行替换，cephfs客户端的内核参数的自定义传递等等，在本博客的第三篇文章就有一个单独编译一个ext4模块的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-08-20</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/x7qsgok0tlsl6cttnul0em0z/pool1.png" alt="pool.png-115.2kB"><br></center>

<h2 id="前言">前言</h2><p>如果用cephfs比较多，应该都知道，在cephfs的客户端进行mount以后，看到的容量显示的是集群的总的容量，也就是你的总的磁盘空间是多少这个地方显示的就是多少</p>
<p>这个一直都是这样显示的，我们之前在hammer版本的时候，阿茂和大黄一起在公司内部实现了这个功能，社区会慢慢的集成一些类似的面向面向商业用户的需求</p>
<p>社区已经开发了一个版本，接口都做的差不多了，那么稍微改改，就能实现想要的需求的</p>
<p>本篇内的改动是基于内核客户端代码的改动，改动很小，应该能够看的懂</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[快速构建ceph可视化监控系统]]></title>
    <link href="http://www.zphj1987.com/2018/07/17/ceph-web-monitor-system-grafana/"/>
    <id>http://www.zphj1987.com/2018/07/17/ceph-web-monitor-system-grafana/</id>
    <published>2018-07-17T10:50:42.000Z</published>
    <updated>2018-07-17T11:04:05.634Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/488o6i5pn327d8udgrcpoilx/image.png" alt="granfa"><br></center>

<h2 id="前言">前言</h2><p>ceph的可视化方案很多，本篇介绍的是比较简单的一种方式，并且对包都进行了二次封装，所以能够在极短的时间内构建出一个可视化的监控系统</p>
<p>本系统组件如下：</p>
<ul>
<li>ceph-jewel版本</li>
<li>ceph_exporter的jewel版本</li>
<li>prometheus的2.3.2版本</li>
<li>grafana的grafana-5.2.1版本</li>
<li>Ceph grafana的插件- Clusterby Cristian Calin</li>
</ul>
<p>适配的系统为centos7</p>
<p>资源如下：</p>
<blockquote>
<p><a href="http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm" target="_blank" rel="external">http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm</a><br><a href="http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json" target="_blank" rel="external">http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json</a><br><a href="http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm" target="_blank" rel="external">http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm</a><br><a href="http://7xweck.com1.z0.glb.clouddn.com/grafana-5.2.1-1.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/grafana-5.2.1-1.x86_64.rpm</a></p>
</blockquote>
<p>以上资源均可以直接用wget进行下载，然后直接安装</p>
<a id="more"></a>
<h2 id="监控的架构介绍">监控的架构介绍</h2><p>通过ceph_exporter抓取的ceph相关的数据并且在本地监听端口9128端口</p>
<p>prometheus抓取ceph_exporter的9128的端口的数据存储在本地的/var/lib/prometheus/目录下面</p>
<p>grafana抓取prometheus的数据进行渲染成web页面</p>
<p>页面的模板就是使用的grafana的ceph模板插件</p>
<p>那么我们就根据上面的架构去一步步的把系统配置起来</p>
<h2 id="配置监控系统">配置监控系统</h2><h3 id="安装ceph_exporter">安装ceph_exporter</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 install]<span class="comment"># wget http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm</span></span><br><span class="line">[root@lab101 install]<span class="comment"># rpm -qpl ceph_exporter-2.0.0-1.x86_64.rpm </span></span><br><span class="line">/usr/bin/ceph_exporter</span><br><span class="line">/usr/lib/systemd/system/ceph_exporter.service</span><br><span class="line">[root@lab101 install]<span class="comment"># rpm -ivh ceph_exporter-2.0.0-1.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:ceph_exporter-<span class="number">2</span>:<span class="number">2.0</span>.<span class="number">0</span>-<span class="number">1</span>          <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl start ceph_exporter</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl enable ceph_exporter</span></span><br><span class="line">[root@lab101 install]<span class="comment"># netstat -tunlp|grep 9128</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">9128</span>                 :::*                    LISTEN      <span class="number">35853</span>/ceph_exporter</span><br></pre></td></tr></table></figure>
<p>可以看到端口起来了就是安装成功了，这个ceph_exporter建议是安装在管理节点上，也就是能够执行出ceph -s的节点上面的</p>
<h3 id="安装prometheus">安装prometheus</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 install]<span class="comment">#  wget http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm</span></span><br><span class="line">[root@lab101 install]<span class="comment"># rpm -qpl prometheus-2.3.2-1.x86_64.rpm </span></span><br><span class="line">/etc/ceph/prometheus.yml</span><br><span class="line">/usr/bin/prometheus</span><br><span class="line">/usr/lib/systemd/system/prometheus.service</span><br><span class="line">[root@lab101 install]<span class="comment"># rpm -ivh prometheus-2.3.2-1.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:prometheus-<span class="number">2</span>:<span class="number">2.3</span>.<span class="number">2</span>-<span class="number">1</span>             <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl start prometheus</span></span><br><span class="line">[root@lab101 install]<span class="comment"># netstat -tunlp|grep 9090</span></span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">9090</span>                 :::*                    LISTEN      <span class="number">36163</span>/prometheus</span><br></pre></td></tr></table></figure>
<p>这个地方默认是认为prometheus和ceph_exporter在一台机器上面，所以配置文件的/etc/ceph/prometheus.yml里面的targets写的是127.0.0.1，根据需要修改成ceph_exporter的ip地址即可</p>
<p>prometheus的默认监听端口为9090，到这个时候直接去web 上面就可以看到prometheus的抓取的数据了</p>
<p><img src="http://static.zybuluo.com/zphj1987/g84qk6x6qye3zjtlninfwrwm/image.png" alt="prometheus"></p>
<p>到这里是数据到prometheus的已经完成了，下面就去做跟grafana相关的配置了</p>
<h3 id="安装grafana">安装grafana</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 install]<span class="comment"># wget http://7xweck.com1.z0.glb.clouddn.com/grafana-5.2.1-1.x86_64.rpm</span></span><br><span class="line">[root@lab101 install]<span class="comment"># yum localinstall grafana-5.2.1-1.x86_64.rpm</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl start grafana-server.service</span></span><br><span class="line">[root@lab101 install]<span class="comment"># netstat -tunlp|grep gra</span></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp6       <span class="number">0</span>      <span class="number">0</span> :::<span class="number">3000</span>                 :::*                    LISTEN      <span class="number">36730</span>/grafana-serve</span><br></pre></td></tr></table></figure>
<p>grafana默认监听的3000的端口</p>
<p><img src="http://static.zybuluo.com/zphj1987/cr0jun59v0gwbhee1mq3ixmw/image.png" alt="grafanalogin"><br>默认登陆的用户名密码为admin admin,登陆成功后会强制修改密码</p>
<h3 id="配置grafana">配置grafana</h3><p><img src="http://static.zybuluo.com/zphj1987/c3czhtv5l7mml4hb6c4ztkvd/image.png" alt="add sour"><br>首先增加数据源<br><img src="http://static.zybuluo.com/zphj1987/kdunzb3s3hz7u0tkri3ntzzw/image.png" alt="配置9090"><br><img src="http://static.zybuluo.com/zphj1987/p2oec87wp1fszlldq1lrdkri/image.png" alt="import"></p>
<p><img src="http://static.zybuluo.com/zphj1987/4gxwmkfkmr2do4g9v58x1kve/image.png" alt="image.png-97.2kB"></p>
<p><img src="http://static.zybuluo.com/zphj1987/iah7t9cud3bnponoddq3iv9q/image.png" alt="image.png-96.2kB"></p>
<p>这里如果能上网就直接输入id 917 ，如果不能上网就把上面的ceph-cluster_rev1.json文件弄到本地去，导入进去即可</p>
<p><img src="http://static.zybuluo.com/zphj1987/488o6i5pn327d8udgrcpoilx/image.png" alt="granfa"></p>
<p>到这里就完成了配置了</p>
<h2 id="总结">总结</h2><p>以上为了方便都把相关的软件做成了rpm包，从安装方便角度来看，grafana，ceph_exporter，还有prometheus都采用的是单二进制文件的方式，稍微组合一下大大的降低了部署难度，比如那个ceph_exporter需要用go进行编译，封好包以后就不需要这个过程，并且接口因为有版本的限制，所以这样直接对应版本安装也避免了出错</p>
<p>本篇的环境所述均为jewel适配版本</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-07-17</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/488o6i5pn327d8udgrcpoilx/image.png" alt="granfa"><br></center>

<h2 id="前言">前言</h2><p>ceph的可视化方案很多，本篇介绍的是比较简单的一种方式，并且对包都进行了二次封装，所以能够在极短的时间内构建出一个可视化的监控系统</p>
<p>本系统组件如下：</p>
<ul>
<li>ceph-jewel版本</li>
<li>ceph_exporter的jewel版本</li>
<li>prometheus的2.3.2版本</li>
<li>grafana的grafana-5.2.1版本</li>
<li>Ceph grafana的插件- Clusterby Cristian Calin</li>
</ul>
<p>适配的系统为centos7</p>
<p>资源如下：</p>
<blockquote>
<p><a href="http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm">http://static.zybuluo.com/zphj1987/jiwx305b8q1hwc5uulo0z7ft/ceph_exporter-2.0.0-1.x86_64.rpm</a><br><a href="http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json">http://static.zybuluo.com/zphj1987/1nu2k4cpcery94q2re3u6s1t/ceph-cluster_rev1.json</a><br><a href="http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm">http://static.zybuluo.com/zphj1987/7ro7up6r03kx52rkwy1qjuwm/prometheus-2.3.2-1.x86_64.rpm</a><br><a href="http://7xweck.com1.z0.glb.clouddn.com/grafana-5.2.1-1.x86_64.rpm">http://7xweck.com1.z0.glb.clouddn.com/grafana-5.2.1-1.x86_64.rpm</a></p>
</blockquote>
<p>以上资源均可以直接用wget进行下载，然后直接安装</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用s3-test进行ceph的接口兼容性测试]]></title>
    <link href="http://www.zphj1987.com/2018/06/27/use-s3-test-to-ceph-compatibility-tests/"/>
    <id>http://www.zphj1987.com/2018/06/27/use-s3-test-to-ceph-compatibility-tests/</id>
    <published>2018-06-27T09:16:54.000Z</published>
    <updated>2018-07-17T11:03:58.930Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/s3.jpg" alt="s3"><br></center>

<h2 id="前言">前言</h2><p>ceph的rgw能够提供一个兼容性的s3的接口，既然是兼容性，当然不可能是所有接口都会兼容，那么我们需要有一个工具来进行接口的验证以及测试，这个在其他测试工具里面有类似的posix接口验证工具，这类的工具就是跑测试用例，来输出通过或者不通过的列表</p>
<p>用此类的工具有个好的地方就是，能够对接口进行验证，来避免版本的更新带来的接口破坏<br><a id="more"></a></p>
<h2 id="安装">安装</h2><p>直接对官方的分支进行clone下来，总文件数不多，下载很快<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 s3]<span class="comment"># git clone https://github.com/ceph/s3-tests.git</span></span><br><span class="line">[root@lab101 s3]<span class="comment"># cd s3-tests/</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方注意下有版本之分，测试的时候需要用对应版本，这里我们测试的jewel版本就切换到jewel的分支(关键步骤)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 s3-tests]<span class="comment"># git branch -a</span></span><br><span class="line">[root@lab101 s3-tests]<span class="comment"># git checkout -b jewel remotes/origin/ceph-jewel</span></span><br><span class="line">[root@lab101 s3-tests]<span class="comment"># ./bootstrap</span></span><br></pre></td></tr></table></figure>
<p>进入到目录当中执行 ./bootstrap进行初始化相关的工作，这个是下载一些相关的库和软件包，并且创建了一个python的虚拟环境，如果从其他地方拷贝过来的代码最好是删除掉python虚拟环境，让程序自己去重新创建一套环境</p>
<p>执行完了以后就是创建测试配置文件test.conf</p>
<figure class="highlight nix"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line"><span class="comment">## this section is just used as default for all the "s3 *"</span></span><br><span class="line"><span class="comment">## sections, you can place these variables also directly there</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## replace with e.g. "localhost" to run against local software</span></span><br><span class="line"><span class="variable">host =</span> <span class="number">192.168</span>.<span class="number">19.101</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## uncomment the port to use something other than 80</span></span><br><span class="line"><span class="variable">port =</span> <span class="number">7481</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## say "no" to disable TLS</span></span><br><span class="line"><span class="variable">is_secure =</span> no</span><br><span class="line"></span><br><span class="line">[fixtures]</span><br><span class="line"><span class="comment">## all the buckets created will start with this prefix;</span></span><br><span class="line"><span class="comment">## &#123;random&#125; will be filled with random characters to pad</span></span><br><span class="line"><span class="comment">## the prefix to 30 characters long, and avoid collisions</span></span><br><span class="line">bucket <span class="variable">prefix =</span> cephtest-&#123;random&#125;-</span><br><span class="line"></span><br><span class="line">[s3 main]</span><br><span class="line"><span class="comment">## the tests assume two accounts are defined, "main" and "alt".</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## user_id is a 64-character hexstring</span></span><br><span class="line"><span class="variable">user_id =</span> test1</span><br><span class="line"></span><br><span class="line"><span class="comment">## display name typically looks more like a unix login, "jdoe" etc</span></span><br><span class="line"><span class="variable">display_name =</span> test1</span><br><span class="line"></span><br><span class="line"><span class="comment">## replace these with your access keys</span></span><br><span class="line"><span class="variable">access_key =</span> test1</span><br><span class="line"><span class="variable">secret_key =</span> test1</span><br><span class="line"></span><br><span class="line"><span class="comment">## replace with key id obtained when secret is created, or delete if KMS not tested</span></span><br><span class="line"><span class="comment">#kms_keyid = 01234567-89ab-cdef-0123-456789abcdef</span></span><br><span class="line"></span><br><span class="line">[s3 alt]</span><br><span class="line"><span class="comment">## another user account, used for ACL-related tests</span></span><br><span class="line"><span class="variable">user_id =</span> test2</span><br><span class="line"><span class="variable">display_name =</span> test2</span><br><span class="line"><span class="comment">## the "alt" user needs to have email set, too</span></span><br><span class="line"><span class="variable">email =</span> test2@qq.com</span><br><span class="line"><span class="variable">access_key =</span> test2</span><br><span class="line"><span class="variable">secret_key =</span> test2</span><br></pre></td></tr></table></figure>
<p>上面的用户信息是需要提前创建好的，这个用集群内的机器radosgw-admin命令创建即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin user create --uid=<span class="built_in">test</span>01 --display-name=<span class="built_in">test</span>01 --access-key=<span class="built_in">test</span>01 --secret-key=<span class="built_in">test</span>01 --email=<span class="built_in">test</span>01@qq.com</span><br><span class="line">radosgw-admin user create --uid=<span class="built_in">test</span>02 --display-name=<span class="built_in">test</span>02 --access-key=<span class="built_in">test</span>02 --secret-key=<span class="built_in">test</span>02 --email=<span class="built_in">test</span>02@qq.com</span><br></pre></td></tr></table></figure></p>
<p>创建好了以后就可以开始测试了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 s3-tests]<span class="comment"># S3TEST_CONF=test.conf ./virtualenv/bin/nosetests -a '!fails_on_rgw'</span></span><br><span class="line">..................................................SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS.....................................................................................................................SSSS.......................................................................................................................................SSSS.......................................................</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">Ran <span class="number">408</span> tests <span class="keyword">in</span> <span class="number">122.087</span>s</span><br><span class="line"></span><br><span class="line">OK (SKIP=<span class="number">51</span>)</span><br></pre></td></tr></table></figure></p>
<p>正常测试完就应该是上面的ok的状态，也有可能某个版本的测试用例是写的支持，但是rgw也不一定就做好了，这个需要自己判断一下</p>
<h2 id="总结">总结</h2><p>了解软件适配的接口，针对接口进行相关测试即可</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-06-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xo9we.com1.z0.glb.clouddn.com/s3.jpg" alt="s3"><br></center>

<h2 id="前言">前言</h2><p>ceph的rgw能够提供一个兼容性的s3的接口，既然是兼容性，当然不可能是所有接口都会兼容，那么我们需要有一个工具来进行接口的验证以及测试，这个在其他测试工具里面有类似的posix接口验证工具，这类的工具就是跑测试用例，来输出通过或者不通过的列表</p>
<p>用此类的工具有个好的地方就是，能够对接口进行验证，来避免版本的更新带来的接口破坏<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph erasure默认的min_size分析]]></title>
    <link href="http://www.zphj1987.com/2018/06/12/ceph-erasure-default-min-size/"/>
    <id>http://www.zphj1987.com/2018/06/12/ceph-erasure-default-min-size/</id>
    <published>2018-06-12T02:43:10.000Z</published>
    <updated>2018-06-12T06:56:17.609Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/w3q75obfzlripq5usixlk73m/desk.jpg" alt="desk.jpg-47.1kB"><br></center>

<h2 id="引言">引言</h2><p>最近接触了两个集群都使用到了erasure code,一个集群是hammer版本的，一个环境是luminous版本的，两个环境都出现了incomplete，触发的原因有类似的地方，都是有osd的离线的问题</p>
<p>准备在本地环境进行复验的时候，发现了一个跟之前接触的erasure不同的地方，这里做个记录，以防后面出现同样的问题<br><a id="more"></a></p>
<h2 id="分析过程">分析过程</h2><p>准备了一个luminous的集群，使用默认的erasure的profile进行了创建存储池的相关工作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph osd erasure-code-profile get default</span></span><br><span class="line">k=<span class="number">2</span></span><br><span class="line">m=<span class="number">1</span></span><br><span class="line">plugin=jerasure</span><br><span class="line">technique=reed_sol_van</span><br></pre></td></tr></table></figure></p>
<p>默认的是2+1的纠删码的配置，创建完了以后存储池的配置是这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">1</span> <span class="string">'rbd'</span> erasure size <span class="number">3</span> min_size <span class="number">3</span> crush_rule <span class="number">2</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">256</span> pgp_num <span class="number">256</span> last_change <span class="number">41</span> flags hashpspool stripe_width <span class="number">8192</span> application rbdrc</span><br></pre></td></tr></table></figure></p>
<p>然后停止了一个osd以后，状态变成了这样的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -s</span></span><br><span class="line">  cluster:</span><br><span class="line">    id:     <span class="number">9</span>ec7768a-<span class="number">5</span>e7c-<span class="number">4</span>f8e-<span class="number">8</span>a85-<span class="number">89895</span>e338cca</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            <span class="number">1</span> osds down</span><br><span class="line">            Reduced data availability: <span class="number">42</span> pgs inactive, <span class="number">131</span> pgs incomplete</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: <span class="number">1</span> daemons, quorum lab102</span><br><span class="line">    mgr: lab102(active)</span><br><span class="line">    osd: <span class="number">6</span> osds: <span class="number">5</span> up, <span class="number">6</span> <span class="keyword">in</span></span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   <span class="number">3</span> pools, <span class="number">288</span> pgs</span><br><span class="line">    objects: <span class="number">1666</span>k objects, <span class="number">13331</span> MB</span><br><span class="line">    usage:   <span class="number">319</span> GB used, <span class="number">21659</span> GB / <span class="number">21979</span> GB avail</span><br><span class="line">    pgs:     <span class="number">45.486</span>% pgs not active</span><br><span class="line">             <span class="number">157</span> active+clean</span><br><span class="line">             <span class="number">131</span> incomplete</span><br></pre></td></tr></table></figure></p>
<p>停止一个osd也会出现incomplete的状态，也就是在默认状态下，是一个osd也不允许down掉的，不然pg就进入了无法使用的状态，这个在我这里感觉无法理解的，开始以为这个是L版本的bug，在查了下资料以后，发现并不是的</p>
<p>查询到一个这样的patch<a href="https://patchwork.kernel.org/patch/8546771/" target="_blank" rel="external">：default min_size for erasure pools</a></p>
<p>这个里面就讨论了min_size的问题，上面的环境我也发现了，默认的配置的2+1,这个在我的理解下，正常应该会配置为min_size 2,在down掉一个的时候还是可写，可读的</p>
<p>实际上在/src/mon/OSDMonitor.cc 这个里面已经把erasure的min_size的控制改为了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">*min_size = erasure_code-&gt;get_data_chunk_count();</span><br><span class="line">变成</span><br><span class="line">*min_size = erasure_code-&gt;get_data_chunk_count() + <span class="number">1</span>;</span><br></pre></td></tr></table></figure></p>
<p>最后面作者提出了自己的担心，假如在K+M的配置下，只有K个的osd允许可以读写的时候，环境是K个OSD是好的，M个OSD挂掉了，这个时候启动一个M中的osd的时候，会进行backfilling，这个时候如果K个osd当中的某个osd挂掉的话，这个时候实际上PG里面的数据就是不完整的，如果是K+1的时候，这个时候做恢复的时候再挂掉一个，实际上还是完整的，也就是开发者考虑的是恢复过程的异常状况还留一个冗余，这个实际我们在日常的维护过程当中也经常遇到恢复过程中确实有osd的挂掉的情况,这个在其他文件系统里面的做法是设计成可读不可写状态</p>
<p>也就是现在ceph的erasure的min_size设计成了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">min_size=K+<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>也就是默认的环境下的是min_size是3</p>
<p>到这里就知道上面为什么会出现上面的状况了，也就是这个编码设置的时候需要自己去控制下，比如4+2的ec，最多能挂掉几个，如果在以前可以很肯定的说是2个，实际在新的情况下是4+1=5也就是只允许挂掉一个是可读可写的</p>
<p>当然真正生产环境出现了4+2挂掉两个变成了incomplete的时候，因为这个时候数据还是完整可拼接的，所以可以强制mark-complete或者自己把代码里面的min_size改掉来触发恢复也是可以的</p>
<h2 id="总结">总结</h2><p>对于ec这块接触的很早，里面还是有很多有意思的可以研究的东西的，ec最适合的场景就是归档，当然在某些配置下面，性能也是很不错的，也能支持一些低延时的任务，这个最大的特点就是一定需要根据实际环境去跑性能测试，拆成几比几性能有多少，这个一般还是不太好预估的，跟写入的文件模型也有关联</p>
<p>虽然作者的设计初衷是没问题的，但是这个默认配置实际是不符合生产要求的，所以个人觉得这个不是很合理，默认的应该是不需要调整也是可用的，一个osd也不允许down的话，真正也没法用起来，所以不清楚是否有其他可改变的配置来处理这个，自己配置的时候注意下这个min_size，如果未来有控制的参数，会补充进这篇文章</p>
<h2 id="补充">补充</h2><p>通过测试发现，可以通过存储池设置这个min_size来实现继续使用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool <span class="built_in">set</span> rbd min_size <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>也就是这个地方跟副本池的设计类似，给定一个初始值，然后可以通过设置进行修改</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-06-12</td>
</tr>
<tr>
<td style="text-align:center">更新ec的min_size设置</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-06-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/w3q75obfzlripq5usixlk73m/desk.jpg" alt="desk.jpg-47.1kB"><br></center>

<h2 id="引言">引言</h2><p>最近接触了两个集群都使用到了erasure code,一个集群是hammer版本的，一个环境是luminous版本的，两个环境都出现了incomplete，触发的原因有类似的地方，都是有osd的离线的问题</p>
<p>准备在本地环境进行复验的时候，发现了一个跟之前接触的erasure不同的地方，这里做个记录，以防后面出现同样的问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[cephfs元数据池故障的恢复]]></title>
    <link href="http://www.zphj1987.com/2018/05/29/cephfs-metadatapool-disaster-recover/"/>
    <id>http://www.zphj1987.com/2018/05/29/cephfs-metadatapool-disaster-recover/</id>
    <published>2018-05-29T15:37:52.000Z</published>
    <updated>2018-05-29T16:01:56.305Z</updated>
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>cephfs 在L版本已经比较稳定了，这个稳定的意义个人觉得是在其故障恢复方面的成熟，一个文件系统可恢复是其稳定必须具备的属性，本篇就是根据官网的文档来实践下这个恢复的过程</p>
<h2 id="实践过程">实践过程</h2><h3 id="部署一个ceph_Luminous集群">部署一个ceph  Luminous集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">12.2</span>.<span class="number">5</span> (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br></pre></td></tr></table></figure>
<p>创建filestore<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd create  lab102  --filestore  --data /dev/sdb1  --journal /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>这里想用filestore进行测试就按上面的方法去创建osd即可</p>
<a id="more"></a>
<p>传入测试数据</p>
<ul>
<li>doc </li>
<li>pic</li>
<li>vidio<br>这里提供下载链接</li>
</ul>
<blockquote>
<p>链接：<a href="https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg" target="_blank" rel="external">https://pan.baidu.com/s/19tlFi4butA2WjnPAdNEMwg</a> 密码：ugjo</p>
</blockquote>
<p>这个是网上下载的模板的数据，方便进行真实的文件的模拟，dd产生的是空文件，有的时候会影响到测试</p>
<p>需要更多的测试文档推荐可以从下面网站下载</p>
<p>视频下载：</p>
<blockquote>
<p><a href="https://videos.pexels.com/popular-videos" target="_blank" rel="external">https://videos.pexels.com/popular-videos</a></p>
</blockquote>
<p>图片下载：</p>
<blockquote>
<p><a href="https://www.pexels.com/" target="_blank" rel="external">https://www.pexels.com/</a></p>
</blockquote>
<p>文档下载：</p>
<blockquote>
<p><a href="http://office.mmais.com.cn/Template/Home.shtml" target="_blank" rel="external">http://office.mmais.com.cn/Template/Home.shtml</a></p>
</blockquote>
<h3 id="元数据模拟故障">元数据模拟故障</h3><p>跟元数据相关的故障无非就是mds无法启动，或者元数据pg损坏了，这里我们模拟的比较极端的情况，把metadata的元数据对象全部清空掉，这个基本能覆盖到最严重的故障了，数据的损坏不在元数据损坏的范畴</p>
<p>清空元数据存储池<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> object <span class="keyword">in</span> `rados -p metadata ls`;<span class="keyword">do</span> rados -p metadata rm <span class="variable">$object</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>重启下mds进程，应该mds是无法恢复正常的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster:</span><br><span class="line">    id:     <span class="number">9</span>ec7768a-<span class="number">5</span>e7c-<span class="number">4</span>f8e-<span class="number">8</span>a85-<span class="number">89895</span>e338cca</span><br><span class="line">    health: HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> filesystem is degraded</span><br><span class="line">            <span class="number">1</span> mds daemon damaged</span><br><span class="line">            too few PGs per OSD (<span class="number">16</span> &lt; min <span class="number">30</span>)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: <span class="number">1</span> daemons, quorum lab102</span><br><span class="line">    mgr: lab102(active)</span><br><span class="line">    mds: ceph-<span class="number">0</span>/<span class="number">1</span>/<span class="number">1</span> up , <span class="number">1</span> up:standby, <span class="number">1</span> damaged</span><br><span class="line">    osd: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>准备开始我们的修复过程</p>
<h3 id="元数据故障恢复">元数据故障恢复</h3><p>设置允许多文件系统<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph fs flag <span class="built_in">set</span> <span class="built_in">enable</span>_multiple <span class="literal">true</span> --yes-i-really-mean-it</span><br></pre></td></tr></table></figure></p>
<p>创建一个新的元数据池，这里是为了不去动原来的metadata的数据，以免损坏原来的元数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd pool create recovery <span class="number">8</span></span><br></pre></td></tr></table></figure></p>
<p>将老的存储池data和新的元数据池recovery关联起来并且创建一个新的recovery-fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph fs new recovery-fs recovery data --allow-dangerous-metadata-overlay</span></span><br><span class="line">new fs with metadata pool <span class="number">3</span> and data pool <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>做下新的文件系统的初始化相关工作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#cephfs-data-scan init --force-init --filesystem recovery-fs --alternate-pool recovery</span></span><br></pre></td></tr></table></figure></p>
<p>reset下新的fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#ceph fs reset recovery-fs --yes-i-really-mean-it</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset session</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset snap</span></span><br><span class="line">[root@lab102 ~]<span class="comment">#cephfs-table-tool recovery-fs:all reset inode</span></span><br></pre></td></tr></table></figure></p>
<p>做相关的恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_extents --force-pool --alternate-pool recovery --filesystem ceph  data</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_inodes --alternate-pool recovery --filesystem ceph --force-corrupt --force-init data</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># cephfs-data-scan scan_links --filesystem recovery-fs</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># systemctl start ceph-mds@lab102</span></span><br><span class="line">等待mds active 以后再继续下面操作</span><br><span class="line">[root@lab102 ~]<span class="comment"># ceph daemon mds.lab102 scrub_path / recursive repair</span></span><br></pre></td></tr></table></figure>
<p>设置成默认的fs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph fs set-default recovery-fs</span></span><br></pre></td></tr></table></figure></p>
<p>挂载检查数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment">#  mount -t ceph 192.168.19.102:/ /mnt</span></span><br><span class="line">[root@lab102 ~]<span class="comment"># ll /mnt</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">drwxr-xr-x <span class="number">1</span> root root <span class="number">1</span> Jan  <span class="number">1</span>  <span class="number">1970</span> lost+found</span><br><span class="line">[root@lab102 ~]<span class="comment"># ll /mnt/lost+found/</span></span><br><span class="line">total <span class="number">226986</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">569306</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000001</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root <span class="number">16240627</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000002</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1356367</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000003</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">137729</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000004</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">155163</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000005</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">118909</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000006</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1587656</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000007</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">252705</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000008</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1825192</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000009</span></span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">156990</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>a</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">3493435</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>b</span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">342390</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>c</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">1172247</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>d</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">2516169</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>e</span><br><span class="line">-r-x------ <span class="number">1</span> root root  <span class="number">3218770</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">1000000000</span>f</span><br><span class="line">-r-x------ <span class="number">1</span> root root   <span class="number">592729</span> May <span class="number">25</span> <span class="number">16</span>:<span class="number">16</span> <span class="number">10000000010</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到在lost+found里面就有数据了<br><figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000010</span> </span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000010</span>: Microsoft PowerPoint <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000011</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000011</span>: Microsoft Word <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000012</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000012</span>: Microsoft Word <span class="number">2007</span>+</span><br><span class="line">[root<span class="annotation">@lab</span>102 ~]# file <span class="regexp">/mnt/</span>lost+found/<span class="number">10000000013</span></span><br><span class="line"><span class="regexp">/mnt/</span>lost+found/<span class="number">10000000013</span>: Microsoft PowerPoint <span class="number">2007</span>+</span><br></pre></td></tr></table></figure></p>
<p>这个生成的文件名称就是实际文件存储的数据的prifix，也就是通过原始inode进行的运算得到的</p>
<p>如果提前备份好了原始的元数据信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph daemon mds.lab102 dump cache &gt; /tmp/mdscache</span></span><br></pre></td></tr></table></figure></p>
<p>那么可以比较轻松的找到丢失的文件</p>
<h2 id="总结">总结</h2><p>在我另外一篇文章当中已经写过了，通过文件的inode可以把文件跟后台的对象结合起来，在以前我的恢复的思路是，把后台的对象全部抓出来，然后自己手动去对对象进行拼接，实际是数据存在的情况下，反向把文件重新link到一个路径，这个是官方提供的的恢复方法，mds最大的担心就是mds自身的元数据的损坏可能引起整个文件系统的崩溃，而现在，基本上只要data的数据还在的话，就不用担心数据丢掉，即使文件路径信息没有了，但是文件还在</p>
<p>通过备份mds cache可以把文件名称，路径，大小和inode关联起来，而恢复的数据是对象前缀，也就是备份好了mds cache 就可以把整个文件信息串联起来了</p>
<p>虽然cephfs的故障不是常发生，但是万一呢</p>
<p>后续准备带来一篇关于cephfs从挂载点误删除数据后的数据恢复的方案，这个目前已经进行了少量文件的恢复试验了，等后续进行大量文件删除的恢复后，再进行分享</p>
<h2 id="参考文档">参考文档</h2><p><a href="http://docs.ceph.com/docs/luminous/cephfs/disaster-recovery/" target="_blank" rel="external">disaster-recovery</a></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-05-29</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前言">前言</h2><p>cephfs 在L版本已经比较稳定了，这个稳定的意义个人觉得是在其故障恢复方面的成熟，一个文件系统可恢复是其稳定必须具备的属性，本篇就是根据官网的文档来实践下这个恢复的过程</p>
<h2 id="实践过程">实践过程</h2><h3 id="部署一个ceph_Luminous集群">部署一个ceph  Luminous集群</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">12.2</span>.<span class="number">5</span> (cad919881333ac92274171586c827e01f554a70a) luminous (stable)</span><br></pre></td></tr></table></figure>
<p>创建filestore<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy osd create  lab102  --filestore  --data /dev/sdb1  --journal /dev/sdb2</span><br></pre></td></tr></table></figure></p>
<p>这里想用filestore进行测试就按上面的方法去创建osd即可</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[cosbench使用方法]]></title>
    <link href="http://www.zphj1987.com/2018/04/12/cosbench-how-to-use/"/>
    <id>http://www.zphj1987.com/2018/04/12/cosbench-how-to-use/</id>
    <published>2018-04-11T17:18:33.000Z</published>
    <updated>2018-04-11T17:38:02.798Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/f1bbph3hxt9twexfcgmh3wtp/cosbenchnnnn.png" alt="cosbench.png-10.4kB"><br></center>

<h2 id="前言">前言</h2><p>cosbench的功能很强大，但是配置起来可能就有点不是太清楚怎么配置了，本篇将梳理一下这个测试的配置过程，以及一些测试注意项目，以免无法完成自己配置模型的情况</p>
<h2 id="安装">安装</h2><p>cosbench模式是一个控制端控制几个driver向后端rgw发起请求</p>
<p>下载最新版本</p>
<blockquote>
<p><a href="https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip" target="_blank" rel="external">https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip</a></p>
</blockquote>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab102 cosbench]<span class="comment"># unzip 0.4.2.zip</span></span><br><span class="line">[root@lab102 cosbench]<span class="comment"># yum install java-1.7.0-openjdk nmap-ncat</span></span><br></pre></td></tr></table></figure>
<p>同时可以执行的workloads的个数通过下面的control参数控制</p>
<blockquote>
<p>concurrency=1</p>
</blockquote>
<p>默认是一个，这个为了保证单机的硬件资源足够，保持单机启用一个workload</p>
<p>创建一个s3用户<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># radosgw-admin user create --uid=test1 --display-name="test1" --access-key=test1  --secret-key=test1</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"user_id"</span>: <span class="string">"test1"</span>,</span><br><span class="line">    <span class="string">"display_name"</span>: <span class="string">"test1"</span>,</span><br><span class="line">    <span class="string">"email"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"suspended"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"max_buckets"</span>: <span class="number">1000</span>,</span><br><span class="line">    <span class="string">"auid"</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">"subusers"</span>: [],</span><br><span class="line">    <span class="string">"keys"</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">"user"</span>: <span class="string">"test1"</span>,</span><br><span class="line">            <span class="string">"access_key"</span>: <span class="string">"test1"</span>,</span><br><span class="line">            <span class="string">"secret_key"</span>: <span class="string">"test1"</span></span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"swift_keys"</span>: [],</span><br><span class="line">    <span class="string">"caps"</span>: [],</span><br><span class="line">    <span class="string">"op_mask"</span>: <span class="string">"read, write, delete"</span>,</span><br><span class="line">    <span class="string">"default_placement"</span>: <span class="string">""</span>,</span><br><span class="line">    <span class="string">"placement_tags"</span>: [],</span><br><span class="line">    <span class="string">"bucket_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span>: -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"user_quota"</span>: &#123;</span><br><span class="line">        <span class="string">"enabled"</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">"max_size_kb"</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">"max_objects"</span>: -<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"temp_url_keys"</span>: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="配置相关">配置相关</h2><p>cosbench的配置文件结构<br><img src="http://static.zybuluo.com/zphj1987/1ftoz8yntuixd6eqmvxm4old/image.png" alt="image.png-47.7kB"></p>
<ul>
<li>一个workload 可以定义一个或者多个work stages</li>
<li>执行多个work stages是顺序的，执行同一个work stage里面的work是可以并行执行的</li>
<li>每个work里面，worker是来调整负载的</li>
<li>认证可以多个级别的定义，低级别的认证会覆盖高级别的配置</li>
</ul>
<p>可以通过配置多个work的方式来实现并发，而在work内通过增加worker的方式增加并发，从而实现多对多的访问，worker的分摊是分到了driver上面，注意多work的时候的containers不要重名，划分好bucker的空间</p>
<p><img src="http://static.zybuluo.com/zphj1987/1rbgzhq4j0wojgy6x6vd9vik/image.png" alt="image.png-144.5kB"></p>
<p>work相关的说明</p>
<ul>
<li>可以通过写入时间，写入容量，写入iops来控制什么时候结束</li>
<li>interval默认是5s是用来对性能快照的间隔，可以理解为采样点</li>
<li>division 控制workers之间的分配工作的方式是bucket还是对象还是none</li>
<li>默认全部的driver参与工作，也可以通过参数控制部分driver参与</li>
<li>时间会控制执行，如果时间没到，但是指定的对象已经写完了的话就会去进行复写的操作，这里要注意是进行对象的控制还是时间的控制进行的测试</li>
</ul>
<p>如果读取测试的时候，如果没有那个对象，会中断的提示，所以测试读之前需要把测试的对象都填充完毕（最好检查下先）</p>
<h2 id="单项的配置文件">单项的配置文件</h2><h3 id="通过单网关创建bucket">通过单网关创建bucket</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,32);containers=r(1,32);objects=r(0,0);sizes=c(0)B;containers=r(1,32)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure>
<p>如上配置的时候，如果设置的是workers=1,那么就会从当前的driver中挑选一个driver出来，然后选择配置storage进行bucket的创建，如果设置的是workers=2，那么就会挑选两个driver出来进行创建，一个driver负责一半的工作，相当于两个客户端同时向一个网关发起创建的操作</p>
<p>rgw的网关是对等的关系，那么这里肯定就有另外一种配置，我想通过不只一个网关进行创建的操作，那么这个地方是通过增加work的配置来实现的，我们看下配置</p>
<h3 id="通过多网关创建bucket">通过多网关创建bucket</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);containers=r(1,16);objects=r(0,0);sizes=c(0)B;containers=r(1,16)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);containers=r(17,32);objects=r(0,0);sizes=c(0)B;containers=r(17,32)"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure>
<p>以上配置就实现了通过两个网关进行创建bucket的配置了，下面是做prepare的相关配置，在cosbench里面有两个部分可以进行写操作，在prepare stage里面和 main stage里面<br>这个地方这样设置的理由是：<br>如果有读和写混合测试的时候，那么就需要提前进行读数据的准备，然后再开始进行读写并发的测试，所以会有一个prepare的阶段，这个在配置文件里面只是type设置的不同，其他没区别，我们可以看下这里web界面里面提供的配置项目，下面其他项目默认都是采取双并发的模式</p>
<p><img src="http://static.zybuluo.com/zphj1987/e11r5pzxy4hpexsodbnj49bi/image.png" alt="prepare"></p>
<p>在写的部分是一样的</p>
<h3 id="通过多网关写数据">通过多网关写数据</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"putobject"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw1-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"container"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">            config=<span class="string">"containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw2-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"container"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">            config=<span class="string">"containers=u(17,32);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;			</span><br><span class="line">&lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<p>这里有几个参数可以注意一下：</p>
<blockquote>
<p>containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB</p>
</blockquote>
<p>控制写入的bucket的名称的，是全部散列还是把负载均分可以自己去控制，objects是指定写入bucke里面的对象的名称的，sizes是指定大小的，如果两个值不同，就是设置的范围，相同就是设置的指定大小的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span> afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span></span><br></pre></td></tr></table></figure></p>
<p>这个是控制写入什么时候中止的，可以通过时间，也可以通过总的ops，或者总的大小来控制，这个需求可以自己定，afr是控制允许的失效率的，单位为1百万分之<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">interval=<span class="string">"5"</span></span><br></pre></td></tr></table></figure></p>
<p>这个是控制抓取性能数据的周期的</p>
<p>写入的配置就完了</p>
<h3 id="并发读取的配置">并发读取的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"getobj"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw1-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"none"</span> runtime=<span class="string">"30"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">            config=<span class="string">"containers=u(1,16);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;</span><br><span class="line">    &lt;work name=<span class="string">"rgw2-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">        division=<span class="string">"none"</span> runtime=<span class="string">"30"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">        afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">        &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">        &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">            config=<span class="string">"containers=u(17,32);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">    &lt;/work&gt;			</span><br><span class="line">&lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<h3 id="删除对象的配置">删除对象的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"cleanup"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">          &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">          &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw1-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);objects=r(1,5);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                  config=<span class="string">"containers=r(1,16);objects=r(1,5);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw2-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);objects=r(1,5);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                  config=<span class="string">"containers=r(17,32);objects=r(1,5);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">      &lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<h3 id="删除bucket的配置">删除bucket的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;workstage name=<span class="string">"dispose"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">          &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">          &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw1-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                  division=<span class="string">"container"</span></span><br><span class="line">                  config=<span class="string">"containers=r(1,16);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;</span><br><span class="line">          &lt;work name=<span class="string">"rgw2-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">              division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">              afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);"</span>&gt;</span><br><span class="line">              &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">              &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">              &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                  division=<span class="string">"container"</span></span><br><span class="line">                  config=<span class="string">"containers=r(17,32);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">          &lt;/work&gt;			</span><br><span class="line">      &lt;/workstage&gt;</span><br></pre></td></tr></table></figure>
<p>上面的workstage一共包括下面几种</p>
<ul>
<li>init 创建bucket</li>
<li>normal write 写入对象</li>
<li>normal read  读取对象</li>
<li>cleanup  清理对象</li>
<li>dispose  清理bucket</li>
</ul>
<p>division是控制多个worker之间的操作怎么去分的控制，最好在operation那层进行控制</p>
<h2 id="测试前自我提问">测试前自我提问</h2><ul>
<li>单机用了几个workload（默认一般一个，保证单个测试资源的独占）</li>
<li>采用了几个driver（决定了客户端的发起是有几个客户端，单机一个就可以）</li>
<li>测试了哪几个项目（init,prepare or normal,remove），单独测试还是混合测试</li>
<li>单个项目的workstage里面启动了几个work（work可以控制请求发向哪里）</li>
<li>单个work里面采用了几个workers(这个是控制几个driver进行并发的)</li>
<li>测试的ceph集群有多少个rgw网关，创建了多少个bucket测试</li>
<li>设置的写入每个bucket的对象为多少？对象大小为多少？测试时间为多久？</li>
</ul>
<p>测试很多文件的时候，可以用ops控制，并且将ops设置大于想测试的文件数目，保证能写入那么多的数据，或者比较确定性能，也可以通过时间控制</p>
<p>那么我来根据自己的需求来进行一个测试模型说明，然后根据说明进行配置</p>
<ul>
<li>采用两个客户端测试，那么准备两个driver</li>
<li>准备配置两个rgw的网关，那么在配置workstage的时候配置两个work对应到两个storage</li>
<li>测试创建，写入，读取，删除对象，删除bucket一套完整测试</li>
<li>wokers设置为2的倍数，初始值为2，让每个driver分得一半的负载，在进行一轮测试后，成倍的增加driver的数目，来增大并发，在性能基本不增加，时延在增加的时候，记录性能值和参数值，这个为本环境的最大性能</li>
<li>创建32个bucket，每个bucket写入5个2M的对象，测试写入时间为600s，读取时间为60s</li>
</ul>
<p>简单框架图<br><img src="http://static.zybuluo.com/zphj1987/6onskxt6rtoqqbq6zwba48bi/cosbench-1.png" alt="cosbench"></p>
<p>配置文件如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span><br><span class="line">&lt;workload name=<span class="string">"create-bucket"</span> description=<span class="string">"create s3 bucket"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">    &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">    &lt;workflow config=<span class="string">""</span>&gt;</span><br><span class="line">        &lt;workstage name=<span class="string">"create bucket"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-create"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);objects=r(0,0);sizes=c(0)B"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-create"</span> <span class="built_in">type</span>=<span class="string">"init"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32)"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"init"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);objects=r(0,0);sizes=c(0)B"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">		</span><br><span class="line">        &lt;workstage name=<span class="string">"putobject"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"600"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=u(1,16);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-put"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"600"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"write"</span> ratio=<span class="string">"100"</span> division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=u(17,32);objects=u(1,5);sizes=u(2,2)MB"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line"></span><br><span class="line">        &lt;workstage name=<span class="string">"getobj"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"none"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">                    config=<span class="string">"containers=u(1,16);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-get"</span> <span class="built_in">type</span>=<span class="string">"normal"</span> workers=<span class="string">"2"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"none"</span> runtime=<span class="string">"60"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"200000"</span> totalOps=<span class="string">"0"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"read"</span> ratio=<span class="string">"100"</span> division=<span class="string">"none"</span></span><br><span class="line">                    config=<span class="string">"containers=u(17,32);objects=u(1,5);"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line">		</span><br><span class="line">		&lt;workstage name=<span class="string">"cleanup"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);objects=r(1,100);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);objects=r(1,100);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-cleanup"</span> <span class="built_in">type</span>=<span class="string">"cleanup"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"object"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);objects=r(1,100);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"cleanup"</span> ratio=<span class="string">"100"</span> division=<span class="string">"object"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);objects=r(1,100);;deleteContainer=false;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">        &lt;/workstage&gt;</span><br><span class="line"></span><br><span class="line">		&lt;workstage name=<span class="string">"dispose"</span> closuredelay=<span class="string">"0"</span> config=<span class="string">""</span>&gt;</span><br><span class="line">            &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw1-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(1,16);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7481;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                    division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(1,16);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;</span><br><span class="line">            &lt;work name=<span class="string">"rgw2-dispose"</span> <span class="built_in">type</span>=<span class="string">"dispose"</span> workers=<span class="string">"1"</span> interval=<span class="string">"5"</span></span><br><span class="line">                division=<span class="string">"container"</span> runtime=<span class="string">"0"</span> rampup=<span class="string">"0"</span> rampdown=<span class="string">"0"</span></span><br><span class="line">                afr=<span class="string">"0"</span> totalOps=<span class="string">"1"</span> totalBytes=<span class="string">"0"</span> config=<span class="string">"containers=r(17,32);"</span>&gt;</span><br><span class="line">                &lt;auth <span class="built_in">type</span>=<span class="string">"none"</span> config=<span class="string">""</span>/&gt;</span><br><span class="line">                &lt;storage <span class="built_in">type</span>=<span class="string">"s3"</span> config=<span class="string">"accesskey=test1;secretkey=test1;endpoint=http://192.168.19.101:7482;path_style_access=true"</span>/&gt;</span><br><span class="line">                &lt;operation <span class="built_in">type</span>=<span class="string">"dispose"</span> ratio=<span class="string">"100"</span></span><br><span class="line">                    division=<span class="string">"container"</span></span><br><span class="line">                    config=<span class="string">"containers=r(17,32);;objects=r(0,0);sizes=c(0)B;;"</span> id=<span class="string">"none"</span>/&gt;</span><br><span class="line">            &lt;/work&gt;			</span><br><span class="line">        &lt;/workstage&gt;		</span><br><span class="line">		</span><br><span class="line">    &lt;/workflow&gt;</span><br><span class="line">&lt;/workload&gt;</span><br></pre></td></tr></table></figure></p>
<p>上面的测试是为了做测试模板，所以采用了比较小的对象数目和比较小的测试时间</p>
<p>可以根据自己的硬件环境或者客户的要求来设计测试模型，环境够大的时候，提供足够的rgw和足够的客户端才能测出比较大的性能值</p>
<p>测试的时候，尽量把写入和读取的测试分开，也就是分两次测试，避免第一次的写入没写足够对象，读取的时候读不到中断了，对于长达数小时的测试的时候，中断是很令人头疼的，分段可以减少这种中断后的继续测试的时间</p>
<p>写入的测试在允许的范围内，尽量写入多点对象，尽量避免复写，也能够在读取的时候尽量能够足够散列</p>
<p>测试时间能够长尽量长</p>
<h2 id="测试结果">测试结果</h2><p><img src="http://static.zybuluo.com/zphj1987/kp625z85jbve3bvw3aoqbzq3/image.png" alt="result"><br><img src="http://static.zybuluo.com/zphj1987/h92rb9bgrw1sq9fs5jyq883i/image.png" alt="graph"></p>
<p>可以通过线图来看指定测试项目的中间情况，一般是去关注是否出现比较大的抖动    ，相同性能下，抖动越小越好</p>
<h2 id="其他调优">其他调优</h2><p>在硬件环境一定的情况下，可以通过增加nginx负载均衡，或者lvs负载均衡来尝试增加性能值，这个不在本篇的讨论范围内</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-04-12</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://static.zybuluo.com/zphj1987/f1bbph3hxt9twexfcgmh3wtp/cosbenchnnnn.png" alt="cosbench.png-10.4kB"><br></center>

<h2 id="前言">前言</h2><p>cosbench的功能很强大，但是配置起来可能就有点不是太清楚怎么配置了，本篇将梳理一下这个测试的配置过程，以及一些测试注意项目，以免无法完成自己配置模型的情况</p>
<h2 id="安装">安装</h2><p>cosbench模式是一个控制端控制几个driver向后端rgw发起请求</p>
<p>下载最新版本</p>
<blockquote>
<p><a href="https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip">https://github.com/intel-cloud/cosbench/releases/download/v0.4.2.c4/0.4.2.c4.zip</a></p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的ISCSI GATEWAY]]></title>
    <link href="http://www.zphj1987.com/2018/04/11/ceph-ISCSI-GATEWAY/"/>
    <id>http://www.zphj1987.com/2018/04/11/ceph-ISCSI-GATEWAY/</id>
    <published>2018-04-11T06:57:55.000Z</published>
    <updated>2018-04-11T07:09:14.189Z</updated>
    <content type="html"><![CDATA[<center><br><br><img src="http://static.zybuluo.com/zphj1987/w0jdxwzox8vxnjagipexheoa/gateway.jpg" alt="gateway"><br><br></center>






<h2 id="前言">前言</h2><p>最开始接触这个是在L版本的监控平台里面看到的，有个iscsi网关，但是没看到有类似的介绍，然后通过接口查询到了一些资料，当时由于有比较多的东西需要新内核，新版本的支持，所以并没有配置出来，由于内核已经更新迭代了几个小版本了，经过测试验证可以跑起来了，这里只是把东西跑起来，性能相关的对比需要根据去做</p>
<a id="more"></a>
<h2 id="实践过程">实践过程</h2><h3 id="架构图">架构图</h3><p><img src="http://static.zybuluo.com/zphj1987/hnknjcp8kkiha844m4fyejm4/Ceph_iSCSI_HA_424879_1116_ECE-01.png" alt="Ceph_iSCSI_HA_424879_1116_ECE-01.png-79.4kB"></p>
<p>这个图是引用的红帽的架构图，可以理解为一个多路径的实现方式，那么这个跟之前的有什么不同</p>
<p>主要是有个新的tcmu-runner来处理LIO TCM后端存储的用户空间端的守护进程，这个是在内核之上多了一个用户态的驱动层，这样只需要根据tcmu的标准来对接接口就可以了，而不用去直接跟内核进行交互</p>
<h3 id="需要的软件">需要的软件</h3><p>Ceph Luminous 版本的集群或者更新的版本<br>RHEL/CentOS 7.5或者Linux kernel v4.16或者更新版本的内核<br>其他控制软件</p>
<blockquote>
<p>targetcli-2.1.fb47 or newer package<br> ython-rtslib-2.1.fb64 or newer package<br> cmu-runner-1.3.0 or newer package<br> eph-iscsi-config-2.4 or newer package<br> eph-iscsi-cli-2.5 or newer package</p>
</blockquote>
<p>以上为配置这个环境需要的软件，下面为我使用的版本的软件，统一打包放在一个下载路径<br>我安装的版本如下：</p>
<blockquote>
<p>kernel-4.16.0-0.rc5.git0.1<br>targetcli-fb-2.1.fb48<br>python-rtslib-2.1.67<br>tcmu-runner-1.3.0-rc4<br>ceph-iscsi-config-2.5<br>ceph-iscsi-cli-2.6</p>
</blockquote>
<p>下载链接：</p>
<blockquote>
<p>链接:<a href="https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg" target="_blank" rel="external">https://pan.baidu.com/s/12OwR5ZNtWFW13feLXy3Ezg</a> 密码:m09k</p>
</blockquote>
<p>如果环境之前有安装过其他版本，需要先卸载掉，并且需要提前部署好一个Luminous 最新版本的集群<br>官方建议调整的参数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_client_watch_timeout 15'</span></span><br><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_heartbeat_grace 20'</span></span><br><span class="line"><span class="comment"># ceph tell osd.* injectargs '--osd_heartbeat_interval 5'</span></span><br></pre></td></tr></table></figure></p>
<h3 id="配置过程">配置过程</h3><p>创建一个存储池<br>需要用到rbd存储池，用来存储iscsi的配置文件，提前创建好一个名字是rbd的存储池</p>
<p>创建iscsi-gateway配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">touch /etc/ceph/iscsi-gateway.cfg</span><br></pre></td></tr></table></figure></p>
<p>修改iscsi-gateway.cfg配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[config]</span><br><span class="line"><span class="comment"># Name of the Ceph storage cluster. A suitable Ceph configuration file allowing</span></span><br><span class="line"><span class="comment"># access to the Ceph storage cluster from the gateway node is required, if not</span></span><br><span class="line"><span class="comment"># colocated on an OSD node.</span></span><br><span class="line">cluster_name = ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place a copy of the ceph cluster's admin keyring in the gateway's /etc/ceph</span></span><br><span class="line"><span class="comment"># drectory and reference the filename here</span></span><br><span class="line">gateway_keyring = ceph.client.admin.keyring</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># API settings.</span></span><br><span class="line"><span class="comment"># The API supports a number of options that allow you to tailor it to your</span></span><br><span class="line"><span class="comment"># local environment. If you want to run the API under https, you will need to</span></span><br><span class="line"><span class="comment"># create cert/key files that are compatible for each iSCSI gateway node, that is</span></span><br><span class="line"><span class="comment"># not locked to a specific node. SSL cert and key files *must* be called</span></span><br><span class="line"><span class="comment"># 'iscsi-gateway.crt' and 'iscsi-gateway.key' and placed in the '/etc/ceph/' directory</span></span><br><span class="line"><span class="comment"># on *each* gateway node. With the SSL files in place, you can use 'api_secure = true'</span></span><br><span class="line"><span class="comment"># to switch to https mode.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To support the API, the bear minimum settings are:</span></span><br><span class="line">api_secure = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Additional API configuration options are as follows, defaults shown.</span></span><br><span class="line"><span class="comment"># api_user = admin</span></span><br><span class="line"><span class="comment"># api_password = admin</span></span><br><span class="line"><span class="comment"># api_port = 5001</span></span><br><span class="line"><span class="comment"># trusted_ip_list = 192.168.0.10,192.168.0.11</span></span><br></pre></td></tr></table></figure></p>
<p>最后一行的trusted_ip_list修改为用来配置网关的主机IP，我的环境为</p>
<blockquote>
<p>trusted_ip_list =192.168.219.128,192.168.219.129</p>
</blockquote>
<p>所有网关节点的这个配置文件的内容需要一致，修改好一台直接scp到每个网关节点上</p>
<p>启动API服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 install]<span class="comment"># systemctl daemon-reload</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl enable rbd-target-api</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl start rbd-target-api</span></span><br><span class="line">[root@lab101 install]<span class="comment"># systemctl status rbd-target-api</span></span><br><span class="line">● rbd-target-api.service - Ceph iscsi target configuration API</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rbd-target-api.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu <span class="number">2018</span>-<span class="number">03</span>-<span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> CST; <span class="number">18</span>min ago</span><br><span class="line"> Main PID: <span class="number">1493</span> (rbd-target-api)</span><br><span class="line">   CGroup: /system.slice/rbd-target-api.service</span><br><span class="line">           └─<span class="number">1493</span> /usr/bin/python /usr/bin/rbd-target-api</span><br><span class="line"></span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> lab101 systemd[<span class="number">1</span>]: Started Ceph iscsi target configuration API.</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">34</span> lab101 systemd[<span class="number">1</span>]: Starting Ceph iscsi target configuration API...</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]: Started the configuration object watcher</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]: Checking <span class="keyword">for</span> config object changes every <span class="number">1</span>s</span><br><span class="line">Mar <span class="number">15</span> <span class="number">09</span>:<span class="number">44</span>:<span class="number">58</span> lab101 rbd-target-api[<span class="number">1493</span>]:  * Running on http://<span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">5000</span>/</span><br></pre></td></tr></table></figure></p>
<p>配置iscsi<br>执行gwcli命令<br><img src="http://static.zybuluo.com/zphj1987/b74q5lumh96ui9uhknf2329i/image.png" alt="image.png-23kB"></p>
<p>默认是这样的</p>
<p>进入icsi-target创建一个target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/&gt; <span class="built_in">cd</span> iscsi-target </span><br><span class="line">/iscsi-target&gt; create iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建iSCSI网关。以下使用的IP是用于iSCSI数据传输的IP,它们可以与trusted_ip_list中列出的用于管理操作的IP相同，也可以不同，看有没有做多网卡分离<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target&gt; <span class="built_in">cd</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw/</span><br><span class="line">/iscsi-target...-gw:iscsi-igw&gt; <span class="built_in">cd</span> gateways </span><br><span class="line">/iscsi-target...-igw/gateways&gt; create lab101 <span class="number">192.168</span>.<span class="number">219.128</span> skipchecks=<span class="literal">true</span></span><br><span class="line">OS version/package checks have been bypassed</span><br><span class="line">Adding gateway, syncing <span class="number">0</span> disk(s) and <span class="number">0</span> client(s)</span><br><span class="line">  /iscsi-target...-igw/gateways&gt; create lab102 <span class="number">192.168</span>.<span class="number">219.129</span> skipchecks=<span class="literal">true</span></span><br><span class="line">OS version/package checks have been bypassed</span><br><span class="line">Adding gateway, sync<span class="string">'ing 0 disk(s) and 0 client(s)</span><br><span class="line">ok</span><br><span class="line">/iscsi-target...-igw/gateways&gt; ls</span><br><span class="line">o- gateways ............. [Up: 2/2, Portals: 2]</span><br><span class="line">  o- lab101 ............. [192.168.219.128 (UP)]</span><br><span class="line">  o- lab102 ............. [192.168.219.129 (UP)]</span></span><br></pre></td></tr></table></figure></p>
<p>创建一个rbd设备disk_1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...-igw/gateways&gt; <span class="built_in">cd</span> /disks </span><br><span class="line">/disks&gt; create pool=rbd image=disk_1 size=<span class="number">100</span>G</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建一个客户端名称iqn.1994-05.com.redhat:75c3d5efde0<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/disks&gt; <span class="built_in">cd</span> /iscsi-target/iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw/hosts </span><br><span class="line">/iscsi-target...csi-igw/hosts&gt; create iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>创建chap的用户名密码，由于用户名密码都有特殊要求，如果你不确定，就按我给的去设置，并且chap必须设置，否则服务端是禁止连接的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; auth chap=iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0/admin@a_12a-bb</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>chap的命名规则可以这样查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; <span class="built_in">help</span> auth</span><br><span class="line"></span><br><span class="line">SYNTAX</span><br><span class="line">======</span><br><span class="line">auth [chap] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DESCRIPTION</span><br><span class="line">===========</span><br><span class="line"></span><br><span class="line">Client authentication can be <span class="built_in">set</span> to use CHAP by supplying the</span><br><span class="line">a string of the form &lt;username&gt;/&lt;password&gt;</span><br><span class="line"></span><br><span class="line">e.g.</span><br><span class="line">auth chap=username/password | nochap</span><br><span class="line"></span><br><span class="line">username ... the username is <span class="number">8</span>-<span class="number">64</span> character string. Each character</span><br><span class="line">             may either be an alphanumeric or use one of the following</span><br><span class="line">             special characters .,:,-,@.</span><br><span class="line">             Consider using the hosts <span class="string">'shortname'</span> or the initiators IQN</span><br><span class="line">             value as the username</span><br><span class="line"></span><br><span class="line">password ... the password must be between <span class="number">12</span>-<span class="number">16</span> chars <span class="keyword">in</span> length</span><br><span class="line">             containing alphanumeric characters, plus the following</span><br><span class="line">             special characters @,_,-</span><br><span class="line"></span><br><span class="line">WARNING: Using unsupported special characters may result <span class="keyword">in</span> truncation,</span><br><span class="line">         resulting <span class="keyword">in</span> failed logins.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Specifying <span class="string">'nochap'</span> will remove chap authentication <span class="keyword">for</span> the client</span><br><span class="line">across all gateways.</span><br></pre></td></tr></table></figure></p>
<p>增加磁盘到客户端<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/iscsi-target...t:<span class="number">75</span>c3d5efde0&gt; disk add rbd.disk_1</span><br><span class="line">ok</span><br></pre></td></tr></table></figure></p>
<p>到这里就配置完成了，我们看下最终应该是怎么样的<br><img src="http://static.zybuluo.com/zphj1987/erpd7tv1gymtrdkny0h33afy/image.png" alt="image.png-38.5kB"></p>
<h2 id="windows客户端配置">windows客户端配置</h2><p>这个地方我配置的时候用的win10配置的时候出现了无法连接的情况，可能是windows10自身的认证要求跟服务端冲突了，这里用windows server 2016 进行连接测试</p>
<p>windows server开启下Multipath IO</p>
<p>修改windows iscsi客户端的名称<br><img src="http://static.zybuluo.com/zphj1987/fqt0s8h75jm5joffglsm7z4t/image.png" alt="image.png-47.5kB"><br>修改为上面创建的客户端名称</p>
<p>发现门户<br><img src="http://static.zybuluo.com/zphj1987/yp7pteay72mms7fbf7jf6z1u/image.png" alt="image.png-37.7kB"><br>点击发现门户，填写好服务端的IP后直接点确定，这里先不用高级里面的配置</p>
<p><img src="http://static.zybuluo.com/zphj1987/3r2crtvesqir0f2qzhqow8dq/image.png" alt="image.png-35.1kB"></p>
<p>这个时候目标里面已经有一个发现的目标了，显示状态是不活动的，准备点击连接</p>
<p><img src="http://static.zybuluo.com/zphj1987/2oliqvnxb9rogfwimy8ogvx6/image.png" alt="image.png-80.7kB"><br>点击高级，选择门户IP，填写chap登陆信息，然后chap名称就是上面设置的用户名称，因为跟客户端名称设置的一致，也就是客户端的名称，密码就是上面设置的admin@a_12a-bb</p>
<p><img src="http://static.zybuluo.com/zphj1987/138747o4l4cxch52sb5z2z5w/image.png" alt="image.png-21.9kB"></p>
<p>切换到卷和设备，点击自动配置<br><img src="http://static.zybuluo.com/zphj1987/qn4zzyofhh2546fv4nxczbs3/image.png" alt="image.png-47.4kB"></p>
<p>可以看到已经装载设备了</p>
<p>在服务管理器，文件存储服务，卷，磁盘里面查看设备<br><img src="http://static.zybuluo.com/zphj1987/hi5flzjcsbdhv0wglrrkenda/image.png" alt="image.png-92.8kB"></p>
<p>可以看到是配置的LIO-ORG TCMU设备，对设备进行格式化即可</p>
<p><img src="http://static.zybuluo.com/zphj1987/h7yj7k87fllhxuqfz5utc4mv/image.png" alt="image.png-42.6kB"></p>
<p>完成了连接了</p>
<h2 id="Linux的客户端连接">Linux的客户端连接</h2><p>Linux客户端选择建议就选择3.10默认内核，选择高版本的内核的时候在配置多路径的时候碰到内核崩溃的问题</p>
<p>安装连接软件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># yum install iscsi-initiator-utils</span></span><br><span class="line">[root@lab103 ~]<span class="comment"># yum install device-mapper-multipath</span></span><br></pre></td></tr></table></figure></p>
<p>配置多路径</p>
<p>开启服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># mpathconf --enable --with_multipathd y</span></span><br></pre></td></tr></table></figure></p>
<p>修改配置文件/etc/multipath.conf<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">devices &#123;</span><br><span class="line">        device &#123;</span><br><span class="line">                vendor                 <span class="string">"LIO-ORG"</span></span><br><span class="line">                hardware_handler       <span class="string">"1 alua"</span></span><br><span class="line">                path_grouping_policy   <span class="string">"failover"</span></span><br><span class="line">                path_selector          <span class="string">"queue-length 0"</span></span><br><span class="line">                failback               <span class="number">60</span></span><br><span class="line">                path_checker           tur</span><br><span class="line">                prio                   alua</span><br><span class="line">                prio_args              exclusive_pref_bit</span><br><span class="line">                fast_io_fail_tmo       <span class="number">25</span></span><br><span class="line">                no_path_retry          queue</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>重启多路径服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># systemctl reload multipathd</span></span><br></pre></td></tr></table></figure></p>
<p>配置chap的认证</p>
<p>修改配置客户端的名称为上面设置的名称<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># cat /etc/iscsi/initiatorname.iscsi </span></span><br><span class="line">InitiatorName=iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br></pre></td></tr></table></figure></p>
<p>修改认证的配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># cat /etc/iscsi/iscsid.conf |grep "node.session.auth.username\|node.session.auth.password\|node.session.auth.authmethod"</span></span><br><span class="line">node.session.auth.authmethod = CHAP</span><br><span class="line">node.session.auth.username = iqn.<span class="number">1994</span>-<span class="number">05</span>.com.redhat:<span class="number">75</span>c3d5efde0</span><br><span class="line">node.session.auth.password = admin@a_12a-bb</span><br></pre></td></tr></table></figure></p>
<p>查询iscsi target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># iscsiadm -m discovery -t st -p 192.168.219.128</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">219.128</span>:<span class="number">3260</span>,<span class="number">1</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line"><span class="number">192.168</span>.<span class="number">219.129</span>:<span class="number">3260</span>,<span class="number">2</span> iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw</span><br></pre></td></tr></table></figure></p>
<p>连接target<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab103 ~]<span class="comment"># iscsiadm -m node -T iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw -l</span></span><br><span class="line">Logging <span class="keyword">in</span> to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] (multiple)</span><br><span class="line">Logging <span class="keyword">in</span> to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] (multiple)</span><br><span class="line">Login to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] successful.</span><br><span class="line">Login to [iface: default, target: iqn.<span class="number">2003</span>-<span class="number">01</span>.com.redhat.iscsi-gw:iscsi-igw, portal: <span class="number">192.168</span>.<span class="number">219.129</span>,<span class="number">3260</span>] successful.</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># multipath -ll</span></span><br><span class="line">mpathb (<span class="number">360014052</span><span class="built_in">fc</span>39ba627874fdba9aefcf6c) dm-<span class="number">4</span> LIO-ORG ,TCMU device     </span><br><span class="line">size=<span class="number">100</span>G features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'1 alua'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'queue-length 0'</span> prio=<span class="number">10</span> status=active</span><br><span class="line">| `- <span class="number">5</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span> sdc <span class="number">8</span>:<span class="number">32</span> active ready running</span><br><span class="line">`-+- policy=<span class="string">'queue-length 0'</span> prio=<span class="number">10</span> status=enabled</span><br><span class="line">  `- <span class="number">6</span>:<span class="number">0</span>:<span class="number">0</span>:<span class="number">0</span> sdd <span class="number">8</span>:<span class="number">48</span> active ready running</span><br></pre></td></tr></table></figure>
<p>查看盘符<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># parted -s /dev/mapper/mpathb print</span></span><br><span class="line">Model: Linux device-mapper (multipath) (dm)</span><br><span class="line">Disk /dev/mapper/mpathb: <span class="number">107</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name                          Flags</span><br><span class="line"> <span class="number">1</span>      <span class="number">17.4</span>kB  <span class="number">134</span>MB  <span class="number">134</span>MB               Microsoft reserved partition  msftres</span><br><span class="line"> <span class="number">2</span>      <span class="number">135</span>MB   <span class="number">107</span>GB  <span class="number">107</span>GB  ntfs         Basic data partition</span><br></pre></td></tr></table></figure></p>
<p>直接使用这个/dev/mapper/mpathb设备即可</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-04-11</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><br><img src="http://static.zybuluo.com/zphj1987/w0jdxwzox8vxnjagipexheoa/gateway.jpg" alt="gateway"><br><br></center>






<h2 id="前言">前言</h2><p>最开始接触这个是在L版本的监控平台里面看到的，有个iscsi网关，但是没看到有类似的介绍，然后通过接口查询到了一些资料，当时由于有比较多的东西需要新内核，新版本的支持，所以并没有配置出来，由于内核已经更新迭代了几个小版本了，经过测试验证可以跑起来了，这里只是把东西跑起来，性能相关的对比需要根据去做</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[parted会启动你的ceph osd，意外不？]]></title>
    <link href="http://www.zphj1987.com/2018/03/23/parted-may-start-your-osd/"/>
    <id>http://www.zphj1987.com/2018/03/23/parted-may-start-your-osd/</id>
    <published>2018-03-23T15:54:55.000Z</published>
    <updated>2018-03-23T16:13:59.765Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yellowhelmet.png" alt="disk"><br></center>

<h2 id="前言">前言</h2><p>如果看到标题，你是不是第一眼觉得写错了，这个怎么可能，完全就是两个不相关的东西，最开始我也是这么想的，直到我发现真的是这样的时候，也是很意外，还是弄清楚下比较好，不然在某个操作下，也许就会出现意想不到的情况</p>
<a id="more"></a>
<h2 id="定位">定位</h2><p>如果你看过我的博客，正好看过这篇<a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/" target="_blank" rel="external">ceph在centos7下一个不容易发现的改变</a>，那么应该还记得这个讲的是centos 7 下面通过udev来实现了osd的自动挂载，这个自动挂载就是本篇需要了解的前提<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># systemctl stop ceph-osd@1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># umount /dev/sdf1 </span></span><br><span class="line">[root@lab101 ~]<span class="comment"># parted -l &amp;&gt;/dev/null</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># ps -ef|grep osd</span></span><br><span class="line">ceph      <span class="number">62701</span>      <span class="number">1</span>  <span class="number">1</span> <span class="number">23</span>:<span class="number">25</span> ?        <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> /usr/bin/ceph-osd <span class="operator">-f</span> --cluster ceph --id <span class="number">1</span> --setuser ceph --setgroup ceph</span><br><span class="line">root      <span class="number">62843</span>  <span class="number">35114</span>  <span class="number">0</span> <span class="number">23</span>:<span class="number">25</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> grep --color=auto osd</span><br></pre></td></tr></table></figure></p>
<p>看这个操作过程，是不是很神奇，是不是很意外，不管怎么说，parted -l的一个操作把我们的osd给自动mount 起来了，也自动给启动了</p>
<p>出现这个以后，我们先看下日志怎么出的，大概看起来的是这样的<br><img src="http://static.zybuluo.com/zphj1987/u7va8eisqwexkyx19zxjhemk/parted.gif" alt="parted.gif-4083.1kB"></p>
<p>可以看到确实是实时去触发的</p>
<p>服务器上面是有一个这个服务的 </p>
<blockquote>
<p>systemd-udevd.service<br>看到在做parted -l 后就会起一个这个子进程的<br><img src="http://static.zybuluo.com/zphj1987/xh8q4om2hyc7obw4xn09w8yx/image.png" alt="image.png-146.3kB"></p>
</blockquote>
<p>在尝试关闭这个服务后，再做parted -l操作就不会出现自动启动进程</p>
<h2 id="原因">原因</h2><p>执行parted -l 对指定设备发起parted命令的时候，就会对内核做一个trigger，而我们的</p>
<blockquote>
<p>/lib/udev/rules.d/95-ceph-osd.rules<br>这个文件一旦触发是会去调用<br>/usr/sbin/ceph-disk —log-stdout -v trigger /dev/$name</p>
</blockquote>
<p>也就是自动挂载加上启动osd的的操作了</p>
<h3 id="可能带来什么困扰">可能带来什么困扰</h3><p>其实这个我也不知道算不算bug，至少在正常使用的时候是没有问题的，以至于这个功能已经有了这么久，而我并没有察觉到，也没有感觉到它给我带来的干扰，那么作为一名测试人员，现在来构思一种可能出现的破坏场景，只要按照正常操作去做的，还会出现的，就是有可能发生的事情<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /var/lib/ceph/osd/</span><br><span class="line">[root@lab101 osd]<span class="comment"># df -h|grep osd</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">34</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># systemctl stop ceph-osd@1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># umount /dev/sdf1</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># parted -l  &amp;&gt;/dev/null</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># rm -rf ceph-1/</span></span><br><span class="line">rm: cannot remove ‘ceph-<span class="number">1</span>/’: Device or resource busy</span><br><span class="line">[root@lab101 osd]<span class="comment"># ll ceph-1/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">[root@lab101 osd]<span class="comment"># df -h|grep ceph</span></span><br><span class="line">/dev/sdf1                <span class="number">233</span>G   <span class="number">33</span>M  <span class="number">233</span>G   <span class="number">1</span>% /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到除了上面的parted -l以外，其他操作都是一个正常的操作，umount掉挂载点，然后清理掉这个目录，然后数据就被删了，当然正常情况下也许没人在正好那个点来了一个parted,但是不是完全没有可能</p>
<p>还有种情况就是我是要做维护，我想umount掉挂载点，不想进程起来，执行parted是很常规的操作了，结果自己给我拉起来了，这个操作应该比较常见的</p>
<h3 id="如何解决这个情况">如何解决这个情况</h3><p>第一种方法<br>什么都不动，你知道这个事情就行，执行过parted后再加上个df多检查下</p>
<p>第二种方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop systemd-udevd</span><br></pre></td></tr></table></figure></p>
<p>这个会带来其他什么影响，暂时不好判断，还没深入研究，影响应该也只会在硬件变动和一些udev触发的需求，不确定的情况可以不改，不推荐此方法</p>
<p>第三种方法<br>不用这个/lib/udev/rules.d/95-ceph-osd.rules做控制了，自己去写配置文件，或者写fstab，都可以，保证启动后能够自动mount，服务能够正常启动就可以了，个人从维护角度还是偏向于第三种方法，记录的信息越多，维护的时候越方便，这个是逼着记录了一些信息，虽然可以什么信息也不记</p>
<h2 id="总结">总结</h2><p>其实这个问题梳理清楚了也还好，最可怕的也许就是不知道为什么，特别是觉得完全不搭边的东西相互起了关联，至少在我们的研发跟我描述这个问题的时候，我想的是，还有这种神操作，是不是哪里加入了钩子程序什么的，花了点时间查到了原因，也方便在日后碰到不那么惊讶了</p>
<p>ceph北京大会已经顺利开完了，等PPT出来以后再学习一下新的东西，内容应该还是很多的，其实干货不干货，都在于你发现了什么，如果有一个PPT里面你提取到了一个知识点，你都是赚到了，何况分享的人并没有告知的义务的，所以每次看到有分享都是很感谢分享者的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-03-23</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/yellowhelmet.png" alt="disk"><br></center>

<h2 id="前言">前言</h2><p>如果看到标题，你是不是第一眼觉得写错了，这个怎么可能，完全就是两个不相关的东西，最开始我也是这么想的，直到我发现真的是这样的时候，也是很意外，还是弄清楚下比较好，不然在某个操作下，也许就会出现意想不到的情况</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[REDHAT 7.5beta 新推出的VDO功能]]></title>
    <link href="http://www.zphj1987.com/2018/02/10/REDHAT-7-5beta-with-VDO/"/>
    <id>http://www.zphj1987.com/2018/02/10/REDHAT-7-5beta-with-VDO/</id>
    <published>2018-02-10T08:25:08.000Z</published>
    <updated>2018-02-11T07:17:27.655Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/hat.jpg" alt="network"><br></center>

<h2 id="前言">前言</h2><h3 id="关于VDO">关于VDO</h3><p>VDO的技术来源于收购的Permabit公司，一个专门从事重删技术的公司，所以技术可靠性是没有问题的</p>
<p>VDO是一个内核模块，目的是通过重删减少磁盘的空间占用，以及减少复制带宽，VDO是基于块设备层之上的，也就是在原设备基础上映射出mapper虚拟设备，然后直接使用即可，功能的实现主要基于以下技术：</p>
<ul>
<li><p>零区块的排除：</p>
<p>在初始化阶段，整块为0的会被元数据记录下来，这个可以用水杯里面的水和沙子混合的例子来解释，使用滤纸（零块排除），把沙子（非零空间）给过滤出来，然后就是下一个阶段的处理</p>
</li>
</ul>
<ul>
<li><p>重复数据删除：</p>
<p>在第二阶段，输入的数据会判断是不是冗余数据（在写入之前就判断），这个部分的数据通过UDS内核模块来判断（U niversal D eduplication S ervice），被判断为重复数据的部分不会被写入，然后对元数据进行更新，直接指向原始已经存储的数据块即可</p>
</li>
<li><p>压缩：</p>
<p>一旦消零和重删完成，LZ4压缩会对每个单独的数据块进行处理，然后压缩好的数据块会以固定大小4KB的数据块存储在介质上，由于一个物理块可以包含很多的压缩块，这个也可以加速读取的性能</p>
</li>
</ul>
<p>上面的技术看起来很容易理解，但是实际做成产品还是相当大的难度的，技术设想和实际输出还是有很大距离，不然redhat也不会通过收购来获取技术，而不是自己去重新写一套了<br><a id="more"></a></p>
<h3 id="如何获取VDO">如何获取VDO</h3><p>主要有两种方式，一种是通过申请测试版的方式申请redhat 7.5的ISO，这个可以进行一个月的测试</p>
<p>另外一种方式是申请测试版本，然后通过源码在你正在使用的ISO上面进行相关的测试，从适配方面在自己的ISO上面进行测试能够更好的对比，由于基于redhat的源码做分发会涉及法律问题，这里就不做过多讲解，也不提供rpm包，自行申请测试即可</p>
<h2 id="实践过程">实践过程</h2><h3 id="安装VDO">安装VDO</h3><p>安装的操作系统为CentOS Linux release 7.4.1708<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># lsb_release -a</span></span><br><span class="line">LSB Version:	:core-<span class="number">4.1</span>-amd64:core-<span class="number">4.1</span>-noarch</span><br><span class="line">Distributor ID:	CentOS</span><br><span class="line">Description:	CentOS Linux release <span class="number">7.4</span>.<span class="number">1708</span> (Core) </span><br><span class="line">Release:	<span class="number">7.4</span>.<span class="number">1708</span></span><br><span class="line">Codename:	Core</span><br></pre></td></tr></table></figure></p>
<p>内核版本如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># uname -a</span></span><br><span class="line">Linux lab101 <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 <span class="comment">#1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -qa|grep kernel</span></span><br><span class="line">kernel-tools-libs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64</span><br><span class="line">abrt-addon-kerneloops-<span class="number">2.1</span>.<span class="number">11</span>-<span class="number">48</span>.el7.centos.x86_64</span><br><span class="line">kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64</span><br></pre></td></tr></table></figure>
<p>我们把内核升级一下，因为这个模块比较新，所以选择目前updates里面最新的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://mirror.centos.org/centos/<span class="number">7</span>/updates/x86_64/Packages/kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693.17</span>.<span class="number">1</span>.el7.x86_64.rpm</span><br></pre></td></tr></table></figure></p>
<p>大版本一致，小版本不同，直接安装即可<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh kernel-3.10.0-693.17.1.el7.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:kernel-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693.17</span>.<span class="number">1</span>.el7       <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># grub2-set-default 'CentOS Linux (3.10.0-693.17.1.el7.x86_64) 7 (Core)'</span></span><br></pre></td></tr></table></figure></p>
<p>重启服务器<br>安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh kmod-kvdo-6.1.0.98-11.el7.centos.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:kmod-kvdo-<span class="number">6.1</span>.<span class="number">0.98</span>-<span class="number">11</span>.el7.centos <span class="comment">################################# [100%]</span></span><br><span class="line">[root@lab101 ~]<span class="comment"># yum install PyYAML   </span></span><br><span class="line">[root@lab101 ~]<span class="comment"># rpm -ivh vdo-6.1.0.98-13.x86_64.rpm </span></span><br><span class="line">Preparing...                          <span class="comment">################################# [100%]</span></span><br><span class="line">Updating / installing...</span><br><span class="line">   <span class="number">1</span>:vdo-<span class="number">6.1</span>.<span class="number">0.98</span>-<span class="number">13</span>                  <span class="comment">################################# [100%]</span></span><br></pre></td></tr></table></figure></p>
<p>到这里安装就完成了</p>
<h3 id="配置VDO">配置VDO</h3><p>创建一个vdo卷<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo create --name=my_vdo  --device=/dev/sdb1   --vdoLogicalSize=80G --writePolicy=sync</span></span><br><span class="line">Creating VDO my_vdo</span><br><span class="line">Starting VDO my_vdo</span><br><span class="line">Starting compression on VDO my_vdo</span><br><span class="line">VDO instance <span class="number">0</span> volume is ready at /dev/mapper/my_vdo</span><br></pre></td></tr></table></figure></p>
<p>参数解释：<br>name是创建的vdo名称，也就是生成的新设备的名称，device是指定的设备，vdoLogicalSize是指定新生成的设备的大小，因为vdo是支持精简配置的，也就是你原来1T的物理空间，这里可以创建出超过1T的逻辑空间，因为内部支持重删，可以根据数据类型进行放大，writePolicy是指定写入的模式的</p>
<p>如果磁盘设备是write back模式的可以设置为aysnc，如果没有的话就设置为sync模式</p>
<p>如果磁盘没有写缓存或者有write throuth cache的时候设置为sync模式<br>如果磁盘有write back cache的时候就必须设置成async模式</p>
<p>默认是sync模式的，这里的同步异步实际上是告诉vdo，我们的底层存储是不是有写缓存，有缓存的话就要告诉vdo我们底层是async的，没有缓存的时候就是sync</p>
<p>检查我们的磁盘的写入方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># cat /sys/block/sdb/device/scsi_disk/0\:0\:1\:0/cache_type </span></span><br><span class="line">write through</span><br></pre></td></tr></table></figure></p>
<p>这个输出的根据上面的规则，我们设置为sync模式</p>
<p>修改缓存模式的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vdo changeWritePolicy --writePolicy=sync_or_async --name=vdo_name</span><br></pre></td></tr></table></figure></p>
<p>格式化硬盘<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># mkfs.xfs -K /dev/mapper/my_vdo </span></span><br><span class="line">meta-data=/dev/mapper/my_vdo     isize=<span class="number">512</span>    agcount=<span class="number">4</span>, agsize=<span class="number">5242880</span> blks</span><br><span class="line">         =                       sectsz=<span class="number">4096</span>  attr=<span class="number">2</span>, projid32bit=<span class="number">1</span></span><br><span class="line">         =                       crc=<span class="number">1</span>        finobt=<span class="number">0</span>, sparse=<span class="number">0</span></span><br><span class="line">data     =                       bsize=<span class="number">4096</span>   blocks=<span class="number">20971520</span>, imaxpct=<span class="number">25</span></span><br><span class="line">         =                       sunit=<span class="number">0</span>      swidth=<span class="number">0</span> blks</span><br><span class="line">naming   =version <span class="number">2</span>              bsize=<span class="number">4096</span>   ascii-ci=<span class="number">0</span> ftype=<span class="number">1</span></span><br><span class="line"><span class="built_in">log</span>      =internal <span class="built_in">log</span>           bsize=<span class="number">4096</span>   blocks=<span class="number">10240</span>, version=<span class="number">2</span></span><br><span class="line">         =                       sectsz=<span class="number">4096</span>  sunit=<span class="number">1</span> blks, lazy-count=<span class="number">1</span></span><br><span class="line">realtime =none                   extsz=<span class="number">4096</span>   blocks=<span class="number">0</span>, rtextents=<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>使用-K参数是加速了格式化的操作，也就是不发送丢弃的请求，因为之前创建了vdo，已经将其初始化为0了，所以可以采用这个操作</p>
<p>我们挂载的时候最好能加上discard的选项，精简配置的设备需要对之前的空间进行回收，一般来说有在线的和离线的回收，离线的就通过fstrim来进行回收即可</p>
<p>挂载设备<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># mount -o discard /dev/mapper/my_vdo /myvod/</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdostats --human-readable </span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.0</span>G     <span class="number">46.0</span>G   <span class="number">8</span>%           <span class="number">99</span>%</span><br></pre></td></tr></table></figure>
<p>默认创建完vdo设备就会占用4G左右的空间，这个用来存储UDS和VDO的元数据</p>
<p>检查重删和压缩是否开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo status -n my_vdo|grep Deduplication</span></span><br><span class="line">    Deduplication: enabled</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdo status -n my_vdo|grep Compress</span></span><br><span class="line">    Compression: enabled</span><br></pre></td></tr></table></figure></p>
<p>如果没有开启，可以通过下面的命令开启<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vdo <span class="built_in">enable</span>Compression -n &lt;vdo_vol_name&gt;</span><br><span class="line">vdo <span class="built_in">enable</span>Deduplication -n &lt;vdo_vol_name&gt;</span><br></pre></td></tr></table></figure></p>
<h3 id="验证重删功能">验证重删功能</h3><figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">[root@lab<span class="number">101</span> ~]<span class="title"># df -h|grep vdo</span><br><span class="line">/dev/mapper/my_vdo   80</span>G   <span class="number">33</span>M   <span class="number">80</span>G   <span class="number">1</span><span class="preprocessor">%</span> /myvod</span><br><span class="line">[root@lab<span class="number">101</span> ~]<span class="title"># vdostats --hu</span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       50</span><span class="number">.0</span>G      <span class="number">4.0</span>G     <span class="number">46.0</span>G   <span class="number">8</span><span class="preprocessor">%</span>           <span class="number">99</span><span class="preprocessor">%</span></span><br></pre></td></tr></table></figure>
<p>传入一个ISO文件CentOS-7-x86_64-NetInstall-1708.iso 422M的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep vdo</span></span><br><span class="line">/dev/mapper/my_vdo   <span class="number">80</span>G  <span class="number">455</span>M   <span class="number">80</span>G   <span class="number">1</span>% /myvod</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdostats --hu</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.4</span>G     <span class="number">45.6</span>G   <span class="number">8</span>%            <span class="number">9</span>%</span><br></pre></td></tr></table></figure></p>
<p>然后重复传入3个相同文件，一共四个文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># df -h|grep vdo</span></span><br><span class="line">/dev/mapper/my_vdo   <span class="number">80</span>G  <span class="number">1.7</span>G   <span class="number">79</span>G   <span class="number">3</span>% /myvod</span><br><span class="line">[root@lab101 ~]<span class="comment"># vdostats --hu</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/my_vdo       <span class="number">50.0</span>G      <span class="number">4.4</span>G     <span class="number">45.6</span>G   <span class="number">8</span>%           <span class="number">73</span>%</span><br></pre></td></tr></table></figure></p>
<p>可以看到后面传入的文件，并没有占用底层存储的实际空间</p>
<h3 id="验证压缩功能">验证压缩功能</h3><p>测试数据来源 silesia的资料库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://sun.aei.polsl.pl/~sdeor/corpus/silesia.zip</span><br></pre></td></tr></table></figure>
<p>通过资料库里面的文件来看下对不同类型的数据的压缩情况</p>
<table>
<thead>
<tr>
<th style="text-align:center">Filename</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">类型</th>
<th style="text-align:center">原始空间（KB）</th>
<th style="text-align:center">实际占用空间（KB）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">dickens</td>
<td style="text-align:center">狄更斯文集</td>
<td style="text-align:center">英文原文</td>
<td style="text-align:center">9953</td>
<td style="text-align:center">9948</td>
</tr>
<tr>
<td style="text-align:center">mozilla</td>
<td style="text-align:center">Mozilla的1.0可执行文件</td>
<td style="text-align:center">执行程序</td>
<td style="text-align:center">50020</td>
<td style="text-align:center">33228</td>
</tr>
<tr>
<td style="text-align:center">mr</td>
<td style="text-align:center">医用resonanse图像</td>
<td style="text-align:center">图片</td>
<td style="text-align:center">9736</td>
<td style="text-align:center">9272</td>
</tr>
<tr>
<td style="text-align:center">nci</td>
<td style="text-align:center">结构化的化学数据库</td>
<td style="text-align:center">数据库</td>
<td style="text-align:center">32767</td>
<td style="text-align:center">10168</td>
</tr>
<tr>
<td style="text-align:center">ooffice</td>
<td style="text-align:center">Open Office.org 1.01 DLL</td>
<td style="text-align:center">可执行程序</td>
<td style="text-align:center">6008</td>
<td style="text-align:center">5640</td>
</tr>
<tr>
<td style="text-align:center">osdb</td>
<td style="text-align:center">基准测试用的MySQL格式示例数据库</td>
<td style="text-align:center">数据库</td>
<td style="text-align:center">9849</td>
<td style="text-align:center">9824</td>
</tr>
<tr>
<td style="text-align:center">reymont</td>
<td style="text-align:center">瓦迪斯瓦夫·雷蒙特的书</td>
<td style="text-align:center">PDF</td>
<td style="text-align:center">6471</td>
<td style="text-align:center">6312</td>
</tr>
<tr>
<td style="text-align:center">samba</td>
<td style="text-align:center">samba源代码</td>
<td style="text-align:center">src源码</td>
<td style="text-align:center">21100</td>
<td style="text-align:center">11768</td>
</tr>
<tr>
<td style="text-align:center">sao</td>
<td style="text-align:center">星空数据</td>
<td style="text-align:center">天文格式的bin文件</td>
<td style="text-align:center">7081</td>
<td style="text-align:center">7036</td>
</tr>
<tr>
<td style="text-align:center">webster</td>
<td style="text-align:center">辞海</td>
<td style="text-align:center">HTML</td>
<td style="text-align:center">40487</td>
<td style="text-align:center">40144</td>
</tr>
<tr>
<td style="text-align:center">xml</td>
<td style="text-align:center">XML文件</td>
<td style="text-align:center">HTML</td>
<td style="text-align:center">5220</td>
<td style="text-align:center">2180</td>
</tr>
<tr>
<td style="text-align:center">x-ray</td>
<td style="text-align:center">透视医学图片</td>
<td style="text-align:center">医院数据</td>
<td style="text-align:center">8275</td>
<td style="text-align:center">8260</td>
</tr>
</tbody>
</table>
<p>可以看到都有不同程度的压缩，某些类型的数据压缩能达到50%的比例</p>
<p>停止vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo stop  -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<p>启动vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo start  -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<p>删除vdo操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># vdo remove -n my_vdo</span></span><br></pre></td></tr></table></figure></p>
<h2 id="VDO和CEPH能产生什么火花？">VDO和CEPH能产生什么火花？</h2><p>在ceph里面可以用到vdo的地方有两个，一个是作为Kernel rbd的前端，在块设备的上层，另外一个是作为OSD的底层，也就是把VDO当OSD来使用，我们看下怎么使用</p>
<h3 id="作为rbd的上层">作为rbd的上层</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># rbd create testvdorbd --size 20G</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># rbd map testvdorbd</span></span><br></pre></td></tr></table></figure>
<p>创建rbd的vdo<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd</span></span><br><span class="line">Creating VDO rbd_vdo</span><br><span class="line">vdo: ERROR -   Device /dev/rbd/rbd/testvdorbd not found (or ignored by filtering).</span><br></pre></td></tr></table></figure></p>
<p>被默认排除掉了，这个以前正好见过类似的问题，比较好处理</p>
<p>这个地方因为vdo添加存储的时候内部调用了lvm相关的配置，然后lvm默认会排除掉rbd，这里修改下lvm的配置文件即可<br>在/etc/lvm/lvm.conf的修改如下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">types = [ <span class="string">"fd"</span>, <span class="number">16</span> ,<span class="string">"rbd"</span>, <span class="number">64</span> ]</span><br></pre></td></tr></table></figure></p>
<p>把types里面增加下rbd 的文件类型即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name=rbd_vdo  --device=/dev/rbd/rbd/testvdorbd</span></span><br><span class="line">Creating VDO rbd_vdo</span><br><span class="line">Starting VDO rbd_vdo</span><br><span class="line">Starting compression on VDO rbd_vdo</span><br><span class="line">VDO instance <span class="number">2</span> volume is ready at /dev/mapper/rbd_vdo</span><br></pre></td></tr></table></figure>
<p>挂载<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -o discard /dev/mapper/rbd_vdo /mnt</span><br></pre></td></tr></table></figure></p>
<p>查看容量<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># vdostats --human-readable</span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/rbd_vdo      <span class="number">20.0</span>G      <span class="number">4.4</span>G     <span class="number">15.6</span>G  <span class="number">22</span>%            <span class="number">3</span>%</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">57316</span>M     <span class="number">49409</span>M        <span class="number">7906</span>M         <span class="number">13.79</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd      <span class="number">0</span>      <span class="number">566</span>M      <span class="number">1.20</span>        <span class="number">46543</span>M         <span class="number">148</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 mnt]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE       AVAIL      RAW USED     %RAW USED </span><br><span class="line">    <span class="number">57316</span>M     <span class="number">48699</span>M        <span class="number">8616</span>M         <span class="number">15.03</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME     ID     USED      %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd      <span class="number">0</span>      <span class="number">1393</span>M      <span class="number">2.95</span>        <span class="number">45833</span>M         <span class="number">355</span></span><br></pre></td></tr></table></figure>
<p>多次传入相同的时候可以看到对于ceph内部来说还是会产生对象的，只是这个在vdo的文件系统来看是不占用物理空间的</p>
<p>对镜像做下copy<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># rbd cp testvdorbd testvdorbdclone</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment">#rbd map  testvdorbdclone</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ~]<span class="comment"># cat /etc/vdoconf.yml |grep device</span></span><br><span class="line">      device: /dev/rbd/rbd/testvdorbdclone</span><br></pre></td></tr></table></figure>
<p>修改配置文件为对应的设备，就可以启动了,这个操作说明vdo设备是不绑定硬件的，只需要有相关的配置文件，即可对文件系统进行启动</p>
<p>那么这个在一个数据转移用途下，就可以利用vdo对数据进行重删压缩，然后把整个img转移到远端去，这个也符合现在的私有云和公有云之间的数据传输量的问题，会节省不少空间</p>
<h3 id="vdo作为ceph的osd">vdo作为ceph的osd</h3><p>ceph对设备的属性有要求，这里直接采用目录部署的方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name sdb1 --device=/dev/sdb1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># vdo create --name sdb2 --device=/dev/sdb2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkfs.xfs -K -f /dev/mapper/sdb1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkfs.xfs -K -f /dev/mapper/sdb2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkdir /osd1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mkdir /osd2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mount /dev/mapper/sdb1 /osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># mount /dev/mapper/sdb2 /osd2/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># chown ceph:ceph /osd1</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># chown ceph:ceph /osd2</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd prepare lab101:/osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd prepare lab101:/osd2/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd activate lab101:/osd1/</span></span><br><span class="line">[root@lab101 ceph]<span class="comment"># ceph-deploy osd activate lab101:/osd2/</span></span><br></pre></td></tr></table></figure></p>
<p>写入测试数据<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># rados  -p rbd bench 60 write --no-cleanup</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab101 ceph]<span class="comment"># df -h</span></span><br><span class="line">Filesystem        Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda2          <span class="number">56</span>G  <span class="number">2.0</span>G   <span class="number">54</span>G   <span class="number">4</span>% /</span><br><span class="line">devtmpfs          <span class="number">983</span>M     <span class="number">0</span>  <span class="number">983</span>M   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs             <span class="number">992</span>M     <span class="number">0</span>  <span class="number">992</span>M   <span class="number">0</span>% /dev/shm</span><br><span class="line">tmpfs             <span class="number">992</span>M  <span class="number">8.8</span>M  <span class="number">983</span>M   <span class="number">1</span>% /run</span><br><span class="line">tmpfs             <span class="number">992</span>M     <span class="number">0</span>  <span class="number">992</span>M   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1        <span class="number">1014</span>M  <span class="number">151</span>M  <span class="number">864</span>M  <span class="number">15</span>% /boot</span><br><span class="line">tmpfs             <span class="number">199</span>M     <span class="number">0</span>  <span class="number">199</span>M   <span class="number">0</span>% /run/user/<span class="number">0</span></span><br><span class="line">/dev/mapper/sdb1   <span class="number">22</span>G  <span class="number">6.5</span>G   <span class="number">16</span>G  <span class="number">30</span>% /osd1</span><br><span class="line">/dev/mapper/sdb2   <span class="number">22</span>G  <span class="number">6.5</span>G   <span class="number">16</span>G  <span class="number">30</span>% /osd2</span><br><span class="line">[root@lab101 ceph]<span class="comment"># vdostats --human-readable </span></span><br><span class="line">Device                    Size      Used Available Use% Space saving%</span><br><span class="line">/dev/mapper/sdb2         <span class="number">25.0</span>G      <span class="number">3.0</span>G     <span class="number">22.0</span>G  <span class="number">12</span>%           <span class="number">99</span>%</span><br><span class="line">/dev/mapper/sdb1         <span class="number">25.0</span>G      <span class="number">3.0</span>G     <span class="number">22.0</span>G  <span class="number">12</span>%           <span class="number">99</span>%</span><br></pre></td></tr></table></figure>
<p>可以看到虽然在df看到了空间的占用，实际上因为rados bench写入的是填充的空洞数据，vdo作为osd对数据直接进行了重删了，测试可以看到vdo是可以作为ceph osd的，由于我的测试环境是在vmware虚拟机里面的，所以并不能做性能测试，有硬件环境的情况下可以对比下开启vdo和不开启的情况的性能区别</p>
<h2 id="参考文档">参考文档</h2><p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-qs-creating-a-volume" target="_blank" rel="external">vdo-qs-creating-a-volume</a><br><a href="https://rhelblog.redhat.com/tag/vdo/" target="_blank" rel="external">Determining the space savings of virtual data optimizer (VDO) in RHEL 7.5 Beta</a></p>
<h2 id="总结">总结</h2><p>本篇从配置和部署以及适配方面对vdo进行一次比较完整的实践，从目前的测试情况来看，配置简单，对环境友好，基本是可以作为一个驱动层嵌入到任何块设备之上的，未来应该有广泛的用途，目前还不清楚红帽是否会把这个属性放到centos下面去，目前可以通过在<a href="https://access.redhat.com/downloads/" target="_blank" rel="external">https://access.redhat.com/downloads/</a> 申请测试版本的ISO进行功能的测试</p>
<p>应该是农历年前的最后一篇文章了，祝新春快乐！</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-02-10</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/hat.jpg" alt="network"><br></center>

<h2 id="前言">前言</h2><h3 id="关于VDO">关于VDO</h3><p>VDO的技术来源于收购的Permabit公司，一个专门从事重删技术的公司，所以技术可靠性是没有问题的</p>
<p>VDO是一个内核模块，目的是通过重删减少磁盘的空间占用，以及减少复制带宽，VDO是基于块设备层之上的，也就是在原设备基础上映射出mapper虚拟设备，然后直接使用即可，功能的实现主要基于以下技术：</p>
<ul>
<li><p>零区块的排除：</p>
<p>在初始化阶段，整块为0的会被元数据记录下来，这个可以用水杯里面的水和沙子混合的例子来解释，使用滤纸（零块排除），把沙子（非零空间）给过滤出来，然后就是下一个阶段的处理</p>
</li>
</ul>
<ul>
<li><p>重复数据删除：</p>
<p>在第二阶段，输入的数据会判断是不是冗余数据（在写入之前就判断），这个部分的数据通过UDS内核模块来判断（U niversal D eduplication S ervice），被判断为重复数据的部分不会被写入，然后对元数据进行更新，直接指向原始已经存储的数据块即可</p>
</li>
<li><p>压缩：</p>
<p>一旦消零和重删完成，LZ4压缩会对每个单独的数据块进行处理，然后压缩好的数据块会以固定大小4KB的数据块存储在介质上，由于一个物理块可以包含很多的压缩块，这个也可以加速读取的性能</p>
</li>
</ul>
<p>上面的技术看起来很容易理解，但是实际做成产品还是相当大的难度的，技术设想和实际输出还是有很大距离，不然redhat也不会通过收购来获取技术，而不是自己去重新写一套了<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[定位一个网络问题引起的ceph异常]]></title>
    <link href="http://www.zphj1987.com/2018/01/16/catch-a-problem-with-network-in-ceph/"/>
    <id>http://www.zphj1987.com/2018/01/16/catch-a-problem-with-network-in-ceph/</id>
    <published>2018-01-16T15:10:59.000Z</published>
    <updated>2018-01-16T15:45:03.002Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ne.png" alt="network"><br></center>


<h2 id="前言">前言</h2><p>有一个ceph环境出现了异常，状态就是恢复异常的慢，但是所有数据又都在走，只是非常的慢，本篇将记录探测出问题的过程，以便以后处理类似的问题有个思路<br><a id="more"></a></p>
<h2 id="处理过程">处理过程</h2><p>问题的现象是恢复的很慢，但是除此以外并没有其它的异常，通过iostat监控磁盘，也没有出现异常的100%的情况，暂时排除了是osd底层慢的问题</p>
<h3 id="检测整体写入的速度">检测整体写入的速度</h3><p>通过rados bench写入<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p rbd bench <span class="number">5</span> write</span><br></pre></td></tr></table></figure></p>
<p>刚开始写入的时候没问题，但是写入了以后不久就会出现一只是0的情况，可以判断在写入某些对象的时候出现了异常</p>
<p>本地生成一些文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">seq <span class="number">0</span> <span class="number">30</span>|xargs -i dd <span class="keyword">if</span>=/dev/zero of=benchmarkzp&#123;&#125; bs=<span class="number">4</span>M count=<span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<p>通过rados put 命令把对象put进去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `ls ./`;<span class="keyword">do</span> time rados -p rbd put <span class="variable">$a</span> <span class="variable">$a</span>;<span class="built_in">echo</span> <span class="variable">$a</span>;ceph osd map rbd <span class="variable">$a</span>;<span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<p>得到的结果里面会有部分是好的，部分是非常长的时间，对结果进行过滤，分为bad 和good</p>
<p>开始怀疑会不会是固定的盘符出了问题，首先把磁盘组合分出来，完全没问题的磁盘全部排除，结果最后都排除完了，所以磁盘本省是没问题的</p>
<h3 id="根据pg的osd组合进行主机分类">根据pg的osd组合进行主机分类</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>  <span class="number">2</span>  <span class="number">4</span>  ok</span><br><span class="line"><span class="number">3</span>  <span class="number">1</span>   <span class="number">2</span>  bad</span><br><span class="line"><span class="number">2</span>  <span class="number">4</span>   <span class="number">1</span> ok</span><br><span class="line"><span class="number">3</span>  <span class="number">1</span> <span class="number">2</span>   bad</span><br><span class="line"><span class="number">3</span>  <span class="number">4</span>  <span class="number">2</span>  bad</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p>上面的编号是写入对象所在的pg对应的osd所在的主机，严格按照顺序写入，第一个主机为发送数据方，第二个和第三个为接收数据方，并且使用了cluster network</p>
<p>通过上面的结果发现了从3往2进行发送副本数据的时候出现了问题，然后去主机上排查网络</p>
<p>在主机2上面做iperf -s<br>在主机3上面做iperf -c host2然后就发现了网络异常了</p>
<p>最终还是定位在了网络上面</p>
<p>已经在好几个环境上面发现没装可以监控实时网络流量dstat工具或者ifstat的动态监控，做操作的时候监控下网络，可以发现一些异常</p>
<h2 id="总结">总结</h2><p>这个环境在最开始的时候就怀疑是网络可能有问题，但是没有去进行全部服务器的网络的检测，这个在出现一些奇奇怪怪的异常的时候，还是可能出现在网络上面，特别是这种坏掉又不是完全坏掉，只是掉速的情况，通过集群的一些内部告警还没法完全体现出来，而主机很多的时候，又没有多少人愿意一个个的去检测，就容易出现这种疏漏了</p>
<p>在做一个ceph的管理平台的时候，对整个集群做全员对等网络带宽测试还是很有必要的，如果有一天我来设计管理平台，一定会加入这个功能进去</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-16</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ne.png" alt="network"><br></center>


<h2 id="前言">前言</h2><p>有一个ceph环境出现了异常，状态就是恢复异常的慢，但是所有数据又都在走，只是非常的慢，本篇将记录探测出问题的过程，以便以后处理类似的问题有个思路<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CTDB使用rados object作为lock file]]></title>
    <link href="http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/"/>
    <id>http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/</id>
    <published>2018-01-06T15:29:59.000Z</published>
    <updated>2018-01-06T16:06:18.867Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/object.jpg" alt="object"><br></center></p>
<h2 id="前言">前言</h2><p>服务器的服务做HA有很多种方式，其中有一种就是是用CTDB，之前这个是独立的软件来做HA的，现在已经跟着SAMBA主线里面了，也就是跟着samba发行包一起发行</p>
<p>之前CTDB的模式是需要有一个共享文件系统，并且在这个共享文件系统里面所有的节点都去访问同一个文件，会有一个Master会获得这个文件的锁</p>
<p>在cephfs的使用场景中可以用cephfs的目录作为这个锁文件的路径，这个有个问题就是一旦有一个节点down掉的时候，可能客户端也会卡住目录，这个目录访问会被卡住，文件锁在其他机器无法获取到，需要等到这个锁超时以后，其它节点才能获得到锁，这个切换的周期就会长一点了</p>
<p>CTDB在最近的版本当中加入了cluster mutex helper using Ceph RADOS的支持，本篇将介绍这个方式锁文件配置方式<br><a id="more"></a></p>
<h2 id="实践过程">实践过程</h2><h3 id="安装CTDB">安装CTDB</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># yum install samba ctdb</span></span><br></pre></td></tr></table></figure>
<p>检查默认包里面是否有rados的支持<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -qpl ctdb-4.6.2-12.el7_4.x86_64.rpm</span></span><br><span class="line">…</span><br><span class="line">usr/libexec/ctdb</span><br><span class="line">/usr/libexec/ctdb/ctdb_event</span><br><span class="line">/usr/libexec/ctdb/ctdb_eventd</span><br><span class="line">/usr/libexec/ctdb/ctdb_killtcp</span><br><span class="line">/usr/libexec/ctdb/ctdb_lock_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_lvs</span><br><span class="line">/usr/libexec/ctdb/ctdb_mutex_fcntl_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_natgw</span><br><span class="line">/usr/libexec/ctdb/ctdb_recovery_helper</span><br><span class="line">/usr/libexec/ctdb/ctdb_takeover_helper</span><br><span class="line">/usr/libexec/ctdb/smnotify</span><br><span class="line">…</span><br></pre></td></tr></table></figure></p>
<p>这个可以看到默认并没有包含这个rados的支持，这个很多通用软件都会这么处理，因为支持第三方插件的时候需要开发库，而开发库又有版本的区别，所以默认并不支持，需要支持就自己编译即可，例如fio支持librbd的接口就是这么处理的，等到插件也通用起来的时候，可能就会默认支持了</p>
<p>很多软件的编译可以采取源码的编译方式，如果不是有很强的代码合入和patch跟踪能力，直接用发行包的方式是最稳妥的，所以为了不破坏这个稳定性，本篇采用的是基于发行版本，增加模块的方式，这样不会破坏核心组件的稳定性，并且后续升级也是比较简单的，这个也是个人推荐的方式</p>
<p>查询当前使用的samba版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -qa|grep samba</span></span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.x86_64</span><br></pre></td></tr></table></figure></p>
<h3 id="打包新的CTDB">打包新的CTDB</h3><p>可以查询得到这个的源码包为samba-4.6.2-12.el7_4.src.rpm,进一步搜索可以查询的到这个src源码rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://vault.centos.org/<span class="number">7.4</span>.<span class="number">1708</span>/updates/Source/SPackages/samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>下载这个rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># wget http://vault.centos.org/7.4.1708/updates/Source/SPackages/samba-4.6.2-12.el7_4.src.rpm</span></span><br></pre></td></tr></table></figure></p>
<p>如果下载比较慢的话就用迅雷下载，会快很多，国内的源里面把源码包的rpm都删除掉了，上面的是官网会有最全的包</p>
<p>解压这个rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm2cpio samba-4.6.2-12.el7_4.src.rpm |cpio -div</span></span><br></pre></td></tr></table></figure></p>
<p>检查包的内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># ls</span></span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12150</span>.patch                                 samba-v4-<span class="number">6</span>-fix-cross-realm-refferals.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12151</span>.patch                                 samba-v4-<span class="number">6</span>-fix-kerberos-debug-message.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">12163</span>.patch                                 samba-v4-<span class="number">6</span>-fix_net_ads_changetrustpw.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">14746</span>.patch                                 samba-v4-<span class="number">6</span>-fix-net-ads-keytab-handling.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">15275</span>.patch                                 samba-v4-<span class="number">6</span>-fix_path_substitutions.patch</span><br><span class="line">CVE-<span class="number">2017</span>-<span class="number">7494</span>.patch                                  samba-v4-<span class="number">6</span>-fix_smbclient_session_setup_info.patch</span><br><span class="line">gpgkey-<span class="number">52</span>FBC0B86D954B0843324CDC6F33915B6568B7EA.gpg  samba-v4-<span class="number">6</span>-fix_smbclient_username_parsing.patch</span><br><span class="line">pam_winbind.conf                                     samba-v4.<span class="number">6</span>-fix_smbpasswd_user_<span class="built_in">pwd</span>_change.patch</span><br><span class="line">README.dc                                            samba-v4-<span class="number">6</span>-fix-spoolss-<span class="number">32</span>bit-driver-upload.patch</span><br><span class="line">README.downgrade                                     samba-v4-<span class="number">6</span>-fix-vfs-expand-msdfs.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>-<span class="number">12</span>.el7_4.src.rpm                         samba-v4-<span class="number">6</span>-fix_winbind_child_crash.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>.tar.asc                                  samba-v4-<span class="number">6</span>-fix_winbind_normalize_names.patch</span><br><span class="line">samba-<span class="number">4.6</span>.<span class="number">2</span>.tar.xz                                   samba-v4.<span class="number">6</span>-graceful_fsctl_validate_negotiate_info.patch</span><br><span class="line">samba.log                                            samba-v4.<span class="number">6</span>-gss_krb5_import_cred.patch</span><br><span class="line">samba.pamd                                           samba-v4.<span class="number">6</span>-lib-crypto-implement-samba.crypto-Python-module-for-.patch</span><br><span class="line">samba.spec                                           samba-v4.<span class="number">7</span>-config-dynamic-rpc-port-range.patch</span><br><span class="line">samba-v4.<span class="number">6</span>-credentials-fix-realm.patch               smb.conf.example</span><br><span class="line">samba-v4-<span class="number">6</span>-fix-building-with-new-glibc.patch         smb.conf.vendor</span><br></pre></td></tr></table></figure></p>
<p>可以看到在源码包基础上还打入了很多的patch，内部的编译采用的是waf编译的方式，内部的过程就不做太多介绍了，这里只去改动我们需要的部分即可，也就是去修改samba.spec文件</p>
<p>我们先获取相关的编译选项，这个我最开始的时候打算独立编译ctdb的rpm包，发现有依赖关系太多，后来多次验证后，发现直接可以在samba编译里面增加选项的，选项获取方式<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab211 samba-<span class="number">4.6</span>.<span class="number">2</span>]<span class="comment"># ./configure --help|grep ceph</span></span><br><span class="line">  --with-libcephfs=LIBCEPHFS_DIR</span><br><span class="line">            Directory under <span class="built_in">which</span> libcephfs is installed</span><br><span class="line">  --enable-cephfs</span><br><span class="line">            Build with cephfs support (default=yes)</span><br><span class="line">  --enable-ceph-reclock</span><br></pre></td></tr></table></figure></p>
<p>这个可以知道需要添加ceph-reclock的支持就添加这个选项，我们把这个选项添加到samba.spec当中<br>修改samba.spec文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">…</span><br><span class="line">%configure \</span><br><span class="line">        --enable-fhs \</span><br><span class="line">        --with-piddir=/run \</span><br><span class="line">        --with-sockets-dir=/run/samba \</span><br><span class="line">        --with-modulesdir=%&#123;_libdir&#125;/samba \</span><br><span class="line">        --with-pammodulesdir=%&#123;_libdir&#125;/security \</span><br><span class="line">        --with-lockdir=/var/lib/samba/lock \</span><br><span class="line">        --with-statedir=/var/lib/samba \</span><br><span class="line">        --with-cachedir=/var/lib/samba \</span><br><span class="line">        --disable-rpath-install \</span><br><span class="line">        --with-shared-modules=%&#123;_samba4_modules&#125; \</span><br><span class="line">        --bundled-libraries=%&#123;_samba4_libraries&#125; \</span><br><span class="line">        --with-pam \</span><br><span class="line">        --with-pie \</span><br><span class="line">        --with-relro \</span><br><span class="line">        --enable-ceph-reclock \</span><br><span class="line">        --without-fam \</span><br><span class="line">…</span><br><span class="line">%dir %&#123;_libexecdir&#125;/ctdb</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_event</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_eventd</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_killtcp</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_lock_helper</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_lvs</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_fcntl_helper</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">…</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdb.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdb_diagnostics.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdbd.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/onnode.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ltdbtool.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ping_pong.<span class="number">1</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br><span class="line">%&#123;_mandir&#125;/man1/ctdbd_wrapper.<span class="number">1</span>.gz</span><br><span class="line">…</span><br></pre></td></tr></table></figure></p>
<p>这个文件当中一共添加了三行内容<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--enable-ceph-reclock \</span><br><span class="line">%&#123;_libexecdir&#125;/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">%&#123;_mandir&#125;/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br></pre></td></tr></table></figure></p>
<p>把解压后的目录里面的所有文件都拷贝到源码编译目录,就是上面ls列出的那些文件，以及修改好的samba.spec文件都一起拷贝过去<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># cp -ra * /root/rpmbuild/SOURCES/</span></span><br></pre></td></tr></table></figure></p>
<p>安装librados2的devel包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># yum install librados2-devel</span></span><br></pre></td></tr></table></figure></p>
<p>如果编译过程缺其他的依赖包就依次安装即可，这个可以通过解压源码先编译一次的方式来把依赖包找全，然后再打rpm包</p>
<p>开始编译rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># rpmbuild -bb samba.spec</span></span><br></pre></td></tr></table></figure></p>
<p>这个可以就在当前的目录执行即可</p>
<p>检查生成的包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos myctdb]<span class="comment"># rpm -qpl /root/rpmbuild/RPMS/x86_64/ctdb-4.6.2-12.el7.centos.x86_64.rpm|grep rados</span></span><br><span class="line">/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper</span><br><span class="line">/usr/share/man/man7/ctdb_mutex_ceph_rados_helper.<span class="number">7</span>.gz</span><br></pre></td></tr></table></figure></p>
<p>可以看到已经生成了这个，把这个包拷贝到需要更新的机器上面</p>
<h3 id="配置ctdb">配置ctdb</h3><p>首先要升级安装下新的ctdb包，因为名称有改变，会提示依赖问题,这里忽略依赖的问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># rpm -Uvh ctdb-4.6.2-12.el7.centos.x86_64.rpm --nodeps</span></span><br></pre></td></tr></table></figure></p>
<p>添加一个虚拟IP配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/public_addresses </span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.99</span>/<span class="number">16</span> ens33</span><br></pre></td></tr></table></figure></p>
<p>添加node配置<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/nodes </span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.18</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.201</span></span><br></pre></td></tr></table></figure></p>
<p>修改配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># cat /etc/ctdb/ctdbd.conf|grep -v "#"</span></span><br><span class="line"> CTDB_RECOVERY_LOCK=<span class="string">"!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb"</span></span><br><span class="line"> CTDB_NODES=/etc/ctdb/nodes</span><br><span class="line"> CTDB_PUBLIC_ADDRESSES=/etc/ctdb/public_addresses</span><br><span class="line"> CTDB_LOGGING=file:/var/<span class="built_in">log</span>/log.ctdb</span><br><span class="line"><span class="comment"># CTDB_DEBUGLEVEL=debug</span></span><br></pre></td></tr></table></figure></p>
<p>上面为了调试，我开启了debug来查看重要的信息</p>
<blockquote>
<p>CTDB_RECOVERY_LOCK=”!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb”<br>最重要的是这行配置文件规则是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CTDB_RECOVERY_LOCK=<span class="string">"!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper [Cluster] [User] [Pool] [Object]"</span></span><br><span class="line">Cluster: Ceph cluster name (e.g. ceph)</span><br><span class="line">User: Ceph cluster user name (e.g. client.admin)</span><br><span class="line">Pool: Ceph RADOS pool name</span><br><span class="line">Object: Ceph RADOS object name</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>在ctdb的机器上面准备好librados2和ceph配置文件，这个配置的rbd的lockctdb对象会由ctdb去生成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ~]<span class="comment"># systemctl restart ctdb</span></span><br></pre></td></tr></table></figure></p>
<p>配置好了以后就可以启动进程了，上面的/etc/ctdb/ctdbd.conf配置文件最好是修改好一台机器的，然后scp到其它机器，里面内容有一点点偏差都会判断为异常的，所以最好是相同的配置文件</p>
<p>查看进程状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@customos ceph]<span class="comment"># ctdb status</span></span><br><span class="line">Number of nodes:<span class="number">2</span></span><br><span class="line">pnn:<span class="number">0</span> <span class="number">192.168</span>.<span class="number">0.18</span>     OK (THIS NODE)</span><br><span class="line">pnn:<span class="number">1</span> <span class="number">192.168</span>.<span class="number">0.201</span>    OK</span><br><span class="line">Generation:<span class="number">1662303628</span></span><br><span class="line">Size:<span class="number">2</span></span><br><span class="line"><span class="built_in">hash</span>:<span class="number">0</span> lmaster:<span class="number">0</span></span><br><span class="line"><span class="built_in">hash</span>:<span class="number">1</span> lmaster:<span class="number">1</span></span><br><span class="line">Recovery mode:NORMAL (<span class="number">0</span>)</span><br><span class="line">Recovery master:<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>查看/var/log/log.ctdb日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399849</span> ctdb-recoverd[<span class="number">129134</span>]: Node:<span class="number">1</span> was <span class="keyword">in</span> recovery mode. Start recovery process</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399879</span> ctdb-recoverd[<span class="number">129134</span>]: ../ctdb/server/ctdb_recoverd.c:<span class="number">1267</span> Starting <span class="keyword">do</span>_recovery</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.399903</span> ctdb-recoverd[<span class="number">129134</span>]: Attempting to take recovery lock (!/usr/libexec/ctdb/ctdb_mutex_ceph_rados_helper ceph client.admin rbd lockctdb)</span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.400657</span> ctdb-recoverd[<span class="number">129134</span>]: ../ctdb/server/ctdb_cluster_mutex.c:<span class="number">251</span> Created PIPE FD:<span class="number">17</span></span><br><span class="line"><span class="number">2018</span>/<span class="number">01</span>/<span class="number">06</span> <span class="number">23</span>:<span class="number">18</span>:<span class="number">11.579865</span> ctdbd[<span class="number">129038</span>]: ../ctdb/server/ctdb_daemon.c:<span class="number">907</span> client request <span class="number">40</span> of <span class="built_in">type</span> <span class="number">7</span> length <span class="number">72</span> from node <span class="number">1</span> to <span class="number">4026531841</span></span><br></pre></td></tr></table></figure></p>
<p>日志中可以看到ctdb-recoverd已经是采用的ctdb_mutex_ceph_rados_helper来获取的recovery lock</p>
<p>停掉ctdb的进程，IP可以正常的切换，到这里，使用对象作为lock文件的功能就实现了，其他更多的ctdb的高级控制就不在这个里作过多的说明</p>
<h2 id="总结">总结</h2><p>本篇是基于发行版本的ctdb包进行模块的加入重新发包，并且把配置做了一次实践，这个可以作为一个ctdb的方案之一，具体跟之前的方案相比切换时间可以改善多少，需要通过数据进行对比，这个进行测试即可</p>
<h2 id="资源">资源</h2><p>已经打好包的ctdb共享一下，可以直接使用</p>
<blockquote>
<p><a href="http://7xweck.com1.z0.glb.clouddn.com/ctdb-4.6.2-12.el7.centos.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/ctdb-4.6.2-12.el7.centos.x86_64.rpm</a></p>
</blockquote>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/object.jpg" alt="object"><br></cemter></p>
<h2 id="前言">前言</h2><p>服务器的服务做HA有很多种方式，其中有一种就是是用CTDB，之前这个是独立的软件来做HA的，现在已经跟着SAMBA主线里面了，也就是跟着samba发行包一起发行</p>
<p>之前CTDB的模式是需要有一个共享文件系统，并且在这个共享文件系统里面所有的节点都去访问同一个文件，会有一个Master会获得这个文件的锁</p>
<p>在cephfs的使用场景中可以用cephfs的目录作为这个锁文件的路径，这个有个问题就是一旦有一个节点down掉的时候，可能客户端也会卡住目录，这个目录访问会被卡住，文件锁在其他机器无法获取到，需要等到这个锁超时以后，其它节点才能获得到锁，这个切换的周期就会长一点了</p>
<p>CTDB在最近的版本当中加入了cluster mutex helper using Ceph RADOS的支持，本篇将介绍这个方式锁文件配置方式<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Kernel RBD的QOS配置方案]]></title>
    <link href="http://www.zphj1987.com/2018/01/05/Kernel-RBD-QOS/"/>
    <id>http://www.zphj1987.com/2018/01/05/Kernel-RBD-QOS/</id>
    <published>2018-01-05T07:23:30.000Z</published>
    <updated>2018-01-06T15:42:00.936Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/io.png" alt="io"><br></center>

<h2 id="前言">前言</h2><p>关于qos的讨论有很多，ceph内部也正在实现着一整套的基于dmclock的qos的方案，这个不是本篇的内容，之前在社区的邮件列表看过有研发在聊qos的相关的实现的，当时一个研发就提出了在使用kernel rbd的时候，可以直接使用linux的操作系统qos来实现，也就是cgroup来控制读取写入</p>
<p>cgroup之前也有接触过，主要测试了限制cpu和内存相关的，没有做io相关的测试，这个当然可以通过ceph内部来实现qos，但是有现成的解决方案的时候，可以减少很多开发周期，以及测试的成本</p>
<p>本篇将介绍的是kernel rbd的qos方案<br><a id="more"></a></p>
<h2 id="时间过长">时间过长</h2><p>首先介绍下几个测试qos相关的命令，用来比较设置前后的效果<br>验证写入IOPS命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=write -ioengine=libaio -bs=<span class="number">4</span>K -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证写入带宽的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=write -ioengine=libaio -bs=<span class="number">4</span>M -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证读取IOPS命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=<span class="built_in">read</span> -ioengine=libaio -bs=<span class="number">4</span>K -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>验证读取带宽命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">fio -filename=/dev/rbd0 -direct=<span class="number">1</span> -iodepth <span class="number">1</span> -thread -rw=<span class="built_in">read</span> -ioengine=libaio -bs=<span class="number">4</span>M -size=<span class="number">1</span>G -numjobs=<span class="number">1</span> -runtime=<span class="number">60</span> -group_reporting -name=mytest</span><br></pre></td></tr></table></figure></p>
<p>上面为什么会设置不同的块大小，这个是因为测试的存储是会受到带宽和iops的共同制约的，当测试小io的时候，这个时候的峰值是受到iops的限制的，测试大io的时候，受到的是带宽限制，所以在做测试的时候，需要测试iops是否被限制住的时候就使用小的bs=4K，需要测试大的带宽的限制的时候就采用bs=4M来测试</p>
<p>测试的时候都是，开始不用做qos来进行测试得到一个当前不配置qos的性能数值，然后根据需要进行qos设置后通过上面的fio去测试是否能限制住</p>
<p>启用cgroup的blkio模块<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir -p  /cgroup/blkio/</span><br><span class="line">mount -t cgroup -o blkio blkio /cgroup/blkio/</span><br></pre></td></tr></table></figure></p>
<p>获取rbd磁盘的major/minor numbers<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab211 ~]<span class="comment"># lsblk </span></span><br><span class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">rbd0   <span class="number">252</span>:<span class="number">0</span>    <span class="number">0</span>  <span class="number">19.5</span>G  <span class="number">0</span> disk </span><br><span class="line">sda      <span class="number">8</span>:<span class="number">0</span>    <span class="number">1</span> <span class="number">238.4</span>G  <span class="number">0</span> disk </span><br><span class="line">├─sda4   <span class="number">8</span>:<span class="number">4</span>    <span class="number">1</span>     <span class="number">1</span>K  <span class="number">0</span> part </span><br><span class="line">├─sda2   <span class="number">8</span>:<span class="number">2</span>    <span class="number">1</span>  <span class="number">99.9</span>G  <span class="number">0</span> part </span><br><span class="line">├─sda5   <span class="number">8</span>:<span class="number">5</span>    <span class="number">1</span>     <span class="number">8</span>G  <span class="number">0</span> part [SWAP]</span><br><span class="line">├─sda3   <span class="number">8</span>:<span class="number">3</span>    <span class="number">1</span>     <span class="number">1</span>G  <span class="number">0</span> part /boot</span><br><span class="line">├─sda1   <span class="number">8</span>:<span class="number">1</span>    <span class="number">1</span>   <span class="number">100</span>M  <span class="number">0</span> part </span><br><span class="line">└─sda6   <span class="number">8</span>:<span class="number">6</span>    <span class="number">1</span> <span class="number">129.4</span>G  <span class="number">0</span> part /</span><br></pre></td></tr></table></figure></p>
<p>通过lsblk命令可以获取到磁盘对应的major number和minor number,这里可以看到rbd0对应的编号为252:0</p>
<p>设置rbd0的iops的qos为10<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10"</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device</span><br></pre></td></tr></table></figure></p>
<p>如果想清理这个规则,把后面的数值设置为0就清理了，后面几个配置也是相同的方法<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 0"</span> &gt; /cgroup/blkio/blkio.throttle.write_iops_device</span><br></pre></td></tr></table></figure></p>
<p>限制写入的带宽为10MB/s<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10485760"</span> &gt; /cgroup/blkio/blkio.throttle.write_bps_device</span><br></pre></td></tr></table></figure></p>
<p>限制读取的qos为10<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10"</span> &gt; /cgroup/blkio/blkio.throttle.read_iops_device</span><br></pre></td></tr></table></figure></p>
<p>限制读取的带宽为10MB/s<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"252:0 10485760"</span> &gt; /cgroup/blkio/blkio.throttle.read_bps_device</span><br></pre></td></tr></table></figure></p>
<p>以上简单的设置就完成了kernel rbd的qos设置了，我测试了下，确实是生效了的</p>
<h2 id="总结">总结</h2><p>这个知识点很久前就看到了，一直没总结，现在记录下，个人观点是能快速，有效，稳定的实现功能是最好的，所以使用这个在kernel rbd方式下可以不用再进行qos的开发了</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-05</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/io.png" alt="io"><br></center>

<h2 id="前言">前言</h2><p>关于qos的讨论有很多，ceph内部也正在实现着一整套的基于dmclock的qos的方案，这个不是本篇的内容，之前在社区的邮件列表看过有研发在聊qos的相关的实现的，当时一个研发就提出了在使用kernel rbd的时候，可以直接使用linux的操作系统qos来实现，也就是cgroup来控制读取写入</p>
<p>cgroup之前也有接触过，主要测试了限制cpu和内存相关的，没有做io相关的测试，这个当然可以通过ceph内部来实现qos，但是有现成的解决方案的时候，可以减少很多开发周期，以及测试的成本</p>
<p>本篇将介绍的是kernel rbd的qos方案<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph对象主本损坏的修复方法]]></title>
    <link href="http://www.zphj1987.com/2018/01/02/ceph-primary-object-damage-recover/"/>
    <id>http://www.zphj1987.com/2018/01/02/ceph-primary-object-damage-recover/</id>
    <published>2018-01-02T14:21:23.000Z</published>
    <updated>2018-01-02T14:41:54.296Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dog.jpg" alt="dog"><br></center>

<h2 id="前言">前言</h2><p>问题的触发是在进行一个目录的查询的时候，osd就会挂掉，开始以为是osd操作超时了，后来发现每次访问这个对象都有问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">log</span> [WRN] ： slow request <span class="number">60.793196</span> seconds old, received at osd_op(mds.<span class="number">0.188</span>:<span class="number">728345234100006</span>c6ddc.<span class="number">00000000</span> [o map-get-header <span class="number">0</span>-<span class="number">0</span>,omap-get-vals <span class="number">0</span>~<span class="number">16</span>,getxattr parent] snapc <span class="number">0</span>=[] ack+<span class="built_in">read</span>+known_<span class="keyword">if</span>_redirected+full_force e218901) currently started</span><br><span class="line">heartbeat_map is_healthy  ··· osd_op_tp thread ··· had timed out after <span class="number">60</span></span><br></pre></td></tr></table></figure></p>
<p>这个对象是元数据的一个空对象，保留数据在扩展属性当中<br><a id="more"></a><br>然后做了一个操作判断是对象损坏了:</p>
<p>直接列取omapkeys</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rados -p metadata listomapvals <span class="number">100006</span>c6ddc.<span class="number">00000000</span></span><br></pre></td></tr></table></figure>
<p>发现会卡住，然后关闭这个osd再次做操作，就可以了，启动后还是不行，这里可以判断是主本的对象已经有问题了，本篇将讲述多种方法来解决这个问题</p>
<h2 id="处理办法">处理办法</h2><p>本章将会根据操作粒度的不同来讲述三种方法的恢复，根据自己的实际情况，和风险的判断来选择自己的操作</p>
<h3 id="方法一：通过repair修复">方法一：通过repair修复</h3><p>首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span></span><br></pre></td></tr></table></figure></p>
<p>通过ceph-object-tool进行移除的时候有bug,无法移除metadata的对象，已经提了一个<a href="http://tracker.ceph.com/issues/22553" target="_blank" rel="external">bug</a><br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># mv 100.00000000__head_C5265AB3__2 ../</span></span><br></pre></td></tr></table></figure></p>
<p>注意一下在老版本的时候，对对象进行删除以后，可能元数据里面记录了对象信息，而对象又不在的时候可能会引起osd无法启动，这个在10.2.10是没有这个问题</p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>对pg做scrub<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph pg scrub 2.0</span></span><br><span class="line">instructing pg <span class="number">2.0</span> on osd.<span class="number">0</span> to scrub</span><br></pre></td></tr></table></figure></p>
<p>这种方法就是需要做scrub的操作，如果对象特别多，并且是线上环境，可能不太好去做scrub的操作<br>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster <span class="number">03580</span>f14-<span class="number">9906</span>-<span class="number">4257</span>-<span class="number">9182</span>-<span class="number">65</span>c886e7f5a7</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            <span class="number">1</span> pgs inconsistent</span><br><span class="line">            <span class="number">1</span> scrub errors</span><br><span class="line">            too few PGs per OSD (<span class="number">3</span> &lt; min <span class="number">30</span>)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab71=<span class="number">20.20</span>.<span class="number">20.71</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">4</span>, quorum <span class="number">0</span> lab71</span><br><span class="line">      fsmap e30: <span class="number">1</span>/<span class="number">1</span>/<span class="number">1</span> up &#123;<span class="number">0</span>=lab71=up:active&#125;</span><br><span class="line">     osdmap e101: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v377: <span class="number">3</span> pgs, <span class="number">3</span> pools, <span class="number">100814</span> bytes data, <span class="number">41</span> objects</span><br><span class="line">            <span class="number">70196</span> kB used, <span class="number">189</span> GB / <span class="number">189</span> GB avail</span><br><span class="line">                   <span class="number">2</span> active+clean</span><br><span class="line">                   <span class="number">1</span> active+clean+inconsistent</span><br></pre></td></tr></table></figure></p>
<p>发起修复请求<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># ceph pg repair 2.0</span></span><br><span class="line">instructing pg <span class="number">2.0</span> on osd.<span class="number">0</span> to repair</span><br></pre></td></tr></table></figure></p>
<p>修复完成后检查集群状态和对象，到这里可以恢复正常了</p>
<h3 id="方法二：通过rsync拷贝数据方式恢复">方法二：通过rsync拷贝数据方式恢复</h3><p>跟上面一样这里首先能确定是主本损坏了，那么先把主本的对象进行一个备份，然后移除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># cp -ra 100.00000000__head_C5265AB3__2 ../../</span></span><br></pre></td></tr></table></figure></p>
<p>移除对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># mv 100.00000000__head_C5265AB3__2 ../</span></span><br></pre></td></tr></table></figure></p>
<p>在副本的机器上执行rsync命令，这里我们直接从副本拷贝对象过来，注意下不能直接使用scp会掉扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab72 <span class="number">2.0</span>_head]<span class="comment"># rsync  -avXH  /var/lib/ceph/osd/ceph-1/current/2.0_head/100.00000000__head_C5265AB3__2 20.20.20.71:/var/lib/ceph/osd/ceph-0/current/2.0_head/100.00000000__head_C5265AB3__2</span></span><br></pre></td></tr></table></figure></p>
<p>在主本机器检查扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># getfattr 100.00000000__head_C5265AB3__2 </span></span><br><span class="line"><span class="comment"># file: 100.00000000__head_C5265AB3__2</span></span><br><span class="line">user.ceph._</span><br><span class="line">user.ceph._@<span class="number">1</span></span><br><span class="line">user.ceph.snapset</span><br><span class="line">user.cephos.spill_out</span><br></pre></td></tr></table></figure></p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>检查对象的扩展属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 <span class="number">2.0</span>_head]<span class="comment"># rados -p metadata listomapvals 100.00000000</span></span><br></pre></td></tr></table></figure></p>
<h3 id="方法三：通过删除PG的方式恢复">方法三：通过删除PG的方式恢复</h3><p>这个方式是删除PG，然后重新启动的方式<br>这种方式操作比较危险，所以提前备份好pg的数据，最好主备pg都备份下，万一出了问题或者数据不对，可以根据需要再导入<br>备份PG<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-objectstore-tool --pgid <span class="number">2.0</span> --op <span class="built_in">export</span> --data-path /var/lib/ceph/osd/ceph-<span class="number">0</span>/ --journal-path   /var/lib/ceph/osd/ceph-<span class="number">0</span>/journal --file /root/<span class="number">2.0</span></span><br></pre></td></tr></table></figure></p>
<p>删除PG的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment"># ceph-objectstore-tool --pgid 2.0  --op remove --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">00</span> c0 <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">00</span> c0 <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"> marking collection <span class="keyword">for</span> removal</span><br><span class="line">setting <span class="string">'_remove'</span> omap key</span><br><span class="line">finish_remove_pgs <span class="number">2.0</span>_head removing <span class="number">2.0</span></span><br><span class="line">Remove successful</span><br></pre></td></tr></table></figure></p>
<p>重启osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment"># systemctl restart ceph-osd@0</span></span><br></pre></td></tr></table></figure></p>
<p>等待回复即可</p>
<p>本方法里面还可以衍生一种就是，通过导出的副本的PG数据,在主本删除了相应的PG以后,进行导入的方法，这样就不会产生迁移<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab71 current]<span class="comment">#  ceph-objectstore-tool --pgid 2.0  --op import --data-path /var/lib/ceph/osd/ceph-0/ --journal-path /var/lib/ceph/osd/ceph-0/journal --file /root/2.0</span></span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>上面用三种方法来实现了副本向主本同步的操作，判断主本是否有问题的方法就是主动的把主本所在的OSD停掉，然后检查请求是否可达，在确定主本已经坏掉的情况下，就可以做将副本同步到主本的操作，可以根据PG的对象的多少来选择需要做哪种操作</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2018-01-02</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/dog.jpg" alt="dog"><br></center>

<h2 id="前言">前言</h2><p>问题的触发是在进行一个目录的查询的时候，osd就会挂掉，开始以为是osd操作超时了，后来发现每次访问这个对象都有问题<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">log</span> [WRN] ： slow request <span class="number">60.793196</span> seconds old, received at osd_op(mds.<span class="number">0.188</span>:<span class="number">728345234100006</span>c6ddc.<span class="number">00000000</span> [o map-get-header <span class="number">0</span>-<span class="number">0</span>,omap-get-vals <span class="number">0</span>~<span class="number">16</span>,getxattr parent] snapc <span class="number">0</span>=[] ack+<span class="built_in">read</span>+known_<span class="keyword">if</span>_redirected+full_force e218901) currently started</span><br><span class="line">heartbeat_map is_healthy  ··· osd_op_tp thread ··· had timed out after <span class="number">60</span></span><br></pre></td></tr></table></figure></p>
<p>这个对象是元数据的一个空对象，保留数据在扩展属性当中<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mds的cpu占用问题分析以及解决办法]]></title>
    <link href="http://www.zphj1987.com/2017/12/04/mds-use-too-more-cpu/"/>
    <id>http://www.zphj1987.com/2017/12/04/mds-use-too-more-cpu/</id>
    <published>2017-12-04T14:47:04.000Z</published>
    <updated>2017-12-04T15:06:37.402Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ganesha.png" alt="ganesha"><br></center>

<h2 id="前言">前言</h2><p>mds是ceph里面处理文件接口的组件，一旦使用文件系统，不可避免的会出现一种场景就是目录很多，目录里面的文件很多，而mds是一个单进程的组件，现在虽然有了muti mds，但稳定的使用的大部分场景还是单acitve mds的</p>
<p>这就会出现一种情况，一旦一个目录里面有很多文件的时候，去查询这个目录里的文件就会在当前目录做一次遍历，这个需要一个比较长的时间，如果能比较好的缓存文件信息，也能避免一些过载情况，本篇讲述的是内核客户端正常，而export nfs后mds的负载长时间过高的情况<br><a id="more"></a></p>
<h2 id="问题复现">问题复现</h2><h3 id="准备测试数据,准备好监控环境">准备测试数据,准备好监控环境</h3><p>监控mds cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pidstat -u  <span class="number">1</span> -p <span class="number">27076</span> &gt; /tmp/mds.cpu.log</span><br><span class="line">UserParameter=mds.cpu,cat /tmp/mds.cpu.log|tail -n <span class="number">1</span>|grep -v Average| awk <span class="string">'&#123;print $8&#125;'</span></span><br></pre></td></tr></table></figure></p>
<p>整个测试避免屏幕的打印影响时间统计,把输出需要重定向<br>测试一：<br>内核客户端写入10000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time seq 10000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line">real	<span class="number">0</span>m30.<span class="number">121</span>s</span><br><span class="line">user	<span class="number">0</span>m1.<span class="number">901</span>s</span><br><span class="line">sys	<span class="number">0</span>m10.<span class="number">420</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/3no4iobedvhwujvtr8jfmrf7/aa.png" alt="aa.png-32.5kB"></p>
<p>测试二：<br>内核客户端写入20000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc20000]<span class="comment"># time seq 20000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line">real	<span class="number">1</span>m38.<span class="number">233</span>s</span><br><span class="line">user	<span class="number">0</span>m3.<span class="number">761</span>s</span><br><span class="line">sys	<span class="number">0</span>m21.<span class="number">510</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/ual9inmekpeth163muql8dyn/bbb.png" alt="bbb.png-39kB"><br>测试三：<br>内核客户端写入40000文件查看时间以及cpu占用<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc40000]<span class="comment">#  time seq 40000|xargs -i dd if=/dev/zero of=a&#123;&#125; bs=1K count=1  2&gt;/dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">2</span>m55.<span class="number">261</span>s</span><br><span class="line">user	<span class="number">0</span>m7.<span class="number">699</span>s</span><br><span class="line">sys	<span class="number">0</span>m42.<span class="number">410</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/pnqr3lpjuydb8dj182cw6ffa/cccc.png" alt="cccc.png-57.3kB"></p>
<p>测试4：<br>内核客户端列目录10000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">228</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">063</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">048</span>s</span><br></pre></td></tr></table></figure></p>
<p>内核客户端列目录20000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">737</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">141</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">092</span>s</span><br></pre></td></tr></table></figure></p>
<p>内核客户端列目录40000文件，第一次写完有缓存情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">658</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">286</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">196</span>s</span><br></pre></td></tr></table></figure></p>
<p>都是比较快的返回，CPU可以忽略不计</p>
<p>现在重启mds后再次列目录<br>客户端如果不umount,直接重启mds的话,还是会缓存在<br>新版本这个地方好像已经改了（重启了mds 显示inode还在，但是随着时间的增长inode会减少，说明还是有周期，会释放，这个还不知道哪个地方控制，用什么参数控制，这个不是本篇着重关注的地方，后续再看下,jewel版本已经比hammer版本的元数据时间快了很多了）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsserver kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">380</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">065</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">041</span>s</span><br><span class="line">[root@nfsserver kc10000]<span class="comment"># cd ../kc20000/</span></span><br><span class="line">[root@nfsserver kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m0.<span class="number">868</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">154</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">074</span>s</span><br><span class="line">[root@nfsserver kc20000]<span class="comment"># cd ../kc40000/</span></span><br><span class="line">[root@nfsserver kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">947</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">300</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">166</span>s</span><br></pre></td></tr></table></figure></p>
<p>测试都是看到很快的返回，以上都是正常的，下面开始将这个目录exportnfs出去，看下是个什么情况</p>
<h3 id="负载问题复现">负载问题复现</h3><p>从nfs客户端第一次列10000个小文件的目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc10000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m4.<span class="number">038</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">095</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">069</span>s</span><br></pre></td></tr></table></figure>
<p><img src="http://static.zybuluo.com/zphj1987/2zkcsg31i5r0972clmsnwu9p/nfs10000.png" alt="nfs10000.png-36.7kB"></p>
<p>从nfs客户端第一次列20000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m17.<span class="number">446</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">175</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">141</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/jsd4ropqf7s80olmib2bm16t/nfs20000.png" alt="nfs20000.png-43.2kB"><br>从nfs客户端第二次列20000个小文件目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m21.<span class="number">215</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">182</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">151</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/im9wby3cyze0fsvnvoifgita/nfs200002.png" alt="nfs200002.png-56.7kB"></p>
<p>从nfs客户端第三次列20000个小文件目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc20000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m16.<span class="number">222</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">189</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">143</span>s</span><br></pre></td></tr></table></figure></p>
<p>可以看到在20000量级的时候列目录维持在20000左右，CPU维持一个高位</p>
<p>从nfs客户端列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">7</span>m15.<span class="number">663</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">319</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">581</span>s</span><br><span class="line">[root@nfsclient kc40000]<span class="comment">#</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/rflb49cxesc1g6uv0cuyfyse/nfs40000.png" alt="nfs40000.png-77.2kB"><br>第一次列完，马上第二次列看下情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">1</span>m12.<span class="number">816</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">163</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">142</span>s</span><br></pre></td></tr></table></figure></p>
<p>可以看到第二次列的时间已经缩短了，再来第三次<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">1</span>m33.<span class="number">549</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">162</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">183</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/nhx161bnwwt3ggiaommufvfx/nfs400003.png" alt="nfs400003.png-61.3kB"><br>可以看到在后面列的时候时间确实缩短了，但是还是维持一个非常高CPU的占用，以及比较长的一个时间，这个很容易造成过载</p>
<p>这个地方目前看应该是内核客户端与内核NFS的结合的问题</p>
<h2 id="解决办法:用ganesha的ceph用户态接口替代kernel_nfs">解决办法:用ganesha的ceph用户态接口替代kernel nfs</h2><p>我们看下另外一种方案用户态的NFS+ceph同样的环境下测试结果：</p>
<p>从nfs客户端第一次列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m3.<span class="number">289</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">335</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">386</span>s</span><br></pre></td></tr></table></figure></p>
<p>从nfs客户端第二次列40000个小文件的目录</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">686</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">351</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">389</span>s</span><br></pre></td></tr></table></figure>
<p>从nfs客户端第三次列40000个小文件的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@nfsclient kc40000]<span class="comment"># time ll 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line">real	<span class="number">0</span>m1.<span class="number">675</span>s</span><br><span class="line">user	<span class="number">0</span>m0.<span class="number">320</span>s</span><br><span class="line">sys	<span class="number">0</span>m0.<span class="number">391</span>s</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/mw0j9nj322p0slwymyicffvc/ganesha.png" alt="ganesha.png-51.5kB"><br>基本mds无多余的负载，非常快的返回</p>
<p>可以从上面的测试看到差别是非常的大的，这个地方应该是内核模块与内核之间的问题，而采用用户态的以后解决了列目录慢以及卡顿的问题</p>
<h2 id="如何配置ganesha支持ceph的nfs接口">如何配置ganesha支持ceph的nfs接口</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b V2.<span class="number">3</span>-stable https://github.com/nfs-ganesha/nfs-ganesha.git</span><br><span class="line"><span class="built_in">cd</span> nfs-ganesha/</span><br><span class="line">git submodule update --init --recursive</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> nfs-ganesha/</span><br><span class="line">ll src/FSAL/FSAL_CEPH/</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">mkdir mybuild</span><br><span class="line"><span class="built_in">cd</span> mybuild/</span><br><span class="line">cmake -DUSE_FSAL_CEPH=ON ../nfs-ganesha/src/</span><br><span class="line">ll FSAL/FSAL_CEPH/</span><br><span class="line">make</span><br><span class="line">make -j <span class="number">12</span></span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>vim /etc/ganesha/ganesha.conf<br>修改配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">EXPORT</span><br><span class="line">&#123;</span><br><span class="line">    Export_ID=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    Path = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">    Pseudo = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">    Access_Type = RW;</span><br><span class="line"></span><br><span class="line">    NFS_Protocols = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    Transport_Protocols = TCP;</span><br><span class="line"></span><br><span class="line">    FSAL &#123;</span><br><span class="line">        Name = CEPH;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>停止掉原生的nfs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl stop nfs</span><br></pre></td></tr></table></figure></p>
<p>启用ganesha nfs<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl start  nfs-ganesha.service</span><br></pre></td></tr></table></figure></p>
<p>然后在客户端进行nfs的挂载即可</p>
<h2 id="总结">总结</h2><p>ganesha在需要用到cephfs又正好是要用到nfs接口的时候，可以考虑这个方案，至少在缓存文件，降低负载上面能够比kernel client有更好的效果，这个可以根据测试情况用数据来做比较</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-12-04</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ganesha.png" alt="ganesha"><br></center>

<h2 id="前言">前言</h2><p>mds是ceph里面处理文件接口的组件，一旦使用文件系统，不可避免的会出现一种场景就是目录很多，目录里面的文件很多，而mds是一个单进程的组件，现在虽然有了muti mds，但稳定的使用的大部分场景还是单acitve mds的</p>
<p>这就会出现一种情况，一旦一个目录里面有很多文件的时候，去查询这个目录里的文件就会在当前目录做一次遍历，这个需要一个比较长的时间，如果能比较好的缓存文件信息，也能避免一些过载情况，本篇讲述的是内核客户端正常，而export nfs后mds的负载长时间过高的情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CentOS GRUB损坏修复方法]]></title>
    <link href="http://www.zphj1987.com/2017/11/30/recovery-from-grub-damage/"/>
    <id>http://www.zphj1987.com/2017/11/30/recovery-from-grub-damage/</id>
    <published>2017-11-30T14:51:55.000Z</published>
    <updated>2017-11-30T15:22:39.682Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/grub.jpg" alt="grub"><br></center>

<h2 id="前言">前言</h2><p>博客很久没有更新了，一个原因就是原来存放部署博客的环境坏了，硬盘使用的是SSD，只要读取到某个文件，整个磁盘就直接识别不到了，还好博客环境之前有做备份，最近一直没有把部署环境做下恢复，今天抽空把环境做下恢复并且记录一篇基础的GRUB的处理文档</p>
<p>这两天正好碰到GRUB损坏的事，很久前处理过，但是没留下文档，正好现在把流程梳理一下，来解决grub.cfg损坏的情况,或者无法启动的情况<br><a id="more"></a></p>
<h2 id="实践步骤">实践步骤</h2><p>安装操作系统的时候会有多种可能分区的方法，一个直接的分区，一个是用了lvm,本篇将几种分区的情况分别写出来</p>
<h3 id="lvm分区的情况">lvm分区的情况</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># df -h</span></span><br><span class="line">Filesystem               Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/mapper/centos-root   <span class="number">17</span>G  <span class="number">927</span>M   <span class="number">17</span>G   <span class="number">6</span>% /</span><br><span class="line">devtmpfs                 <span class="number">901</span>M     <span class="number">0</span>  <span class="number">901</span>M   <span class="number">0</span>% /dev</span><br><span class="line">tmpfs                    <span class="number">912</span>M     <span class="number">0</span>  <span class="number">912</span>M   <span class="number">0</span>% /dev/shm</span><br><span class="line">tmpfs                    <span class="number">912</span>M  <span class="number">8.6</span>M  <span class="number">904</span>M   <span class="number">1</span>% /run</span><br><span class="line">tmpfs                    <span class="number">912</span>M     <span class="number">0</span>  <span class="number">912</span>M   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1               <span class="number">1014</span>M  <span class="number">143</span>M  <span class="number">872</span>M  <span class="number">15</span>% /boot</span><br><span class="line">tmpfs                    <span class="number">183</span>M     <span class="number">0</span>  <span class="number">183</span>M   <span class="number">0</span>% /run/user/<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>模拟/boot/grub2/grub.cfg的破坏</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># mv /boot/grub2/grub.cfg /boot/grub2/grub.cfgbk</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># reboot</span></span><br></pre></td></tr></table></figure>
<p>重启后就会出现这个</p>
<p><img src="http://static.zybuluo.com/zphj1987/agdslyms36u15eaar4h4d8gr/image.png" alt="image.png-13.4kB"></p>
<p>使用ls查询当前的分区情况</p>
<p><img src="http://static.zybuluo.com/zphj1987/fv6pjy9a3aw09ut819k8lvj9/image.png" alt="image.png-7.7kB"><br>查询分区情况<br><img src="http://static.zybuluo.com/zphj1987/uuj04u8y2dguvhqg85iavbe5/image.png" alt="image.png-29.1kB"></p>
<p>可以看到(hd0,msdos1)可以列出/boot里面的内容，可以确定这个就是启动分区</p>
<p>设置root<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos1)</span><br></pre></td></tr></table></figure></p>
<p>命令后面的路径可以用tab键补全,/dev/mapper/centos-root为根分区，因为当前的分区模式是lvm的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/mapper/centos-root</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h3 id="一个完整/分区形式">一个完整/分区形式</h3><p>这种情况，整个安装的系统就一个分区，boot是作为/分区的一个子目录的情况<br>ls 查询分区<br><img src="http://static.zybuluo.com/zphj1987/o3d1wegh1nfpm6a3u7w5q8yo/image.png" alt="image.png-4.6kB"></p>
<p>设置根分区<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos3)</span><br></pre></td></tr></table></figure></p>
<p>可以看到上面是msdos3分区对应的就是root=/dev/sda3,下面就设置这个root</p>
<p>设置linux16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /root/vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/sda3</span><br></pre></td></tr></table></figure></p>
<p>设置initrd16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /root/initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure></p>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h3 id="/分区和/boot分区独立分区的情况">/分区和/boot分区独立分区的情况</h3><p><img src="http://static.zybuluo.com/zphj1987/bd481u09kfonua77zwc87jsc/image.png" alt="image.png-16.3kB"></p>
<p>设置根分区<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; <span class="built_in">set</span> root=(hd0,msdos1)</span><br></pre></td></tr></table></figure></p>
<p>根据/分区为msdos2可以知道root分区为/dev/sda2<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; linux16 /vmlinuz-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64 root=/dev/sda2</span><br></pre></td></tr></table></figure></p>
<p>设置initrd16<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; initrd16 /initramfs-<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">693</span>.el7.x86_64.img</span><br></pre></td></tr></table></figure></p>
<p>启动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub&gt; boot</span><br></pre></td></tr></table></figure></p>
<p>进入系统后重新生成grub.cfg<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grub2-mkconfig -o /boot/grub2/grub.cfg</span><br></pre></td></tr></table></figure></p>
<p>然后重启下系统验证是否好了</p>
<h2 id="总结">总结</h2><p>主要的处理流程如下：</p>
<ul>
<li>首先通过<code>ls</code>得到分区的情况</li>
<li>通过<code>set</code>设置/boot所在的分区为root</li>
<li>分别设置linux16，initrd16并且指定root分区为/分区所在的目录</li>
<li>重启后重新生成grub即可</li>
</ul>
<p>本篇作为一个总结以备不时之需</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-11-30</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/grub.jpg" alt="grub"><br></center>

<h2 id="前言">前言</h2><p>博客很久没有更新了，一个原因就是原来存放部署博客的环境坏了，硬盘使用的是SSD，只要读取到某个文件，整个磁盘就直接识别不到了，还好博客环境之前有做备份，最近一直没有把部署环境做下恢复，今天抽空把环境做下恢复并且记录一篇基础的GRUB的处理文档</p>
<p>这两天正好碰到GRUB损坏的事，很久前处理过，但是没留下文档，正好现在把流程梳理一下，来解决grub.cfg损坏的情况,或者无法启动的情况<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[推荐一本书《Ceph设计原理与实现》]]></title>
    <link href="http://www.zphj1987.com/2017/09/28/a-new-ceph-book/"/>
    <id>http://www.zphj1987.com/2017/09/28/a-new-ceph-book/</id>
    <published>2017-09-28T14:23:47.000Z</published>
    <updated>2017-11-30T14:47:50.773Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/book-rocket.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇不是一篇技术文，而是推荐的一本书，对于写书来说，在多年以前觉得是一件可望而不可及的事情，而看到几本经典书籍的作者在讲述自己写书的过程的时候，都是自己注入了大量的精力的，所以我自己目前也只能是一个个知识点的以博客的方式进行记录</p>
<p>对于买书来说，很多人会觉得很贵，其实一本书几百面，只要里面的书有两面是能够帮助到你的，书的价值其实已经回来了，所以对于技术书籍来说，基本不评价书的好坏，而是去看多少能提取的东西，多少未知的东西</p>
<p>ceph的书籍最初的起步也是国外的一个作者写的，社区进行翻译，社区自己也出了一本书，还有ZETTAKIT（泽塔云）的研发常涛也出了一本《Ceph源码分析》，这些都是很好的书籍，之前也都有推荐，这些书籍都是一线的研发在繁忙之中抽出空余的时间写下来的</p>
<p>本篇推荐的一篇是来自中兴的书籍，中兴也是国内ceph开发里面代码提交量很高的公司</p>
<p>目前没能拿到书籍，所以只能从目录来讲下本书会提供哪些相关的知识了<br><a id="more"></a></p>
<h2 id="书籍简介">书籍简介</h2><h3 id="straw及straw2相关内容">straw及straw2相关内容</h3><p>这个是ceph里面的crush算法的内容，straw2算法优化了再平衡的时候的数据迁移量，以及能提供更好的分布，让数据更平均，相关内容里面还讲了数据分布的相关知识，整个能解决的应该是数据平均分布相关的知识，让你的数据更加平衡</p>
<h3 id="BlueStore_相关内容">BlueStore 相关内容</h3><p>BlueStore 是Ceph Luminous版本作为默认存储的新型的底层存储，这个是用来替换掉linux下的底层的文件系统的，而实现的一个新型的文件系统，这个是为了带来一个更好的性能的提升的，目前是测试可用，生产慎用的情况，应该会越来越稳定的</p>
<h3 id="纠删码原理与overwrites支持">纠删码原理与overwrites支持</h3><p>纠删码是为了解决副本的空间占用的问题，用更少的空间损失来获取更大的安全性，相当于计算换空间，纠删这个在很久以前就接触过一个另外一套文件系统，使用场景个人觉得是冷数据比较合适，而如果性能足够好，计算能力足够强，也能支撑比较大的带宽的<br>在之前的版本当中，ec的启用必须启动缓冲池，需要副本缓冲池的缓冲池做一层转发，这个转发实际上意味着写放大，并且还会出现缓冲池下刷数据的时候性能急剧下降的问题<br>在新版本中加入了overwrites支持，这个现在新版的bluestore的已经支持数据直接写到ec存储池了，也就是无需缓冲了</p>
<h3 id="PG_读写流程与状态迁移详解">PG 读写流程与状态迁移详解</h3><p>PG在恢复过程中会有各种状态，什么情况下会出现什么状态，什么状态进行什么处理，什么情况下不能乱动，这些都是需要好好的了解PG状态再进行操作的，否则把PG状态弄坏了，意味着数据也就无法读取了</p>
<h3 id="存储服务质量QoS">存储服务质量QoS</h3><p>ceph里面一直没有qos这个，也就是对读写相关的限流，kernel rbd的场景下是可以用cgroup进行qos相关的控制的，其他场景就没有什么好的方法了,所以在比较新的版本里面引入了dmclock来进行限流的相关的控制，这个以后可以在恢复以及写入当中做更精准的控制了，qos也是商用存储里面必要的功能，所以说ceph在功能完善方面更进了一步，需求推动研发</p>
<h3 id="存储RBD">存储RBD</h3><p>这个讲了rbd相关的一些知识，结构和功能方面的</p>
<h3 id="对象存储网关RGW">对象存储网关RGW</h3><p>这个讲了对象存储方面的一些功能特性和相关的操作</p>
<h3 id="分布式文件系统_CephFS">分布式文件系统 CephFS</h3><p>这个讲了cephfs相关的一些知识，讲了负载均衡和故障恢复的相关内容，负载均衡是相对于多active mds的场景的，可以对目录进行mds的负载划分，把负载分摊到多个mds上面，这个在新版本已经可以使用了，并且目前已经是生产可用</p>
<h3 id="定时scrub">定时scrub</h3><p>scrub这个不要让默认触发，自己做相关的策略，指定时间一个个PG的去scrub就可以了，书中应该会提及相关的具体做法</p>
<h3 id="Full的紧急处理">Full的紧急处理</h3><p>这个是集群出现Full以后的紧急处理，对于full以后的情况，一般不要乱动，因为full以后，其他osd也会是快full的状态，并且还有backfill full的控制，所以需要比较精准的控制，相当于游戏里面的微操了，书中应该会系统的讲解</p>
<h3 id="快照在增量备份中的应用">快照在增量备份中的应用</h3><p>通过快照的方式可以进行增量的备份，从而减少备份的需要获取的数据量，这个之前也有介绍过</p>
<h3 id="异常watcher的处理">异常watcher的处理</h3><p>这个应该是通过黑名单的方式进行watcher的相关的处理，这个建议是先处理能处理的，最后无法处理的异常情况用黑名单处理，这个等书出来以后可以看到更详细的内容</p>
<h2 id="作者简介">作者简介</h2><p>谢型果<br>中兴通讯资深软件工程师，5年存储开发经验，精通本地文件系统ZFS和分布式存储系统Ceph。2014 年开始研究 Ceph，2015 年加入 Ceph 开源社区，目前是 Ceph 开源社区的 Ceph Member。</p>
<p>任焕文<br>中兴通讯高级软件工程师，有10余年研发经验，曾就职于浪潮和华为，擅长数据库、网络和存储相关技术。Ceph Member成员，现主要负责Ceph文件系统、NAS存储和分布式一致性方面的研发工作。</p>
<p>严　军<br>中兴通讯高级软件工程师，从事存储系统开发工作多年，熟悉DPDK开发框架；2015年加入Ceph开源项目，对分布式存储系统QoS有深入研究，目前是Ceph开源社区的积极贡献者。</p>
<p>罗润兵<br>华中科技大学微电子专业研究生，中兴通讯高级软件工程师，精通TCP/IP协议栈和分布式存储系统，2014年开始接触并参与Ceph开源项目，目前是Ceph开源社区的积极贡献者。</p>
<p>韦巧苗<br>中兴通讯高级软件工程师，擅长C/C++编程，有5年存储系统研发经验，对Ceph RGW模块有深入研究，同时在Cache技术及性能优化上也有丰富的经验。</p>
<p>骆科学<br>中兴通讯高级软件工程师，有5年存储产品相关开发经验，擅长虚拟化及存储相关技术，2016年于Ceph中国社区年终盛典中被评为“2016年度社区十佳贡献者”。<br>总结</p>
<p>从目录上面看书中的内容包含的方面很多，可以看到这些很多都是我之前在生产环境当中使用到了或者接触过的东西，所以可以很系统的把这些知识提取出来，这样可以更了解整个系统<br>购买地址：</p>
<blockquote>
<p><a href="http://item.jd.com/12196497.html" target="_blank" rel="external">http://item.jd.com/12196497.html</a></p>
</blockquote>
<p>或者扫描二维码购买</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/miyt9s24b9e9x2jzf8arkzzq/scancode.png" alt=""><br></center>

<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-28</td>
</tr>
<tr>
<td style="text-align:center">更正常涛的公司名称</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-28</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xopdu.com1.z0.glb.clouddn.com/book-rocket.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇不是一篇技术文，而是推荐的一本书，对于写书来说，在多年以前觉得是一件可望而不可及的事情，而看到几本经典书籍的作者在讲述自己写书的过程的时候，都是自己注入了大量的精力的，所以我自己目前也只能是一个个知识点的以博客的方式进行记录</p>
<p>对于买书来说，很多人会觉得很贵，其实一本书几百面，只要里面的书有两面是能够帮助到你的，书的价值其实已经回来了，所以对于技术书籍来说，基本不评价书的好坏，而是去看多少能提取的东西，多少未知的东西</p>
<p>ceph的书籍最初的起步也是国外的一个作者写的，社区进行翻译，社区自己也出了一本书，还有ZETTAKIT（泽塔云）的研发常涛也出了一本《Ceph源码分析》，这些都是很好的书籍，之前也都有推荐，这些书籍都是一线的研发在繁忙之中抽出空余的时间写下来的</p>
<p>本篇推荐的一篇是来自中兴的书籍，中兴也是国内ceph开发里面代码提交量很高的公司</p>
<p>目前没能拿到书籍，所以只能从目录来讲下本书会提供哪些相关的知识了<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[掉电后osdmap丢失无法启动osd的解决方案]]></title>
    <link href="http://www.zphj1987.com/2017/09/27/lost-osdmap-recovery/"/>
    <id>http://www.zphj1987.com/2017/09/27/lost-osdmap-recovery/</id>
    <published>2017-09-27T06:03:59.000Z</published>
    <updated>2017-09-27T08:48:41.122Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recuva.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇讲述的是一个比较极端的故障的恢复场景，在整个集群全部服务器突然掉电的时候，osd里面的osdmap可能会出现没刷到磁盘上的情况，这个时候osdmap的最新版本为空或者为没有这个文件</p>
<p>还有一种情况就是机器宕机了，没有马上处理，等了一段时间以后，服务器机器启动了起来，而这个时候osdmap已经更新了，全局找不到需要的旧版本的osdmap和incmap，osd无法启动</p>
<p>一般情况下能找到的就直接从其他osd上面拷贝过来，然后就可以启动了，本篇讲述的是无法启动的情况<br><a id="more"></a></p>
<h2 id="解决方案">解决方案</h2><h3 id="获取运行的ceph集群当前版本">获取运行的ceph集群当前版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version <span class="number">10.2</span>.<span class="number">9</span> (<span class="number">2</span>ee413f77150c0f375ff6f10edd6c8f9c7d060d0)</span><br></pre></td></tr></table></figure>
<p>获取最新的osdmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 ~]<span class="comment"># ceph osd getmap -o /tmp/productosdmap</span></span><br><span class="line">got osdmap epoch <span class="number">142</span></span><br></pre></td></tr></table></figure></p>
<p>通过osdmap可以得到crushmap，fsid，osd，存储池，pg等信息</p>
<p>提取crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool /tmp/productosdmap --export-crush /tmp/productcrushmap</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'/tmp/productosdmap'</span></span><br><span class="line">osdmaptool: exported crush map to /tmp/productcrushmap</span><br></pre></td></tr></table></figure></p>
<p>拷贝到开发环境的机器上面</p>
<p>通过osdmap获取集群的fsid<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep fsid</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">fsid d153844c-<span class="number">16</span>f5-<span class="number">4</span>f48-<span class="number">829</span>d-<span class="number">87</span>fb49120bbe</span><br></pre></td></tr></table></figure></p>
<p>获取存储池相关的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep  pool</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">pool <span class="number">0</span> <span class="string">'rbd'</span> replicated size <span class="number">2</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">64</span> pgp_num <span class="number">64</span> last_change <span class="number">1</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>获取osd相关的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 tmp]<span class="comment"># osdmaptool --print productosdmap |grep  osd</span></span><br><span class="line">osdmaptool: osdmap file <span class="string">'productosdmap'</span></span><br><span class="line">flags sortbitwise,require_jewel_osds</span><br><span class="line">max_osd <span class="number">3</span></span><br><span class="line">osd.<span class="number">0</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">135</span> up_thru <span class="number">141</span> down_at <span class="number">127</span> last_clean_interval [<span class="number">23</span>,<span class="number">24</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6801</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6802</span>/<span class="number">28245</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6803</span>/<span class="number">28245</span> exists,up d8040272-<span class="number">7</span>afb-<span class="number">49</span>c0-bb78-<span class="number">9</span>ff13cf7d31b</span><br><span class="line">osd.<span class="number">1</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">140</span> up_thru <span class="number">141</span> down_at <span class="number">131</span> last_clean_interval [<span class="number">33</span>,<span class="number">130</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6808</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6809</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6810</span>/<span class="number">28698</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6811</span>/<span class="number">28698</span> exists,up c6ac4c7a-<span class="number">0227</span>-<span class="number">4</span>af4-ac3f-bd844b2480f8</span><br><span class="line">osd.<span class="number">2</span> up   <span class="keyword">in</span>  weight <span class="number">1</span> up_from <span class="number">137</span> up_thru <span class="number">141</span> down_at <span class="number">133</span> last_clean_interval [<span class="number">29</span>,<span class="number">132</span>) <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6804</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6805</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6806</span>/<span class="number">28549</span> <span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6807</span>/<span class="number">28549</span> exists,up <span class="number">2170260</span>b-bb05-<span class="number">4965</span>-baf2-<span class="number">12</span>d1c41b3ba0</span><br></pre></td></tr></table></figure></p>
<h3 id="构建新集群">构建新集群</h3><p>下载这个版本的源码<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS/ceph-<span class="number">10.2</span>.<span class="number">9</span>-<span class="number">0</span>.el7.src.rpm</span><br></pre></td></tr></table></figure></p>
<p>放到一台独立的机器上面</p>
<p>解压rpm包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 bianyi]<span class="comment"># rpm2cpio ceph-10.2.9-0.el7.src.rpm |cpio -div</span></span><br><span class="line">[root@lab8106 bianyi]<span class="comment"># tar -xvf ceph-10.2.9.tar.bz2</span></span><br></pre></td></tr></table></figure></p>
<p>编译环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ceph</span><br><span class="line">./install-deps.sh</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make -j <span class="number">12</span></span><br><span class="line"><span class="built_in">cd</span> src</span><br></pre></td></tr></table></figure></p>
<p>修改vstart.sh里面的fsid<br>启动集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./vstart.sh -n  --mon_num <span class="number">1</span> --osd_num <span class="number">3</span> --mds_num <span class="number">0</span>  --short  <span class="operator">-d</span></span><br></pre></td></tr></table></figure></p>
<p>检查集群状态：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf -s</span></span><br><span class="line">    cluster d153844c-<span class="number">16</span>f5-<span class="number">4</span>f48-<span class="number">829</span>d-<span class="number">87</span>fb49120bbe</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;a=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">3</span>, quorum <span class="number">0</span> a</span><br><span class="line">     osdmap e12: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v16: <span class="number">8</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">115</span> GB used, <span class="number">1082</span> GB / <span class="number">1197</span> GB avail</span><br><span class="line">                   <span class="number">8</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>导入crushmap<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd setcrushmap -i /root/rpmbuild/bianyi/productcrushmap </span></span><br><span class="line"><span class="built_in">set</span> crush map</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">26</span> <span class="number">14</span>:<span class="number">13</span>:<span class="number">29.052246</span> <span class="number">7</span>f19fd01d700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>设置PG<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pg_num <span class="number">64</span></span><br><span class="line">./ceph -c ceph.conf osd pool <span class="built_in">set</span> rbd pgp_num <span class="number">64</span></span><br></pre></td></tr></table></figure></p>
<p>模拟正式集群上的故障<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/osdmap.153__0_AC977A95__none  /tmp/</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># mv /var/lib/ceph/osd/ceph-0/current/meta/inc\\uosdmap.153__0_C67D77C2__none  /tmp/</span></span><br></pre></td></tr></table></figure></p>
<p>相当于无法读取这个osdmap和incmap了</p>
<p>尝试启动osd<br>设置debug_osd=20后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-osd@<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>检查日志<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/var/<span class="built_in">log</span>/ceph/ceph-osd.<span class="number">0</span>.log</span><br></pre></td></tr></table></figure></p>
<p><img src="http://static.zybuluo.com/zphj1987/bj2fr68otco4oh6lloy9j7ly/image.png" alt="image.png-56.9kB"></p>
<p>可以看到153 epoch的osdmap是有问题的，那么我们需要的就是这个版本的osdmap</p>
<p>检查当前开发集群的osdmap的版本<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e18: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>那么先快速把osdmap版本提高到153附近，这里我选择120<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd thrash 120</span></span><br><span class="line">will thrash map <span class="keyword">for</span> <span class="number">120</span> epochs</span><br></pre></td></tr></table></figure></p>
<p>检查快速变化后的osdmap epoch<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e138: <span class="number">3</span> osds: <span class="number">2</span> up, <span class="number">1</span> <span class="keyword">in</span>; <span class="number">64</span> remapped pgs</span><br></pre></td></tr></table></figure></p>
<p>做了上面的thrash后，集群的osd会是比较乱的，比如我的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-<span class="number">1</span> <span class="number">0.80338</span> root default                                       </span><br><span class="line">-<span class="number">2</span> <span class="number">0.80338</span>     host lab8107                                   </span><br><span class="line"> <span class="number">0</span> <span class="number">0.26779</span>         osd.<span class="number">0</span>         up        <span class="number">0</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">1</span> <span class="number">0.26779</span>         osd.<span class="number">1</span>       down        <span class="number">0</span>          <span class="number">1.00000</span> </span><br><span class="line"> <span class="number">2</span> <span class="number">0.26779</span>         osd.<span class="number">2</span>         up  <span class="number">1.00000</span>          <span class="number">1.00000</span> </span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">43</span>:<span class="number">24.817177</span> <span class="number">7</span>fbcc7cdb700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>做下恢复，启动下相关osd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd reweight 0 1</span></span><br><span class="line">reweighted osd.<span class="number">0</span> to <span class="number">1</span> (<span class="number">10000</span>)</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">01.439009</span> <span class="number">7</span>f56c147b700  <span class="number">0</span> lockdep stop</span><br><span class="line">[root@lab8106 src]<span class="comment"># ./ceph -c ceph.conf osd reweight 1 1</span></span><br><span class="line">reweighted osd.<span class="number">1</span> to <span class="number">1</span> (<span class="number">10000</span>)</span><br><span class="line"><span class="number">2017</span>-<span class="number">09</span>-<span class="number">27</span> <span class="number">09</span>:<span class="number">45</span>:<span class="number">04.020686</span> <span class="number">7</span>fea3345c700  <span class="number">0</span> lockdep stop</span><br></pre></td></tr></table></figure></p>
<p>注意提取下开发集群上面新生成的osdmap的文件（多次执行以免刷掉了）<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment">#rsync -qvzrtopg   dev/osd0/current/meta/ /root/meta/</span></span><br></pre></td></tr></table></figure></p>
<p>重启一遍开发集群<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># ./vstart.sh   --mon_num 1 --osd_num 3 --mds_num 0  --short  -d</span></span><br></pre></td></tr></table></figure></p>
<p>注意这里少了一个参数 -n,n是重建集群，这里我们只需要重启即可<br>再次检查<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osdmap e145: <span class="number">3</span> osds: <span class="number">3</span> up, <span class="number">3</span> <span class="keyword">in</span></span><br></pre></td></tr></table></figure></p>
<p>还是不够，不够的时候就执行上面的这个多次即可，一直到epoch到满足即可</p>
<p>将得到的osdmap拷贝到无法启动的osd的主机上面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 src]<span class="comment"># scp /root/meta/osdmap.153__0_AC977A95__none 192.168.8.107:/root</span></span><br><span class="line">osdmap.<span class="number">153</span>__0_AC977A95__none                            <span class="number">100</span>% <span class="number">2824</span>     <span class="number">2.8</span>KB/s   <span class="number">00</span>:<span class="number">00</span>    </span><br><span class="line">[root@lab8106 src]<span class="comment"># scp /root/meta/inc\\uosdmap.153__0_C67D77C2__none 192.168.8.107:/root</span></span><br><span class="line">inc\uosdmap.<span class="number">153</span>__0_C67D77C2__none                       <span class="number">100</span>%  <span class="number">198</span>     <span class="number">0.2</span>KB/s   <span class="number">00</span>:<span class="number">00</span></span><br></pre></td></tr></table></figure></p>
<p>拷贝到osdmap的路径下面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># cp /root/osdmap.153__0_AC977A95__none ./</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># cp /root/inc\\uosdmap.153__0_C67D77C2__none ./</span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># chown ceph:ceph osdmap.153__0_AC977A95__none </span></span><br><span class="line">[root@lab8107 meta]<span class="comment"># chown ceph:ceph inc\\uosdmap.153__0_C67D77C2__none</span></span><br></pre></td></tr></table></figure></p>
<p>启动并且观测<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8107 meta]<span class="comment"># systemctl start ceph-osd@0</span></span><br><span class="line">[root@lab8107 meta]<span class="comment">#tailf /var/log/ceph/ceph-osd.0.log</span></span><br></pre></td></tr></table></figure></p>
<p>检查集群状态，可以看到已经可以启动了</p>
<h2 id="总结">总结</h2><p>一般来说，出问题的时候都会说一句，如果备份了，就没那多事情，在一套生产环境当中，可以考虑下，什么是可以备份的，备份对环境的影响大不大，这种关键数据，并且可以全局共用，数据量也不大的数据，就需要备份好，比如上面的osdmap就可以在一个osd节点上面做一个实时的备份，或者短延时备份</p>
<p>本篇讲的是已经没有备份的情况下的做的一个恢复，掉电不是没有可能发生，至少解决了一个在osdmap无法找回的情况下的恢复办法</p>
<p>当然这里如果能够通过直接基于最新的osdmap和incmap做一定的解码，修改，编码，这样的方式应该也是可行的，这个就需要有一定的开发基础了，如果后面有找到这个方法会补充进本篇文章</p>
<p>你备份osdmap了么？</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-27</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/recuva.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>本篇讲述的是一个比较极端的故障的恢复场景，在整个集群全部服务器突然掉电的时候，osd里面的osdmap可能会出现没刷到磁盘上的情况，这个时候osdmap的最新版本为空或者为没有这个文件</p>
<p>还有一种情况就是机器宕机了，没有马上处理，等了一段时间以后，服务器机器启动了起来，而这个时候osdmap已经更新了，全局找不到需要的旧版本的osdmap和incmap，osd无法启动</p>
<p>一般情况下能找到的就直接从其他osd上面拷贝过来，然后就可以启动了，本篇讲述的是无法启动的情况<br>]]>
    
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[Luminous监控界面中文语言包]]></title>
    <link href="http://www.zphj1987.com/2017/09/13/maybe-the-first-chinese-for-luminous-dashboard/"/>
    <id>http://www.zphj1987.com/2017/09/13/maybe-the-first-chinese-for-luminous-dashboard/</id>
    <published>2017-09-13T09:30:32.000Z</published>
    <updated>2017-09-13T10:11:06.694Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/china.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前有各种ceph的管理平台，在部署方面大部分都比较麻烦，现在在luminous版本当中有一个原生的dashboard，虽然目前这个只能看，但是从界面上面，从接口方面都是非常不错的一个版本</p>
<p>原生版本目前没有语言的选择，虽然IT方面都是推荐用英语去做，但是在数据展示方面因为毕竟是要人来看，所以这里做了一个中文的语言包，方便转换成中文的界面，这个语言包是跟着ceph版本走的，因为界面可能会调整，所以只能一一匹配，同时提供了原版语言包，可以方便的回退回去，如果版本有更新以最后一个链接为准</p>
<p>如果有翻译的建议，欢迎在下面留言，或者其他方式告知我<br><a id="more"></a></p>
<h2 id="语言包">语言包</h2><h3 id="ceph版本（ceph_version_12-2-0_(32ce2a3ae5239ee33d6150705cdb24d43bab910c)_luminous_(rc)">ceph版本（ceph version 12.2.0 (32ce2a3ae5239ee33d6150705cdb24d43bab910c) luminous (rc)</h3><p>中文包：</p>
<p><a href="http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-chinese-12.2.0-1.0-1.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-chinese-12.2.0-1.0-1.x86_64.rpm</a></p>
<p>英文原版包：<br><a href="http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-english-12.2.0-1.0-1.x86_64.rpm" target="_blank" rel="external">http://7xweck.com1.z0.glb.clouddn.com/dashboard/luminous-dashboard-english-12.2.0-1.0-1.x86_64.rpm</a></p>
<h3 id="安装方法">安装方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -Uvh  http://xxxxx.rpm --force</span><br></pre></td></tr></table></figure>
<h2 id="在线预览">在线预览</h2><p>为了方便看到效果，专门在本篇博客内放了一个预览，可以看看效果，数据是离线的，但是可以点击</p>
<div class="video-container"> <object><br><embed src="http://ow7obg32z.bkt.clouddn.com" <="" embed=""></object><br></div>

<h2 id="总结">总结</h2><p>一直有这个想法，花了点时间去实现，慢慢优化</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-13</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/china.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>之前有各种ceph的管理平台，在部署方面大部分都比较麻烦，现在在luminous版本当中有一个原生的dashboard，虽然目前这个只能看，但是从界面上面，从接口方面都是非常不错的一个版本</p>
<p>原生版本目前没有语言的选择，虽然IT方面都是推荐用英语去做，但是在数据展示方面因为毕竟是要人来看，所以这里做了一个中文的语言包，方便转换成中文的界面，这个语言包是跟着ceph版本走的，因为界面可能会调整，所以只能一一匹配，同时提供了原版语言包，可以方便的回退回去，如果版本有更新以最后一个链接为准</p>
<p>如果有翻译的建议，欢迎在下面留言，或者其他方式告知我<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[怎样禁止Ceph OSD的自动挂载]]></title>
    <link href="http://www.zphj1987.com/2017/09/07/how-to-disable-Ceph-OSD-automount/"/>
    <id>http://www.zphj1987.com/2017/09/07/how-to-disable-Ceph-OSD-automount/</id>
    <published>2017-09-06T16:29:55.000Z</published>
    <updated>2017-09-06T16:31:34.346Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mount.png" alt="此处输入图片的描述"><br></center>

<h2 id="前言">前言</h2><p>本篇来源于群里一个人的问题，有没有办法让ceph的磁盘不自动挂载，一般人的问题都是怎样让ceph能够自动挂载，在centos 7 平台下 ceph jewel版本以后都是有自动挂载的处理的，这个我之前也写过两篇文章 <a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/" target="_blank" rel="external">ceph在centos7下一个不容易发现的改变</a>和<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/" target="_blank" rel="external">Ceph数据盘怎样实现自动挂载</a>，来讲述这个自动挂载的<br><a id="more"></a><br>这里讲下流程：</p>
<blockquote>
<p>开机后udev匹配95-ceph-osd.rules规则，触发ceph-disk  trigger，遍历磁盘，匹配到磁盘的标记后就触发了自动挂载</p>
</blockquote>
<p>为什么要取消挂载？<br>也许一般都会想：不就是停掉osd，然后umount掉，检查磁盘吗<br>这个想法如果放在一般情况下都没有问题，但是为什么有这个需求就是有不一般的情况，这个我在很久前遇到过，所以对这个需求的场景比较清楚</p>
<p>在很久以前碰到过一次，机器启动都是正常的，但是只要某个磁盘一挂载，机器就直接挂掉了，所以这个是不能让它重启机器自动挂载的，也许还有其他的情况，这里总结成一个简单的需求就是不想它自动挂载</p>
<h2 id="解决方法">解决方法</h2><p>从上面的自启动后的自动挂载流程里面，我们可以知道这里可以有两个方案去解决这个问题，第一种是改变磁盘的标记，第二种就是改变udev的rule的规则匹配，这里两个方法都行，一个是完全不动磁盘，一个是动了磁盘的标记</p>
<h3 id="修改udev规则的方式">修改udev规则的方式</h3><p>这个因为曾经有一段时间看过udev相关的一些东西，所以处理起来还是比较简单的，这里顺便把调试过程也记录下来<br>/lib/udev/rules.d/95-ceph-osd.rules这个文件里面就是集群自动挂载的触发规则，所以在这里我们在最开始匹配上我们需要屏蔽的盘，然后绕过内部的所有匹配规则，具体办法就是<br>在这个文件里面第一行加上</p>
<blockquote>
<p>KERNEL==”sdb1|sdb2”, GOTO=”not_auto_mount”</p>
</blockquote>
<p>在最后一行加上</p>
<blockquote>
<p>LABEL=”not_auto_mount”</p>
</blockquote>
<p>验证规则是否正确<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">udevadm <span class="built_in">test</span> /sys/block/sdb/sdb1</span><br></pre></td></tr></table></figure></p>
<p>我们先看下正常的可以挂载的盘符的触发测试显示<br><img src="http://static.zybuluo.com/zphj1987/4fuopv2z3ys36e3462svo72t/image.png" alt="image.png-17.2kB"><br>再看下屏蔽了后的规则是怎样的<br><img src="http://static.zybuluo.com/zphj1987/3phv4b3x8d2nf6mhaio68zk4/image.png" alt="image.png-16kB"><br>可以看到在加入屏蔽条件以后，就没有触发挂载了，这里要注意，做屏蔽规则的时候需要把这个osd相关的盘都屏蔽，不然在触发相关分区的时候可能顺带挂载起来了，上面的sdb1就是数据盘，sdb2就是bluestore的block盘</p>
<p>测试没问题后就执行下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">udevadm control --reload-rules</span><br></pre></td></tr></table></figure></p>
<p>重启后验证是否自动挂载了</p>
<h3 id="修改磁盘标记的方式">修改磁盘标记的方式</h3><p>查询磁盘的标记typecode,也就是ID_PART_ENTRY_TYPE这个属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># blkid -o udev -p /dev/sdb1</span></span><br><span class="line">ID_FS_UUID=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_UUID_ENC=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_TYPE=xfs</span><br><span class="line">ID_FS_USAGE=filesystem</span><br><span class="line">ID_PART_ENTRY_SCHEME=gpt</span><br><span class="line">ID_PART_ENTRY_NAME=ceph\x20data</span><br><span class="line">ID_PART_ENTRY_UUID=<span class="number">7</span>b321ca3-<span class="number">402</span>c-<span class="number">4557</span>-b121-<span class="number">887266</span>a1e1b8</span><br><span class="line">ID_PART_ENTRY_TYPE=<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff05d</span><br><span class="line">ID_PART_ENTRY_NUMBER=<span class="number">1</span></span><br><span class="line">ID_PART_ENTRY_OFFSET=<span class="number">2048</span></span><br><span class="line">ID_PART_ENTRY_SIZE=<span class="number">204800</span></span><br><span class="line">ID_PART_ENTRY_DISK=<span class="number">8</span>:<span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p>匹配到这个属性就认为是集群的节点，可以挂载的，那么我们先改变这个<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff0f9 -- /dev/sdb</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># blkid -o udev -p /dev/sdb1</span></span><br><span class="line">ID_FS_UUID=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_UUID_ENC=<span class="number">7</span>a852eec-b32d-<span class="number">4</span>c0a-<span class="number">8</span>b8e-<span class="number">1</span>e056a67ee35</span><br><span class="line">ID_FS_TYPE=xfs</span><br><span class="line">ID_FS_USAGE=filesystem</span><br><span class="line">ID_PART_ENTRY_SCHEME=gpt</span><br><span class="line">ID_PART_ENTRY_NAME=ceph\x20data</span><br><span class="line">ID_PART_ENTRY_UUID=<span class="number">7</span>b321ca3-<span class="number">402</span>c-<span class="number">4557</span>-b121-<span class="number">887266</span>a1e1b8</span><br><span class="line">ID_PART_ENTRY_TYPE=<span class="number">4</span>fbd7e29-<span class="number">9</span>d25-<span class="number">41</span>b8-afd0-<span class="number">062</span>c0ceff0f9</span><br><span class="line">ID_PART_ENTRY_NUMBER=<span class="number">1</span></span><br><span class="line">ID_PART_ENTRY_OFFSET=<span class="number">2048</span></span><br><span class="line">ID_PART_ENTRY_SIZE=<span class="number">204800</span></span><br><span class="line">ID_PART_ENTRY_DISK=<span class="number">8</span>:<span class="number">16</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到type的属性已经被修改了<br>再次测试，可以看到已经不匹配了<br><img src="http://static.zybuluo.com/zphj1987/ek3ocgg9w584u07x0pg8lqc0/image.png" alt="image.png-14.1kB"></p>
<p>如果需要恢复就执行<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># /usr/sbin/sgdisk --typecode=1:4fbd7e29-9d25-41b8-afd0-062c0ceff05d -- /dev/sdb</span></span><br></pre></td></tr></table></figure></p>
<p>这里同样需要改掉相关的block盘的标记，否则一样被关联的挂载起来了</p>
<h2 id="总结">总结</h2><p>本篇用两种方法来实现了ceph osd的盘符的不自动挂载，这个一般情况下都不会用到，比较特殊的情况遇到了再这么处理就可以了，或者比较暴力的方法就是直接把挂载的匹配的规则全部取消掉，使用手动触发挂载的方式也行，这个方法很多，能够快速，简单的满足需求即可</p>
<p>此mount非彼mount，题图无关</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-07</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/mount.png" alt="此处输入图片的描述"><br></center>

<h2 id="前言">前言</h2><p>本篇来源于群里一个人的问题，有没有办法让ceph的磁盘不自动挂载，一般人的问题都是怎样让ceph能够自动挂载，在centos 7 平台下 ceph jewel版本以后都是有自动挂载的处理的，这个我之前也写过两篇文章 <a href="http://www.zphj1987.com/2016/03/31/ceph%E5%9C%A8centos7%E4%B8%8B%E4%B8%80%E4%B8%AA%E4%B8%8D%E5%AE%B9%E6%98%93%E5%8F%91%E7%8E%B0%E7%9A%84%E6%94%B9%E5%8F%98/">ceph在centos7下一个不容易发现的改变</a>和<a href="http://www.zphj1987.com/2016/12/22/Ceph%E6%95%B0%E6%8D%AE%E7%9B%98%E6%80%8E%E6%A0%B7%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%8C%82%E8%BD%BD/">Ceph数据盘怎样实现自动挂载</a>，来讲述这个自动挂载的<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph OSD服务失效自动启动控制]]></title>
    <link href="http://www.zphj1987.com/2017/09/06/Ceph-OSD-autorestart-when-fail/"/>
    <id>http://www.zphj1987.com/2017/09/06/Ceph-OSD-autorestart-when-fail/</id>
    <published>2017-09-06T04:32:41.000Z</published>
    <updated>2017-09-06T05:34:31.655Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/restart.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>服务器上面的服务会因为各种各样的原因失败，磁盘故障，权限问题，或者是服务过载引起超时，这些都可能引起</p>
<p>这个在ceph里面systemctl unit 默认有个on-fail restart,默认的可能并不适合所有的场景，所以自动化的服务应该是尽量去适配你手动处理的过程，手动怎么处理的，就怎么去设置<br><a id="more"></a></p>
<h2 id="启动分析">启动分析</h2><p>如果有osd失败了，一般上去会先启动一次，尽快让服务启动，然后去检查是否有故障，如果失败了，就开启调试日志，再次重启，在问题解决之前，是不会再启动了，所以这里我们的自动启动设置也这么设置</p>
<h2 id="参数配置">参数配置</h2><p>ceph的osd的启动配置在这个配置文件</p>
<blockquote>
<p>/usr/lib/systemd/system/ceph-osd@.service</p>
</blockquote>
<p>默认参数：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Restart=on-failure</span><br><span class="line">StartLimitInterval=<span class="number">30</span>min</span><br><span class="line">StartLimitBurst=<span class="number">30</span></span><br><span class="line">RestartSec=<span class="number">20</span>s</span><br></pre></td></tr></table></figure></p>
<p>默认的参数意思是<br>在30min的周期内，如果没启动成功，那么在失败后20s进行启动，这样的启动尝试30次</p>
<p>这个在启动机器的时候，是尽量在osd启动失败的情况下，能够在30min分钟内尽量把服务都启动起来，这个对于关机启动后的控制是没问题的</p>
<p>参数解释：<br>StartLimitInterval不能设置太小，在osd崩溃的情况里面有一种是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，所以RestartSec*StartLimitBurst 必须小于StartLimitInterval，否则可能出现无限重启的情况</p>
<p>restart的触发条件</p>
<table>
<thead>
<tr>
<th style="text-align:left">Restart settings/Exit causes</th>
<th style="text-align:center">always</th>
<th style="text-align:center">on-success</th>
<th style="text-align:center">on-failure</th>
<th style="text-align:center">on-abnormal</th>
<th style="text-align:center">on-abort</th>
<th style="text-align:center">on-watchdog</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Clean exit code or signal</td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Unclean exit code</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Unclean signal</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Timeout</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:left">Watchdog</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
<td style="text-align:center">X</td>
<td style="text-align:center"></td>
<td style="text-align:center">X</td>
</tr>
</tbody>
</table>
<p>可调整项目<br>Restart=always就是只要非正常的退出了，就满足重启的条件，kill -9 进程也能够自动启动</p>
<p>在osd崩溃的情况里面有一种情况是对象异常了，这个在启动了后，内部会加载一段时间的数据以后才会崩溃，这种崩溃的情况我们不需要尝试多次重启,所以适当降低重启频率<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">StartLimitBurst=<span class="number">3</span></span><br><span class="line">RestartSec=<span class="number">10</span>s</span><br></pre></td></tr></table></figure></p>
<p>这个设置后能够在运行的集群当中比较好的处理异常退出的情况，但是设置后就要注意关机osd osd启动的问题，一般关机的时候肯定是有人在维护的，所以这个问题不大，人为处理下就行</p>
<p>所以建议的参数是</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Restart=always</span><br><span class="line">StartLimitInterval=<span class="number">30</span>min</span><br><span class="line">StartLimitBurst=<span class="number">3</span></span><br><span class="line">RestartSec=<span class="number">10</span>s</span><br></pre></td></tr></table></figure>
<p>可以根据自己的需要进行设置，这个设置下，停止osd就用systemctl 命令去 stop，然后其他的任何异常退出情况都会把osd给拉起来</p>
<h2 id="总结">总结</h2><p>systemctl在服务控制方面有着很丰富的功能，可以根据自己的需求进行调整，特别是对启动条件有约束的场景，这个是最适合的</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2017-09-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/restart.png" alt=""><br></center>

<h2 id="前言">前言</h2><p>服务器上面的服务会因为各种各样的原因失败，磁盘故障，权限问题，或者是服务过载引起超时，这些都可能引起</p>
<p>这个在ceph里面systemctl unit 默认有个on-fail restart,默认的可能并不适合所有的场景，所以自动化的服务应该是尽量去适配你手动处理的过程，手动怎么处理的，就怎么去设置<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
</feed>
