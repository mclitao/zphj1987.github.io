<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-09-20T08:18:11.845Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Ceph的Mon数据重新构建工具]]></title>
    <link href="http://www.zphj1987.com/2016/09/20/Ceph%E7%9A%84Mon%E6%95%B0%E6%8D%AE%E9%87%8D%E6%96%B0%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/"/>
    <id>http://www.zphj1987.com/2016/09/20/Ceph的Mon数据重新构建工具/</id>
    <published>2016-09-20T08:09:53.000Z</published>
    <updated>2016-09-20T08:18:11.845Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心这个mon万一三个同时都挂掉了，那么集群所有的数据是不是都丢了，关于真实数据恢复的，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，ceph的中国的一位开发者 <a href="https://github.com/tchaikov" target="_blank" rel="external">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到ceph的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292" target="_blank" rel="external">issue</a></p>
<a id="more"></a>
<h2 id="打包一个合进新代码的master版本的ceph包">打包一个合进新代码的master版本的ceph包</h2><h3 id="从github上面获取代码">从github上面获取代码</h3><p>默认的分支就是master的直接去clone就可以了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># git clone https://github.com/ceph/ceph.git</span></span><br></pre></td></tr></table></figure></p>
<h3 id="检查是否是master分支">检查是否是master分支</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># git branch</span></span><br><span class="line">* master</span><br></pre></td></tr></table></figure>
<h3 id="检查代码是否是合进需要的代码了">检查代码是否是合进需要的代码了</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat ceph/doc/rados/troubleshooting/troubleshooting-mon.rst |grep rebuild</span></span><br><span class="line">  <span class="comment"># rebuild the monitor store from the collected map, if the cluster does not</span></span><br><span class="line">  <span class="comment"># i.e. use "ceph-monstore-tool /tmp/mon-store rebuild" instead</span></span><br><span class="line">  ceph-monstore-tool /tmp/mon-store rebuild -- --keyring /path/to/admin.keyring</span><br><span class="line"><span class="comment">#. then rebuild the store</span></span><br></pre></td></tr></table></figure>
<p>因为这个代码是最近才合进去的 ，所以一定要检查代码的正确性</p>
<h3 id="创建一个源码包">创建一个源码包</h3><p>进入到代码的根目录，修改make-dist文件里面的一个地方(第46行)，否则打出来的包可能没有版本号，因为打包的时候检查了有没有git目录<br>修改下面<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#tar cvf $outfile.version.tar $outfile/src/.git_version $outfile/src/ceph_ver.h $outfile/ceph.spec</span></span><br><span class="line">tar cvf <span class="variable">$outfile</span>.version.tar <span class="variable">$outfile</span>/src/.git_version <span class="variable">$outfile</span>/src/ceph_ver.h <span class="variable">$outfile</span>/ceph.spec <span class="variable">$outfile</span>/.git</span><br></pre></td></tr></table></figure></p>
<h4 id="如果不改，就可能出现">如果不改，就可能出现</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph -v</span></span><br><span class="line">ceph version HEAD-HASH-NOTFOUND (GITDIR-NOTFOUND)</span><br></pre></td></tr></table></figure>
<h4 id="创建源码包">创建源码包</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment">#cd ceph</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment">#./make-dist</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp ceph-11.0.0-2460-g22053d0.tar.bz2 /root/rpmbuild/SOURCES/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># cp -f ceph.spec /root/rpmbuild/SPECS/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># rpmbuild -bb /root/rpmbuild/SPECS/ceph.spec</span></span><br></pre></td></tr></table></figure>
<p>执行完了以后就去这个路径取包<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ll /root/rpmbuild/RPMS/x86_64/</span></span><br><span class="line">total <span class="number">1643964</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root      <span class="number">1972</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">42259096</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">32</span> ceph-base-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">320843080</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">35</span> ceph-common-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">58138088</span> Sep <span class="number">20</span> <span class="number">10</span>:<span class="number">36</span> ceph-mds-<span class="number">11.0</span>.<span class="number">0</span>-<span class="number">2460</span>.g22053d0.el7.centos.x86_64.rpm</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<h3 id="准备测试环境">准备测试环境</h3><p>使用打好的包进行集群的配置，创建一个正常的集群</p>
<h4 id="模拟mon损坏">模拟mon损坏</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># systemctl stop ceph-mon@lab8106</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># mv /var/lib/ceph/mon/ceph-lab8106/  /var/lib/ceph/mon/ceph-lab8106bk</span></span><br></pre></td></tr></table></figure>
<p>按上面的操作以后，mon的数据相当于全部丢失了，本测试环境是单mon的，多mon原来一样</p>
<h4 id="重构数据">重构数据</h4><p>创建一个临时目录,停止掉所有的osd，这个地方因为mon已经完全挂掉了,所以停止所有osd也没什么大的影响了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># mkdir /tmp/mon-store</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-0/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-1/ --op update-mon-db --mon-store-path /tmp/mon-store/</span></span><br></pre></td></tr></table></figure></p>
<p>注意如果有多台机器，那么一台台的进行上面的操作，这个目录的数据要保持递增的，也就是一只对着这个目录弄，假如换了一台机器那么先把这个数据传递到另外一台机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8106 ~]<span class="comment"># rsync -avz /tmp/mon-store 192.168.8.107:/tmp/mon-store</span></span><br><span class="line">sending incremental file list</span><br><span class="line">created directory /tmp/mon-store</span><br><span class="line">mon-store/</span><br><span class="line">mon-store/kv_backend</span><br><span class="line">mon-store/store.db/</span><br><span class="line">mon-store/store.db/<span class="number">000005</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000008</span>.sst</span><br><span class="line">mon-store/store.db/<span class="number">000009</span>.log</span><br><span class="line">mon-store/store.db/CURRENT</span><br><span class="line">mon-store/store.db/LOCK</span><br><span class="line">mon-store/store.db/MANIFEST-<span class="number">000007</span></span><br><span class="line"></span><br><span class="line">sent <span class="number">11490</span> bytes  received <span class="number">153</span> bytes  <span class="number">7762.00</span> bytes/sec</span><br><span class="line">total size is <span class="number">74900</span>  speedup is <span class="number">6.43</span></span><br></pre></td></tr></table></figure></p>
<p>等192.168.8.106的机器全部做完了，然后这个/tmp/mon-store传递到了192.168.8.107的机器上，然后再开始做192.168.8.107这台机器的，等全部做外了，把这个/tmp/mon-store弄到需要恢复mon的机器上</p>
<h3 id="根据获得的数据进行重构">根据获得的数据进行重构</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># mkdir /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-monstore-tool /tmp/mon-store rebuild</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># cp -ra /tmp/mon-store/* /var/lib/ceph/mon/ceph-lab8106</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/done</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># touch /var/lib/ceph/mon/ceph-lab8106/systemd</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph -R /var/lib/ceph/mon/</span></span><br></pre></td></tr></table></figure>
<h3 id="启动mon">启动mon</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-mon@lab8106</span></span><br></pre></td></tr></table></figure>
<p>检查状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到可以好了，在实践过程中，发现如果对修复的数据，马上进行破坏，再次进行修复的时候，就无法恢复了，应该是个bug，已经提交给作者 Issue:<a href="https://github.com/ceph/ceph/pull/11126" target="_blank" rel="external">11226</a></p>
<h2 id="总结">总结</h2><p>因为工具才出来，可能难免有些bug，这个是为未来提供一种恢复数据的方式，使得 Ceph 变得更加的健壮</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rebuild/rebuild-shot.png" alt="rebuild"><br></center><br>关于mon的数据的问题，一般正常情况下都是配置的3个mon的，但是还是有人会担心这个mon万一三个同时都挂掉了，那么集群所有的数据是不是都丢了，关于真实数据恢复的，有去后台取对象，然后一个个拼接起来的方案，这个是确定可以成功的，但是这个方法对于生产的集群耗时巨大，并且需要导出数据，然后又配置新的集群，工程比较耗大，考虑到这个问题，ceph的中国的一位开发者 <a href="https://github.com/tchaikov">tchaikov</a> 就写了一个新的工具，来对损坏的MON的数据进行原集群的重构，这个比起其他方案要好很多，本篇将讲述怎么使用这个工具，代码已经合并到ceph的master分支当中去了</p>
<p>关于这个工具相关的<a href="http://tracker.ceph.com/issues/17292">issue</a></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[替换OSD操作的优化与分析]]></title>
    <link href="http://www.zphj1987.com/2016/09/19/%E6%9B%BF%E6%8D%A2OSD%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BC%98%E5%8C%96%E4%B8%8E%E5%88%86%E6%9E%90/"/>
    <id>http://www.zphj1987.com/2016/09/19/替换OSD操作的优化与分析/</id>
    <published>2016-09-19T02:56:54.000Z</published>
    <updated>2016-09-19T03:07:09.282Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/" target="_blank" rel="external">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br><a id="more"></a></p>
<h2 id="二、测试三种方法">二、测试三种方法</h2><h3 id="方法一：首先_out_一个_OSD，然后剔除_OSD，然后增加_OSD">方法一：首先 out 一个 OSD，然后剔除 OSD，然后增加 OSD</h3><ol>
<li>停止指定 OSD 进程</li>
<li>out 指定 OSD</li>
<li>crush remove 指定 OSD</li>
<li>增加一个新的 OSD</li>
</ol>
<p>一般生产环境会设置为 <code>noout</code>，当然不设置也可以，那就交给程序去控制节点的 out，默认是在进程停止后的五分钟，总之这个地方如果有 out 触发，不管是人为触发，还是自动触发数据流是一定的，我们这里为了便于测试，使用的是人为触发，上面提到的预制环境就是设置的 <code>noout</code></p>
<p>开始测试前获取最原始的分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布,保存到文件pg1.txt，这个 PG 分布记录是 PG 所在的 OSD，记录下来，方便后面进行比较，从而得出需要迁移的数据 </p>
<h4 id="停止指定的_OSD_进程">停止指定的 OSD 进程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@15</span></span><br></pre></td></tr></table></figure>
<p>停止进程并不会触发迁移，只会引起 PG 状态的变化，比如原来主 PG 在停止的 OSD 上，那么停止掉 OSD 以后，原来的副本的那个 PG 就会角色升级为主 PG 了</p>
<h4 id="out_掉一个_OSD">out 掉一个 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd out 15</span></span><br></pre></td></tr></table></figure>
<p>在触发 out 以前，当前的 PG 状态应该有 <code>active+undersized+degraded</code>,触发 out 以后，所有的 PG 的状态应该会慢慢变成 <code>active+clean</code>,等待集群正常后，再次查询当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg2.txt</span></span><br></pre></td></tr></table></figure></p>
<p>保存当前的 PG 分布为pg2.txt<br>比较 out 前后的 PG 的变化情况，下面是比较具体的变化情况，只列出变化的部分<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们关心的是变动的数目，只统计变动的 PG 的数目<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg1.txt pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">102</span></span><br></pre></td></tr></table></figure></p>
<p>第一次 out 以后有102个 PG 的变动,这个数字记住，后面的统计会用到</p>
<h4 id="从_crush_里面删除_OSD">从 crush 里面删除 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<p>crush 删除以后同样会触发迁移，等待 PG 的均衡，也就是全部变成 <code>active+clean</code> 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>获取当前的 PG 分布的状态<br>现在来比较 crush remove 前后的 PG 变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg2.txt pg3.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">137</span></span><br></pre></td></tr></table></figure></p>
<p>我们重新加上新的 OSD<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure></p>
<p>加完以后统计当前的新的 PG 状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; pg4.txt</span></span><br></pre></td></tr></table></figure></p>
<p>比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 pg3.txt pg4.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">167</span></span><br></pre></td></tr></table></figure></p>
<p>整个替换流程完毕，统计上面的 PG 总的变动</p>
<blockquote>
<p>102 +137 +167 = 406</p>
</blockquote>
<p>也就是按这个方法的变动为406个 PG，因为是只有双主机，里面可能存在某些放大问题，这里不做深入的讨论，因为我的三组测试环境都是一样的情况，只做横向比较，原理相通，这里是用数据来分析出差别</p>
<h3 id="方法二：先crush_reweight_0_，然后out，然后再增加osd">方法二：先crush reweight 0 ，然后out，然后再增加osd</h3><p>首先恢复环境为测试前的环境<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg1.txt</span></span><br></pre></td></tr></table></figure></p>
<p>记录最原始的 PG 分布情况</p>
<h4 id="crush_reweight_指定OSD">crush reweight 指定OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.16 0</span></span><br><span class="line">reweighted item id <span class="number">16</span> name <span class="string">'osd.16'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>等待平衡了以后记录当前的 PG 分布状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg2.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<p>比较前后的变动<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg1.txt 2pg2.txt  --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">166</span></span><br></pre></td></tr></table></figure></p>
<h4 id="crush_remove_指定_OSD">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush remove osd.16</span></span><br><span class="line">removed item id <span class="number">16</span> name <span class="string">'osd.16'</span> from crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为上面 crush 已经是0了所以删除也不会引起 PG 变动<br>然后直接 <code>ceph osd rm osd.16</code> 同样没有 PG 变动</p>
<h4 id="增加新的_OSD">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>等待平衡以后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 2pg3.txt</span></span><br></pre></td></tr></table></figure></p>
<p>来比较前后的变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 2pg2.txt 2pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">159</span></span><br></pre></td></tr></table></figure></p>
<p>总的 PG 变动为</p>
<blockquote>
<p>166+159=325</p>
</blockquote>
<h3 id="方法3：开始做norebalance，然后做crush_remove，然后做add">方法3：开始做norebalance，然后做crush remove，然后做add</h3><p>恢复环境为初始环境，然后获取当前的 PG 分布<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg1.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br></pre></td></tr></table></figure></p>
<h4 id="给集群做多种标记，防止迁移">给集群做多种标记，防止迁移</h4><p>设置为 norebalance，nobackfill，norecover,后面是有地方会解除这些设置的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norebalance</span></span><br><span class="line"><span class="built_in">set</span> norebalance</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set nobackfill</span></span><br><span class="line"><span class="built_in">set</span> nobackfill</span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set norecover</span></span><br><span class="line"><span class="built_in">set</span> norecover</span><br></pre></td></tr></table></figure></p>
<h4 id="crush_reweight_指定_OSD">crush reweight 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd crush reweight osd.15 0</span></span><br><span class="line">reweighted item id <span class="number">15</span> name <span class="string">'osd.15'</span> to <span class="number">0</span> <span class="keyword">in</span> crush map</span><br></pre></td></tr></table></figure>
<p>这个地方因为已经做了上面的标记，所以只会出现状态变化，而没有真正的迁移，我们也先统计一下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg2.txt</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg2.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">158</span></span><br></pre></td></tr></table></figure></p>
<p>注意这里只是计算了，并没有真正的数据变动，可以通过监控两台的主机的网络流量来判断,所以这里的变动并不用计算到需要迁移的 PG 数目当中</p>
<h4 id="crush_remove_指定_OSD-1">crush remove 指定 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph osd crush remove osd.15</span></span><br></pre></td></tr></table></figure>
<h4 id="删除指定的_OSD">删除指定的 OSD</h4><p>删除以后同样是没有 PG 的变动的<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph osd rm osd.<span class="number">15</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方有个小地方需要注意一下，不做 ceph auth del osd.15 把15的编号留着，这样好判断前后的 PG 的变化，不然相同的编号，就无法判断是不是做了迁移了</p>
<h4 id="增加新的_OSD-1">增加新的 OSD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd prepare lab8107:/dev/sdi</span></span><br><span class="line">[root@lab8106 ~]<span class="comment">#ceph-deploy osd activate lab8107:/dev/sdi1</span></span><br></pre></td></tr></table></figure>
<p>我的环境下，新增的 OSD 的编号为16了</p>
<h4 id="解除各种标记">解除各种标记</h4><p>我们放开上面的设置，看下数据的变动情况<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norebalance</span></span><br><span class="line"><span class="built_in">unset</span> norebalance</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset nobackfill</span></span><br><span class="line"><span class="built_in">unset</span> nobackfill</span><br><span class="line">[root@lab8106 ceph]<span class="comment"># ceph osd unset norecover</span></span><br><span class="line"><span class="built_in">unset</span> norecover</span><br></pre></td></tr></table></figure></p>
<p>设置完了后数据才真正开始变动了，可以通过观察网卡流量看到，来看下最终pg变化<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># ceph pg dump pgs|awk '&#123;print $1,$15&#125;'|grep -v pg   &gt; 3pg3.txt</span></span><br><span class="line">dumped pgs <span class="keyword">in</span> format plain</span><br><span class="line">[root@lab8106 ~]<span class="comment"># diff -y -W 100 3pg1.txt 3pg3.txt --suppress-common-lines|wc -l</span></span><br><span class="line"><span class="number">195</span></span><br></pre></td></tr></table></figure></p>
<p>这里我们只需要跟最开始的 PG 分布状况进行比较就可以了，因为中间的状态实际上都没有做数据的迁移，所以不需要统计进去，可以看到这个地方动了195个 PG<br>总共的 PG 迁移量为</p>
<blockquote>
<p>195</p>
</blockquote>
<h2 id="三、数据汇总">三、数据汇总</h2><p>现在通过表格来对比下三种方法的迁移量的比较(括号内为迁移 PG 数目)</p>
<table>
<thead>
<tr>
<th style="text-align:center">　</th>
<th style="text-align:left">方法一</th>
<th style="text-align:left">方法二</th>
<th style="text-align:left">方法三</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">所做操作</td>
<td style="text-align:left">stop osd (0)<br>out osd(102)<br>crush remove osd (137)<br> add osd(167)</td>
<td style="text-align:left">crush reweight osd(166)<br>out osd(0)<br>crush remove osd (0)<br>add osd(159)</td>
<td style="text-align:left">set 标记(0)<br>crush reweight osd(0)<br>crush remove osd (0)<br>add osd(195)</td>
</tr>
<tr>
<td style="text-align:center">PG迁移数量</td>
<td style="text-align:left">406</td>
<td style="text-align:left">325</td>
<td style="text-align:left">195</td>
</tr>
</tbody>
</table>
<p>可以很清楚的看到三种不同的方法，最终的触发的迁移量是不同的，处理的好的话，能节约差不多一半的迁移的数据量，这个对于生产环境来说还是很好的，关于这个建议先在测试环境上进行测试，然后再操作，上面的操作只要不对磁盘进行格式化，操作都是可逆的，也就是可以比较放心的做，记住所做的操作，每一步都做完都去检查 PG 的状态是否是正常的</p>
<h2 id="四、总结">四、总结</h2><p>从我自己的操作经验来看，最开始是用的第一种方法，后面就用第二种方法减少了一部分迁移量，最近看到资料写做剔除OSD的时候可以关闭迁移防止无效的过多的迁移，然后就测试了一下，确实能够减少不少的迁移量，这个减少在某些场景下还是很好的，当然如果不太熟悉，用哪一种都可以，最终能达到的目的是一样的</p>
<h2 id="五、变更记录">五、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-19</td>
</tr>
</tbody>
</table>
<h2 id="六、For_me">六、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/reolaceosd/terminal.png" alt="replaceosd"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>之前有写过一篇<a href="http://www.zphj1987.com/2016/01/12/%E5%88%A0%E9%99%A4osd%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E5%BC%8F/">删除OSD的正确方式</a>，里面只是简单的讲了下删除的方式怎样能减少迁移量，本篇属于一个扩展，讲述了 Ceph 运维当中经常出现的坏盘提换盘的步骤的优化</p>
<p>基础环境两台主机每台主机8个 OSD，一共 16 个 OSD，副本设置为2，PG 数设置为800，计算下来平均每个 OSD 上的 P G数目为100个，本篇将通过数据来分析不同的处理方法的差别</p>
<p>开始测试前先把环境设置为 <code>noout</code>，然后通过停止 OSD 来模拟 OSD 出现了异常，之后进行不同处理方法<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Centos7下Jewel版本radosgw服务启动]]></title>
    <link href="http://www.zphj1987.com/2016/09/12/Centos7%E4%B8%8BJewel%E7%89%88%E6%9C%ACradosgw%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/09/12/Centos7下Jewel版本radosgw服务启动/</id>
    <published>2016-09-12T05:47:47.000Z</published>
    <updated>2016-09-12T06:00:17.703Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>
<a id="more"></a>
<h2 id="二、预备环境">二、预备环境</h2><h3 id="一个完整的集群">一个完整的集群</h3><p>拥有一个正常的集群是需要提前准备好的，ceph -s检查正确的输出</p>
<h3 id="关闭各种auth">关闭各种auth</h3><p>这个地方也可以不关闭，注意配置好用户认证就可以了，这里关闭了，配置起来方便，我是从来不开的,也避免了新手不会配置用户造成认证的各种异常<br>关闭认证就是在ceph.conf里面添加下面字段<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">auth_cluster_required = none</span><br><span class="line">auth_service_required = none</span><br><span class="line">auth_client_required = none</span><br></pre></td></tr></table></figure></p>
<h3 id="安装ceph-radosgw的包">安装ceph-radosgw的包</h3><p>这个因为默认不会安装，所以要安装好<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install ceph-radosgw</span><br></pre></td></tr></table></figure></p>
<h2 id="三、默认启动过程">三、默认启动过程</h2><p>我们先什么都不配置，看下一般的会怎么处理</p>
<h3 id="3-1_启动服务">3.1 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart ceph-radosgw.target</span><br></pre></td></tr></table></figure>
<h3 id="3-2_检查服务的状态">3.2 检查服务的状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw.target </span></span><br><span class="line">● ceph-radosgw.target - ceph target allowing to start/stop all ceph-radosgw@.service instances at once</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw.target; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> CST; <span class="number">51</span>s ago</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Stopping ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">03</span> lab8106 systemd[<span class="number">1</span>]: Starting ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">13</span>:<span class="number">51</span> lab8106 systemd[<span class="number">1</span>]: Reached target ceph target allowing to start/stop all ceph-radosgw@.service instances at once.</span><br></pre></td></tr></table></figure>
<p>可以看到进程是启动的，没有任何异常</p>
<h3 id="3-3_检查端口是否启动">3.3 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br></pre></td></tr></table></figure>
<p>但是并没有生成任何端口，这个是因为还没有配置实例,这个地方就是新手经常卡住的地方</p>
<h2 id="四、下面开始配置默认单实例">四、下面开始配置默认单实例</h2><h3 id="4-1_写配置文件">4.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写一个最简配置文件<br>注意下面的client.radosgw1这个包起来的，这个是固定写法，在 <code>systemctl</code> 启动服务的时候 <code>@</code> 取后面的radosgw1<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="4-2_启动服务">4.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br></pre></td></tr></table></figure>
<h3 id="4-3_检查服务状态">4.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> CST; <span class="number">17</span>s ago</span><br><span class="line"> Main PID: <span class="number">19996</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">19996</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">17</span>:<span class="number">34</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="4-4_检查端口是否启动">4.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7480</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">19996</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到默认的端口是7480</p>
<h2 id="五、配置多个自定义端口实例">五、配置多个自定义端口实例</h2><h3 id="5-1_写配置文件">5.1 写配置文件</h3><p>在配置文件 /etc/ceph/ceph.conf的最下面写下配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[client.radosgw1]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7481</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">[client.radosgw2]</span><br><span class="line">host = lab8106</span><br><span class="line">rgw_frontends = civetweb port=<span class="number">7482</span></span><br><span class="line">rgw_content_length_compat = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>这个地方配置两个实例，用了不同的名称，用了不同的端口</p>
<h3 id="5-2_启动服务">5.2 启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw1</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-radosgw@radosgw2</span></span><br></pre></td></tr></table></figure>
<h3 id="5-3_检查服务状态">5.3 检查服务状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw1</span></span><br><span class="line">● ceph-radosgw@radosgw1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> CST; <span class="number">1</span>min <span class="number">4</span>s ago</span><br><span class="line"> Main PID: <span class="number">20509</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw1.service</span><br><span class="line">           └─<span class="number">20509</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">06</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br><span class="line">[root@lab8106 ~]<span class="comment"># systemctl status ceph-radosgw@radosgw2</span></span><br><span class="line">● ceph-radosgw@radosgw2.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon <span class="number">2016</span>-<span class="number">09</span>-<span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> CST; <span class="number">1</span>min <span class="number">3</span>s ago</span><br><span class="line"> Main PID: <span class="number">20696</span> (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@radosgw2.service</span><br><span class="line">           └─<span class="number">20696</span> /usr/bin/radosgw <span class="operator">-f</span> --cluster ceph --name client.radosgw2 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Started Ceph rados gateway.</span><br><span class="line">Sep <span class="number">12</span> <span class="number">13</span>:<span class="number">20</span>:<span class="number">09</span> lab8106 systemd[<span class="number">1</span>]: Starting Ceph rados gateway...</span><br></pre></td></tr></table></figure>
<h3 id="5-4_检查端口是否启动">5.4 检查端口是否启动</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># netstat -tunlp|grep radosgw</span></span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7481</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20509</span>/radosgw       </span><br><span class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">7482</span>            <span class="number">0.0</span>.<span class="number">0.0</span>:*               LISTEN      <span class="number">20696</span>/radosgw</span><br></pre></td></tr></table></figure>
<p>可以看到服务和端口都能正常的启动了</p>
<p>好了，关于centos7下jewel版本的radosgw配置的启动已经介绍完了，这里不涉及更多深入的东西，其他的东西可以参照其他文档配置即可，这个地方只是对启动服务这里专门的介绍一下</p>
<h2 id="六、总结">六、总结</h2><p>从上面的过程可以看出大致的流程如下</p>
<ul>
<li>安装软件</li>
<li>启动服务</li>
<li>检查服务状态</li>
<li>检查服务端口</li>
</ul>
<p>这些很多都是基础的做法，在centos7下面虽然比6做了一些改变，但是掌握了一些通用的排查方法后，是很容易举一反三的，因为看到有新手不熟悉启动，所以写下这篇文章，自己因为也没经常用，所以也写下当个笔记了</p>
<h2 id="七、For_me">七、For me</h2><p>如果本文有帮助到你，愿意欢迎进入<a href="http://www.zphj1987.com/payforask" target="_blank" rel="external">打赏</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/radosgw/gateway1.png" alt="rgw"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>本篇介绍了centos7下jewel版本的radosgw配置，这里的配置是指将服务能够正常起来，不涉及到S3的配置，以及其他的更多的配置,radosgw后面的gw就是gateway的意思，也就是我们说的网关的意思，本篇中所提及的实例也就是网关的意思，说实例是将每个单独的网关更细化一点的说法</p>
<p>很多人不清楚在centos7下面怎么去控制这个radosgw网关的服务的控制，这个地方是会去读取配置文件的，所以配置文件得写正确</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何统计Ceph的RBD真实使用容量]]></title>
    <link href="http://www.zphj1987.com/2016/09/08/%E5%A6%82%E4%BD%95%E7%BB%9F%E8%AE%A1Ceph%E7%9A%84RBD%E7%9C%9F%E5%AE%9E%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F/"/>
    <id>http://www.zphj1987.com/2016/09/08/如何统计Ceph的RBD真实使用容量/</id>
    <published>2016-09-08T09:17:08.000Z</published>
    <updated>2016-09-12T05:54:30.809Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>
<a id="more"></a>
<p>详细介绍</p>
<h3 id="2-1_方法一：使用rbd_du查询">2.1 方法一：使用rbd du查询</h3><p>这个参考我之前的文章：<a href="http://www.zphj1987.com/2016/03/24/ceph%E6%9F%A5%E8%AF%A2rbd%E7%9A%84%E4%BD%BF%E7%94%A8%E5%AE%B9%E9%87%8F%EF%BC%88%E5%BF%AB%E9%80%9F%EF%BC%89/" target="_blank" rel="external">查询rbd的使用容量</a></p>
<h3 id="2-2_方法二：使用rbd_diff">2.2 方法二：使用rbd diff</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd diff rbd/zp | awk '&#123; SUM += $2 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<h3 id="2-3_方法三：根据对象统计的方法进行统计">2.3 方法三：根据对象统计的方法进行统计</h3><p>这个是本篇着重介绍的一点，在集群非常大的时候，再去按上面的一个个的查询，需要花很长的时间，并且需要时不时的跟集群进行交互，这里采用的方法是把统计数据一次获取下来，然后进行数据的统计分析，从而获取结果，获取的粒度是以存储池为基准的</p>
<h4 id="拿到所有对象的信息">拿到所有对象的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<p>这个获取的时间长短是根据对象的多少来的，如果担心出问题，可以换个终端查看进度<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tail <span class="operator">-f</span>  obj.txt</span><br></pre></td></tr></table></figure></p>
<h4 id="获取RBD的镜像列表">获取RBD的镜像列表</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd ls</span></span><br><span class="line"><span class="built_in">test</span>1</span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<h3 id="获取RBD的镜像的prefix">获取RBD的镜像的prefix</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> `rbd -p rbd ls`;<span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$a</span> ;rbd -p rbd info <span class="variable">$a</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span> ;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="获取指定RBD镜像的大小">获取指定RBD镜像的大小</h3><p>查询 test1 的镜像大小<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># cat obj.txt |grep rbd_data.3ac16b8b4567|awk  '&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span></span><br><span class="line"><span class="number">4014.27</span> MB</span><br></pre></td></tr></table></figure></p>
<h3 id="将上面的汇总，使用脚本一次查询出所有的">将上面的汇总，使用脚本一次查询出所有的</h3><h4 id="第一步获取：">第一步获取：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> `rados -p rbd ls`;<span class="keyword">do</span> rados -p rbd <span class="built_in">stat</span> <span class="variable">$obj</span> &gt;&gt; obj.txt;<span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>
<h4 id="第二步计算：">第二步计算：</h4><p>创建一个获取的脚本getused.sh<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line"><span class="comment">##default pool name use rbd,you can change it </span></span><br><span class="line"><span class="comment">##default objfile is obj.txt,you can change it</span></span><br><span class="line">objfile=obj.txt</span><br><span class="line">Poolname=rbd</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> `rbd -p <span class="variable">$Poolname</span> ls`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">Imagename=<span class="variable">$image</span></span><br><span class="line">Prefix=`rbd  -p <span class="variable">$Poolname</span> info <span class="variable">$image</span>|grep prefix |awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line">Used=`cat <span class="variable">$objfile</span> |grep <span class="variable">$Prefix</span>|awk <span class="string">'&#123; SUM += $6 &#125; END &#123; print SUM/1024/1024 " MB" &#125;'</span>`</span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$Imagename</span> <span class="variable">$Prefix</span></span><br><span class="line"><span class="built_in">echo</span> Used: <span class="variable">$Used</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h4 id="我的输出如下：">我的输出如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sh getused.sh </span></span><br><span class="line"><span class="built_in">test</span>1 rbd_data.<span class="number">3</span>ac16b8b4567</span><br><span class="line">Used: <span class="number">4014.27</span> MB</span><br><span class="line">zp rbd_data.<span class="number">11</span>f66b8b4567</span><br><span class="line">Used: <span class="number">828.844</span> MB</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意这里只统计了image里面的真实容量，如果是用了快速clone的,存在容量复用的问题，需要自己看是否需要统计那一部分的对象，方法同上</p>
</blockquote>
<h2 id="三、总结">三、总结</h2><p>对于已存在的系统，并且数据量很大的系统，不要频繁的去做请求，最好把统计请求，集中起来，并且就单线程的处理，慢一点不要紧，然后拉取到数据后，慢慢处理，这样能把影响降低到最少，可以在最不忙的时候去进行相关的操作</p>
<h2 id="四、变更记录">四、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-08</td>
</tr>
</tbody>
</table>
<h2 id="五、For_me">五、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/rbdtongji/storage.png" alt="storage"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>ceph的rbd一直有个问题就是无法清楚的知道这个分配的空间里面到底使用了多少，这个在Jewel里面提供了一个新的接口去查询，对于老版本来说可能同样有这个需求，本篇将详细介绍如何解决这个问题</p>
<h2 id="二、查询的各种方法">二、查询的各种方法</h2><p>目前已知的有三种方法<br>1、使用rbd du查询（Jewel才支持）<br>2、使用rbd diff<br>3、根据对象统计的方法进行统计</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph中的Copyset概念和使用方法]]></title>
    <link href="http://www.zphj1987.com/2016/09/06/Ceph%E4%B8%AD%E7%9A%84Copyset%E6%A6%82%E5%BF%B5%E5%92%8C%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/09/06/Ceph中的Copyset概念和使用方法/</id>
    <published>2016-09-06T09:39:15.000Z</published>
    <updated>2016-09-07T03:31:05.791Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>
<a id="more"></a>
<h2 id="二、copyset的概念">二、copyset的概念</h2><p>首先我们要理解copyset的概念，用通俗的话说就是，包含一个数据的所有副本的节点，也就是一个copyset损坏的情况下，数据就是全丢的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomcopy.png" alt="radomcopy"><br></center><br>如上图所示，这里的copyset就是：<br>{1,5,6}，{2,6,8} 两组</p>
<p>如果不做特殊的设置，那么基本上就是会随机的去分布</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/radomall.png" alt="allcopy"><br></center></p>
<h3 id="2-1_最大copyset">2.1 最大copyset</h3><p>如上图的所示，一般来说，最终组合将是一个最大的随机组合，比如这样的一个9个node随机组合3个的，这样的组合数有：<br>从 n个元素中取出  k个元素， k个元素的组合数量为：</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/gongshi.png" alt="计算公式"><br></center><br>9个随机3个的组合为84<br>如果3个节点down掉，那么有数据丢失概率就是100%</p>
<h3 id="2-2_最小copyset">2.2 最小copyset</h3><p>如果存在一种情况，分布是这样的</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/mincopy.png" alt="mincopy"><br></center><br>那么copyset为<br>{1,5,7},{2,4,9},{3,6,8}<br>如果3个节点down掉,只有正好是上面的3种组合中的一种出现的时候，才会出现数据丢失<br>那么数据丢失的概率为 3/84</p>
<p>最小copyset可能带来的不好的地方</p>
<ul>
<li>真出现丢失的时候（概率极低），丢失的数据量将是最大化的，这个是因为出现丢的时候，那么三个上面的组合配对为100%，其他情况不是100%</li>
<li>失效恢复时间将会增大一些，根据facebook的报告100GB的39节点的HDFS随机分布恢复时间在60s,最小分布为700s，这个是因为可用于恢复的点相对减少了，恢复时间自然长了</li>
</ul>
<h3 id="2-3_比较好的处理方式">2.3 比较好的处理方式</h3><p>比较好的方式就是取copyset值为介于纯随机和最小之间的数，那么失效的概率计算方式就是：</p>
<blockquote>
<p>当前的copyset数目/最大copyset</p>
</blockquote>
<h2 id="三、这个概念在ceph当中的实现">三、这个概念在ceph当中的实现</h2><p>其实这个概念在ceph当中就是bucket的概念，PG为最小故障单元，PG就可以理解为上图当中的node上的元素，默认的分组方式为host，这个copyset就是全随机的在这些主机当中进行组合，我们在提升故障域为rack的时候，实际上就是将copyset进行了减少，一个rack之内的主机是形成不了copyset，这样down掉rack的时候，就不会数据丢失了，这个地方的实际可以做的控制方式有三种，下面将详细的介绍三种模式</p>
<h3 id="3-1、缩小最小主机单位">3.1、缩小最小主机单位</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hostzu.png" alt="最小主机组"><br></center><br>默认的为主机组，这样的主机间的copyset为<br>{1,2}，{1,3}，{1,4}，{2,3}，{2,4}，{3,4}<br>这样的有六组</p>
<p>现在我们对host进行一个合并看下</p>
<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/hebing.png" alt="此处输入图片的描述"><br></center><br>注意这个地方并不是往上加了一层bucket，而是把最底层的host给拆掉了，加入一台机器有24个osd，那么这里的vhost1里面的osd个数实际是48个osd，那么当前的copyset为<br>{vhost1,vhost2}<br>copyset已经为上面默认情况的1/6<br>这样会带来两个好处</p>
<ul>
<li>减少了copyset，减少的好处就见上面的分析</li>
<li>增加可接收恢复的osd数目，之前坏了一个osd的时候，能接收数据的osd为n-1,那么现在坏一个osd，可接收的osd为2n-1(n为单node上的osd个数)</li>
</ul>
<h3 id="3-2、增加分组">3.2、增加分组</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/rackfenzu.png" alt="rack分组"><br></center><br>这个地方是增加了rack分组的，同一个rack里面不会出现copyset，那么当前的模式的copyset就是<br>{1,3}，{1,4}，{2,3}，{2,4}</p>
<p>同没有处理相比copyset为4/6</p>
<h3 id="3-3、增加分组的情况进行PG分流">3.3、增加分组的情况进行PG分流</h3><p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/zone.png" alt="zone"><br></center><br>这里看上去跟上面的分组很像，但是在做crush的时候是有区别的，上面的分组以后，会让PG分布在两个rack当中，这里的crush写的时候会让PG只在一个zone当中，在进入zone的下层再去进行分离主副PG，那么这种方式的copyset为<br>{1,2} {3,4}<br>为上面默认情况的2/6</p>
<h2 id="四、总结">四、总结</h2><p>关于ceph中的ceph的copyset的三种模式已经总结完了，需要补充的是，上面的node都是一个虚拟的概念，你可以扩充为row，或者rack都行，这里只是说明了不同的处理方式，针对每个集群都可以有很多种组合，这个关键看自己怎么处理，减少copyset会明显的减低机器上的线程数目和资源的占用，这一点可以自行研制，从原理上来说少了很多配对的通信，crush的是非常灵活的一个分布控制，可以做很精细的控制，当然也会增加了维护的难度</p>
<h2 id="五、参考资料：">五、参考资料：</h2><p><a href="https://www.ustack.com/blog/build-block-storage-service/" target="_blank" rel="external">打造高性能高可靠块存储系统</a><br><a href="https://www.usenix.org/conference/atc13/technical-sessions/presentation/cidon" target="_blank" rel="external">Copysets: Reducing the Frequency of Data Loss in Cloud Storage</a></p>
<h2 id="六、变更记录">六、变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
<h2 id="七、For_me">七、For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/copyset/octo-guitar.gif" alt="ceph"><br></center></p>
<h2 id="一、前言">一、前言</h2><p>copyset运用好能带来什么好处</p>
<ul>
<li>降低故障情况下的数据丢失概率（增加可用性）</li>
<li>降低资源占用，从而降低负载</li>
</ul>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Proftp最简匿名访问配置]]></title>
    <link href="http://www.zphj1987.com/2016/09/01/Proftp%E6%9C%80%E7%AE%80%E5%8C%BF%E5%90%8D%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/"/>
    <id>http://www.zphj1987.com/2016/09/01/Proftp最简匿名访问配置/</id>
    <published>2016-09-01T06:04:24.000Z</published>
    <updated>2016-09-01T06:24:34.153Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>
<a id="more"></a>
<h2 id="配置proftp过程">配置proftp过程</h2><p>1、配置过程尽量少的动原配置文件，需要共享的为/share/a目录，首先修改默认的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">DefaultRoot                     ~ !adm</span><br></pre></td></tr></table></figure></p>
<p>修改为:<br><figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="title">DefaultRoot</span>                     /share</span><br></pre></td></tr></table></figure></p>
<p>让默认的根目录为 /share,默认的为用户的根目录，匿名用户对应的ftp用户的根目录</p>
<p>2、修改匿名用户的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous ~ftp&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改为<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Anonymous /share&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改原匿名用户ftp的用户目录为/share</p>
<p>3、修改默认屏蔽权限WRITE<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit WRITE SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>改成<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">  DenyAll</span><br><span class="line">&lt;/Limit&gt;</span><br></pre></td></tr></table></figure></p>
<p>默认会屏蔽掉写的操作，就没法上传了</p>
<p>5、配置访问的目录<br>默认启用了vroot，所以写路径的时候写相对路径即可，添加如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br><span class="line">&lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">    AllowOverwrite          no</span><br><span class="line">    &lt;Limit ALL&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure></p>
<p>/a就代表的是/share/a</p>
<p>6、开启匿名<br>修改配置vim /etc/sysconfig/proftpd<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">""</span></span><br></pre></td></tr></table></figure></p>
<p>改成:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PROFTPD_OPTIONS=<span class="string">"-DANONYMOUS_FTP"</span></span><br></pre></td></tr></table></figure></p>
<p>7、给目录访问权限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chown ftp:ftp /share/a</span><br><span class="line">chmod <span class="number">755</span>  /share/a</span><br></pre></td></tr></table></figure></p>
<p>8、启动proftp服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart proftpd</span><br></pre></td></tr></table></figure></p>
<h2 id="完整配置文件">完整配置文件</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ServerName			<span class="string">"ProFTPD server"</span></span><br><span class="line">ServerIdent			on <span class="string">"FTP Server ready."</span></span><br><span class="line">ServerAdmin			root@localhost</span><br><span class="line">DefaultServer			on</span><br><span class="line">DefaultRoot			~ !adm</span><br><span class="line">AuthPAMConfig			proftpd</span><br><span class="line">AuthOrder			mod_auth_pam.c* mod_auth_unix.c</span><br><span class="line">UseReverseDNS			off</span><br><span class="line">User				nobody</span><br><span class="line">Group				nobody</span><br><span class="line">MaxInstances			<span class="number">20</span></span><br><span class="line">UseSendfile			off</span><br><span class="line">LogFormat			default	<span class="string">"%h %l %u %t \"%r\" %s %b"</span></span><br><span class="line">LogFormat			auth	<span class="string">"%v [%P] %h %t \"%r\" %s"</span></span><br><span class="line">LoadModule mod_ctrls_admin.c</span><br><span class="line">LoadModule mod_vroot.c</span><br><span class="line">ModuleControlsACLs		insmod,rmmod allow user root</span><br><span class="line">ModuleControlsACLs		lsmod allow user *</span><br><span class="line">ControlsEngine			on</span><br><span class="line">ControlsACLs			all allow user root</span><br><span class="line">ControlsSocketACL		allow user *</span><br><span class="line">ControlsLog			/var/<span class="built_in">log</span>/proftpd/controls.log</span><br><span class="line">&lt;IfModule mod_ctrls_admin.c&gt;</span><br><span class="line">  AdminControlsEngine		on</span><br><span class="line">  AdminControlsACLs		all allow user root</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfModule mod_vroot.c&gt;</span><br><span class="line">  VRootEngine			on</span><br><span class="line">&lt;/IfModule&gt;</span><br><span class="line">&lt;IfDefine TLS&gt;</span><br><span class="line">  TLSEngine			on</span><br><span class="line">  TLSRequired			on</span><br><span class="line">  TLSRSACertificateFile		/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSRSACertificateKeyFile	/etc/pki/tls/certs/proftpd.pem</span><br><span class="line">  TLSCipherSuite		ALL:!ADH:!DES</span><br><span class="line">  TLSOptions			NoCertRequest</span><br><span class="line">  TLSVerifyClient		off</span><br><span class="line">  TLSLog			/var/<span class="built_in">log</span>/proftpd/tls.log</span><br><span class="line">  &lt;IfModule mod_tls_shmcache.c&gt;</span><br><span class="line">    TLSSessionCache		shm:/file=/var/run/proftpd/sesscache</span><br><span class="line">  &lt;/IfModule&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine DYNAMIC_BAN_LISTS&gt;</span><br><span class="line">  LoadModule			mod_ban.c</span><br><span class="line">  BanEngine			on</span><br><span class="line">  BanLog			/var/<span class="built_in">log</span>/proftpd/ban.log</span><br><span class="line">  BanTable			/var/run/proftpd/ban.tab</span><br><span class="line">  BanOnEvent			MaxLoginAttempts <span class="number">2</span>/<span class="number">00</span>:<span class="number">10</span>:<span class="number">00</span> <span class="number">01</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">  BanMessage			<span class="string">"Host %a has been banned"</span></span><br><span class="line">  BanControlsACLs		all allow user ftpadm</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;IfDefine QOS&gt;</span><br><span class="line">  LoadModule			mod_qos.c</span><br><span class="line">  QoSOptions			dataqos throughput ctrlqos lowdelay</span><br><span class="line">&lt;/IfDefine&gt;</span><br><span class="line">&lt;Global&gt;</span><br><span class="line">  Umask				<span class="number">022</span></span><br><span class="line">  AllowOverwrite		yes</span><br><span class="line">  &lt;Limit ALL SITE_CHMOD&gt;</span><br><span class="line">    AllowAll</span><br><span class="line">  &lt;/Limit&gt;</span><br><span class="line">&lt;/Global&gt;</span><br><span class="line">&lt;IfDefine ANONYMOUS_FTP&gt;</span><br><span class="line">  &lt;Anonymous /share/&gt;</span><br><span class="line">    User			ftp</span><br><span class="line">    Group			ftp</span><br><span class="line">    AccessGrantMsg		<span class="string">"Anonymous login ok, restrictions apply."</span></span><br><span class="line">    UserAlias			anonymous ftp</span><br><span class="line">    MaxClients			<span class="number">10</span> <span class="string">"Sorry, max %m users -- try again later"</span></span><br><span class="line">    DisplayLogin		/welcome.msg</span><br><span class="line">    DisplayChdir		.message</span><br><span class="line">    DisplayReadme		README*</span><br><span class="line">    DirFakeUser			on ftp</span><br><span class="line">    DirFakeGroup		on ftp</span><br><span class="line">    &lt;Limit  SITE_CHMOD&gt;</span><br><span class="line">      DenyAll</span><br><span class="line">    &lt;/Limit&gt;</span><br><span class="line">    &lt;IfModule mod_vroot.c&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/*"</span>&gt;</span><br><span class="line">	       AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">        DenyAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">        &lt;Limit DIRS&gt;</span><br><span class="line">        AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">       &lt;Directory <span class="string">"/a"</span>&gt;</span><br><span class="line">              AllowOverwrite          no</span><br><span class="line">        &lt;Limit ALL&gt;</span><br><span class="line">          AllowAll</span><br><span class="line">        &lt;/Limit&gt;</span><br><span class="line">       &lt;/Directory&gt;</span><br><span class="line">    &lt;/IfModule&gt;</span><br><span class="line">    WtmpLog			off</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/access.log WRITE,READ default</span><br><span class="line">    ExtendedLog			/var/<span class="built_in">log</span>/proftpd/auth.log AUTH auth</span><br><span class="line">  &lt;/Anonymous&gt;</span><br><span class="line">&lt;/IfDefine&gt;</span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2><p>最简配置就完成了，也可以根据需要再去做更复杂的配置，这里就不做过多的介绍，比较容易错误的点就是容易出现权限问题无法访问，或者是上下的设置关联错误，可以开启调试模式进行调试<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">proftpd   -n <span class="operator">-d</span> <span class="number">10</span> -c /etc/proftpd.conf -DANONYMOUS_FTP</span><br></pre></td></tr></table></figure></p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-01</td>
</tr>
</tbody>
</table>
<h2 id="For_me">For me</h2><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ftpfile.png" alt="ftpfile"><br></center></p>
<h2 id="前言">前言</h2><p>每一次做ftp的配置都要弄半天，找文档，各种权限控制的坑，折腾半天，这次还是准备记录下来，以备不时之需，这里不配置什么高级的功能，就去实现一个最简单的配置</p>
<blockquote>
<p>匿名用户的上传和下载</p>
</blockquote>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cephday北京总结-August 20, 2016（未完待续）]]></title>
    <link href="http://www.zphj1987.com/2016/08/29/Cephday%E5%8C%97%E4%BA%AC%E6%80%BB%E7%BB%93-August-20-2016%EF%BC%88%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD%EF%BC%89/"/>
    <id>http://www.zphj1987.com/2016/08/29/Cephday北京总结-August-20-2016（未完待续）/</id>
    <published>2016-08-29T15:57:52.000Z</published>
    <updated>2016-09-06T04:19:47.616Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>
<a id="more"></a>
<h2 id="分享的PPT">分享的PPT</h2><h3 id="开幕致辞-张建">开幕致辞-张建</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/00-%E5%BC%80%E5%B9%95%E8%87%B4%E8%BE%9E-%E5%BC%A0%E5%BB%BA.pdf" width="850" height="700"></center>

<p>首先说下这位分享者，之前在2015的Ceph Hackathon上，就是他最先发现的老版本的ceph与 TCMalloc结合的一个bug，然后提出了用jemalloc获取了随机IO的提升，并且降低了资源占用， 这对于老版本的环境提升还是比较大的，在新的环境下，差别没有那么大了，不过分享者还是非常无私的分享了他们的发现<br>本篇主要讲了下面几点：ceph在中国很火，intel投入很多，并且参与了很多的功能的开发，这只是一个致辞，发出的信号就是Intel 很关注ceph</p>
<h3 id="Ceph社区进展-Patrick">Ceph社区进展-Patrick</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/01-Ceph%E7%A4%BE%E5%8C%BA%E8%BF%9B%E5%B1%95-Patrick.pdf" width="850" height="700"></center>

<p>Patrick是红帽的ceph社区的总监，负责推进ceph各方面的发展，<br>本篇主要讲了：ceph当前的发展情况，各大厂对ceph的关注，ceph的固定的活动，cephfs在jewel版本会稳定下来</p>
<h3 id="Ceph中国社区-孙琦">Ceph中国社区-孙琦</h3><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/02-Ceph%E4%B8%AD%E5%9B%BD%E7%A4%BE%E5%8C%BA-%E5%AD%99%E7%90%A6.pdf" width="850" height="700"></center><br>这篇是由孙琦进行的演讲，他对推动ceph在中国的发展做了很多工作<br>本篇主要讲了：ceph中国社区在中国做了哪些推广方面的活动，主要是建立圈子，关注的人很多，发布了一本翻译的技术书籍，未来会做的事情，需要关注的是社区自己写的书会在10月份出来<br><br>###SSD-Ceph在360游戏云的应用-谷忠言<br><br><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/03-SSD%20Ceph%E5%9C%A8360%E6%B8%B8%E6%88%8F%E4%BA%91%E7%9A%84%E5%BA%94%E7%94%A8-%E8%B0%B7%E5%BF%A0%E8%A8%80.pdf" width="850" height="700"></center>

<p>本篇是由360游戏的谷忠言进行演讲的，主要讲述了ceph在360游戏中使用的经验<br>提出了IO容量计算模型；概括了ceph主要调优的方法;相同负载情况下分池对线程和资源的占用帮助很大;如果不限流很有可能因为过载造成心跳超时，进程自杀了;扩容采用扩池的方式避免数据大量变动；网络问题不好定位，根据osd的提交时间的异常来追踪问题（这个数据看下采集方法）；图形化监控采用的是grafana；纯ssd才能满足360游戏主机对性能的需求</p>
<h3 id="SPDK加速Ceph-XSKY_Bluestore案例分享-扬子夜-王豪迈">SPDK加速Ceph-XSKY Bluestore案例分享-扬子夜-王豪迈</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/04-SPDK%E5%8A%A0%E9%80%9FCeph-XSKY%20Bluestore%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB-%E6%89%AC%E5%AD%90%E5%A4%9C-%E7%8E%8B%E8%B1%AA%E8%BF%88.pdf" width="850" height="700"></center><br>本篇是由xsky的扬子夜-王豪迈进行演讲的，ceph设计是在低性能硬件基础上设计的，现在的网络磁盘都是高性能的，软件设计和实现是性能瓶颈，介绍了底层对象存储的写入模型的优点和缺点；设计了新的写入方式，解决这些问题，介绍了spdk；spdk的nvme 驱动比内核的nvme驱动带来了6倍随机读性能的提升；spdk对iscsi场景也能带来很大性能提升；替换内核驱动NVME SSD的OSD为spdk驱动，OSD网络用dpdk替换；介绍bluestore，性能全线提升，当前还在完善功能；总之这个地方会对性能提升很多，但是目前资料太少，目前还没普及，只能有一定功底研发实力的才能参与进来，目前主要是xsky和Intel还有Redhat等大厂在进行驱动在</p>
<h3 id="Ceph_Tiering高性能架构-Thor_Chin">Ceph Tiering高性能架构-Thor Chin</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/05-Ceph%20Tiering%E9%AB%98%E6%80%A7%E8%83%BD%E6%9E%B6%E6%9E%84-Thor%20Chin.pdf" width="850" height="700"></center><br>本篇是由 Thor Chin 进行的演讲，首先介绍了自己的使用场景，然后介绍了下crushmap文件里面各个字段的意思；介绍了各种测试的工具；然后给出了测试的情况，这里作者没有给出测试模型，并且没有说明是否在cache满载的情况下，这个方案是可以使用的，但是性能数据就不做过多的评价</p>
<h3 id="Ceph在视频应用上的性能优化-何营">Ceph在视频应用上的性能优化-何营</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/06-Ceph%E5%9C%A8%E8%A7%86%E9%A2%91%E5%BA%94%E7%94%A8%E4%B8%8A%E7%9A%84%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E4%BD%95%E8%90%A5.pdf" width="850" height="700"></center><br>本篇来自浪潮的何营的演讲，主要介绍了ceph在视频行业的运用，并且提出了直接纠删码的实现方法，研发可以看看，实现起来代码量还是很高的</p>
<h3 id="借助当今的NVM_Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘">借助当今的NVM Express固态盘和未来的英特尔Optane技术打造经济高效的高性能存储解决方案-周渊-张缘</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/07-%E5%80%9F%E5%8A%A9%E5%BD%93%E4%BB%8A%E7%9A%84NVM%20Express%E5%9B%BA%E6%80%81%E7%9B%98%E5%92%8C%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%8B%B1%E7%89%B9%E5%B0%94Optane%E6%8A%80%E6%9C%AF%E6%89%93%E9%80%A0%E7%BB%8F%E6%B5%8E%E9%AB%98%E6%95%88%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD%E5%AD%98%E5%82%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E5%91%A8%E6%B8%8A-%E5%BC%A0%E7%BC%98.pdf" width="850" height="700"></center><br>本篇来自Intel的周渊-张缘作的演讲，开始介绍了Intel在ceph上的贡献，有三大工具，CeTune性能调优工具，Vsm部署管理工具，COSbench压力测试工具，三大工具目前都是开源可部署的，然后介绍了基于Intel的硬件的Ceph方案，介绍了几个调优点，4K随机写提高了6倍，4K随机读提高了16倍，介绍了Bluestore和Filestore在Intel硬件上性能的差别，根据火焰图的输出提出rocksdb需要调优；介绍了Intel的3D Xpoint，最后给出了参考配置文件，这个是针对全闪存的调优</p>
<h3 id="基于ARM的Ceph可扩展高效解决方案-罗旭">基于ARM的Ceph可扩展高效解决方案-罗旭</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/08-%E5%9F%BA%E4%BA%8EARM%E7%9A%84Ceph%E5%8F%AF%E6%89%A9%E5%B1%95%E9%AB%98%E6%95%88%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-%E7%BD%97%E6%97%AD.pdf " width="850" height="700"></center><br>本篇来自罗旭的演讲，介绍了arm在存储方面的应用，社区也在发布arm版本的发行包，提供了ceph的arm基本解决方案，介绍了功耗的优势，西数之前做了一个504个osd的arm的测试，arm目前在国内还属于起步概念的东西，很多人想上，但是因为因为不是通用平台，目前的成本其实并没有太大的优势，未来还是值得期待，在冷数据存储的场景上，还是大有可为的，还有一个原因，国内在功耗这一块并没有特别的重视</p>
<h3 id="Ceph存储设备案例研究与S3对象存储性能优化-刘志刚">Ceph存储设备案例研究与S3对象存储性能优化-刘志刚</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/09-Ceph%E5%AD%98%E5%82%A8%E8%AE%BE%E5%A4%87%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%E4%B8%8ES3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%88%98%E5%BF%97%E5%88%9A.pdf " width="850" height="700"></center><br>本篇来自富士通的刘志刚演讲，这也是本次分享里面唯一的RGW方面的分享，开始介绍了富士通在ceph上的投入，介绍了cache tiering存在性能衰减的问题，介绍了一些方案上调优的点，介绍了ownCloud与对象存储对接的方案和性能，介绍了cosbench测试出来的性能的情况，介绍了rgw调优的参数</p>
<h3 id="Ceph全闪存存储-周皓">Ceph全闪存存储-周皓</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/10-Ceph%E5%85%A8%E9%97%AA%E5%AD%98%E5%AD%98%E5%82%A8-%E5%91%A8%E7%9A%93.pdf" width="850" height="700"></center><br>本篇来自SanDisk的周皓的演讲，介绍了Sandisk的全闪存ceph的方案InfiniFlash，性能确实非常的好，并且TCO非常的低；介绍了一些调优的点BlueStore，KV Store，Memory allocation等等</p>
<h3 id="将Ceph引入企业-在30分钟安装一个50T移动集群">将Ceph引入企业-在30分钟安装一个50T移动集群</h3><p><center><embed src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/11-%E5%B0%86Ceph%E5%BC%95%E5%85%A5%E4%BC%81%E4%B8%9A-%E5%9C%A830%E5%88%86%E9%92%9F%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AA50T%E7%A7%BB%E5%8A%A8%E9%9B%86%E7%BE%A4-Alex%20Lau.pdf" width="850" height="700"></center><br>本篇来自Suse的劉俊賢的演讲，主要讲的是快速部署一个50T的ceph集群，介绍了Suse的iscsi的方案，目前Suse这块做的不错，商业版本提供了基于rbd的iscsi方案的高可用，介绍了基于LIO的LRBD，介绍了openATTIC，这个是之前一家做存储管理平台的公司，后来和Suse合作比较紧密，这个在openATTIC更稳定一点我会写下部署的相关文档，介绍了基于salt的快速部署</p>
<h2 id="完结">完结</h2><p>现在一些大厂的分享都会带上一些优化的方法和优化的参数，这个比前几年已经好了很多，这些参数建议都自己在自己的环境上跑一跑，因为优化是基于当前环境的优化，如果有通用优化，那不用优化了，直接固定参数值就行了，举个简单的例子，在ssd场景上常用的一个优化需要调高IO的线程，如果直接参数硬套到sata的场景，性能不会提高，反而增大了延时，IO一般在增大到一个峰值后，就不会增大，延时反而会增大，所以调优就是找到自己环境的最适合的参数，上面只是简单的介绍了PPT里面的内容点，如果感兴趣可以深挖里面的东西</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-29</td>
</tr>
<tr>
<td style="text-align:center">完成</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-06</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/cephdaybeijing201608/beijing.jpg" alt=""><br></center>

<h2 id="进度">进度</h2><p>已完结，因为BLOG已经被抓取，为了防止链接失效，就不做标题的修改了</p>
<h2 id="前言">前言</h2><p>这次的ceph day 在北京举办的，随着中国IT业的发展，中国的程序员在一些开源项目中做出了自己的贡献，同样的，国外的大厂也越来越关注中国的市场，这就促成了越来越多的交流活动，这次的北京站应该是CEPH DAY APAC ROADSHOW – BEIJING，这个是ceph的亚洲行的其中的一站，来中国，当然就有更多的中国的开发者进行的分享，作为一个长期关注ceph的爱好者，本篇将从我自己的角度来看下这次北京站讲了哪些东西</p>
<p>由于工作的地方在武汉，没有那么多的机会去参加分享活动，就从分享的PPT当中进行解读了，所有的知识都是需要去根据环境进行实践的，也就是别人的经验只有适配好你的环境，对你才是有用的，废话不多说开始了</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重写ceph-lazy]]></title>
    <link href="http://www.zphj1987.com/2016/08/28/%E9%87%8D%E5%86%99ceph-lazy/"/>
    <id>http://www.zphj1987.com/2016/08/28/重写ceph-lazy/</id>
    <published>2016-08-28T15:58:04.000Z</published>
    <updated>2016-08-29T01:05:09.111Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/" target="_blank" rel="external">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>
<a id="more"></a>
<h2 id="项目地址">项目地址</h2><p>原作者项目地址：<a href="https://github.com/gcharot/ceph-lazy" target="_blank" rel="external">https://github.com/gcharot/ceph-lazy</a><br>我重写的地址：<a href="https://github.com/zphj1987/ceph-lazy/tree/lazy-python" target="_blank" rel="external">https://github.com/zphj1987/ceph-lazy/tree/lazy-python</a></p>
<h3 id="安装方法">安装方法</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget -O /sbin/ceph-lazy https://raw.githubusercontent.com/zphj1987/ceph-lazy/lazy-python/ceph-lazy.py</span><br><span class="line">chmod <span class="number">777</span> /sbin/ceph-lazy</span><br></pre></td></tr></table></figure>
<h3 id="详细使用说明">详细使用说明</h3><h4 id="列出节点上的所有的OSD">列出节点上的所有的OSD</h4><p>命令：ceph-lazy host-get-osd {hostname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-osd lab8106</span></span><br><span class="line">osd.<span class="number">0</span> </span><br><span class="line">osd.<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有的存储主机节点">列出所有的存储主机节点</h4><p>命令：ceph-lazy host-get-nodes<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy host-get-nodes </span></span><br><span class="line">lab8106</span><br><span class="line">lab8107</span><br></pre></td></tr></table></figure></p>
<h4 id="列出存储节点上的存储使用的情况(detail看详细信息)">列出存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-osd-usage {hostname} {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106</span></span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy  host-osd-usage lab8106 detail</span></span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出所有存储节点上的存储使用的情况(detail看详细信息)">列出所有存储节点上的存储使用的情况(detail看详细信息)</h4><p>命令：ceph-lazy host-all-usage {detail}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br><span class="line"></span><br><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy host-all-usage detail</span></span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">0</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">4.6</span>GB | Available:<span class="number">273.6</span>GB</span><br><span class="line">OSD:<span class="number">1</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">8.4</span>GB | Available:<span class="number">269.8</span>GB</span><br><span class="line">Host:lab8106 | OSDs:<span class="number">2</span> | Total_Size:<span class="number">556.5</span>GB | Total_Used:<span class="number">13.0</span>GB | Total_Available:<span class="number">543.5</span>GB</span><br><span class="line">----------------------------------------------</span><br><span class="line">OSD:<span class="number">2</span> | Size:<span class="number">278.3</span>GB | Used:<span class="number">3.8</span>GB | Available:<span class="number">274.4</span>GB</span><br><span class="line">Host:lab8107 | OSDs:<span class="number">1</span> | Total_Size:<span class="number">278.3</span>GB | Total_Used:<span class="number">3.8</span>GB | Total_Available:<span class="number">274.4</span>GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出PG所在的节点(first_is_primary)">列出PG所在的节点(first is primary)</h4><p>命令： ceph-lazy pg-get-host {pg_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-get-host   10.2</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host :lab8107</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(_operations_number)">列出写操作最多的PG ( operations number)</h4><p>命令：ceph-lazy pg-most-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-most-write</span></span><br><span class="line">PG:<span class="number">10.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(_operations_number)">列出写操作最少的PG ( operations number)</h4><p>命令：ceph-lazy pg-less-write<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-lazy]<span class="comment"># ceph-lazy pg-less-write</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最多的PG_(data_written)">列出写操作最多的PG (data written)</h4><p>命令：ceph-lazy pg-most-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-write-kb</span></span><br><span class="line">PG:<span class="number">10.0</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出写操作最少的PG_(data_written)">列出写操作最少的PG (data written)</h4><p>命令：ceph-lazy pg-less-write-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-write-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(operations_number)">列出读操作最多的PG (operations number)</h4><p>命令：ceph-lazy pg-most-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read</span></span><br><span class="line">PG:<span class="number">10.1</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(operations_number)">列出读操作最少的PG (operations number)</h4><p>命令：ceph-lazy pg-less-read<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最多的PG_(data_read)">列出读操作最多的PG (data read)</h4><p>命令：ceph-lazy pg-most-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-most-read-kb</span></span><br><span class="line">PG:<span class="number">10.4</span> | OSD:osd.<span class="number">0</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出读操作最少的PG_(data_read)">列出读操作最少的PG (data read)</h4><p>命令：ceph-lazy pg-less-read-kb<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-less-read-kb</span></span><br><span class="line">PG:<span class="number">11.3</span> | OSD:osd.<span class="number">1</span> | Host:lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出空的PG_(没有存储对象)">列出空的PG (没有存储对象)</h4><p>命令：ceph-lazy pg-empty<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy pg-empty</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">11.2</span></span><br><span class="line"><span class="number">11.1</span></span><br><span class="line"><span class="number">11.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">11.4</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的prefix">列出RBD的prefix</h4><p>命令：ceph-lazy rbd-prefix {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-prefix rbd zp</span></span><br><span class="line">rbd_data.<span class="number">1</span>b93a6b8b4567</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的对象数目">列出RBD的对象数目</h4><p>命令：ceph-lazy rbd-count {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-count rbd zp</span></span><br><span class="line"></span><br><span class="line">    RBD image rbd/zp has prefix rbd_data.<span class="number">1</span>b93a6b8b4567; now couning objects...</span><br><span class="line">    count: <span class="number">27</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的存储主机">列出RBD的Primary所在的存储主机</h4><p>命令：ceph-lazy rbd-host {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-host rbd zp</span></span><br><span class="line">Primary Host: lab8107</span><br><span class="line">Primary Host: lab8106</span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Primary所在的OSD节点">列出RBD的Primary所在的OSD节点</h4><p>命令：ceph-lazy rbd-osd {poolname} {imgname}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-osd rbd zp</span></span><br><span class="line">Primary Osd: <span class="number">0</span></span><br><span class="line">Primary Osd: <span class="number">1</span></span><br><span class="line">Primary Osd: <span class="number">2</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出RBD的Image的真实大小">列出RBD的Image的真实大小</h4><p>命令：ceph-lazy rbd-size rbd zp<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy rbd-size rbd zp</span></span><br><span class="line">Pool: rbd | Image:zp | Real_size:<span class="number">71.5586</span> MB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最多的OSD">列出容量使用最多的OSD</h4><p>命令：ceph-lazy osd-most-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-most-used</span></span><br><span class="line">OSD:osd.<span class="number">1</span> | Host: lab8106 | Used: <span class="number">8</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出容量使用最少的OSD">列出容量使用最少的OSD</h4><p>命令：ceph-lazy osd-less-used<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-less-used</span></span><br><span class="line">OSD:osd.<span class="number">2</span> | Host: lab8107 | Used: <span class="number">3</span> GB</span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上所有的primary_PG">列出指定OSD上所有的primary PG</h4><p>命令： ceph-lazy osd-get-ppg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-ppg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定OSD上的所有PG">列出指定OSD上的所有PG</h4><p>命令：ceph-lazy osd-get-pg {osd_id}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy osd-get-pg 1</span></span><br><span class="line"><span class="number">11.3</span></span><br><span class="line"><span class="number">10.2</span></span><br><span class="line"><span class="number">10.3</span></span><br><span class="line"><span class="number">10.0</span></span><br><span class="line"><span class="number">10.1</span></span><br><span class="line"><span class="number">11.7</span></span><br><span class="line"><span class="number">10.6</span></span><br><span class="line"><span class="number">11.6</span></span><br><span class="line"><span class="number">10.7</span></span><br><span class="line"><span class="number">11.5</span></span><br><span class="line"><span class="number">10.4</span></span><br><span class="line"><span class="number">10.5</span></span><br></pre></td></tr></table></figure></p>
<h4 id="列出指定对象所在的主机（第一个是主）">列出指定对象所在的主机（第一个是主）</h4><p>命令：ceph-lazy object-get-host   {poolname} {obj_name}<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-lazy object-get-host   rbd rbd_data.1b93a6b8b4567.00000000000000a0</span></span><br><span class="line">Pg: <span class="number">10.4</span></span><br><span class="line">OSD:osd.<span class="number">0</span> | Host :lab8106</span><br><span class="line">OSD:osd.<span class="number">1</span> | Host :lab8106</span><br></pre></td></tr></table></figure></p>
<h3 id="总结">总结</h3><p>本篇只是暂时结束了，目前完成了原作者的一些想法，等有空再写点自己比较注重的数据</p>
<p>最近一直在关注冯大辉的事情，看完后还是原来的感觉，在利益面前，公司总是会追求最大化，当出现分离的时候，总会显得无情，还是自己让自己强大一点，拿到属于自己的那一部分就好</p>
<h3 id="变更记录">变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
<h3 id="For_me">For me</h3><p>愿意打赏欢迎联系，另有私人ceph小群，可以联系我</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph-lazy/lazy.jpg" alt="ceph-lzy"><br></center></p>
<h2 id="前言">前言</h2><p>这个工具最开始是从sebastien的blog里面看到的，这个是 <a href="https://github.com/gcharot/">Gregory Charot</a>（工具的作者）写的，通常我们在获取一个ceph的信息的时候，需要敲一连串的命令去获得自己需要的信息，可能需要一大堆的解析才能完成，而经常出现的是，使用了后，下次使用的时候，又要重来一遍，所以作者把这些常用的操作做了一些归纳，形成了一个查询的工具，很多人有个相同的观点就是，越懒，就会想办法提高效率，当然，首先得有提高效率的意识，否则只剩下懒了</p>
<p>我做的事情就是把作者用shell的逻辑转换成了python的版本，这样也方便自己以后的扩展，这里感谢作者做的一些工作，让我很快就能完成了，这里并不是重复造车轮，本来自己就不会python，权当练手了</p>
<p>在linux下面我是不建议用中文的，但是这个工具里面还是改成用中文提示，因为中文可能看上去更清楚需要做的是一个什么事情，这个仅仅是一个查询工具</p>
<p>有一段时间没有更新blog了，主要是最近比较忙，没有时间去看太多的资料，没有时间来写下更多的东西，有时间还是会坚持写下去</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux配置邮件发送信息]]></title>
    <link href="http://www.zphj1987.com/2016/08/19/Linux%E9%85%8D%E7%BD%AE%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E4%BF%A1%E6%81%AF/"/>
    <id>http://www.zphj1987.com/2016/08/19/Linux配置邮件发送信息/</id>
    <published>2016-08-18T16:48:17.000Z</published>
    <updated>2016-08-28T16:08:27.386Z</updated>
    <content type="html"><![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<h3 id="编写一个发送的脚本">编写一个发送的脚本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /root/sendmail.sh </span><br><span class="line"><span class="shebang">#! /bin/sh</span></span><br><span class="line">timeout <span class="number">20</span> date &gt; /tmp/mail</span><br><span class="line">timeout <span class="number">20</span> ceph <span class="operator">-s</span> &gt;&gt; /tmp/mail</span><br><span class="line">timeout <span class="number">600</span> mail <span class="operator">-s</span> <span class="string">"cephstatus-`date`"</span> zbkc2016@sina.com &lt; /tmp/mail</span><br></pre></td></tr></table></figure>
<h3 id="在crontab中添加定期执行">在crontab中添加定期执行</h3><p>修改crontab配置文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim crontab</span><br><span class="line">*/<span class="number">5</span> * * * *  root  sh /root/sendmail.sh  <span class="number">2</span>&gt;&amp;<span class="number">1</span>  &gt; /dev/null</span><br></pre></td></tr></table></figure></p>
<p>让crontab服务生效<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">crontab crontab</span><br><span class="line">/etc/init.d/crontab restart</span><br></pre></td></tr></table></figure></p>
<h2 id="总结">总结</h2><p>这个东西很简单，不过自己真去配置的时候，还是找半天资料，还是自己写好文档，方便以后使用，最快最简单的实现需求</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-08-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p><center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/email/email.png" alt=""><br></center></p>
<h2 id="背景">背景</h2><p>一般情况下，我们的IT系统都会有相关的告警的处理，有的是邮件，有的是短信，这些都能很方便的获得一些有用的信息<br>在某些时候我们没有这样的系统，而自己又需要定期的获取一些信息的时候，配置一个邮件发送是很有用的</p>
<h2 id="配置方法">配置方法</h2><p>网上的大部分的方法使用的是sendmail的发送方法，这个地方我们只需要简单的发送邮件的需求，可以直接配置SMTP发送的模式</p>
<h3 id="修改配置文件，填写发送的相关信息">修改配置文件，填写发送的相关信息</h3><p>修改配置文件 <code>/etc/mail.rc</code><br>在最下面添加发送邮箱的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> from=<span class="built_in">test</span>@sina.com smtp=smtp.sina.com</span><br><span class="line"><span class="built_in">set</span> smtp-auth-user=<span class="built_in">test</span>@sina.com smtp-auth-password=<span class="built_in">test</span>123456 smtp-auth=login</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ceph 状态报警告 pool rbd has many more objects per pg than average (too few pgs?)]]></title>
    <link href="http://www.zphj1987.com/2016/07/27/Ceph-%E7%8A%B6%E6%80%81%E6%8A%A5%E8%AD%A6%E5%91%8A-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/"/>
    <id>http://www.zphj1987.com/2016/07/27/Ceph-状态报警告-pool-rbd-has-many-more-objects-per-pg-than-average-too-few-pgs/</id>
    <published>2016-07-27T13:42:05.000Z</published>
    <updated>2016-07-27T14:45:55.641Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>
<a id="more"></a>
<p>这个地方查看集群详细信息：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph health detail</span></span><br><span class="line">HEALTH_WARN pool rbd has many more objects per pg than average (too few pgs?); mon.lab8106 low disk space</span><br><span class="line">pool rbd objects per pg (<span class="number">1912</span>) is more than <span class="number">50.3158</span> <span class="built_in">times</span> cluster average (<span class="number">38</span>)</span><br></pre></td></tr></table></figure></p>
<p>看下集群的pool的对象状态<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph df</span></span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE     AVAIL     RAW USED     %RAW USED </span><br><span class="line">    <span class="number">556</span>G      <span class="number">538</span>G       <span class="number">18668</span>M          <span class="number">3.28</span> </span><br><span class="line">POOLS:</span><br><span class="line">    NAME       ID     USED       %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd        <span class="number">6</span>      <span class="number">16071</span>M      <span class="number">2.82</span>          <span class="number">536</span>G       <span class="number">15296</span> </span><br><span class="line">    pool1      <span class="number">7</span>        <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool2      <span class="number">8</span>        <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool3      <span class="number">9</span>        <span class="number">188</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">48</span> </span><br><span class="line">    pool4      <span class="number">10</span>       <span class="number">192</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">49</span> </span><br><span class="line">    pool5      <span class="number">11</span>       <span class="number">204</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">52</span> </span><br><span class="line">    pool6      <span class="number">12</span>       <span class="number">148</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">38</span> </span><br><span class="line">    pool7      <span class="number">13</span>       <span class="number">184</span>M      <span class="number">0.03</span>          <span class="number">536</span>G          <span class="number">47</span> </span><br><span class="line">    pool8      <span class="number">14</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool9      <span class="number">15</span>       <span class="number">200</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">51</span> </span><br><span class="line">    pool10     <span class="number">16</span>       <span class="number">248</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">63</span> </span><br><span class="line">    pool11     <span class="number">17</span>       <span class="number">232</span>M      <span class="number">0.04</span>          <span class="number">536</span>G          <span class="number">59</span> </span><br><span class="line">    pool12     <span class="number">18</span>       <span class="number">264</span>M      <span class="number">0.05</span>          <span class="number">536</span>G          <span class="number">67</span></span><br></pre></td></tr></table></figure></p>
<p>查看存储池的pg个数<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd dump|grep pool</span></span><br><span class="line">pool <span class="number">6</span> <span class="string">'rbd'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">8</span> pgp_num <span class="number">8</span> last_change <span class="number">132</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">7</span> <span class="string">'pool1'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">134</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">8</span> <span class="string">'pool2'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">136</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">9</span> <span class="string">'pool3'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">138</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">10</span> <span class="string">'pool4'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">140</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">11</span> <span class="string">'pool5'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">142</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">12</span> <span class="string">'pool6'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">144</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">13</span> <span class="string">'pool7'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">146</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">14</span> <span class="string">'pool8'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">148</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">15</span> <span class="string">'pool9'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">1</span> pgp_num <span class="number">1</span> last_change <span class="number">150</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">16</span> <span class="string">'pool10'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">152</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">17</span> <span class="string">'pool11'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">100</span> pgp_num <span class="number">100</span> last_change <span class="number">154</span> flags hashpspool stripe_width <span class="number">0</span></span><br><span class="line">pool <span class="number">18</span> <span class="string">'pool12'</span> replicated size <span class="number">1</span> min_size <span class="number">1</span> crush_ruleset <span class="number">0</span> object_<span class="built_in">hash</span> rjenkins pg_num <span class="number">200</span> pgp_num <span class="number">200</span> last_change <span class="number">156</span> flags hashpspool stripe_width <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>我们看下这个是怎么得到的</p>
<blockquote>
<p>pool rbd objects per pg (1912) is more than 50.3158 times cluster average (38)</p>
</blockquote>
<p>rbd objects_per_pg = 15296 / 8 = 1912<br>objects_per_pg = 15920 /417  ≈ 38<br>50.3158 =  rbd objects_per_pg / objects_per_pg =  1912 / 38 </p>
<p>也就是出现其他pool的对象太少，而这个pg少，对象多，就会提示这个了，我们看下代码里面的判断</p>
<p><a href="https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc" target="_blank" rel="external">https://github.com/ceph/ceph/blob/master/src/mon/PGMonitor.cc</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">int average_objects_per_pg = pg_map.pg_sum.stats.sum.num_objects / pg_map.pg_stat.size();</span><br><span class="line">     <span class="keyword">if</span> (average_objects_per_pg &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">         pg_map.pg_sum.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_objects &amp;&amp;</span><br><span class="line">         p-&gt;second.stats.sum.num_objects &gt;= g_conf-&gt;mon_pg_warn_min_pool_objects) &#123;</span><br><span class="line">int objects_per_pg = p-&gt;second.stats.sum.num_objects / pi-&gt;get_pg_num();</span><br><span class="line"><span class="built_in">float</span> ratio = (<span class="built_in">float</span>)objects_per_pg / (<span class="built_in">float</span>)average_objects_per_pg;</span><br><span class="line"><span class="keyword">if</span> (g_conf-&gt;mon_pg_warn_max_object_skew &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">    ratio &gt; g_conf-&gt;mon_pg_warn_max_object_skew) &#123;</span><br><span class="line">  ostringstream ss;</span><br><span class="line">  ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" has many more objects per pg than average (too few pgs?)"</span>;</span><br><span class="line">  summary.push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  <span class="keyword">if</span> (detail) &#123;</span><br><span class="line">    ostringstream ss;</span><br><span class="line">    ss &lt;&lt; <span class="string">"pool "</span> &lt;&lt; name &lt;&lt; <span class="string">" objects per pg ("</span></span><br><span class="line">       &lt;&lt; objects_per_pg &lt;&lt; <span class="string">") is more than "</span> &lt;&lt; ratio &lt;&lt; <span class="string">" times cluster average ("</span></span><br><span class="line">       &lt;&lt; average_objects_per_pg &lt;&lt; <span class="string">")"</span>;</span><br><span class="line">    detail-&gt;push_back(make_pair(HEALTH_WARN, ss.str()));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>主要下面的几个限制条件</p>
<blockquote>
<p>mon_pg_warn_min_objects = 10000   //总的对象超过10000<br>mon_pg_warn_min_pool_objects = 1000     //存储池对象超过1000<br>mon_pg_warn_max_object_skew = 10        //就是上面的存储池的平均对象与所有pg的平均值的倍数关系</p>
</blockquote>
<h2 id="解决问题">解决问题</h2><p>有三个方法解决这个警告的提示：</p>
<ul>
<li><p>删除无用的存储池<br>如果集群中有一些不用的存储池，并且相对的pg数目还比较高，那么可以删除一些这样的存储池，从而降低<code>mon_pg_warn_max_object_skew</code>这个值，警告就会没有了</p>
</li>
<li><p>增加提示的pool的pg数目<br>有可能的情况就是，这个存储池的pg数目从一开始就不够，增加pg和pgp数目，同样降低了<code>mon_pg_warn_max_object_skew</code>这个值了</p>
</li>
<li>增加<code>mon_pg_warn_max_object_skew</code>的参数值<br>如果集群里面已经有足够多的pg了，再增加pg会不稳定，如果想去掉这个警告，就可以增加这个参数值，默认为10</li>
</ul>
<h2 id="总结">总结</h2><p>这个警告是比较的是存储池中的对象数目与整个集群的pg的平均对象数目的偏差，如果偏差太大就会发出警告</p>
<p>检查的步骤：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">ceph df</span><br><span class="line">ceph osd dump | grep pool</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>mon_pg_warn_max_object_skew = 10.0</p>
</blockquote>
<p>((objects/pg_num) in the affected pool)/(objects/pg_num in the entire system) &gt;= 10.0 警告就会出现</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/ceph%E7%8A%B6%E6%80%81%E8%AD%A6%E5%91%8A/d-a.gif" alt=""><br></center>


<h2 id="定位问题">定位问题</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph -s</span></span><br><span class="line">    cluster fa7ec1a1-<span class="number">662</span>a-<span class="number">4</span>ba3-b478-<span class="number">7</span>cb570482b62</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            pool rbd has many more objects per pg than average (too few pgs?)</span><br><span class="line">     monmap e1: <span class="number">1</span> mons at &#123;lab8106=<span class="number">192.168</span>.<span class="number">8.106</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">30</span>, quorum <span class="number">0</span> lab8106</span><br><span class="line">     osdmap e157: <span class="number">2</span> osds: <span class="number">2</span> up, <span class="number">2</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v1023: <span class="number">417</span> pgs, <span class="number">13</span> pools, <span class="number">18519</span> MB data, <span class="number">15920</span> objects</span><br><span class="line">            <span class="number">18668</span> MB used, <span class="number">538</span> GB / <span class="number">556</span> GB avail</span><br><span class="line">                 <span class="number">417</span> active+clean</span><br></pre></td></tr></table></figure>
<p>集群出现了这个警告，<code>pool rbd has many more objects per pg than average (too few pgs?)</code> 这个警告在hammer版本里面的提示是<code>pool rbd has too few pgs</code></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何替换Ceph的Journal]]></title>
    <link href="http://www.zphj1987.com/2016/07/26/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2Ceph%E7%9A%84Journal/"/>
    <id>http://www.zphj1987.com/2016/07/26/如何替换Ceph的Journal/</id>
    <published>2016-07-26T14:32:18.000Z</published>
    <updated>2016-07-26T17:31:44.315Z</updated>
    <content type="html"><![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>
<a id="more"></a>
<h2 id="第一种方法">第一种方法</h2><p>在开始处理前，最开始要设置OSD状态为<code>noout</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd set noout</span></span><br><span class="line"><span class="built_in">set</span> noout</span><br></pre></td></tr></table></figure>
<p>停止需要替换journal的osd(这里是osd.1)</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl stop ceph-osd@1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>我的版本是jewel的，如果是hammer版本，就使用 /etc/init.d/ceph stop osd.1</p>
</blockquote>
<p>下刷journal到osd，使用 -i 指定需要替换journal的 osd的编号</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --flush-journal</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line">SG_IO: bad/missing sense data, sb[]:  <span class="number">70</span> <span class="number">00</span> <span class="number">05</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">0</span>a <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">20</span> <span class="number">00</span> <span class="number">01</span> cf <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">26</span> <span class="number">22</span>:<span class="number">47</span>:<span class="number">20.185292</span> <span class="number">7</span><span class="built_in">fc</span>54a6c3800 -<span class="number">1</span> flushed journal /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal <span class="keyword">for</span> object store /var/lib/ceph/osd/ceph-<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h3 id="创建一个新的journal">创建一个新的journal</h3><p>删除原来的journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ll /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> ceph ceph <span class="number">58</span> Jul <span class="number">25</span> <span class="number">09</span>:<span class="number">25</span> /var/lib/ceph/osd/ceph-<span class="number">1</span>/journal -&gt; /dev/disk/by-partuuid/<span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7</span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /var/lib/ceph/osd/ceph-1/journal</span></span><br></pre></td></tr></table></figure></p>
<p>准备一个新的分区</p>
<p>我的环境准备使用/dev/sdd1,分区大小为10G，这个注意磁盘大小比参数设置的要大一点即可</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ls -l /dev/disk/by-partuuid/</span></span><br><span class="line">total <span class="number">0</span></span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">25</span> <span class="number">14</span>:<span class="number">25</span> <span class="number">4766</span>ce93<span class="operator">-a</span>476-<span class="number">4</span>e97-<span class="number">9</span>aac-<span class="number">894</span>d461b367e -&gt; ../../sdb2</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">51</span> <span class="number">5</span>bb48687-<span class="number">6</span>be6-<span class="number">4</span>aef-<span class="number">82</span>f6-<span class="number">5</span>af822c3fad8 -&gt; ../../sdd1</span><br><span class="line">lrwxrwxrwx <span class="number">1</span> root root <span class="number">10</span> Jul <span class="number">26</span> <span class="number">22</span>:<span class="number">47</span> <span class="number">872</span>f8b40<span class="operator">-a</span>750-<span class="number">4</span>be3-<span class="number">9150</span>-<span class="number">033</span>b990553f7 -&gt; ../../sdc2</span><br></pre></td></tr></table></figure>
<p>我的新的journal的uuid的路径为<code>/dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8</code></p>
<p>将这个磁盘的分区链接到原始路径<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ln -s /dev/disk/by-partuuid/5bb48687-6be6-4aef-82f6-5af822c3fad8 /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># echo 5bb48687-6be6-4aef-82f6-5af822c3fad8 &gt; /var/lib/ceph/osd/ceph-1/journal_uuid</span></span><br></pre></td></tr></table></figure></p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<p>启动后检查集群的状态</p>
<hr>
<h2 id="第二种方法">第二种方法</h2><p>这个属于备份和转移分区表的方法<br>首先进行上面方法的停进程，下刷journal</p>
<p>备份需要替换journal的分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --backup=/tmp/backup_journal_sdd /dev/sdd</span></span><br></pre></td></tr></table></figure></p>
<p>还原分区表<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># sgdisk --load-backup=/tmp/backup_journal_sde /dev/sde</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># parted -s /dev/sde print</span></span><br></pre></td></tr></table></figure></p>
<p>新的journal磁盘现在跟老的journal的磁盘的分区表一样的了。这意味着新的分区的UUID和老的相同的。如果选择的是这种备份还原分布的方法，那么journal的那个软连接是不需要进行修改的，因为两个磁盘的uuid是一样的，所以需要注意将老的磁盘拔掉或者清理掉分区，以免冲突</p>
<p>在做完这个以后同样跟上面的方法一样需要重建journal</p>
<p>创建journal<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># chown ceph:ceph /var/lib/ceph/osd/ceph-1/journal</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-osd -i 1 --mkjournal</span></span><br></pre></td></tr></table></figure></p>
<p>启动进程<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># systemctl restart ceph-osd@1</span></span><br></pre></td></tr></table></figure></p>
<p>去除<code>noout</code>的标记<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure></p>
<h2 id="第一种方法的实践记录">第一种方法的实践记录</h2><p>这样你可以看到完整的操作过程，而不是枯燥的文档了，虽然命令行看上去也是那么的枯燥</p>
<iframe src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/%E5%A6%82%E4%BD%95%E6%9B%BF%E6%8D%A2journal.html" height="530px" width="90%" align="center"></iframe>

<p>支持暂停复制，是不是很屌？</p>
<h2 id="变更记录">变更记录</h2><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-27</td>
</tr>
</tbody>
</table>
<h2 id="打赏通道">打赏通道</h2><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h2 id="广告">广告</h2><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<center><br><img src="http://7xweck.com1.z0.glb.clouddn.com/movejournal/fixjournal.png" alt=""><br></center>

<p>很多人会提出这样的问题：</p>
<ul>
<li>能不能够将 Ceph journal 分区从一个磁盘替换到另一个磁盘？</li>
<li>怎样替换 Ceph 的 journal 分区？</li>
</ul>
<p>有两种方法来修改Ceph的journal：</p>
<ul>
<li>创建一个journal分区，在上面创建一个新的journal</li>
<li>转移已经存在的journal分区到新的分区上，这个适合整盘替换</li>
</ul>
<blockquote>
<p>Ceph 的journal是基于事务的日志，所以正确的下刷journal数据，然后重新创建journal并不会引起数据丢失，因为在下刷journal的数据的时候，osd是停止的，一旦数据下刷后，这个journal是不会再有新的脏数据进来的</p>
</blockquote>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[modprobe: FATAL: Module ceph not found解决办法]]></title>
    <link href="http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>http://www.zphj1987.com/2016/07/24/modprobe-FATAL-Module-ceph-not-found解决办法/</id>
    <published>2016-07-24T14:48:23.000Z</published>
    <updated>2016-07-24T15:10:25.730Z</updated>
    <content type="html"><![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>
<a id="more"></a>
<pre><code class="bash">[root@lab8106 ~]<span class="comment"># uname -a</span>
Linux ciserver <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 <span class="comment">#1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux</span>
[root@lab8106 ~]<span class="comment"># modinfo ceph</span>
filename:       /lib/modules/<span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64/kernel/fs/ceph/ceph.ko
license:        GPL
description:    Ceph filesystem <span class="keyword">for</span> Linux
author:         Patience Warnick &lt;patience@newdream.net&gt;
author:         Yehuda Sadeh &lt;yehuda@hq.newdream.net&gt;
author:         Sage Weil &lt;sage@newdream.net&gt;
<span class="built_in">alias</span>:          fs-ceph
rhelversion:    <span class="number">7.1</span>
srcversion:     <span class="number">2086</span>D500AFAF47B7260E08A
depends:        libceph
intree:         Y
vermagic:       <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">229</span>.el7.x86_64 SMP mod_unload modversions 
signer:         CentOS Linux kernel signing key
sig_key:        A6:<span class="number">2</span>A:<span class="number">0</span>E:<span class="number">1</span>D:<span class="number">6</span>A:<span class="number">6</span>E:<span class="number">48</span>:<span class="number">4</span>E:<span class="number">9</span>B:FD:<span class="number">73</span>:<span class="number">68</span>:AF:<span class="number">34</span>:<span class="number">08</span>:<span class="number">10</span>:<span class="number">48</span>:E5:<span class="number">35</span>:E5
sig_hashalgo:   sha256
</code></pre>
<p>可以从上面的输出可以看到有个路径为 <code>/lib/modules/3.10.0-229.el7.x86_64/kernel/fs/ceph/ceph.ko</code> 的内核模块，这个就是 cephfs 客户端需要使用到的模块</p>
<h3 id="二、解决办法">二、解决办法</h3><p>解决这个缺失的模块的办法就是升级内核，并且在编译内核的时候需要选上这个模块，在某些商用的 Ceph 里面都是默认把这个模块给屏蔽了，这是因为 Cephfs 并没有达到稳定的标准，而这个在后端版本升级到 10.2 版本（jewel）版本，才正式宣布为第一个稳定版本，当然这个还是慎用为好，除非有比较强大的技术力量支撑，否则也不会出现那么多的大的商用厂家也不开放 Cephfs。</p>
<h3 id="三、总结">三、总结</h3><p>Cephfs这块是比rbd和radosgw这两个部分都复杂的部分，而真正能控制住这个开发的目前主要是 Intel 的<code>zhengyan</code>，从邮件列表里面可以看到主要都是他在修bug，这一块未知的可能性太多，任何小的故障抖动都可能是致命的</p>
<p>Bug不会自己消失，都是在那里的，只是看你有没有碰到</p>
<h3 id="四、变更记录">四、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-24</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>收费小群（适合新手，大牛忽略）：</p>
<center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、问题">一、问题</h3><p>有可能你在进行 Ceph 文件系统挂载的时候出现下面的提示：</p>
<blockquote>
<p>modprobe: FATAL: Module ceph not found.<br>mount.ceph: modprobe failed, exit status 1<br>mount error: ceph filesystem not supported by the system</p>
</blockquote>
<p>这个是因为你的内核当中没有cephfs的相关模块，这个 centos6 下面比较常见，因为 centos6 的内核是 2.6.32,这个版本的内核中还没有集成cephfs的内核模块，而在 centos7 默认内核 3.10中已经默认集成了这个模块，我们看下集成的模块是怎样的显示</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[解决自动安装Freebsd系统盘符无法确定问题]]></title>
    <link href="http://www.zphj1987.com/2016/07/19/%E8%A7%A3%E5%86%B3%E8%87%AA%E5%8A%A8%E5%AE%89%E8%A3%85Freebsd%E7%B3%BB%E7%BB%9F%E7%9B%98%E7%AC%A6%E6%97%A0%E6%B3%95%E7%A1%AE%E5%AE%9A%E9%97%AE%E9%A2%98/"/>
    <id>http://www.zphj1987.com/2016/07/19/解决自动安装Freebsd系统盘符无法确定问题/</id>
    <published>2016-07-18T16:35:37.000Z</published>
    <updated>2016-07-18T17:06:52.181Z</updated>
    <content type="html"><![CDATA[<p>最近因为需要用到Freebsd，所以研究了打包的一些方法，这个没什么太大问题，通过网上的一些资料可以解决，但是由于确实不太熟悉这套系统，还是碰上了一些比较麻烦的地方，目前也没看到有人写如何处理，那就自己总结一下，以免以后再用忘记如何处理</p>
<h3 id="一、问题来源">一、问题来源</h3><p>在linux下的iso自动安装的时候，在无法确定盘符的情况下，可以不写盘符，从而在遇到任何奇怪的磁盘的时候也是能安装的，比如 sda,xvda，vda,这些都可以通过不精确盘符的方式解决</p>
<p>而在freebsd当中处理就不一样了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ./etc/installerconfig</span><br><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">DISTRIBUTIONS=<span class="string">"custom_kernel.txz base.txz lib32.txz custom_files.txz"</span></span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>这个地方写配置文件的第一句就要告诉安装环境需要安装到哪里，这个地方是写死的一个数据，而碰上ada为系统盘就没法解决了，得不断的适配这个盘符</p>
<a id="more"></a>
<h3 id="二、解决问题">二、解决问题</h3><p>最开始的时候写 etc/installerconfig这个配置文件我也不知道为什么要写这里就可以，根据网上的资料是写这个就可以了，在查阅更多的资料后，可以发现是在光盘的etc/rc.local里面会去调用这个脚本，然后去安装</p>
<p>最开始的思路是直接修改这个脚本，后来发现在安装过程中，这个文件实际是只读的，无法去修改的，所以这个地方需要做一个折中的修改</p>
<p>先准备好etc/installerconfig，写死几个值<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">···</span><br><span class="line"><span class="comment">#changge fstab to gpt id</span></span><br><span class="line">systemuuid=`gpart list | grep -A <span class="number">11</span> <span class="string">'da0p2'</span> | grep <span class="string">'rawuuid'</span> | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line">sed -i <span class="operator">-e</span> <span class="string">"s/da0p2/gptid\/<span class="variable">$systemuuid</span>/g"</span> /etc/fstab</span><br></pre></td></tr></table></figure></p>
<p>下面的那个部分是解决盘符变动，在安装过程中就处理好盘符的uuid挂载，这个在linux下面，是操作系统默认就处理好了，这个地方写定一个da0,等下后面处理的时候可以去匹配这个da0</p>
<p>处理默认的./etc/rc.local<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> TERM</span><br><span class="line"></span><br><span class="line">cp /etc/installerconfig /tmp/installerconfig</span><br><span class="line">sh -c <span class="string">'. /usr/share/bsdconfig/device.subr;f_device_menu "" "" "" DISK'</span></span><br><span class="line"><span class="built_in">echo</span> -n  <span class="string">"Which disk your what install :"</span></span><br><span class="line"><span class="built_in">read</span> mydisk</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$mydisk</span>"</span></span><br><span class="line">sed -i <span class="operator">-e</span> <span class="string">"s/da0/<span class="variable">$mydisk</span>/g"</span> /tmp/installerconfig</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="operator">-f</span> /tmp/installerconfig ]; <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">if</span> bsdinstall script /tmp/installerconfig; <span class="keyword">then</span></span><br><span class="line">                dialog --backtitle <span class="string">"FreeBSD Installer"</span> --title <span class="string">"Complete"</span> --no-cancel --ok-label <span class="string">"Reboot"</span> --pause <span class="string">"Inst</span><br><span class="line">allation of FreeBSD complete! Rebooting in 10 seconds"</span> <span class="number">10</span> <span class="number">30</span> <span class="number">10</span></span><br><span class="line">                reboot</span><br></pre></td></tr></table></figure></p>
<p>处理思路就是先拷贝到一个临时的环境下面，然后去修改它，利用系统接口去获取可以安装的磁盘，这个地方只是起一个告诉有哪些盘可以安装的作用，然后根据提示输入想安装的磁盘的盘符名称，这个地方是什么名称就输入什么名称就可以安装了，然后系统就会根据改好的脚本去安装操作系统了</p>
<h3 id="三、总结">三、总结</h3><p>这是一个遗留问题，之前一直没解决，造成了越来越多的问题，在花了一个晚上的时间后，终于能够解决了，对系统越熟悉越能够知道怎么去处理问题，未知的东西太多，只能一点点花时间解决</p>
<h3 id="四、变更记录">四、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-19</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近因为需要用到Freebsd，所以研究了打包的一些方法，这个没什么太大问题，通过网上的一些资料可以解决，但是由于确实不太熟悉这套系统，还是碰上了一些比较麻烦的地方，目前也没看到有人写如何处理，那就自己总结一下，以免以后再用忘记如何处理</p>
<h3 id="一、问题来源">一、问题来源</h3><p>在linux下的iso自动安装的时候，在无法确定盘符的情况下，可以不写盘符，从而在遇到任何奇怪的磁盘的时候也是能安装的，比如 sda,xvda，vda,这些都可以通过不精确盘符的方式解决</p>
<p>而在freebsd当中处理就不一样了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ./etc/installerconfig</span><br><span class="line">PARTITIONS=<span class="string">"da0 &#123; 512K freebsd-boot, auto freebsd-ufs / &#125;"</span></span><br><span class="line">DISTRIBUTIONS=<span class="string">"custom_kernel.txz base.txz lib32.txz custom_files.txz"</span></span><br><span class="line"><span class="shebang">#!/bin/sh</span></span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>这个地方写配置文件的第一句就要告诉安装环境需要安装到哪里，这个地方是写死的一个数据，而碰上ada为系统盘就没法解决了，得不断的适配这个盘符</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[通过ceph-deploy安装不同版本ceph]]></title>
    <link href="http://www.zphj1987.com/2016/07/14/%E9%80%9A%E8%BF%87ceph-deploy%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%ACceph/"/>
    <id>http://www.zphj1987.com/2016/07/14/通过ceph-deploy安装不同版本ceph/</id>
    <published>2016-07-14T15:28:33.000Z</published>
    <updated>2016-07-15T10:28:06.373Z</updated>
    <content type="html"><![CDATA[<p>之前有在论坛写了怎么用 yum 安装 ceph，但是看到ceph社区的群里还是有人经常用 ceph-deploy 进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49" target="_blank" rel="external">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装 ceph-deploy 这个工具，机器上应该安装好 epel 源和 base 源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从 epel 的 repo 里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>
<a id="more"></a>
<p>现在什么都不修改，看下默认的安装会什么样的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">25</span>): /usr/bin/ceph-deploy install lab8106</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version hammer on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">[lab8106][WARNIN] http://ceph.com/rpm-hammer/rhel7/x86_64/repodata/repomd.xml: [Errno <span class="number">14</span>] HTTP Error <span class="number">404</span> - Not Found</span><br></pre></td></tr></table></figure>
<p>这个默认的版本没安装成功<br>这个地方的原因是默认会去下载<a href="http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm" target="_blank" rel="external">http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm</a> 这个包，而这个包是有问题的，安装以后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># cat /etc/yum.repos.d/ceph.repo |grep baseurl</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/<span class="variable">$basearch</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/noarch</span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/SRPMS</span><br></pre></td></tr></table></figure></p>
<p>这路径rhel7是根本就没有的，所以这个地方所以会出错，可以去修改repo的方式解决，这里先忽略这个问题，我们换一个ceph-deploy看看会怎样</p>
<h3 id="二、安装另外版本的ceph-deploy">二、安装另外版本的ceph-deploy</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum remove ceph-deploy</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rpm -ivh http://download.ceph.com/rpm/el7/noarch/ceph-deploy-1.5.34-0.noarch.rpm</span></span><br></pre></td></tr></table></figure>
<p>安装好了后，再次执行安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">···</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://download.ceph.com/keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[lab8106][DEBUG ] ---&gt; Package ceph.x86_64 <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">2</span>-<span class="number">0</span>.el7 will be installed</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>如果网络好的话，那么可以看到，执行这个命令后会在ceph.com的官网上去下载安装包了，如果网络不好的话，就会卡住了，这里是要说明的是</p>
<blockquote>
<p>不同的 ceph-deploy 去 install 的时候会安装不同的版本，这个因为代码里面会写上当时的版本，这样默认安装的就是当时的版本了</p>
</blockquote>
<p>到了这里要开始本篇的主题了，主要的目的有两个:</p>
<ul>
<li>自己选择想安装的 ceph 版本</li>
<li>自己选择通过什么地址安装</li>
</ul>
<p>第一个是解决了安装自己的版本，第二个是避免ceph.com无法访问的时候无法安装，通过国内的源进行加速</p>
<h3 id="三、自定义安装ceph">三、自定义安装ceph</h3><h4 id="通过阿里云安装ceph-hammer">通过阿里云安装ceph-hammer</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<p>通过这个命令，就通过阿里云的源安装了ceph的hammer版本的ceph</p>
<h4 id="通过阿里云安装ceph-jewel">通过阿里云安装ceph-jewel</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum clean all</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<h3 id="四、总结">四、总结</h3><p>安装的方式有很多，对于新手来说如果想用 ceph-deploy 去安装的话，可以根据上面的很简单的命令就解决了，这里没有写本地做源的相关的知识，安装这一块怎么顺手怎么来，不要在安装上面耗费太多的时间</p>
<h3 id="五、变更记录">五、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-14</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>私人朋友群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜（收费入群）</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前有在论坛写了怎么用 yum 安装 ceph，但是看到ceph社区的群里还是有人经常用 ceph-deploy 进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装 ceph-deploy 这个工具，机器上应该安装好 epel 源和 base 源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从 epel 的 repo 里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重构rbd镜像的元数据]]></title>
    <link href="http://www.zphj1987.com/2016/07/02/%E9%87%8D%E6%9E%84rbd%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/"/>
    <id>http://www.zphj1987.com/2016/07/02/重构rbd镜像的元数据/</id>
    <published>2016-07-01T18:52:33.000Z</published>
    <updated>2016-07-01T19:01:08.994Z</updated>
    <content type="html"><![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>将这几个镜像下载下来<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_header.60276b8b4567 rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_id.zp rbd_id.zp</span></span><br></pre></td></tr></table></figure></p>
<p>查看下载下来的几个镜像的元数据的文件信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># ll</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_directory</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">16</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<p>有没有发现有两个镜像的文件大小是0，这个是因为rbd format 2 格式下（默认格式），这两个对象的元数据信息是存储在扩展属性里面的，所以下载下来的对象是没有内容，那我们怎么查看这个属性，看下面讲述的查询相关的操作</p>
<h4 id="2、查询这个image的信息">2、查询这个image的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure>
<p>这里可以看到这个image文件的大小，对象大小，前缀信息，属性相关信息，这是用我们比较常规的方式来查询到的信息，现在用另外一种方式来查询信息，查到的会是另外一种方式，也就是上面一节提到的空对象的扩展属性的查询</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure>
<p>先来查询 rbd_directory 这个的元数据信息，这个里面的信息可以看到两组对应关系<br>id_60276b8b4567,就是这个image的id，也是前缀信息，后面对应的是一个名称zp<br>第二组name_zp,对应的就是后面的60276b8b4567，也就是名称对应到id<br>，那个value值就是后面的字符串对应的16进制的一种方式，这个地方就是需要备份的元数据信息，现在准备做第一次重构，重构rbd_directory这个的元数据信息，这个rbd_directory记录所属存储池有哪些镜像</p>
<h3 id="三、恢复rbd_directory的元数据信息">三、恢复rbd_directory的元数据信息</h3><p>先来破坏这个元数据信息，破坏的方式很简单，就是做删除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到删除了元数据信息以后，再进行镜像的ls，是查询不到信息的</p>
<p>开始做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># touch rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br></pre></td></tr></table></figure></p>
<p>上面做的三步是创建一个空文件，然后上传，然后列属性，可以看到，都是空的（这个地方也可以不创建空对象，直接做后面的给属性的时候，集群会自动创建相关的对象）<br>现在给这个对象写入属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x02\\x00\\x00\\x00\\x7a\\x70|rados -p rbd setomapval rbd_directory id_60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37|rados -p rbd setomapval rbd_directory name_zp</span></span><br></pre></td></tr></table></figure></p>
<p>写入的值就是上面让记录下来的信息，这个地方就用这个格式就行了，为什么要这么写，因为16进制的字符是需要转义的，之前不清楚怎么写，在邮件列表中提问后，有一个人低调的给回复了怎么写入这种进制数据，现在就这么固定写法就行了，现在再查询写入以后的属性情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<p>到这里 rbd_directory这个的信息就恢复了，下面再进行image的元数据的信息的恢复</p>
<h3 id="四、恢复image的元数据信息">四、恢复image的元数据信息</h3><p>先查询下这个对象包含的元数据信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>记录下这个信息，然后进行破坏，跟上面一样的删除掉对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd info zp</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">00</span>:<span class="number">57</span>:<span class="number">50.150559</span> <span class="number">7</span>ff4b56b3700 -<span class="number">1</span> librbd::image::OpenRequest: failed to retreive immutable metadata: (<span class="number">2</span>) No such file or directory</span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在删除了这个对象以后，已经无法查询到镜像信息了，当然也就无法使用了，下面开始进行image的元数据信息的重构<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00|rados -p rbd setomapval rbd_header.60276b8b4567 features</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x15\\x00\\x00\\x00\\x72\\x62\\x64\\x5f\\x64\\x61\\x74\\x61\</span></span><br><span class="line">\x2e\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37    |rados -p rbd setomapval rbd_header.<span class="number">60276</span>b8b4567  object_prefix</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x16|rados -p rbd setomapval rbd_header.60276b8b4567 order</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\xc4\\x09\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 size</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 snap_seq</span><br></pre></td></tr></table></figure></p>
<p>设置完了所有属性后查询，验证是否恢复了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>元数据完整的回来了<br>上面已经将两个导出的空对象元数据信息恢复好了，再看最后一个有文件大小的对象怎么做恢复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># cat rbd_id.zp </span></span><br><span class="line"></span><br><span class="line"><span class="number">60276</span>b8b4567[root@lab8106 zp]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>这个第一种方式是直接备份好,然后倒入的方式<br>跟上面的方法一样，开始通过删除对象来破坏<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到破坏了就无法访问镜像了，下面直接利用备份对象倒入的方式进行恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_id.zp rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到，倒入后即可，也可以用另外一种方式，记录字符串的方式进行备份<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># xxd rbd_id.zp</span></span><br><span class="line"><span class="number">0000000</span>: <span class="number">0</span>c00 <span class="number">0000</span> <span class="number">3630</span> <span class="number">3237</span> <span class="number">3662</span> <span class="number">3862</span> <span class="number">3435</span> <span class="number">3637</span>  ....<span class="number">60276</span>b8b4567</span><br></pre></td></tr></table></figure></p>
<p>我们可以查看这个文件的16进制的信息输出，这个信息就是要保留的字符串信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zp</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>需要保留的就是这个信息,我们根据这个信息来重新创建一个文件，然后检查文件内容是不是能跟下载下来的对象一样<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37   &gt;rbd_id.zpre</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zpre</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，可以用字符串完整恢复这个对象了，然后put进集群即可恢复了</p>
<h3 id="五、总结">五、总结</h3><p>可以看到，所有的元数据信息都可以以字符串的形式保留下来，然后进行元数据重构，其中的rbd_id.zp这个可以保存对象方式，也可以是获取对象后，然后保存16进制字符串信息，然后再进行本地创建对象,然后put的方式，其它的两个空对象可以用设置属性的方式进行恢复，在openstack场景下，这些元数据信息最好都保留下来，一旦有问题的时候，可以很方便的进行数据的重构，备份并不是说所有数据都需要备份，对于这种数据量很小，而且很重要的信息，定期备份一下，也许哪天就用上了</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-02</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

<h3 id="打个广告">打个广告</h3><p>私人朋友ceph技术讨论收费群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[parted分区对齐]]></title>
    <link href="http://www.zphj1987.com/2016/06/24/parted%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90/"/>
    <id>http://www.zphj1987.com/2016/06/24/parted分区对齐/</id>
    <published>2016-06-24T08:32:43.000Z</published>
    <updated>2016-06-24T08:35:39.321Z</updated>
    <content type="html"><![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>
<a id="more"></a>
<h3 id="二、通过计算分区">二、通过计算分区</h3><h4 id="获取磁盘的几个参数（这里是软raid）">获取磁盘的几个参数（这里是软raid）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /sys/block/md127/queue/optimal_io_size</span></span><br><span class="line"><span class="number">3670016</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/minimum_io_size</span></span><br><span class="line"><span class="number">524288</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/alignment_offset</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/physical_block_size</span></span><br><span class="line"><span class="number">512</span></span><br></pre></td></tr></table></figure>
<p>optimal_io_size 加上 alignment_offset 的和 然后除以  physical_block_size<br>在这个环境下是：<br>(3670016 + 0) / 512 = 7168</p>
<p>那么分区的时候命令就应该是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">7168</span>s <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>如果上面的顺利的完成检查一下  (‘1’是分区的编号):</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 aligned</p>
</blockquote>
<p>这个是正常的结果，如果没对齐就会是</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 not aligned</p>
</blockquote>
<h3 id="三、其他情况">三、其他情况</h3><p>默认情况下直接用下列的分区参数就可以，出现提示再用上面的计算，总之最后align-check 验证下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<h3 id="四、相关文章">四、相关文章</h3><p><a href="http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/" target="_blank" rel="external">How to align partitions for best performance using parted</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-24</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd的增量备份和恢复]]></title>
    <link href="http://www.zphj1987.com/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/06/22/rbd的增量备份和恢复/</id>
    <published>2016-06-21T17:17:24.000Z</published>
    <updated>2016-06-22T01:40:00.378Z</updated>
    <content type="html"><![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br><a id="more"></a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v1 testimage_v1</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v1那个时间点的差异数据导出来了testimage_v1，导出成本地文件testimage_v1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 testimage_v2</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v2那个时间点的差异数据导出来了，导出成本地文件testimage_v2<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 --from-snap v1 testimage_v1_v2</span><br></pre></td></tr></table></figure></p>
<p>这个命令是导出了从v1快照时间点到v2快照时间点的差异数据，导出成本地文件testimage_v1_v2</p>
<p>这个地方上面的导出的数据：</p>
<blockquote>
<p>v1时间点数据 + v1_v2之间数据 = v2 时间点数据</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage testimage_now</span><br></pre></td></tr></table></figure>
<p>这个就是导出了从image创建到当前的时间点的差异数据</p>
<h3 id="三、快照的数据恢复">三、快照的数据恢复</h3><p><img src="http://static.zybuluo.com/zphj1987/ue1feys17yiya6doa2audkbo/image_1alpuprird31dltilpro7kf52a.png" alt=""></p>
<p>快照的恢复过程使用的是刚刚上面提到的备份到本地的那些文件<br>首先随便创建一个image,名称大小都不限制，因为后面恢复的时候会覆盖掉大小的信息<br><figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">create</span> testbacknew <span class="comment">--size 1</span></span><br></pre></td></tr></table></figure></p>
<p>现在假如想恢复到v2那个快照的时间点，那么可以用两个方法</p>
<p>1、直接基于v2的时间点的快照做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd import-diff testimage_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>2、直接基于v1的时间点的数据，和后面的增量的v1_v2数据(要按顺序导入)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd impot-diff testimage_v1 rbd/testbacknew</span><br><span class="line">rbd impot-diff testimage_v1_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>到这里数据就已经恢复了</p>
<h3 id="四、如何利用这个">四、如何利用这个</h3><p>实际项目当中就是，定期做快照，然后导出某个时间点快照的数据，然后导出增量的快照的数据，就可以了，例如：<br>今天对所有的rbd的image做一个基础快照，然后导出这个快照的数据，然后从今天开始，每天晚上做一个快照，然后导出快照时间点之间的数据，这样每天导出来的就是一个增量的数据了，在做恢复的时候，就从第一个快照导入，然后按顺序导入增量的快照即可，也可以定期做一个快照，导出完整的快照数据，以防中间的增量快照漏了，然后就是要注意可以定期清理快照，如果是做备份的模式，在导入了快照数据后，也可以清理一些本地的数据，本地数据做异地机房复制的时候也可以做一下数据的压缩，来减少数据量的传输</p>
<h3 id="五、相关文章">五、相关文章</h3><p><a href="https://github.com/skuicloud/openstack-hacker/tree/master/tsinghua-cluster/script/ceph/volume_backup" target="_blank" rel="external">rbd备份还原的脚本</a><br><a href="http://ceph.com/dev-notes/incremental-snapshots-with-rbd/" target="_blank" rel="external">INCREMENTAL SNAPSHOTS WITH RBD</a><br><a href="http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication" target="_blank" rel="external">RBD Replication</a><br><a href="http://www.evil0x.com/posts/14638.html" target="_blank" rel="external">云杉网络：基于Ceph RBD的快照技术实现异地灾备</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-22</td>
</tr>
<tr>
<td style="text-align:center">修改错别字</td>
<td style="text-align:center">武汉-运维-磨渣 -from- 运维-北京-小白</td>
<td style="text-align:center">2016-06-22</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rgw实现nfs的首测]]></title>
    <link href="http://www.zphj1987.com/2016/06/19/rgw%E5%AE%9E%E7%8E%B0nfs%E7%9A%84%E9%A6%96%E6%B5%8B/"/>
    <id>http://www.zphj1987.com/2016/06/19/rgw实现nfs的首测/</id>
    <published>2016-06-18T18:47:56.000Z</published>
    <updated>2016-09-08T15:04:06.109Z</updated>
    <content type="html"><![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS" target="_blank" rel="external">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718" target="_blank" rel="external">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>
<a id="more"></a>
<h3 id="二、功能架构图">二、功能架构图</h3><p><img src="http://static.zybuluo.com/zphj1987/o5ruvtr9f7nyegbv0ly7ekv5/image_1alibfc78g96dsa1c26crkgis1e.png" alt="image_1alibfc78g96dsa1c26crkgis1e.png-78.3kB"><br>简单说明一下：<br>集群配置s3接口，nfs-genesha将s3接口转换成nfs，然后nfs客户端挂载后访问的就是s3的bucket里面的数据了</p>
<h3 id="三、准备工作">三、准备工作</h3><p>准备代码，这个是需要从源码编译的，并且需要将模块编译进去才可以的，源码分支地址：</p>
<blockquote>
<p><a href="https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable" target="_blank" rel="external">https://github.com/nfs-ganesha/nfs-ganesha/tree/V2.3-stable</a></p>
</blockquote>
<p>这个地方要注意下，需要使用next分支(此分支开发中有编译BUG)，换分支V2.3-stable<br>使用git 进行clone分支到本地<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b V2.<span class="number">3</span>-stable https://github.com/nfs-ganesha/nfs-ganesha.git</span><br></pre></td></tr></table></figure></p>
<p>检查是否有这个RGW模块目录</p>
<blockquote>
<p>nfs-ganesha/src/FSAL/FSAL_RGW/</p>
</blockquote>
<p>默认clone下来后  <code>nfs-ganesha/src/libntirpc/</code> 这个目录是空的，而这个是因为如果在git里面某个目录嵌套的用了其他项目的代码，并且也是有git的分支的话，clone下来就会是空的，这个在ceph的源码里面也会这样，具体的看看下图：<br><img src="http://static.zybuluo.com/zphj1987/00dbog7s6nzbze55qyjilzt9/libntir.png" alt="libntir.png-38.4kB"><br>下载下面的链接的这个版本，然后把代码解压到nfs-ganesha/src/libntirpc/这个目录当中去<br><a href="https://github.com/nfs-ganesha/ntirpc/archive/e9cefd2ebfa0a5fc25932cc1088663d39c27e549.zip" target="_blank" rel="external">https://github.com/nfs-ganesha/ntirpc/archive/e9cefd2ebfa0a5fc25932cc1088663d39c27e549.zip</a></p>
<p>代码的编译采用的是cmake的模式(cmake目录后面接的是nfs-ganesha代码的src目录)</p>
<blockquote>
<p>注意在执行cmake之前编译环境需要安装librgw2-devel这个包，才能编译成功，执行cmake的时候检查下是否真的开启了</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/u3xku4jf3swljl0bub9zkwv0/image_1alian0db17e91gg1mhg866i1q11.png" alt="image_1alian0db17e91gg1mhg866i1q11.png-11.1kB"></p>
<p>开始编译安装过程，创建一个用于编译的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 newbian]<span class="comment">#mkdir mybuild</span></span><br><span class="line">[root@lab8106 newbian]<span class="comment">#cd mybuild</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#cmake -DUSE_FSAL_RGW=ON ../nfs-ganesha/src/</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment"># ll FSAL/FSAL_RGW/</span></span><br><span class="line">total <span class="number">16</span></span><br><span class="line">drwxr-xr-x <span class="number">3</span> root root    <span class="number">83</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> CMakeFiles</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">2979</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> cmake_install.cmake</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">10164</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> Makefile</span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make install</span></span><br></pre></td></tr></table></figure></p>
<p>编译安装工作就到此完成了，还是比较简单的</p>
<h3 id="四、配置服务">四、配置服务</h3><h4 id="1、准备一个s3的环境，我的如下：">1、准备一个s3的环境，我的如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin user create --uid=admin --display-name=<span class="string">"admin"</span>   --access-key=admin  --secret=admin</span><br></pre></td></tr></table></figure>
<p>用户信息如下：</p>
<ul>
<li>s3的User_Id：admin </li>
<li>s3的Access_Key:admin </li>
<li>s3的Secret_Access_Key:admin</li>
</ul>
<p>注意，配置ganesha-nfs服务的机器需要安装librgw</p>
<h4 id="2、修改ganesha-nfs的配置文件">2、修改ganesha-nfs的配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/ganesha/ganesha.conf</span><br></pre></td></tr></table></figure>
<p>修改如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">EXPORT</span><br><span class="line">&#123;</span><br><span class="line">        Export_ID=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        Path = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Pseudo = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Access_Type = RW;</span><br><span class="line"></span><br><span class="line">        NFS_Protocols = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">        Transport_Protocols = TCP;</span><br><span class="line"></span><br><span class="line">        FSAL &#123;</span><br><span class="line">                Name = RGW;</span><br><span class="line">                User_Id = <span class="string">"admin"</span>;</span><br><span class="line">                Access_Key_Id =<span class="string">"admin"</span>;</span><br><span class="line">                Secret_Access_Key = <span class="string">"admin"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RGW &#123;</span><br><span class="line">    ceph_conf = <span class="string">"/etc/ceph/ceph.conf"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>RGW-NFS配置文件的模板路径在：</p>
<blockquote>
<p>/usr/share/doc/ganesha/config_samples/rgw.conf</p>
</blockquote>
<h4 id="4、启动ganesha-nfs服务">4、启动ganesha-nfs服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart  nfs-ganesha.service</span><br></pre></td></tr></table></figure>
<h4 id="5、NFS客户端挂载ganesha-nfs服务">5、NFS客户端挂载ganesha-nfs服务</h4><p>找一台其它的客户端机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t nfs <span class="number">192.168</span>.<span class="number">8.106</span>:/ /mnt</span><br></pre></td></tr></table></figure></p>
<p>直接挂载即可，这里注意因为rgw是没有文件系统的容量概念的，这里df是看不到的，所以用mount命令检测<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># mount|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/ on /mnt <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/testnfsrgw on /mnt/testnfsrgw <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,port=<span class="number">0</span>,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以查看挂载的目录里面的子目录对应的就是bucket<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># ll /mnt/</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> ./</span><br><span class="line">drwxr-xr-x <span class="number">25</span> root root <span class="number">4096</span> Apr <span class="number">13</span> <span class="number">03</span>:<span class="number">04</span> ../</span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> testnfsrgw/</span><br></pre></td></tr></table></figure></p>
<h3 id="五、总结">五、总结</h3><p>在实现这个功能以后，实际上为文件接口和对象接口打通了一个通道，能够方便的实现传统的文件接口的数据到对象接口的转移，在性能方面，本篇并没有做测试，这个交给实际项目中去检测了，如果有问题欢迎探讨</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-19</td>
</tr>
<tr>
<td style="text-align:center">修改无法编译的BUG</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-09-08</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的分答-付费语音回答问题-￥1]]></title>
    <link href="http://www.zphj1987.com/2016/06/15/%E6%88%91%E7%9A%84%E5%88%86%E7%AD%94-%E4%BB%98%E8%B4%B9%E8%AF%AD%E9%9F%B3%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98-%EF%BF%A51/"/>
    <id>http://www.zphj1987.com/2016/06/15/我的分答-付费语音回答问题-￥1/</id>
    <published>2016-06-14T17:34:06.000Z</published>
    <updated>2016-06-15T14:31:23.666Z</updated>
    <content type="html"><![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>
<a id="more"></a>
<center><br><img src="http://static.zybuluo.com/zphj1987/qxfdnyf1c6o1hd4irdaz7xp4/liantu.png" alt="微信扫一扫"><br></center>




]]></content>
    <summary type="html">
    <![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CPU相关的学习]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/CPU%E7%9B%B8%E5%85%B3%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.zphj1987.com/2016/06/13/CPU相关的学习/</id>
    <published>2016-06-13T10:13:49.000Z</published>
    <updated>2016-06-13T10:14:21.018Z</updated>
    <content type="html"><![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br><a id="more"></a></p>
<h3 id="单核和多核">单核和多核</h3><p>在英文里面，单核（single-core）和多核（multi-core）多称作uniprocessor和multiprocessor，这里先对这些概念做一个说明：</p>
<blockquote>
<p>这里所说的core（或processor），是一个泛指，是从使用者（或消费者）的角度看计算机系统。因此，core，或者processor，或者处理器（CPU），都是逻辑概念，指的是一个可以独立运算、处理的核心。<br>而这个核心，可以以任何形式存在，例如：单独的一个chip（如通常意义上的单核处理器）；一个chip上集成多个核心（如SMP，symmetric multiprocessing）；一个核心上实现多个hardware context，以支持多线程（如SMT，Simultaneous multithreading）；等等。这是从硬件实现的角度看的。<br>最后，从操作系统进程调度的角度，又会统一看待这些不同硬件实现的核心，例如上面开始所提及的CPU（24个CPUs，从0编号开始），因为它们都有一个共同的特点：执行进程（或线程）。</p>
</blockquote>
<h3 id="NUNA与SMP的概念">NUNA与SMP的概念</h3><p>NUMA(Non-Uniform Memory Access，非一致性内存访问)和SMP(Symmetric Multi-Processor，对称多处理器系统)是两种不同的CPU硬件体系架构</p>
<p>SMP（Symmetric Multi-Processing）的主要特征是共享，所有的CPU共享使用全部资源，例如内存、总线和I/O，多个CPU对称工作，彼此之间没有主次之分，平等地访问共享的资源，这样势必引入资源的竞争问题，从而导致它的扩展内力非常有限。特别是在现在一台机器CPU核心比较多，内存比较大的情况</p>
<p>NUMA技术将CPU划分成不同的组（Node)，每个Node由多个CPU组成，并且有独立的本地内存、I/O等资源。Node之间通过互联模块连接和沟通，因此除了本地内存外，每个CPU仍可以访问远端Node的内存，只不过效率会比访问本地内存差一些，我们用Node之间的距离（Distance，抽象的概念）来定义各个Node之间互访资源的开销。</p>
<p>本章主要是去做NUMA的相关探索，下图是一个多核系统简单的topology</p>
<center><img src="http://static.zybuluo.com/zphj1987/vg7eprp72ibucwyq0fljvfl2/coremuti.gif" alt="coremuti.gif-23.7kB"></center>

<h3 id="Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)">Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)</h3><p>如果你只知道CPU这么一个概念，那么是无法理解CPU的拓扑的。事实上，在NUMA架构下，CPU的概念从大到小依次是：Node、Socket、Core、Processor</p>
<ul>
<li>Sockets 可以理解成主板上cpu的插槽数，物理cpu的颗数，一般同一socket上的core共享三级缓存</li>
<li>Cores 而Socket中的每个核心被称为Core,常说的核,核有独立的物理资源.比如单独的一级二级缓存什么的</li>
<li>Threads 为了进一步提升CPU的处理能力，Intel又引入了HT（Hyper-Threading，超线程)的技术，一个Core打开HT之后，在OS看来就是两个核，当然这个核是逻辑上的概念，所以也被称为Logical Processor,如果不开超线程,threads应该与cores相等,如果开了超线程,threads应该是cores的倍数.相互之间共享物理资源</li>
<li>Nodes 上图的多核图中没有涉及， Node是NUMA体系中的概念．由于SMP体系中各个CPU访问内存只能通过单一的通道．导致内存访问成为瓶颈,cpu再多也无用．后来引入了NUMA．通过划分node,每个node有本地RAM,这样node内访问RAM速度会非常快．但跨Node的RAM访问代价会相对高一点，下面看一下两种架构的明显区别</li>
</ul>
<center><img src="http://static.zybuluo.com/zphj1987/y0thygclxkbl9y8e1f6rl9yj/smpnuma.png" alt="smpnuma.png-67.5kB"></center>


<p>由此可以总结这样的逻辑关系(包含):Node &gt; Socket &gt; Core &gt; Thread 区分这几个概念为了了解cache的分布,因为cpu绑定的目的就是提高cache的命中率,降低cpu颠簸.所以了解cache与cpu之间的mapping关系是非常重要的.通常来讲:</p>
<ul>
<li>同Socket内的cpu共享三级级缓存</li>
<li>每个Core有自己独立的二级缓存</li>
<li>一个Core上超线程出来的Threads,避免绑定，看似可能会提高L2 cache命中率,但也可能有严重的cpu争抢，导致性能非常差.</li>
</ul>
<h3 id="查看CPU信息">查看CPU信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lscpu </span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        <span class="number">32</span>-bit, <span class="number">64</span>-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                <span class="number">24</span></span><br><span class="line">On-line CPU(s) list:   <span class="number">0</span>-<span class="number">23</span></span><br><span class="line">Thread(s) per core:    <span class="number">2</span></span><br><span class="line">Core(s) per socket:    <span class="number">6</span></span><br><span class="line">Socket(s):             <span class="number">2</span></span><br><span class="line">NUMA node(s):          <span class="number">2</span></span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            <span class="number">6</span></span><br><span class="line">Model:                 <span class="number">62</span></span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br><span class="line">Stepping:              <span class="number">4</span></span><br><span class="line">CPU MHz:               <span class="number">1607.894</span></span><br><span class="line">BogoMIPS:              <span class="number">4205.65</span></span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             <span class="number">32</span>K</span><br><span class="line">L1i cache:             <span class="number">32</span>K</span><br><span class="line">L2 cache:              <span class="number">256</span>K</span><br><span class="line">L3 cache:              <span class="number">15360</span>K</span><br><span class="line">NUMA node0 CPU(s):     <span class="number">0</span>-<span class="number">5</span>,<span class="number">12</span>-<span class="number">17</span></span><br><span class="line">NUMA node1 CPU(s):     <span class="number">6</span>-<span class="number">11</span>,<span class="number">18</span>-<span class="number">23</span></span><br></pre></td></tr></table></figure>
<p>2颗6核双线程，一共是24 processors,也可以看到是NUMA体系，可以使用以下命令详细查看numa信息.非NUMA体系时,所有cpu都划分为一个Node<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># numactl --hardware</span></span><br><span class="line">available: <span class="number">2</span> nodes (<span class="number">0</span>-<span class="number">1</span>)</span><br><span class="line">node <span class="number">0</span> cpus: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span></span><br><span class="line">node <span class="number">0</span> size: <span class="number">31880</span> MB</span><br><span class="line">node <span class="number">0</span> free: <span class="number">19634</span> MB</span><br><span class="line">node <span class="number">1</span> cpus: <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span></span><br><span class="line">node <span class="number">1</span> size: <span class="number">32253</span> MB</span><br><span class="line">node <span class="number">1</span> free: <span class="number">29315</span> MB</span><br><span class="line">node distances:</span><br><span class="line">node   <span class="number">0</span>   <span class="number">1</span> </span><br><span class="line">  <span class="number">0</span>:  <span class="number">10</span>  <span class="number">21</span> </span><br><span class="line">  <span class="number">1</span>:  <span class="number">21</span>  <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>cpu的id不连续的原因是开启了超线程，超线程的cpuid是从新的ID开始计数的，也就是从12开始计数的</p>
</blockquote>
<p>两个node，每个node32G内存左右，这台机器我的物理内存是64G</p>
<h3 id="通过命令行查看cpu信息">通过命令行查看cpu信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取cpu名称与主频</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | cut <span class="operator">-f</span>2 <span class="operator">-d</span>: | head -n1 | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取逻辑核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取物理核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'physical id'</span> | sort | uniq | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cpu的flags</span></span><br><span class="line">cat /proc/cpuinfo | grep flags | uniq | cut <span class="operator">-f</span>2 <span class="operator">-d</span> : | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否打开超线程（检查 physical id * cpu cores 与 processor的比例 1:1为未开启）</span></span><br><span class="line">cat /proc/cpuinfo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cache大小,X自省替换</span></span><br><span class="line">sudo cat /sys/devices/system/cpu/cpuX/cache/indexX/size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看各个cpu之间与cache的mapping</span></span><br><span class="line">cat /sys/devices/system/cpu/cpuX/cache/indexX/shared_cpu_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取CPU分布的信息（id-&gt; core信息）（这一个可以看出来CPU0和CPU12在同一个core）</span></span><br><span class="line">egrep <span class="string">'processor|core id|physical id'</span> /proc/cpuinfo | cut <span class="operator">-d</span> : <span class="operator">-f</span> <span class="number">2</span> | paste - - -  | awk <span class="string">'&#123;print "CPU"$1"\tsocket "$2" core "$3&#125;'</span></span><br><span class="line">CPU0	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU1	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU2	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU3	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU4	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU5	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU6	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU7	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU8	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU9	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU10	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU11	socket <span class="number">1</span> core <span class="number">5</span></span><br><span class="line">CPU12	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU13	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU14	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU15	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU16	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU17	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU18	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU19	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU20	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU21	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU22	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU23	socket <span class="number">1</span> core <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>lscpu,numactl都是读取proc,sys文件系统信息并进行格式化，输出人性化的内容．当没有网络,而lscpu,numactl都没有安装时，只能使用这种命令行方式了</p>
<p>能用工具还是用工具，工具就是解放双手的</p>
<h3 id="Cpu_Topology可视化">Cpu Topology可视化</h3><p>lstopo 指令由 hwloc 数据包提供，创建了用户的系统示意图。lstopo-no-graphics 指令提供详尽的文本输出<br>通过lscpu与numactl获取的信息，必要的时候查询了/sys/devices/system/cpu/cpuX/*的数据将正在使用的 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz的topology进行可视化<br>详细的cache信息可以通过sysfs查看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">index0 index1 index2 index3</span><br></pre></td></tr></table></figure></p>
<p>包含以下4个目录：</p>
<ul>
<li>index0:1级数据cache </li>
<li>index1:1级指令cache </li>
<li>index2:2级cache </li>
<li>index3:3级cache,对应cpuinfo里的cache</li>
</ul>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">文件</th>
<th style="text-align:center">内容</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">type</td>
<td style="text-align:center">Data</td>
<td style="text-align:center">数据cache，如果查看index1就是Instruction</td>
</tr>
<tr>
<td style="text-align:center">Level</td>
<td style="text-align:center">1</td>
<td style="text-align:center">L1</td>
</tr>
<tr>
<td style="text-align:center">Size</td>
<td style="text-align:center">32K</td>
<td style="text-align:center">大小为32K</td>
</tr>
<tr>
<td style="text-align:center">coherency_line_size</td>
<td style="text-align:center">64</td>
<td style="text-align:center">64<em>4</em>128=32K</td>
</tr>
<tr>
<td style="text-align:center">physical_line_partition</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">ways_of_associativity</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">number_of_sets</td>
<td style="text-align:center">128</td>
</tr>
<tr>
<td style="text-align:center">shared_cpu_map</td>
<td style="text-align:center">00000101</td>
<td style="text-align:center">表示这个cache被CPU0和CPU8 share</td>
</tr>
</tbody>
</table>
<p>解释一下shared_cpu_map内容的格式：<br>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu 截取00000101的后4位，转换为2进制表示</p>
<table>
<thead>
<tr>
<th style="text-align:center">CPU id</th>
<th style="text-align:center">15</th>
<th style="text-align:center">14</th>
<th style="text-align:center">13</th>
<th style="text-align:center">12</th>
<th style="text-align:center">11</th>
<th style="text-align:center">10</th>
<th style="text-align:center">9</th>
<th style="text-align:center">8</th>
<th style="text-align:center">7</th>
<th style="text-align:center">6</th>
<th style="text-align:center">5</th>
<th style="text-align:center">4</th>
<th style="text-align:center">3</th>
<th style="text-align:center">2</th>
<th style="text-align:center">1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0×0101的2进制表示</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。<br>也可以使用上面提到的lstopo-no-graphics命令进行查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lstopo-no-graphics </span></span><br><span class="line">Machine (<span class="number">63</span>GB)</span><br><span class="line">  NUMANode L<span class="comment">#0 (P#0 31GB)</span></span><br><span class="line">    Socket L<span class="comment">#0 + L3 L#0 (15MB)</span></span><br><span class="line">      L2 L<span class="comment">#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0</span></span><br><span class="line">        PU L<span class="comment">#0 (P#0)</span></span><br><span class="line">        PU L<span class="comment">#1 (P#12)</span></span><br><span class="line">      L2 L<span class="comment">#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1</span></span><br><span class="line">        PU L<span class="comment">#2 (P#1)</span></span><br><span class="line">        PU L<span class="comment">#3 (P#13)</span></span><br><span class="line">      L2 L<span class="comment">#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2</span></span><br><span class="line">        PU L<span class="comment">#4 (P#2)</span></span><br><span class="line">        PU L<span class="comment">#5 (P#14)</span></span><br><span class="line">      L2 L<span class="comment">#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3</span></span><br><span class="line">        PU L<span class="comment">#6 (P#3)</span></span><br><span class="line">        PU L<span class="comment">#7 (P#15)</span></span><br><span class="line">      L2 L<span class="comment">#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4</span></span><br><span class="line">        PU L<span class="comment">#8 (P#4)</span></span><br><span class="line">        PU L<span class="comment">#9 (P#16)</span></span><br><span class="line">      L2 L<span class="comment">#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5</span></span><br><span class="line">        PU L<span class="comment">#10 (P#5)</span></span><br><span class="line">        PU L<span class="comment">#11 (P#17)</span></span><br><span class="line">    HostBridge L<span class="comment">#0</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">1000</span>:<span class="number">0086</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#0 "enp4s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#1 "enp4s0f1"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#2 "enp4s0f2"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#3 "enp4s0f3"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#4 "enp6s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#5 "enp6s0f1"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1</span>d6b</span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">102</span>b:<span class="number">0532</span></span><br><span class="line">          GPU L<span class="comment">#6 "card0"</span></span><br><span class="line">          GPU L<span class="comment">#7 "controlD64"</span></span><br><span class="line">      PCI <span class="number">8086</span>:<span class="number">1</span>d02</span><br><span class="line">        Block L<span class="comment">#8 "sda"</span></span><br><span class="line">  NUMANode L<span class="comment">#1 (P#1 31GB) + Socket L#1 + L3 L#1 (15MB)</span></span><br><span class="line">    L2 L<span class="comment">#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6</span></span><br><span class="line">      PU L<span class="comment">#12 (P#6)</span></span><br><span class="line">      PU L<span class="comment">#13 (P#18)</span></span><br><span class="line">    L2 L<span class="comment">#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7</span></span><br><span class="line">      PU L<span class="comment">#14 (P#7)</span></span><br><span class="line">      PU L<span class="comment">#15 (P#19)</span></span><br><span class="line">    L2 L<span class="comment">#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8</span></span><br><span class="line">      PU L<span class="comment">#16 (P#8)</span></span><br><span class="line">      PU L<span class="comment">#17 (P#20)</span></span><br><span class="line">    L2 L<span class="comment">#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9</span></span><br><span class="line">      PU L<span class="comment">#18 (P#9)</span></span><br><span class="line">      PU L<span class="comment">#19 (P#21)</span></span><br><span class="line">    L2 L<span class="comment">#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10</span></span><br><span class="line">      PU L<span class="comment">#20 (P#10)</span></span><br><span class="line">      PU L<span class="comment">#21 (P#22)</span></span><br><span class="line">    L2 L<span class="comment">#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11</span></span><br><span class="line">      PU L<span class="comment">#22 (P#11)</span></span><br><span class="line">      PU L<span class="comment">#23 (P#23)</span></span><br></pre></td></tr></table></figure></p>
<p>这个得到的是文本的拓扑，这个转换成一个图看的要清楚一些<br><img src="http://static.zybuluo.com/zphj1987/u4t0qbmeh2i4nemf007vqepr/nodesock.png" alt="nodesock.png-45.9kB"></p>
<h4 id="NUMA分组信息">NUMA分组信息</h4><ul>
<li>通过图可以看到cpu为numa架构,且有两个node</li>
<li>将同一socket内的cpu(threads)都划分在一个node中.通过上图也解释了node中cpu序列不连续的问题.因为同一个Core上的两个Threads是超线程出来的.超线程Thread的cpu id在原有的core id基础上增长的</li>
<li>每个node中有32G左右的本地RAM可用</li>
</ul>
<h4 id="cache信息">cache信息</h4><ul>
<li>每个core都有独立的二级缓存,而不是socket中所有的core共享二级缓存</li>
<li>同node中的cpu共享三级缓存</li>
<li>跨node的内存访问的花费要大些</li>
</ul>
<h3 id="cpu绑定注意的几点">cpu绑定注意的几点</h3><ul>
<li>Numa体系中,如果夸node绑定,性能会下降.因为L3 cache命中率低,跨node内存访问代价高.</li>
<li>绑定同Node,同一个Core中的两个超线程出来的cpu,性能会急剧下降.cpu密集型的线程硬件争用严重.”玩转CPU Topology”中也提到了.</li>
<li>Numa架构可能引起swap insanity.需要注意</li>
</ul>
<h3 id="测试CPU绑定性能">测试CPU绑定性能</h3><p>这个部分就不在这里赘述了，上面是把cpu比较清晰的剥离出来，至于效果，需要在实际环境当中去验证了，有可能变坏，也有可能变好</p>
<p>本篇参考了很多网络上的很多其他资料</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
</feed>
