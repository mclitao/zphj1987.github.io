<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[zphj1987'Blog]]></title>
  <subtitle><![CDATA[现在所学，终有所用]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.zphj1987.com/"/>
  <updated>2016-07-14T15:38:51.328Z</updated>
  <id>http://www.zphj1987.com/</id>
  
  <author>
    <name><![CDATA[zphj1987]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[通过ceph-deploy安装不同版本ceph]]></title>
    <link href="http://www.zphj1987.com/2016/07/14/%E9%80%9A%E8%BF%87ceph-deploy%E5%AE%89%E8%A3%85%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%ACceph/"/>
    <id>http://www.zphj1987.com/2016/07/14/通过ceph-deploy安装不同版本ceph/</id>
    <published>2016-07-14T15:28:33.000Z</published>
    <updated>2016-07-14T15:38:51.328Z</updated>
    <content type="html"><![CDATA[<p>之前有在论坛写了怎么用yum安装ceph，但是看到ceph社区的群里还是有人经常用ceph-deploy进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49" target="_blank" rel="external">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装ceph-deploy这个工具，机器上应该安装好epel源和base源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从epel的repo里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>
<a id="more"></a>
<p>现在什么都不修改，看下默认的安装会什么样的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (<span class="number">1.5</span>.<span class="number">25</span>): /usr/bin/ceph-deploy install lab8106</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version hammer on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs http://ceph.com/rpm-hammer/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">[lab8106][WARNIN] http://ceph.com/rpm-hammer/rhel7/x86_64/repodata/repomd.xml: [Errno <span class="number">14</span>] HTTP Error <span class="number">404</span> - Not Found</span><br></pre></td></tr></table></figure>
<p>这个默认的版本没安装成功<br>这个地方的原因是默认会去下载<a href="http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm" target="_blank" rel="external">http://ceph.com/rpm-hammer/el7/noarch/ceph-release-1-0.el7.noarch.rpm</a> 这个包，而这个包是有问题的，安装以后<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># cat /etc/yum.repos.d/ceph.repo |grep baseurl</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/<span class="variable">$basearch</span></span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/noarch</span><br><span class="line">baseurl=http://ceph.com/rpm-hammer/rhel7/SRPMS</span><br></pre></td></tr></table></figure></p>
<p>这路径rhel7是根本就没有的，所以这个地方所以会出错，可以去修改repo的方式解决，这里先忽略这个问题，我们换一个ceph-deploy看看会怎样</p>
<h3 id="二、安装另外版本的ceph-deploy">二、安装另外版本的ceph-deploy</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum remove ceph-deploy</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rpm -ivh http://download.ceph.com/rpm/el7/noarch/ceph-deploy-1.5.34-0.noarch.rpm</span></span><br></pre></td></tr></table></figure>
<p>安装好了后，再次执行安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install lab8106</span></span><br><span class="line">···</span><br><span class="line">[ceph_deploy.install][DEBUG ] Installing stable version jewel on cluster ceph hosts lab8106</span><br><span class="line">···</span><br><span class="line">lab8106][INFO  ] Running <span class="built_in">command</span>: rpm --import https://download.ceph.com/keys/release.asc</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: rpm -Uvh --replacepkgs https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-<span class="number">1</span>-<span class="number">0</span>.el7.noarch.rpm</span><br><span class="line">[lab8106][INFO  ] Running <span class="built_in">command</span>: yum -y install ceph ceph-radosgw</span><br><span class="line">···</span><br><span class="line">[lab8106][DEBUG ] --&gt; Running transaction check</span><br><span class="line">[lab8106][DEBUG ] ---&gt; Package ceph.x86_64 <span class="number">1</span>:<span class="number">10.2</span>.<span class="number">2</span>-<span class="number">0</span>.el7 will be installed</span><br><span class="line">···</span><br></pre></td></tr></table></figure></p>
<p>如果网络好的话，那么可以看到，执行这个命令后会在ceph.com的官网上去下载安装包了，如果网络不好的话，就会卡住了，这里是要说明的是</p>
<blockquote>
<p>不同的 ceph-deploy 去 install 的时候会安装不同的版本，这个因为代码里面会写上当时的版本，这样默认安装的就是当时的版本了</p>
</blockquote>
<p>到了这里在准备开始本篇的主题了，主要的目的有两个</p>
<ul>
<li>自己选择想安装的版本</li>
<li>自己选择通过什么地址安装</li>
</ul>
<p>第一个是解决了安装自己的版本，第二个是避免ceph.com无法访问的时候无法安装，通过国内的源进行加速</p>
<h3 id="三、自定义安装ceph">三、自定义安装ceph</h3><h4 id="通过阿里云安装ceph-hammer">通过阿里云安装ceph-hammer</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-hammer/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<p>通过这个命令，就通过阿里云的源安装了ceph的hammer版本的ceph</p>
<h4 id="通过阿里云安装ceph-jewel">通过阿里云安装ceph-jewel</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># yum clean all</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rm -rf /etc/yum.repos.d/ceph*</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># ceph-deploy install  lab8106 --repo-url=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/ --gpg-url=http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7</span></span><br></pre></td></tr></table></figure>
<h3 id="四、总结">四、总结</h3><p>安装的方式有很多，对于新手来说如果想用 ceph-deploy 去安装的话，可以根据上面的很简单的命令就解决了，这里没有写本地做源的相关的知识，安装这一块怎么顺手怎么来，不要在安装上面耗费太多的时间</p>
<h3 id="五、变更记录">五、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-14</td>
</tr>
</tbody>
</table>
<h3 id="六、打赏通道">六、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


<h3 id="八、广告">八、广告</h3><p>私人朋友群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜（收费入群）</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前有在论坛写了怎么用yum安装ceph，但是看到ceph社区的群里还是有人经常用ceph-deploy进行安装，然后会出现各种不可控的情况，虽然不建议用ceph-deploy安装，但是既然想用，那就研究下怎么用好</p>
<p>先给一个连接： <a href="http://bbs.ceph.org.cn/article/49">centos7通过yum安装ceph</a></p>
<p>首先机器需要安装ceph-deploy这个工具，机器上应该安装好epel源和base源，这个可以参考上面的那个连接，也可以自己准备好</p>
<h3 id="一、安装ceph-deploy">一、安装ceph-deploy</h3><p>使用yum直接安装<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 yum.repos.d]<span class="comment"># yum install ceph-deploy</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks, priorities</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Resolving Dependencies</span><br><span class="line">--&gt; Running transaction check</span><br><span class="line">---&gt; Package ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7 will be installed</span><br><span class="line">···</span><br><span class="line">===================================================================================================</span><br><span class="line"> Package            Arch            Version             Repository                    Size</span><br><span class="line">===================================================================================================</span><br><span class="line">Installing:</span><br><span class="line"> ceph-deploy        noarch          <span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7         epel                         <span class="number">156</span> k</span><br><span class="line">···</span><br><span class="line">Installed:</span><br><span class="line">  ceph-deploy.noarch <span class="number">0</span>:<span class="number">1.5</span>.<span class="number">25</span>-<span class="number">1</span>.el7</span><br><span class="line">Complete!</span><br></pre></td></tr></table></figure></p>
<p>可以看到是从epel的repo里面下载的版本为1.5.25，如果从ceph源里面下载的这个版本可能会更高一点，这个没什么问题</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[重构rbd镜像的元数据]]></title>
    <link href="http://www.zphj1987.com/2016/07/02/%E9%87%8D%E6%9E%84rbd%E9%95%9C%E5%83%8F%E7%9A%84%E5%85%83%E6%95%B0%E6%8D%AE/"/>
    <id>http://www.zphj1987.com/2016/07/02/重构rbd镜像的元数据/</id>
    <published>2016-07-01T18:52:33.000Z</published>
    <updated>2016-07-01T19:01:08.994Z</updated>
    <content type="html"><![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>将这几个镜像下载下来<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_header.60276b8b4567 rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd get rbd_id.zp rbd_id.zp</span></span><br></pre></td></tr></table></figure></p>
<p>查看下载下来的几个镜像的元数据的文件信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># ll</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_directory</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">0</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">16</span> Jul  <span class="number">1</span> <span class="number">23</span>:<span class="number">28</span> rbd_id.zp</span><br></pre></td></tr></table></figure></p>
<p>有没有发现有两个镜像的文件大小是0，这个是因为rbd format 2 格式下（默认格式），这两个对象的元数据信息是存储在扩展属性里面的，所以下载下来的对象是没有内容，那我们怎么查看这个属性，看下面讲述的查询相关的操作</p>
<h4 id="2、查询这个image的信息">2、查询这个image的信息</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure>
<p>这里可以看到这个image文件的大小，对象大小，前缀信息，属性相关信息，这是用我们比较常规的方式来查询到的信息，现在用另外一种方式来查询信息，查到的会是另外一种方式，也就是上面一节提到的空对象的扩展属性的查询</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure>
<p>先来查询 rbd_directory 这个的元数据信息，这个里面的信息可以看到两组对应关系<br>id_60276b8b4567,就是这个image的id，也是前缀信息，后面对应的是一个名称zp<br>第二组name_zp,对应的就是后面的60276b8b4567，也就是名称对应到id<br>，那个value值就是后面的字符串对应的16进制的一种方式，这个地方就是需要备份的元数据信息，现在准备做第一次重构，重构rbd_directory这个的元数据信息，这个rbd_directory记录所属存储池有哪些镜像</p>
<h3 id="三、恢复rbd_directory的元数据信息">三、恢复rbd_directory的元数据信息</h3><p>先来破坏这个元数据信息，破坏的方式很简单，就是做删除<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到删除了元数据信息以后，再进行镜像的ls，是查询不到信息的</p>
<p>开始做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># touch rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_directory rbd_directory</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br></pre></td></tr></table></figure></p>
<p>上面做的三步是创建一个空文件，然后上传，然后列属性，可以看到，都是空的（这个地方也可以不创建空对象，直接做后面的给属性的时候，集群会自动创建相关的对象）<br>现在给这个对象写入属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x02\\x00\\x00\\x00\\x7a\\x70|rados -p rbd setomapval rbd_directory id_60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37|rados -p rbd setomapval rbd_directory name_zp</span></span><br></pre></td></tr></table></figure></p>
<p>写入的值就是上面让记录下来的信息，这个地方就用这个格式就行了，为什么要这么写，因为16进制的字符是需要转义的，之前不清楚怎么写，在邮件列表中提问后，有一个人低调的给回复了怎么写入这种进制数据，现在就这么固定写法就行了，现在再查询写入以后的属性情况</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_directory</span></span><br><span class="line">id_60276b8b4567</span><br><span class="line">value (<span class="number">6</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">02</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">7</span>a <span class="number">70</span>                                 |....zp|</span><br><span class="line"><span class="number">00000006</span></span><br><span class="line"></span><br><span class="line">name_zp</span><br><span class="line">value (<span class="number">16</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br></pre></td></tr></table></figure>
<p>到这里 rbd_directory这个的信息就恢复了，下面再进行image的元数据的信息的恢复</p>
<h3 id="四、恢复image的元数据信息">四、恢复image的元数据信息</h3><p>先查询下这个对象包含的元数据信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>记录下这个信息，然后进行破坏，跟上面一样的删除掉对象<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_header.60276b8b4567</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd ls</span></span><br><span class="line">zp</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd info zp</span></span><br><span class="line"><span class="number">2016</span>-<span class="number">07</span>-<span class="number">02</span> <span class="number">00</span>:<span class="number">57</span>:<span class="number">50.150559</span> <span class="number">7</span>ff4b56b3700 -<span class="number">1</span> librbd::image::OpenRequest: failed to retreive immutable metadata: (<span class="number">2</span>) No such file or directory</span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在删除了这个对象以后，已经无法查询到镜像信息了，当然也就无法使用了，下面开始进行image的元数据信息的重构<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00|rados -p rbd setomapval rbd_header.60276b8b4567 features</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x15\\x00\\x00\\x00\\x72\\x62\\x64\\x5f\\x64\\x61\\x74\\x61\</span></span><br><span class="line">\x2e\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37    |rados -p rbd setomapval rbd_header.<span class="number">60276</span>b8b4567  object_prefix</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x16|rados -p rbd setomapval rbd_header.60276b8b4567 order</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\xc4\\x09\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 size</span><br><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00   |rados -p rbd seto</span></span><br><span class="line">mapval rbd_header.<span class="number">60276</span>b8b4567 snap_seq</span><br></pre></td></tr></table></figure></p>
<p>设置完了所有属性后查询，验证是否恢复了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd listomapvals rbd_header.60276b8b4567</span></span><br><span class="line">features</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">01</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">object_prefix</span><br><span class="line">value (<span class="number">25</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">15</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">72</span> <span class="number">62</span> <span class="number">64</span> <span class="number">5</span>f  <span class="number">64</span> <span class="number">61</span> <span class="number">74</span> <span class="number">61</span> <span class="number">2</span>e <span class="number">36</span> <span class="number">30</span> <span class="number">32</span>  |....rbd_data.<span class="number">602</span>|</span><br><span class="line"><span class="number">00000010</span>  <span class="number">37</span> <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span>  <span class="number">37</span>                       |<span class="number">76</span>b8b4567|</span><br><span class="line"><span class="number">00000019</span></span><br><span class="line"></span><br><span class="line">order</span><br><span class="line">value (<span class="number">1</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">16</span>                                                |.|</span><br><span class="line"><span class="number">00000001</span></span><br><span class="line"></span><br><span class="line">size</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> c4 <span class="number">09</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br><span class="line"></span><br><span class="line">snap_seq</span><br><span class="line">value (<span class="number">8</span> bytes) :</span><br><span class="line"><span class="number">00000000</span>  <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">00</span>                           |........|</span><br><span class="line"><span class="number">00000008</span></span><br></pre></td></tr></table></figure></p>
<p>元数据完整的回来了<br>上面已经将两个导出的空对象元数据信息恢复好了，再看最后一个有文件大小的对象怎么做恢复</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># cat rbd_id.zp </span></span><br><span class="line"></span><br><span class="line"><span class="number">60276</span>b8b4567[root@lab8106 zp]<span class="comment">#</span></span><br></pre></td></tr></table></figure>
<p>这个第一种方式是直接备份好,然后倒入的方式<br>跟上面的方法一样，开始通过删除对象来破坏<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd rm rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd: error opening image zp: (<span class="number">2</span>) No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>可以看到破坏了就无法访问镜像了，下面直接利用备份对象倒入的方式进行恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># rados -p rbd put rbd_id.zp rbd_id.zp</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># rbd -p rbd info zp</span></span><br><span class="line">rbd image <span class="string">'zp'</span>:</span><br><span class="line">	size <span class="number">40000</span> MB <span class="keyword">in</span> <span class="number">10000</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">60276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到，倒入后即可，也可以用另外一种方式，记录字符串的方式进行备份<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># xxd rbd_id.zp</span></span><br><span class="line"><span class="number">0000000</span>: <span class="number">0</span>c00 <span class="number">0000</span> <span class="number">3630</span> <span class="number">3237</span> <span class="number">3662</span> <span class="number">3862</span> <span class="number">3435</span> <span class="number">3637</span>  ....<span class="number">60276</span>b8b4567</span><br></pre></td></tr></table></figure></p>
<p>我们可以查看这个文件的16进制的信息输出，这个信息就是要保留的字符串信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zp</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>需要保留的就是这个信息,我们根据这个信息来重新创建一个文件，然后检查文件内容是不是能跟下载下来的对象一样<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 zp]<span class="comment"># echo -en \\x0c\\x00\\x00\\x00\\x36\\x30\\x32\\x37\\x36\\x62\\x38\\x62\\x34\\x35\\x36\\x37   &gt;rbd_id.zpre</span></span><br><span class="line">[root@lab8106 zp]<span class="comment"># hexdump -C rbd_id.zpre</span></span><br><span class="line"><span class="number">00000000</span>  <span class="number">0</span>c <span class="number">00</span> <span class="number">00</span> <span class="number">00</span> <span class="number">36</span> <span class="number">30</span> <span class="number">32</span> <span class="number">37</span>  <span class="number">36</span> <span class="number">62</span> <span class="number">38</span> <span class="number">62</span> <span class="number">34</span> <span class="number">35</span> <span class="number">36</span> <span class="number">37</span>  |....<span class="number">60276</span>b8b4567|</span><br><span class="line"><span class="number">00000010</span></span><br></pre></td></tr></table></figure></p>
<p>可以看到，可以用字符串完整恢复这个对象了，然后put进集群即可恢复了</p>
<h3 id="五、总结">五、总结</h3><p>可以看到，所有的元数据信息都可以以字符串的形式保留下来，然后进行元数据重构，其中的rbd_id.zp这个可以保存对象方式，也可以是获取对象后，然后保存16进制字符串信息，然后再进行本地创建对象,然后put的方式，其它的两个空对象可以用设置属性的方式进行恢复，在openstack场景下，这些元数据信息最好都保留下来，一旦有问题的时候，可以很方便的进行数据的重构，备份并不是说所有数据都需要备份，对于这种数据量很小，而且很重要的信息，定期备份一下，也许哪天就用上了</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-02</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

<h3 id="打个广告">打个广告</h3><p>私人朋友ceph技术讨论收费群：</p>
<p><center><br><img src="http://7xi6lo.com1.z0.glb.clouddn.com/qqqun2.png" alt=""><br></center><br>欢迎咨询入群事宜</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这个已经很久之前已经实践成功了，现在正好有时间就来写一写，目前并没有在其他地方有类似的分享，虽然我们自己的业务并没有涉及到云计算的场景，之前还是对rbd镜像这一块做了一些基本的了解，因为一直比较关注故障恢复这一块，东西并不难，总之一切不要等到出了问题再去想办法，提前准备总是好的，如果你有集群的问题，生产环境需要恢复的欢迎找我</p>
<h3 id="一、前言">一、前言</h3><p>rbd的镜像的元数据，这个是什么？这里所提到的元数据信息，是指跟这个image信息有关的元数据信息，就是image的大小名称等等一系列的信息，本篇将讲述怎么去重构这些信息，重构的前提就是做好了信息的记录，然后做重构</p>
<h3 id="二、记录元数据信息">二、记录元数据信息</h3><h4 id="1、创建一个image">1、创建一个image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd -p rbd create zp --size 40000</span></span><br></pre></td></tr></table></figure>
<p>这里是在rbd存储池当中创建的一个名称为zp的，大小为40G的image文件</p>
<p>如果没有其他的image的情况下，我们来查看下对象信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rados -p rbd ls</span></span><br><span class="line">rbd_header.<span class="number">60276</span>b8b4567</span><br><span class="line">rbd_directory</span><br><span class="line">rbd_id.zp</span><br></pre></td></tr></table></figure></p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[parted分区对齐]]></title>
    <link href="http://www.zphj1987.com/2016/06/24/parted%E5%88%86%E5%8C%BA%E5%AF%B9%E9%BD%90/"/>
    <id>http://www.zphj1987.com/2016/06/24/parted分区对齐/</id>
    <published>2016-06-24T08:32:43.000Z</published>
    <updated>2016-06-24T08:35:39.321Z</updated>
    <content type="html"><![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>
<a id="more"></a>
<h3 id="二、通过计算分区">二、通过计算分区</h3><h4 id="获取磁盘的几个参数（这里是软raid）">获取磁盘的几个参数（这里是软raid）</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cat /sys/block/md127/queue/optimal_io_size</span></span><br><span class="line"><span class="number">3670016</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/minimum_io_size</span></span><br><span class="line"><span class="number">524288</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/alignment_offset</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="comment"># cat /sys/block/md127/queue/physical_block_size</span></span><br><span class="line"><span class="number">512</span></span><br></pre></td></tr></table></figure>
<p>optimal_io_size 加上 alignment_offset 的和 然后除以  physical_block_size<br>在这个环境下是：<br>(3670016 + 0) / 512 = 7168</p>
<p>那么分区的时候命令就应该是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">7168</span>s <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>如果上面的顺利的完成检查一下  (‘1’是分区的编号):</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 aligned</p>
</blockquote>
<p>这个是正常的结果，如果没对齐就会是</p>
<blockquote>
<p>(parted) align-check optimal 1<br>1 not aligned</p>
</blockquote>
<h3 id="三、其他情况">三、其他情况</h3><p>默认情况下直接用下列的分区参数就可以，出现提示再用上面的计算，总之最后align-check 验证下<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<h3 id="四、相关文章">四、相关文章</h3><p><a href="http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/" target="_blank" rel="external">How to align partitions for best performance using parted</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-24</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、分区提示未对齐">一、分区提示未对齐</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph]<span class="comment"># parted /dev/sdd </span></span><br><span class="line">GNU Parted <span class="number">3.1</span></span><br><span class="line">Using /dev/sdd</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">'help'</span> to view a list of commands.</span><br><span class="line">(parted) p                                                                </span><br><span class="line">Model: SEAGATE ST3300657SS (scsi)</span><br><span class="line">Disk /dev/sdd: <span class="number">300</span>GB</span><br><span class="line">Sector size (logical/physical): <span class="number">512</span>B/<span class="number">512</span>B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart primary <span class="number">0</span> <span class="number">100</span>%                                            </span><br><span class="line">Warning: The resulting partition is not properly aligned <span class="keyword">for</span> best performance.</span><br><span class="line">Ignore/Cancel?</span><br></pre></td></tr></table></figure>
<p>Warning: The resulting partition is not properly aligned for best performance.<br>分区的时候提示不是最好的模式，这个是因为没有对齐的原因，在默认情况下我都是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkpart primary <span class="number">1</span> <span class="number">100</span>%</span><br></pre></td></tr></table></figure></p>
<p>这个一般都是对齐的，但是最近遇到一个做了raid5的怎么都提示不行，然后搜索了下资料，这个地方是要计算下比较好的</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd的增量备份和恢复]]></title>
    <link href="http://www.zphj1987.com/2016/06/22/rbd%E7%9A%84%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/"/>
    <id>http://www.zphj1987.com/2016/06/22/rbd的增量备份和恢复/</id>
    <published>2016-06-21T17:17:24.000Z</published>
    <updated>2016-06-22T01:40:00.378Z</updated>
    <content type="html"><![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br><a id="more"></a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v1 testimage_v1</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v1那个时间点的差异数据导出来了testimage_v1，导出成本地文件testimage_v1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 testimage_v2</span><br></pre></td></tr></table></figure>
<p>这个命令是导出了从开始创建image到快照v2那个时间点的差异数据导出来了，导出成本地文件testimage_v2<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage@v2 --from-snap v1 testimage_v1_v2</span><br></pre></td></tr></table></figure></p>
<p>这个命令是导出了从v1快照时间点到v2快照时间点的差异数据，导出成本地文件testimage_v1_v2</p>
<p>这个地方上面的导出的数据：</p>
<blockquote>
<p>v1时间点数据 + v1_v2之间数据 = v2 时间点数据</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">export</span>-diff rbd/testimage testimage_now</span><br></pre></td></tr></table></figure>
<p>这个就是导出了从image创建到当前的时间点的差异数据</p>
<h3 id="三、快照的数据恢复">三、快照的数据恢复</h3><p><img src="http://static.zybuluo.com/zphj1987/ue1feys17yiya6doa2audkbo/image_1alpuprird31dltilpro7kf52a.png" alt=""></p>
<p>快照的恢复过程使用的是刚刚上面提到的备份到本地的那些文件<br>首先随便创建一个image,名称大小都不限制，因为后面恢复的时候会覆盖掉大小的信息<br><figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line">rbd <span class="built_in">create</span> testbacknew <span class="comment">--size 1</span></span><br></pre></td></tr></table></figure></p>
<p>现在假如想恢复到v2那个快照的时间点，那么可以用两个方法</p>
<p>1、直接基于v2的时间点的快照做恢复<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd import-diff testimage_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>2、直接基于v1的时间点的数据，和后面的增量的v1_v2数据(要按顺序导入)<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd impot-diff testimage_v1 rbd/testbacknew</span><br><span class="line">rbd impot-diff testimage_v1_v2 rbd/testbacknew</span><br></pre></td></tr></table></figure></p>
<p>到这里数据就已经恢复了</p>
<h3 id="四、如何利用这个">四、如何利用这个</h3><p>实际项目当中就是，定期做快照，然后导出某个时间点快照的数据，然后导出增量的快照的数据，就可以了，例如：<br>今天对所有的rbd的image做一个基础快照，然后导出这个快照的数据，然后从今天开始，每天晚上做一个快照，然后导出快照时间点之间的数据，这样每天导出来的就是一个增量的数据了，在做恢复的时候，就从第一个快照导入，然后按顺序导入增量的快照即可，也可以定期做一个快照，导出完整的快照数据，以防中间的增量快照漏了，然后就是要注意可以定期清理快照，如果是做备份的模式，在导入了快照数据后，也可以清理一些本地的数据，本地数据做异地机房复制的时候也可以做一下数据的压缩，来减少数据量的传输</p>
<h3 id="五、相关文章">五、相关文章</h3><p><a href="https://github.com/skuicloud/openstack-hacker/tree/master/tsinghua-cluster/script/ceph/volume_backup" target="_blank" rel="external">rbd备份还原的脚本</a><br><a href="http://ceph.com/dev-notes/incremental-snapshots-with-rbd/" target="_blank" rel="external">INCREMENTAL SNAPSHOTS WITH RBD</a><br><a href="http://cephnotes.ksperis.com/blog/2014/08/12/rbd-replication" target="_blank" rel="external">RBD Replication</a><br><a href="http://www.evil0x.com/posts/14638.html" target="_blank" rel="external">云杉网络：基于Ceph RBD的快照技术实现异地灾备</a></p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-22</td>
</tr>
<tr>
<td style="text-align:center">修改错别字</td>
<td style="text-align:center">武汉-运维-磨渣 -from- 运维-北京-小白</td>
<td style="text-align:center">2016-06-22</td>
</tr>
</tbody>
</table>
<h3 id="七、打赏通道">七、打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、前言">一、前言</h3><p>快照的功能一般是基于时间点做一个标记，然后在某些需要的时候，将状态恢复到标记的那个点，这个有一个前提是底层的东西没用破坏，举个简单的例子，<strong>Vmware</strong> 里面对虚拟机做了一个快照，然后做了一些系统的操作，想恢复快照，前提是存储快照的存储系统没用破坏，一旦破坏了是无法恢复的</p>
<p>ceph里面也有快照的功能，同样的，在这里的快照是用来保存存储系统上的状态的，数据的快照能成功恢复的前提是存储系统是好的，而一旦存储系统坏了，快照同时会失效的，本篇文章利用ceph的快照去实现一个增量的备份功能，网上也有很多这个脚本，这里主要是对里面细节做一个实践，具体集成到一套系统里面去，自己去做一个策略就行了，总之多备份一下，以备不时之需，并且也可以实现跨机房的增量备份，这个在某些云计算公司已经实现了，这样一旦发生故障的时候，能够把损失减到最小</p>
<h3 id="二、快照的创建和数据的导出">二、快照的创建和数据的导出</h3><p><img src="http://static.zybuluo.com/zphj1987/t933vz49n801mowt0gj3mbts/image_1alqs3lm81ss11n1vg7k1mle1eq39.png" alt=""></p>
<p>上图是一个快照的创建和导出的过程，这里详细的描述下这些操作<br>创建快照<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rbd snap create testimage@v1</span><br><span class="line">rbd snap create testimage@v2</span><br></pre></td></tr></table></figure></p>
<p>这两个命令是在时间点v1和时间点v2分别做了两个快照<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rgw实现nfs的首测]]></title>
    <link href="http://www.zphj1987.com/2016/06/19/rgw%E5%AE%9E%E7%8E%B0nfs%E7%9A%84%E9%A6%96%E6%B5%8B/"/>
    <id>http://www.zphj1987.com/2016/06/19/rgw实现nfs的首测/</id>
    <published>2016-06-18T18:47:56.000Z</published>
    <updated>2016-06-18T18:56:51.676Z</updated>
    <content type="html"><![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS" target="_blank" rel="external">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718" target="_blank" rel="external">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>
<a id="more"></a>
<h3 id="二、功能架构图">二、功能架构图</h3><p><img src="http://static.zybuluo.com/zphj1987/o5ruvtr9f7nyegbv0ly7ekv5/image_1alibfc78g96dsa1c26crkgis1e.png" alt="image_1alibfc78g96dsa1c26crkgis1e.png-78.3kB"><br>简单说明一下：<br>集群配置s3接口，nfs-genesha将s3接口转换成nfs，然后nfs客户端挂载后访问的就是s3的bucket里面的数据了</p>
<h3 id="三、准备工作">三、准备工作</h3><p>准备代码，这个是需要从源码编译的，并且需要将模块编译进去才可以的，源码分支地址：</p>
<blockquote>
<p><a href="https://github.com/nfs-ganesha/nfs-ganesha/tree/next" target="_blank" rel="external">https://github.com/nfs-ganesha/nfs-ganesha/tree/next</a></p>
</blockquote>
<p>这个地方要注意下，需要使用next分支<br>使用git 进行clone分支到本地<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b next https://github.com/nfs-ganesha/nfs-ganesha.git</span><br></pre></td></tr></table></figure></p>
<p>检查是否有这个RGW模块目录</p>
<blockquote>
<p>nfs-ganesha/src/FSAL/FSAL_RGW/</p>
</blockquote>
<p>默认clone下来后  <code>nfs-ganesha/src/libntirpc/</code> 这个目录是空的，而这个是因为如果在git里面某个目录嵌套的用了其他项目的代码，并且也是有git的分支的话，clone下来就会是空的，这个在ceph的源码里面也会这样，具体的看看下图：<br><img src="http://static.zybuluo.com/zphj1987/00dbog7s6nzbze55qyjilzt9/libntir.png" alt="libntir.png-38.4kB"><br>下载下面的链接的这个版本，然后把代码解压到nfs-ganesha/src/libntirpc/这个目录当中去<br><a href="https://codeload.github.com/nfs-ganesha/ntirpc/zip/7e61a9b23e078ce922a5697669d46f9eaf16ccd4" target="_blank" rel="external">https://codeload.github.com/nfs-ganesha/ntirpc/zip/7e61a9b23e078ce922a5697669d46f9eaf16ccd4</a></p>
<p>代码的编译采用的是cmake的模式(cmake目录后面接的是nfs-ganesha代码的src目录)</p>
<blockquote>
<p>注意在执行cmake之前编译环境需要安装librgw2-devel这个包，才能编译成功，执行cmake的时候检查下是否真的开启了</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/u3xku4jf3swljl0bub9zkwv0/image_1alian0db17e91gg1mhg866i1q11.png" alt="image_1alian0db17e91gg1mhg866i1q11.png-11.1kB"></p>
<p>开始编译安装过程，创建一个用于编译的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 newbian]<span class="comment">#mkdir mybuild</span></span><br><span class="line">[root@lab8106 newbian]<span class="comment">#cd mybuild</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#cmake -DUSE_FSAL_RGW=ON ../nfs-ganesha/src/</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment"># ll FSAL/FSAL_RGW/</span></span><br><span class="line">total <span class="number">16</span></span><br><span class="line">drwxr-xr-x <span class="number">3</span> root root    <span class="number">83</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> CMakeFiles</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root  <span class="number">2979</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> cmake_install.cmake</span><br><span class="line">-rw-r--r-- <span class="number">1</span> root root <span class="number">10164</span> Jun <span class="number">19</span> <span class="number">01</span>:<span class="number">59</span> Makefile</span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make</span></span><br><span class="line">[root@lab8106 mybuild]<span class="comment">#make install</span></span><br></pre></td></tr></table></figure></p>
<p>编译安装工作就到此完成了，还是比较简单的</p>
<h3 id="四、配置服务">四、配置服务</h3><h4 id="1、准备一个s3的环境，我的如下：">1、准备一个s3的环境，我的如下：</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">radosgw-admin user create --uid=admin --display-name=<span class="string">"admin"</span>   --access-key=admin  --secret=admin</span><br></pre></td></tr></table></figure>
<p>用户信息如下：</p>
<ul>
<li>s3的User_Id：admin </li>
<li>s3的Access_Key:admin </li>
<li>s3的Secret_Access_Key:admin</li>
</ul>
<p>注意，配置ganesha-nfs服务的机器需要安装librgw</p>
<h4 id="2、修改ganesha-nfs的配置文件">2、修改ganesha-nfs的配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/ganesha/ganesha.conf</span><br></pre></td></tr></table></figure>
<p>修改如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">EXPORT</span><br><span class="line">&#123;</span><br><span class="line">        Export_ID=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        Path = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Pseudo = <span class="string">"/"</span>;</span><br><span class="line"></span><br><span class="line">        Access_Type = RW;</span><br><span class="line"></span><br><span class="line">        NFS_Protocols = <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">        Transport_Protocols = TCP;</span><br><span class="line"></span><br><span class="line">        FSAL &#123;</span><br><span class="line">                Name = RGW;</span><br><span class="line">                User_Id = <span class="string">"admin"</span>;</span><br><span class="line">                Access_Key_Id =<span class="string">"admin"</span>;</span><br><span class="line">                Secret_Access_Key = <span class="string">"admin"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RGW &#123;</span><br><span class="line">    ceph_conf = <span class="string">"/etc/ceph/ceph.conf"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>RGW-NFS配置文件的模板路径在：</p>
<blockquote>
<p>/usr/share/doc/ganesha/config_samples/rgw.conf</p>
</blockquote>
<h4 id="4、启动ganesha-nfs服务">4、启动ganesha-nfs服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">systemctl restart  nfs-ganesha.service</span><br></pre></td></tr></table></figure>
<h4 id="5、NFS客户端挂载ganesha-nfs服务">5、NFS客户端挂载ganesha-nfs服务</h4><p>找一台其它的客户端机器<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mount -t nfs <span class="number">192.168</span>.<span class="number">8.106</span>:/ /mnt</span><br></pre></td></tr></table></figure></p>
<p>直接挂载即可，这里注意因为rgw是没有文件系统的容量概念的，这里df是看不到的，所以用mount命令检测<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># mount|grep mnt</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/ on /mnt <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br><span class="line"><span class="number">192.168</span>.<span class="number">8.106</span>:/testnfsrgw on /mnt/testnfsrgw <span class="built_in">type</span> nfs4 (rw,relatime,vers=<span class="number">4.0</span>,rsize=<span class="number">1048576</span>,wsize=<span class="number">1048576</span>,namlen=<span class="number">255</span>,hard,proto=tcp,port=<span class="number">0</span>,timeo=<span class="number">600</span>,retrans=<span class="number">2</span>,sec=sys,clientaddr=<span class="number">192.168</span>.<span class="number">8.107</span>,<span class="built_in">local</span>_lock=none,addr=<span class="number">192.168</span>.<span class="number">8.106</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以查看挂载的目录里面的子目录对应的就是bucket<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@lab8107:~<span class="comment"># ll /mnt/</span></span><br><span class="line">total <span class="number">4</span></span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> ./</span><br><span class="line">drwxr-xr-x <span class="number">25</span> root root <span class="number">4096</span> Apr <span class="number">13</span> <span class="number">03</span>:<span class="number">04</span> ../</span><br><span class="line">drwxrwxrwx  <span class="number">3</span> root root    <span class="number">0</span> Jan  <span class="number">1</span>  <span class="number">1970</span> testnfsrgw/</span><br></pre></td></tr></table></figure></p>
<h3 id="五、总结">五、总结</h3><p>在实现这个功能以后，实际上为文件接口和对象接口打通了一个通道，能够方便的实现传统的文件接口的数据到对象接口的转移，在性能方面，本篇并没有做测试，这个交给实际项目中去检测了，如果有问题欢迎探讨</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-19</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="一、功能介绍">一、功能介绍</h3><p>关于rgw实现nfs接口这个，刚接触的人可能并不清楚这个是个什么样的服务架构，rgw是ceph里面的对象存储接口，而nfs则是纯正的网络文件系统接口，这二者如何结合在一起,关于这个,有几个相关的链接供大家了解</p>
<ul>
<li><a href="http://tracker.ceph.com/projects/ceph/wiki/RGW_-_NFS">ceph官方的RGW_NFS项目规划</a></li>
<li><a href="http://chuansong.me/n/2385718">麦子迈关于RGW_NFS的文章</a></li>
</ul>
<p>之所以这个功能能实现这么快，原因是nfs-ganesha的开发者Matt Benjamin加入到了Redhat，而ceph目前的开发是Redhat在主导开发，所以功能的实现是非常快的，但是目前官方并没有提供相关的文档，个人推测是功能并未完全开发完成，一旦未完全开发完成的功能放出来，邮件列表和Bug列表就会有很多相关问题，开发者应该还是希望安静的把功能做好，再提供相关的文档，而这个功能也是在ceph 的jewel版本里面才加入的</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[我的分答-付费语音回答问题-￥1]]></title>
    <link href="http://www.zphj1987.com/2016/06/15/%E6%88%91%E7%9A%84%E5%88%86%E7%AD%94-%E4%BB%98%E8%B4%B9%E8%AF%AD%E9%9F%B3%E5%9B%9E%E7%AD%94%E9%97%AE%E9%A2%98-%EF%BF%A51/"/>
    <id>http://www.zphj1987.com/2016/06/15/我的分答-付费语音回答问题-￥1/</id>
    <published>2016-06-14T17:34:06.000Z</published>
    <updated>2016-06-15T14:31:23.666Z</updated>
    <content type="html"><![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>
<a id="more"></a>
<center><br><img src="http://static.zybuluo.com/zphj1987/qxfdnyf1c6o1hd4irdaz7xp4/liantu.png" alt="微信扫一扫"><br></center>




]]></content>
    <summary type="html">
    <![CDATA[<p>在行推出的新产品，在行我也有注册，不过是线下的时间分享就暂时没使用了，现在推出了微信的付费的语音Q&amp;A产品，是一个不错的产品</p>
<p>目的是知识变现的一种模式，也是对等的一种模式，一是自愿，二来公平，获取知识的方式有很多种，只是时间和路径的差别，殊途同归，我会根据自己的经验回答您的提问，语音的方式也是不错的一种方式，依托微信也能很快推广，目前暂定为价格 1，欢迎来问，使用微信扫一扫下面的二维码向我提问</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[CPU相关的学习]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/CPU%E7%9B%B8%E5%85%B3%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.zphj1987.com/2016/06/13/CPU相关的学习/</id>
    <published>2016-06-13T10:13:49.000Z</published>
    <updated>2016-06-13T10:14:21.018Z</updated>
    <content type="html"><![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br><a id="more"></a></p>
<h3 id="单核和多核">单核和多核</h3><p>在英文里面，单核（single-core）和多核（multi-core）多称作uniprocessor和multiprocessor，这里先对这些概念做一个说明：</p>
<blockquote>
<p>这里所说的core（或processor），是一个泛指，是从使用者（或消费者）的角度看计算机系统。因此，core，或者processor，或者处理器（CPU），都是逻辑概念，指的是一个可以独立运算、处理的核心。<br>而这个核心，可以以任何形式存在，例如：单独的一个chip（如通常意义上的单核处理器）；一个chip上集成多个核心（如SMP，symmetric multiprocessing）；一个核心上实现多个hardware context，以支持多线程（如SMT，Simultaneous multithreading）；等等。这是从硬件实现的角度看的。<br>最后，从操作系统进程调度的角度，又会统一看待这些不同硬件实现的核心，例如上面开始所提及的CPU（24个CPUs，从0编号开始），因为它们都有一个共同的特点：执行进程（或线程）。</p>
</blockquote>
<h3 id="NUNA与SMP的概念">NUNA与SMP的概念</h3><p>NUMA(Non-Uniform Memory Access，非一致性内存访问)和SMP(Symmetric Multi-Processor，对称多处理器系统)是两种不同的CPU硬件体系架构</p>
<p>SMP（Symmetric Multi-Processing）的主要特征是共享，所有的CPU共享使用全部资源，例如内存、总线和I/O，多个CPU对称工作，彼此之间没有主次之分，平等地访问共享的资源，这样势必引入资源的竞争问题，从而导致它的扩展内力非常有限。特别是在现在一台机器CPU核心比较多，内存比较大的情况</p>
<p>NUMA技术将CPU划分成不同的组（Node)，每个Node由多个CPU组成，并且有独立的本地内存、I/O等资源。Node之间通过互联模块连接和沟通，因此除了本地内存外，每个CPU仍可以访问远端Node的内存，只不过效率会比访问本地内存差一些，我们用Node之间的距离（Distance，抽象的概念）来定义各个Node之间互访资源的开销。</p>
<p>本章主要是去做NUMA的相关探索，下图是一个多核系统简单的topology</p>
<center><img src="http://static.zybuluo.com/zphj1987/vg7eprp72ibucwyq0fljvfl2/coremuti.gif" alt="coremuti.gif-23.7kB"></center>

<h3 id="Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)">Node-&gt;Socket-&gt;Core-&gt;Processor(Threads)</h3><p>如果你只知道CPU这么一个概念，那么是无法理解CPU的拓扑的。事实上，在NUMA架构下，CPU的概念从大到小依次是：Node、Socket、Core、Processor</p>
<ul>
<li>Sockets 可以理解成主板上cpu的插槽数，物理cpu的颗数，一般同一socket上的core共享三级缓存</li>
<li>Cores 而Socket中的每个核心被称为Core,常说的核,核有独立的物理资源.比如单独的一级二级缓存什么的</li>
<li>Threads 为了进一步提升CPU的处理能力，Intel又引入了HT（Hyper-Threading，超线程)的技术，一个Core打开HT之后，在OS看来就是两个核，当然这个核是逻辑上的概念，所以也被称为Logical Processor,如果不开超线程,threads应该与cores相等,如果开了超线程,threads应该是cores的倍数.相互之间共享物理资源</li>
<li>Nodes 上图的多核图中没有涉及， Node是NUMA体系中的概念．由于SMP体系中各个CPU访问内存只能通过单一的通道．导致内存访问成为瓶颈,cpu再多也无用．后来引入了NUMA．通过划分node,每个node有本地RAM,这样node内访问RAM速度会非常快．但跨Node的RAM访问代价会相对高一点，下面看一下两种架构的明显区别</li>
</ul>
<center><img src="http://static.zybuluo.com/zphj1987/y0thygclxkbl9y8e1f6rl9yj/smpnuma.png" alt="smpnuma.png-67.5kB"></center>


<p>由此可以总结这样的逻辑关系(包含):Node &gt; Socket &gt; Core &gt; Thread 区分这几个概念为了了解cache的分布,因为cpu绑定的目的就是提高cache的命中率,降低cpu颠簸.所以了解cache与cpu之间的mapping关系是非常重要的.通常来讲:</p>
<ul>
<li>同Socket内的cpu共享三级级缓存</li>
<li>每个Core有自己独立的二级缓存</li>
<li>一个Core上超线程出来的Threads,避免绑定，看似可能会提高L2 cache命中率,但也可能有严重的cpu争抢，导致性能非常差.</li>
</ul>
<h3 id="查看CPU信息">查看CPU信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lscpu </span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        <span class="number">32</span>-bit, <span class="number">64</span>-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                <span class="number">24</span></span><br><span class="line">On-line CPU(s) list:   <span class="number">0</span>-<span class="number">23</span></span><br><span class="line">Thread(s) per core:    <span class="number">2</span></span><br><span class="line">Core(s) per socket:    <span class="number">6</span></span><br><span class="line">Socket(s):             <span class="number">2</span></span><br><span class="line">NUMA node(s):          <span class="number">2</span></span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            <span class="number">6</span></span><br><span class="line">Model:                 <span class="number">62</span></span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br><span class="line">Stepping:              <span class="number">4</span></span><br><span class="line">CPU MHz:               <span class="number">1607.894</span></span><br><span class="line">BogoMIPS:              <span class="number">4205.65</span></span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             <span class="number">32</span>K</span><br><span class="line">L1i cache:             <span class="number">32</span>K</span><br><span class="line">L2 cache:              <span class="number">256</span>K</span><br><span class="line">L3 cache:              <span class="number">15360</span>K</span><br><span class="line">NUMA node0 CPU(s):     <span class="number">0</span>-<span class="number">5</span>,<span class="number">12</span>-<span class="number">17</span></span><br><span class="line">NUMA node1 CPU(s):     <span class="number">6</span>-<span class="number">11</span>,<span class="number">18</span>-<span class="number">23</span></span><br></pre></td></tr></table></figure>
<p>2颗6核双线程，一共是24 processors,也可以看到是NUMA体系，可以使用以下命令详细查看numa信息.非NUMA体系时,所有cpu都划分为一个Node<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># numactl --hardware</span></span><br><span class="line">available: <span class="number">2</span> nodes (<span class="number">0</span>-<span class="number">1</span>)</span><br><span class="line">node <span class="number">0</span> cpus: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span></span><br><span class="line">node <span class="number">0</span> size: <span class="number">31880</span> MB</span><br><span class="line">node <span class="number">0</span> free: <span class="number">19634</span> MB</span><br><span class="line">node <span class="number">1</span> cpus: <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span> <span class="number">21</span> <span class="number">22</span> <span class="number">23</span></span><br><span class="line">node <span class="number">1</span> size: <span class="number">32253</span> MB</span><br><span class="line">node <span class="number">1</span> free: <span class="number">29315</span> MB</span><br><span class="line">node distances:</span><br><span class="line">node   <span class="number">0</span>   <span class="number">1</span> </span><br><span class="line">  <span class="number">0</span>:  <span class="number">10</span>  <span class="number">21</span> </span><br><span class="line">  <span class="number">1</span>:  <span class="number">21</span>  <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>cpu的id不连续的原因是开启了超线程，超线程的cpuid是从新的ID开始计数的，也就是从12开始计数的</p>
</blockquote>
<p>两个node，每个node32G内存左右，这台机器我的物理内存是64G</p>
<h3 id="通过命令行查看cpu信息">通过命令行查看cpu信息</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取cpu名称与主频</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | cut <span class="operator">-f</span>2 <span class="operator">-d</span>: | head -n1 | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取逻辑核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'model name'</span>  | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取物理核数</span></span><br><span class="line">cat /proc/cpuinfo | grep <span class="string">'physical id'</span> | sort | uniq | wc <span class="operator">-l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cpu的flags</span></span><br><span class="line">cat /proc/cpuinfo | grep flags | uniq | cut <span class="operator">-f</span>2 <span class="operator">-d</span> : | sed <span class="string">'s/^ //'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否打开超线程（检查 physical id * cpu cores 与 processor的比例 1:1为未开启）</span></span><br><span class="line">cat /proc/cpuinfo </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看cache大小,X自省替换</span></span><br><span class="line">sudo cat /sys/devices/system/cpu/cpuX/cache/indexX/size</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看各个cpu之间与cache的mapping</span></span><br><span class="line">cat /sys/devices/system/cpu/cpuX/cache/indexX/shared_cpu_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取CPU分布的信息（id-&gt; core信息）（这一个可以看出来CPU0和CPU12在同一个core）</span></span><br><span class="line">egrep <span class="string">'processor|core id|physical id'</span> /proc/cpuinfo | cut <span class="operator">-d</span> : <span class="operator">-f</span> <span class="number">2</span> | paste - - -  | awk <span class="string">'&#123;print "CPU"$1"\tsocket "$2" core "$3&#125;'</span></span><br><span class="line">CPU0	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU1	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU2	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU3	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU4	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU5	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU6	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU7	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU8	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU9	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU10	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU11	socket <span class="number">1</span> core <span class="number">5</span></span><br><span class="line">CPU12	socket <span class="number">0</span> core <span class="number">0</span></span><br><span class="line">CPU13	socket <span class="number">0</span> core <span class="number">1</span></span><br><span class="line">CPU14	socket <span class="number">0</span> core <span class="number">2</span></span><br><span class="line">CPU15	socket <span class="number">0</span> core <span class="number">3</span></span><br><span class="line">CPU16	socket <span class="number">0</span> core <span class="number">4</span></span><br><span class="line">CPU17	socket <span class="number">0</span> core <span class="number">5</span></span><br><span class="line">CPU18	socket <span class="number">1</span> core <span class="number">0</span></span><br><span class="line">CPU19	socket <span class="number">1</span> core <span class="number">1</span></span><br><span class="line">CPU20	socket <span class="number">1</span> core <span class="number">2</span></span><br><span class="line">CPU21	socket <span class="number">1</span> core <span class="number">3</span></span><br><span class="line">CPU22	socket <span class="number">1</span> core <span class="number">4</span></span><br><span class="line">CPU23	socket <span class="number">1</span> core <span class="number">5</span></span><br></pre></td></tr></table></figure>
<p>lscpu,numactl都是读取proc,sys文件系统信息并进行格式化，输出人性化的内容．当没有网络,而lscpu,numactl都没有安装时，只能使用这种命令行方式了</p>
<p>能用工具还是用工具，工具就是解放双手的</p>
<h3 id="Cpu_Topology可视化">Cpu Topology可视化</h3><p>lstopo 指令由 hwloc 数据包提供，创建了用户的系统示意图。lstopo-no-graphics 指令提供详尽的文本输出<br>通过lscpu与numactl获取的信息，必要的时候查询了/sys/devices/system/cpu/cpuX/*的数据将正在使用的 Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz的topology进行可视化<br>详细的cache信息可以通过sysfs查看<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls /sys/devices/system/cpu/cpu0/cache/</span><br><span class="line">index0 index1 index2 index3</span><br></pre></td></tr></table></figure></p>
<p>包含以下4个目录：</p>
<ul>
<li>index0:1级数据cache </li>
<li>index1:1级指令cache </li>
<li>index2:2级cache </li>
<li>index3:3级cache,对应cpuinfo里的cache</li>
</ul>
<p>目录里的文件是cache信息描述，以本机的cpu0/index0为例简单解释一下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">文件</th>
<th style="text-align:center">内容</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">type</td>
<td style="text-align:center">Data</td>
<td style="text-align:center">数据cache，如果查看index1就是Instruction</td>
</tr>
<tr>
<td style="text-align:center">Level</td>
<td style="text-align:center">1</td>
<td style="text-align:center">L1</td>
</tr>
<tr>
<td style="text-align:center">Size</td>
<td style="text-align:center">32K</td>
<td style="text-align:center">大小为32K</td>
</tr>
<tr>
<td style="text-align:center">coherency_line_size</td>
<td style="text-align:center">64</td>
<td style="text-align:center">64<em>4</em>128=32K</td>
</tr>
<tr>
<td style="text-align:center">physical_line_partition</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">ways_of_associativity</td>
<td style="text-align:center">4</td>
</tr>
<tr>
<td style="text-align:center">number_of_sets</td>
<td style="text-align:center">128</td>
</tr>
<tr>
<td style="text-align:center">shared_cpu_map</td>
<td style="text-align:center">00000101</td>
<td style="text-align:center">表示这个cache被CPU0和CPU8 share</td>
</tr>
</tbody>
</table>
<p>解释一下shared_cpu_map内容的格式：<br>表面上看是2进制，其实是16进制表示，每个bit表示一个cpu，1个数字可以表示4个cpu 截取00000101的后4位，转换为2进制表示</p>
<table>
<thead>
<tr>
<th style="text-align:center">CPU id</th>
<th style="text-align:center">15</th>
<th style="text-align:center">14</th>
<th style="text-align:center">13</th>
<th style="text-align:center">12</th>
<th style="text-align:center">11</th>
<th style="text-align:center">10</th>
<th style="text-align:center">9</th>
<th style="text-align:center">8</th>
<th style="text-align:center">7</th>
<th style="text-align:center">6</th>
<th style="text-align:center">5</th>
<th style="text-align:center">4</th>
<th style="text-align:center">3</th>
<th style="text-align:center">2</th>
<th style="text-align:center">1</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0×0101的2进制表示</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>0101表示cpu8和cpu0，即cpu0的L1 data cache是和cpu8共享的。<br>也可以使用上面提到的lstopo-no-graphics命令进行查询<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@server9 ~]<span class="comment"># lstopo-no-graphics </span></span><br><span class="line">Machine (<span class="number">63</span>GB)</span><br><span class="line">  NUMANode L<span class="comment">#0 (P#0 31GB)</span></span><br><span class="line">    Socket L<span class="comment">#0 + L3 L#0 (15MB)</span></span><br><span class="line">      L2 L<span class="comment">#0 (256KB) + L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0</span></span><br><span class="line">        PU L<span class="comment">#0 (P#0)</span></span><br><span class="line">        PU L<span class="comment">#1 (P#12)</span></span><br><span class="line">      L2 L<span class="comment">#1 (256KB) + L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1</span></span><br><span class="line">        PU L<span class="comment">#2 (P#1)</span></span><br><span class="line">        PU L<span class="comment">#3 (P#13)</span></span><br><span class="line">      L2 L<span class="comment">#2 (256KB) + L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2</span></span><br><span class="line">        PU L<span class="comment">#4 (P#2)</span></span><br><span class="line">        PU L<span class="comment">#5 (P#14)</span></span><br><span class="line">      L2 L<span class="comment">#3 (256KB) + L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3</span></span><br><span class="line">        PU L<span class="comment">#6 (P#3)</span></span><br><span class="line">        PU L<span class="comment">#7 (P#15)</span></span><br><span class="line">      L2 L<span class="comment">#4 (256KB) + L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4</span></span><br><span class="line">        PU L<span class="comment">#8 (P#4)</span></span><br><span class="line">        PU L<span class="comment">#9 (P#16)</span></span><br><span class="line">      L2 L<span class="comment">#5 (256KB) + L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5</span></span><br><span class="line">        PU L<span class="comment">#10 (P#5)</span></span><br><span class="line">        PU L<span class="comment">#11 (P#17)</span></span><br><span class="line">    HostBridge L<span class="comment">#0</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">1000</span>:<span class="number">0086</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#0 "enp4s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#1 "enp4s0f1"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#2 "enp4s0f2"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1521</span></span><br><span class="line">          Net L<span class="comment">#3 "enp4s0f3"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#4 "enp6s0f0"</span></span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">10</span>fb</span><br><span class="line">          Net L<span class="comment">#5 "enp6s0f1"</span></span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">8086</span>:<span class="number">1</span>d6b</span><br><span class="line">      PCIBridge</span><br><span class="line">        PCI <span class="number">102</span>b:<span class="number">0532</span></span><br><span class="line">          GPU L<span class="comment">#6 "card0"</span></span><br><span class="line">          GPU L<span class="comment">#7 "controlD64"</span></span><br><span class="line">      PCI <span class="number">8086</span>:<span class="number">1</span>d02</span><br><span class="line">        Block L<span class="comment">#8 "sda"</span></span><br><span class="line">  NUMANode L<span class="comment">#1 (P#1 31GB) + Socket L#1 + L3 L#1 (15MB)</span></span><br><span class="line">    L2 L<span class="comment">#6 (256KB) + L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6</span></span><br><span class="line">      PU L<span class="comment">#12 (P#6)</span></span><br><span class="line">      PU L<span class="comment">#13 (P#18)</span></span><br><span class="line">    L2 L<span class="comment">#7 (256KB) + L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7</span></span><br><span class="line">      PU L<span class="comment">#14 (P#7)</span></span><br><span class="line">      PU L<span class="comment">#15 (P#19)</span></span><br><span class="line">    L2 L<span class="comment">#8 (256KB) + L1d L#8 (32KB) + L1i L#8 (32KB) + Core L#8</span></span><br><span class="line">      PU L<span class="comment">#16 (P#8)</span></span><br><span class="line">      PU L<span class="comment">#17 (P#20)</span></span><br><span class="line">    L2 L<span class="comment">#9 (256KB) + L1d L#9 (32KB) + L1i L#9 (32KB) + Core L#9</span></span><br><span class="line">      PU L<span class="comment">#18 (P#9)</span></span><br><span class="line">      PU L<span class="comment">#19 (P#21)</span></span><br><span class="line">    L2 L<span class="comment">#10 (256KB) + L1d L#10 (32KB) + L1i L#10 (32KB) + Core L#10</span></span><br><span class="line">      PU L<span class="comment">#20 (P#10)</span></span><br><span class="line">      PU L<span class="comment">#21 (P#22)</span></span><br><span class="line">    L2 L<span class="comment">#11 (256KB) + L1d L#11 (32KB) + L1i L#11 (32KB) + Core L#11</span></span><br><span class="line">      PU L<span class="comment">#22 (P#11)</span></span><br><span class="line">      PU L<span class="comment">#23 (P#23)</span></span><br></pre></td></tr></table></figure></p>
<p>这个得到的是文本的拓扑，这个转换成一个图看的要清楚一些<br><img src="http://static.zybuluo.com/zphj1987/u4t0qbmeh2i4nemf007vqepr/nodesock.png" alt="nodesock.png-45.9kB"></p>
<h4 id="NUMA分组信息">NUMA分组信息</h4><ul>
<li>通过图可以看到cpu为numa架构,且有两个node</li>
<li>将同一socket内的cpu(threads)都划分在一个node中.通过上图也解释了node中cpu序列不连续的问题.因为同一个Core上的两个Threads是超线程出来的.超线程Thread的cpu id在原有的core id基础上增长的</li>
<li>每个node中有32G左右的本地RAM可用</li>
</ul>
<h4 id="cache信息">cache信息</h4><ul>
<li>每个core都有独立的二级缓存,而不是socket中所有的core共享二级缓存</li>
<li>同node中的cpu共享三级缓存</li>
<li>跨node的内存访问的花费要大些</li>
</ul>
<h3 id="cpu绑定注意的几点">cpu绑定注意的几点</h3><ul>
<li>Numa体系中,如果夸node绑定,性能会下降.因为L3 cache命中率低,跨node内存访问代价高.</li>
<li>绑定同Node,同一个Core中的两个超线程出来的cpu,性能会急剧下降.cpu密集型的线程硬件争用严重.”玩转CPU Topology”中也提到了.</li>
<li>Numa架构可能引起swap insanity.需要注意</li>
</ul>
<h3 id="测试CPU绑定性能">测试CPU绑定性能</h3><p>这个部分就不在这里赘述了，上面是把cpu比较清晰的剥离出来，至于效果，需要在实际环境当中去验证了，有可能变坏，也有可能变好</p>
<p>本篇参考了很多网络上的很多其他资料</p>
]]></content>
    <summary type="html">
    <![CDATA[<center><img src="http://static.zybuluo.com/zphj1987/gky2uc8l9xww4ozupmjxocdt/socket.jpg" alt="socket.jpg-59.6kB"></center>

<h3 id="我理解的CPU">我理解的CPU</h3><p>目前对cpu的了解停留在这个水平<br>查看CPU型号：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep model |tail -n <span class="number">1</span></span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E5-<span class="number">2620</span> v2 @ <span class="number">2.10</span>GHz</span><br></pre></td></tr></table></figure></p>
<p>查看有多少processor：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/cpuinfo |grep processor|tail -n <span class="number">1</span></span><br><span class="line">processor	: <span class="number">23</span></span><br></pre></td></tr></table></figure></p>
<p>然后对性能要求就是主频越高越好，processor越多越好，其它的知道的很少，由于需要做性能相关调优，所以对CPU这一块做一个系统的学习，如果参考网上的一些CEPH性能调优的资料，很多地方都是让关闭numa，以免影响性能，这个从来都是只有人给出答案，至于为什么，对不对，适合不适合你的环境，没有人给出来，没有数据支持的调优都是耍流氓<br>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[推荐一本性能相关的书]]></title>
    <link href="http://www.zphj1987.com/2016/06/13/%E6%8E%A8%E8%8D%90%E4%B8%80%E6%9C%AC%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%E7%9A%84%E4%B9%A6/"/>
    <id>http://www.zphj1987.com/2016/06/13/推荐一本性能相关的书/</id>
    <published>2016-06-13T08:57:38.000Z</published>
    <updated>2016-06-13T09:19:01.017Z</updated>
    <content type="html"><![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>

<a id="more"></a>
<p>京东纸质书购买链接：<a href="http://item.jd.com/11755695.html" target="_blank" rel="external">性能之巅</a></p>
<h3 id="英文版本下载地址：">英文版本下载地址：</h3><p>地址：<a href="http://pan.baidu.com/s/1i5LJTxN" target="_blank" rel="external">http://pan.baidu.com/s/1i5LJTxN</a><br>密码：nam5</p>
<h3 id="中文电子版">中文电子版</h3><p>联系我<br>QQ:199383004</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>听说这本书从池建强老师的订阅号上看到的，然后一搜索发现是 <code>Brendan Gregg</code> 性能调优大神写的，关于大神的资料可以自己google下，然后搜索了一下发现只有英文版本的，对于这样接近800面的篇幅，即使比较喜欢看英文文档的我也是抗拒的，然后看了下实体书的，大概要100左右，这本书的价值肯定不只这个价格的，但是还是比较喜欢看电子书，所以买了电子版本的，如果你自己能找到也可以，本篇会提供免费的英文版本的下载地址，如果你希望得到中文电子版本的，可以联系我，很优惠两瓶可乐，书我也还没有看完，还在学习中，欢迎交流</p>
<p>从书的目录来看这本书是非常系统的，并且原作者和翻译的作者的文章质量都非常的高，系统的学习一下还是很有必要的，毕竟真的没几个人会性能调优，我也只会皮毛，建议买一本纸质书，然后买一个电子书方便的时候看看</p>
<center><br><img src="http://static.zybuluo.com/zphj1987/sw95fibeff214g1quf456etd/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.png" alt="性能之巅.png-878kB"><br></center>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[加速OSD的启动]]></title>
    <link href="http://www.zphj1987.com/2016/06/07/%E5%8A%A0%E9%80%9FOSD%E7%9A%84%E5%90%AF%E5%8A%A8/"/>
    <id>http://www.zphj1987.com/2016/06/07/加速OSD的启动/</id>
    <published>2016-06-07T14:50:02.000Z</published>
    <updated>2016-06-07T15:03:50.697Z</updated>
    <content type="html"><![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br><a id="more"></a></p>
<h3 id="一、问题分析">一、问题分析</h3><p>OSD重启是需要重视的，这个地方是ceph的一个设计的弱点。ceph集群有很多的OSD进程，OSD管理对磁盘上的对象的访问，磁盘的对象被分布到PG组当中，对象有相同的分布，副本会在相同的PG当中存在，如果不理解可以看看（<a href="http://docs.ceph.com/docs/master/architecture/" target="_blank" rel="external">ceph概览</a>）</p>
<p>当集群OSD进程出现down的情况，会被mon认为 “OUT” 了，这个 “OUT” 不是触发迁移的那个 “OUT”，是不服务的 “OUT” ,这个OSD上受影响的PG的I/O请求会被其他拥有这个PG的OSD接管，当OSD重新启动的时候,OSD会被加入进来，将会检查PG，看是否有在down的期间错过东西，然后进行更新，这里问题就来了，启动之后会访问磁盘检查PG是否有缺失的东西进行更新，会进行一定量的数据恢复，同时会开始接受新的IO的请求，如果本来磁盘就只剩很少的余量，那么一旦请求发送到这个OSD上，那么性能将会开始下降</p>
<p>如果去看ceph的邮件列表，在极端情况下，这种效应会让整个集群停机，这发生在OSD太忙了，连心跳都无法回复，然后mon就会把它标记为down，这个时候OSD的进程都还在的，这个时候客户端的请求会导入到其他的OSD上，然后负载小了，OSD又会自己进来，然后又开始响应请求了，然后之前没有受影响的OSD节点，需要把新写入的数据同步过来，这个又增加了其他的OSD的负载了，一旦集群接近I/O的限制，也会让其他的OSD无法响应了，结果就是整个集群的OSD在反复的”in”和”out”状态之间变化了，集群在这种情况下，就无法接收客户端的请求了，如果不人工干预甚至无法恢复正常，这个在高负载下是很好复现出来的;另外一种较轻的情况，在OSD重启过程，I/O可能会hung住，影响性能.如果不能避免，至少能想办法去降低这个影响</p>
<p>我们能做些什么？在ceph开发者列表当中有开发者提出了这个<a href="http://thread.gmane.org/gmane.comp.file-systems.ceph.user/25881/focus=25890" target="_blank" rel="external">设计上需要修复</a>，这个估计需要等很久以后的事情了，我们能做什么来降低这个的影响？最明显的一点是要保证集群有足够的I/O的余量，另一种思路就是减少关键过程启动检查和接收I/O的竞争</p>
<h3 id="二、减少OSD启动过程当中的IO">二、减少OSD启动过程当中的IO</h3><p>OSD在启动的时候可以预测到磁盘的访问的模式。我们可以了解这个访问模式，然后提前将文件读取到内核的缓存当中。这样这些文件在启动的时候就不需要再次访问磁盘了，意味着更少的磁盘消耗和更好的性能</p>
<p>现在来定位下OSD启动过程中做了哪些事情，使用性能大师 Brendan Gregg 的 <a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a> 工具，一个OSD启动的过程如下：</p>
<h4 id="2-1_OSD启动过程">2.1 OSD启动过程</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># opensnoop ceph-7</span></span><br><span class="line">Tracing open()s <span class="keyword">for</span> filenames containing <span class="string">"ceph-7"</span>. Ctrl-C to end.</span><br><span class="line">COMM             PID      FD FILE</span><br><span class="line">ceph             osd     <span class="number">0</span>x3 /var/lib/ceph/osd/ceph-<span class="number">7</span>/</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/<span class="built_in">type</span></span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/magic</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/whoami</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/ceph_fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>x4 /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xb /var/lib/ceph/osd/ceph-<span class="number">7</span>/fsid</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span>/superblock</span><br><span class="line">ceph             osd     <span class="number">0</span>xc /var/lib/ceph/osd/ceph-<span class="number">7</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/fiemap_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/xattr_<span class="built_in">test</span></span><br><span class="line">ceph             osd     <span class="number">0</span>xd /var/lib/ceph/osd/ceph-<span class="number">7</span>/current</span><br><span class="line">ceph             osd     <span class="number">0</span>xe /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/commit_op_seq</span><br><span class="line">ceph             osd     <span class="number">0</span>xf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/LOCK</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/CURRENT</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000135</span></span><br></pre></td></tr></table></figure>
<p>开始的时候，OSD读取了很多元数据文件，没有什么特别的<br>下面读取omap的数据库文件，读取了一部分的osdmap文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000137</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000143</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x10 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000144</span>.log</span><br><span class="line">ceph             osd    <span class="number">0</span>x11 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/MANIFEST-<span class="number">000142</span></span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000142</span>.dbtmp</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000138</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x12 /var/lib/ceph/osd/ceph-<span class="number">7</span>/journal</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/store_version</span><br><span class="line">ceph             osd    <span class="number">0</span>x13 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">298</span>__0_AC96EE75__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.3</span>b_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">297</span>__0_AC96EEA5__none</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.34</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.20</span>_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.22</span>_head</span><br></pre></td></tr></table></figure></p>
<p>可以看到读取一个sst后，就会继续读取pg的目录<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000139</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.ec_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.7</span>e_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>b_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000141</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>fb_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0</span>.cf_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>f_head</span><br><span class="line">[···]</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/omap/<span class="number">000140</span>.sst</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.8</span>f_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.10</span>c_head</span><br><span class="line">ceph             osd    <span class="number">0</span>x16 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.14</span>e_head</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会读取每个pg里面的<em>head</em>文件<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x17 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.23</span>a_head/__head_0000023A__0</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x18 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.1</span>a2_head/DIR_2/DIR_A/DIR_1/__head_000001A2__0</span><br><span class="line">&lt;...&gt;            <span class="number">23689</span>  <span class="number">0</span>x19 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/<span class="number">0.2</span>ea_head/__head_000002EA__0</span><br><span class="line">[···]</span><br></pre></td></tr></table></figure></p>
<p>然后会进行osdmap文件的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x3a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">299</span>__0_C67CF872__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">299</span>__0_AC96EF05__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x14 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">300</span>__0_C67CF142__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x4f /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">300</span>__0_AC96E415__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x15 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/osd\usuperblock__0_23C2FCDE__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x6e /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">301</span>__0_C67CF612__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x55 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">301</span>__0_AC96E5A5__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>xbf /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">302</span>__0_C67CF7A2__none</span><br><span class="line">tp_fstore_op     <span class="number">23689</span>  <span class="number">0</span>x60 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">302</span>__0_AC96E575__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x86 /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_2/inc\uosdmap.<span class="number">303</span>__0_C67CF772__none</span><br><span class="line">tp_fstore_op     <span class="number">23688</span>  <span class="number">0</span>x6a /var/lib/ceph/osd/ceph-<span class="number">7</span>/current/meta/DIR_5/osdmap.<span class="number">303</span>__0_AC96FA05__none</span><br><span class="line">s</span><br></pre></td></tr></table></figure></p>
<p>我们无法确定哪些对象将需要读取，单我们知道，所有的OMAP和元数据文件将会打开，<em>head</em>文件将会打开</p>
<h4 id="2-2_使用vmtouch进行预读取">2.2 使用vmtouch进行预读取</h4><p>下面将进入 <a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a> ,这个小工具能够读取文件并锁定到内存当中，这样后续的I/O请求能够从缓存当中读取它们，这样就减少了对磁盘的访问请求<br>在这里我们的访问模式是这样的：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ceph-<span class="number">7</span>]<span class="comment"># vmtouch -t /var/lib/ceph/osd/ceph-7/current/meta/ /var/lib/ceph/osd/ceph-7/current/omap/</span></span><br><span class="line">          Files: <span class="number">618</span></span><br><span class="line">     Directories: <span class="number">6</span></span><br><span class="line">   Touched Pages: <span class="number">3972</span> (<span class="number">15</span>M)</span><br><span class="line">         Elapsed: <span class="number">0.009621</span> seconds</span><br></pre></td></tr></table></figure></p>
<p>关于这个vmtouch很好使用也很强大，可以使用  <code>vmtouch -L</code> 将数据锁定到内存当中去，这里用 <code>-t</code> 也可以，使用 <code>-v</code>  参数能打印更多详细的信息,这个效果有多大？这个原作者的效果很好，我的环境太小，看不出太多的效果，但是从原理上看，应该是会有用的，我的读取过程跟原作者的读取过程有一定的差别，作者的数据库文件是 <code>ldb</code> ，我的环境是 <code>sst</code>，并且作者的压力应该是很大的情况下的，我的环境较小</p>
<h3 id="三、判断是否有作用">三、判断是否有作用</h3><p>一个很好的衡量的方法就是看启动过程当中的 <code>peering</code> 的阶段的长度,<code>peering</code>状态是osd做相互的协调的，PG的请求在这个时候是无法响应的，理想状况下这个过程会很快，无法察觉，如果集群集群处于高负载或者过载状态，这个持续的时间就会很久，然后关闭一个OSD，然后等待一分钟，以便让一部分写入只写到了其他OSD，在down掉的OSD启动后，需要从其他OSD恢复一些数据，然后重新打开，从日志当中，绘制一段时间的<code>peering</code>状态PG的数目，score是统计的所有时间线上<code>peering</code>状态的计数的总和</p>
<p>为了验证这个vmtouch将会减少 <code>peering</code> 的状态,将负载压到略小于集群满载情况</p>
<h4 id="第一个实验是OSD重启（无vmtouch）">第一个实验是OSD重启（无vmtouch）</h4><p><img src="http://static.zybuluo.com/zphj1987/xisprl91q5ljn99rknb1ku8g/vmtouchno.png" alt="vmtouchno.png-22.3kB"><br>可以看到超过30s时，大量的pg是peering状态，导致集群出现缓慢</p>
<h4 id="第二个实验中使用vmtouch预读取OMAP的数据库文件">第二个实验中使用vmtouch预读取OMAP的数据库文件</h4><p><img src="http://static.zybuluo.com/zphj1987/d64ak6lbnzywokpl6j8zkune/vmtouch.png" alt="vmtouch.png-17.9kB"><br>这些 <code>peering</code> 状态并没有消失，但是可以看到有很大的改善 <code>peering</code> 会更早的开始（OMAP已经加载），总体的score也要小很多，这个是一个很不错的结果</p>
<h3 id="四、结论">四、结论</h3><p>根据之前监测到的读取数据的情况，预读取文件，能够有不错的改善，虽然不是完整的解决方案，但是能够帮助改善一个痛点，从长远来看，希望ceph能改进设计，是这个情况消失</p>
<h3 id="五、总结">五、总结</h3><p>本章节里面介绍了两个工具<br><strong>opensnoop</strong><br>这个工具已经存在了很久很久了，也是到现在才看到的，一个用于监控文件的操作，是Gregg 大师的作品，仅仅是一个shell脚本就能实现监控，关键还在于其对操作系统的了解<br><strong>vmtouch</strong><br>这个是将数据加载到内存的，以前关注的是清理内存，其实在某些场景下，能够预加载到内存将会解决很多问题，关键看怎么去用了</p>
<h3 id="六、参考文章">六、参考文章</h3><p><a href="https://blog.flyingcircus.io/2016/03/11/improving-ceph-osd-start-up-behaviour-with-vmtouch/" target="_blank" rel="external">Improving Ceph OSD start-up behaviour with vmtouch</a><br><a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="external">opensnoop</a><br><a href="https://hoytech.com/vmtouch/" target="_blank" rel="external">vmtouch</a></p>
<h3 id="七、变更记录">七、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-06-07</td>
</tr>
</tbody>
</table>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>


]]></content>
    <summary type="html">
    <![CDATA[<p>ceph是目前开源分布式存储里面最好的一个，但是在高负载下会有很多异常的情况会发生，有些问题无法完全避免，但是可以进行一定的控制，比如：在虚拟化场景下，重启osd会让虚拟机挂起的情况</p>
<p>重新启动osd会给这个osd进程所在的磁盘带来额外的负载，随着前面业务的需求的增长，会增加对存储的I/O的需求，虽然这个对于整个业务系统来说是好事，但是在某些情况下，会越来越接近存储吞吐量的极限，通常情况下没有异常发生的时候，都是正常的，一旦发生异常，集群超过了临界值，性能会变得剧烈的抖动</p>
<p>对于这种情况，一般会升级硬件来避免集群从一个高负载的集群变成一个过载的集群。本章节的重点在重启osd进程这个问题<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[rbd无法map(rbd feature disable)]]></title>
    <link href="http://www.zphj1987.com/2016/06/07/rbd%E6%97%A0%E6%B3%95map(rbd-feature-disable)/"/>
    <id>http://www.zphj1987.com/2016/06/07/rbd无法map(rbd-feature-disable)/</id>
    <published>2016-06-07T05:02:44.000Z</published>
    <updated>2016-06-07T05:03:53.613Z</updated>
    <content type="html"><![CDATA[<p>在jewel版本下默认开启了rbd的一些属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph --show-config|grep rbd|grep features </span></span><br><span class="line">rbd_default_features = <span class="number">61</span></span><br></pre></td></tr></table></figure></p>
<p>RBD属性表：<br><img src="http://static.zybuluo.com/zphj1987/1amw9dopkoohq90dfwme7z6p/%E5%B1%9E%E6%80%A7.png" alt="此处输入图片的描述"><br>61的意思是上面图中的bit码相加得到的值<br><a id="more"></a><br>对rbd进行内核的map操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map mytest</span></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature <span class="built_in">set</span> mismatch. You can <span class="built_in">disable</span> features unsupported by the kernel with <span class="string">"rbd feature disable"</span>.</span><br><span class="line">In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">"dmesg | tail"</span> or so.</span><br><span class="line">rbd: map failed: (<span class="number">6</span>) No such device or address</span><br></pre></td></tr></table></figure></p>
<p>根据提示查询打印的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># dmesg | tail</span></span><br><span class="line">[<span class="number">10440.462708</span>] rbd: image mytest: image uses unsupported features: <span class="number">0</span>x3c</span><br></pre></td></tr></table></figure></p>
<p>这个地方提示的很清楚了，不支持的属性0x3c，0x3c是16进制的数值，换算成10进制是3*16+12=60<br>60的意思是不支持：</p>
<blockquote>
<p>32+16+8+4 = exclusive-lock, object-map, fast-diff, deep-flatten<br>也就是不支持这些属性，现在动态关闭这些属性</p>
</blockquote>
<p>查看当前使用的image属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info mytest</span></span><br><span class="line">rbd image <span class="string">'mytest'</span>:</span><br><span class="line">	size <span class="number">2000</span> MB <span class="keyword">in</span> <span class="number">500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>开启的属性有4个是不支持的，关闭这些属性<br>语法是：</p>
<blockquote>
<p>rbd feature disable {poolname}/{imagename} {feature}<br>具体到这个测试的命令<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest deep-flatten</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest fast-diff</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest object-map</span></span><br><span class="line">[root@lab8106 ~]<span class="comment"># rbd feature disable rbd/mytest exclusive-lock</span></span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>再次查询image的info信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info mytest</span></span><br><span class="line">rbd image <span class="string">'mytest'</span>:</span><br><span class="line">	size <span class="number">2000</span> MB <span class="keyword">in</span> <span class="number">500</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10276</span>b8b4567</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<p>可以看到已经关闭了不支持的属性<br>进行kernel rbd 的map的操作<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map mytest</span></span><br><span class="line">/dev/rbd1</span><br></pre></td></tr></table></figure></p>
<p>如果不想动态的关闭，那么在创建rbd之前，在配置文件中设置这个参数即可</p>
<blockquote>
<p>rbd_default_features = 3</p>
</blockquote>
<p>关于属性支持的，目前到内核4.6仍然只支持</p>
<blockquote>
<p>layering,striping = 1 + 2</p>
</blockquote>
<p>这两个属性</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>在jewel版本下默认开启了rbd的一些属性<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># ceph --show-config|grep rbd|grep features </span></span><br><span class="line">rbd_default_features = <span class="number">61</span></span><br></pre></td></tr></table></figure></p>
<p>RBD属性表：<br><img src="http://static.zybuluo.com/zphj1987/1amw9dopkoohq90dfwme7z6p/%E5%B1%9E%E6%80%A7.png" alt="此处输入图片的描述"><br>61的意思是上面图中的bit码相加得到的值<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[利用虚拟化环境虚拟nvme盘]]></title>
    <link href="http://www.zphj1987.com/2016/06/04/%E5%88%A9%E7%94%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E7%8E%AF%E5%A2%83%E8%99%9A%E6%8B%9Fnvme%E7%9B%98/"/>
    <id>http://www.zphj1987.com/2016/06/04/利用虚拟化环境虚拟nvme盘/</id>
    <published>2016-06-03T16:02:50.000Z</published>
    <updated>2016-06-03T16:21:05.955Z</updated>
    <content type="html"><![CDATA[<h2 id="前情介绍">前情介绍</h2><h3 id="SPDK">SPDK</h3><p>SPDK的全称为Storage Performance Development Kit ，是Intel发起的一个开源驱动项目，这个是一个开发套件，可以让应用程序在用户态去访问存储资源，具体做能做什么可以去官网看一下 <a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk" target="_blank" rel="external">SPDK官网</a></p>
<h3 id="NVME">NVME</h3><p>NVMe其实与AHCI一样都是逻辑设备接口标准，NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p>
<h3 id="Bluestore">Bluestore</h3><p>BlueStore 是用来存储ceph的数据的地方，提供了一种在块设备上直接写入方式的存储。这个是因为之前ceph社区尝试做了一个kvstore，但是性能达不到想要的效果，然后基于rocksdb的原型，重新开发了一套存储系统，BlueStore直接消耗原始分区。还有一个分区是存储元数据的，实际上就是一个RocksDB键/值数据库存储，这个比之前的filestore最大的优势就是去掉了journal，从而提供了更平滑的IO</p>
<a id="more"></a>
<h3 id="SPDK+NVME+Bluestore能产生什么化学反应">SPDK+NVME+Bluestore能产生什么化学反应</h3><p>目前这一块走的比较前沿的就是xsky了，这块的最初的推动力量是Intel，NVME的硬件的推出，需要一个很好的催化剂，传统的内核中断式的访问磁盘的方式，已经不能最大化发挥NVME的性能了，因此推出了SPDK的套件，可以在用户态的去访问磁盘数据，Bluestore按照这个标准就可以去以最大化的跑出磁盘的性能了，从而给上层提供一个非常强悍的IO性能，目前来说这几项都是很新的东西，如果没有特别强的技术，或者找Intel做技术支持话，用好还是需要再等一段时间</p>
<h2 id="开篇">开篇</h2><h3 id="本篇文章做什么">本篇文章做什么</h3><p>之前有篇文章已经实现了bluestore的配置，这个配置并不难，并且用普通的硬盘就能实现，这里是讲怎么弄出来NVME磁盘，因为NVME的磁盘很贵，并不是每个人都能有环境的，这里是用虚拟化的方式虚拟出nvme，以供以后进行相关的功能验证</p>
<h3 id="准备工作">准备工作</h3><p>准备好kvm虚拟化的环境，这个地方就不在这里赘述了，本环境采用的是ubuntu的宿主机，如果是centos需要另做改动，如果有需要欢迎留言</p>
<h3 id="安装操作系统">安装操作系统</h3><h4 id="创建两个磁盘">创建两个磁盘</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">qemu-img create <span class="operator">-f</span> raw /mnt/localdisk.raw  <span class="number">40</span>G</span><br><span class="line">qemu-img create <span class="operator">-f</span> raw  /mnt/nvme.raw <span class="number">50</span>G</span><br></pre></td></tr></table></figure>
<h4 id="执行安装操作系统">执行安装操作系统</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virt-install --name nvmetest --ram <span class="number">4096</span> --vcpus=<span class="number">2</span> --disk path=/mnt/localdisk.raw --network bridge=br0 --cdrom /mnt/CentOS-<span class="number">7</span>-x86_64-DVD-<span class="number">1503</span>-<span class="number">01</span>.iso --vnclisten=<span class="number">192.168</span>.<span class="number">8.107</span> --vncport=<span class="number">7000</span> --vnc --autostart</span><br></pre></td></tr></table></figure>
<p>上面的iso文件需要提前准备，vnclisten就用宿主机的IP即可</p>
<p>都安装好了以后，先停止虚拟机，需要对配置文件做一些改动，因为virsh管理的时候有一些参数是不支持的，这个需要自己做一个  qemu:commandline 的改动</p>
<h4 id="停止掉虚拟机">停止掉虚拟机</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh destroy nvmetest</span><br></pre></td></tr></table></figure>
<h4 id="编辑配置文件">编辑配置文件</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh edit nvmetest</span><br></pre></td></tr></table></figure>
<p>内容如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;domain <span class="built_in">type</span>=<span class="string">'kvm'</span>&gt;</span><br><span class="line">····</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure></p>
<p>修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;domain <span class="built_in">type</span>=<span class="string">'kvm'</span> xmlns:qemu=<span class="string">'http://libvirt.org/schemas/domain/qemu/1.0'</span>&gt;</span><br><span class="line">····</span><br><span class="line">  &lt;qemu:commandline&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'-drive'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'file=/mnt/nvme.raw,if=none,id=D22,format=raw'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'-device'</span>/&gt;</span><br><span class="line">    &lt;qemu:arg value=<span class="string">'nvme,drive=D22,serial=1235'</span>/&gt;</span><br><span class="line">  &lt;/qemu:commandline&gt;</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>这个地方一定要注意加入的这个固定格式一定要写到最后的位置，否则不生效</p>
</blockquote>
<h3 id="检查虚拟机的磁盘是否生成">检查虚拟机的磁盘是否生成</h3><h4 id="启动测试的虚拟机">启动测试的虚拟机</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">virsh destroy nvmetest</span><br></pre></td></tr></table></figure>
<p>使用vnc连接登陆刚刚的那个vnc的端口<br>登陆上机器后就可以fdisk -l,<br><img src="http://static.zybuluo.com/zphj1987/nc79igpyb1hgxny09d5w9nk0/nvmedisk.png" alt="nvme"><br>lspci看到的如下<br><img src="http://static.zybuluo.com/zphj1987/8klt54xfe4w05mdkr41rspwk/lspci.png" alt="lspci.png-1.5kB"></p>
<h4 id="结束">结束</h4><p>本次实践当中还有一部分是对spdk的代码进行编译的，编译没有问题，并且可以根据测试脚本加载驱动，将nvme磁盘排它性的从内核态移出，但是无法找到如何使用这个用户态的磁盘，在ceph的代码分支中已经包含了spdk部分的代码，在ceph中应该默认可以直接使用这个驱动，使用的方式是 spdk：sdasdasdasd (disk SN) ，但是配置当中如何使用还是无从得知，这一块如果资料会第一时间分析，目前xsky应该能够配置出环境来，本篇涉及的几个东西都是比较新的一些东西，在未来将会极大的提高性能的，目前这个阶段还处于开发阶段</p>
<h4 id="异常处理">异常处理</h4><p>执行virsh start nvmetest的时候会提示nvme.raw的磁盘没有访问权限，这个地方卡了很久，不清楚在ubuntu下面居然还有个apparmor的权限问题，是调看系统日志才发现的，下面是处理办法:<br>执行下面的命令为libvirt禁用 apparmor:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ln <span class="operator">-s</span> /etc/apparmor.d/usr.sbin.libvirtd  /etc/apparmor.d/<span class="built_in">disable</span>/</span><br><span class="line">ln <span class="operator">-s</span> /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper  /etc/apparmor.d/<span class="built_in">disable</span>/</span><br><span class="line">apparmor_parser -R  /etc/apparmor.d/usr.sbin.libvirtd</span><br><span class="line">apparmor_parser -R  /etc/apparmor.d/usr.lib.libvirt.virt-aa-helper</span><br></pre></td></tr></table></figure></p>
<p>/etc/libvirt/qemu.conf去掉认证的,修改为：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">security_driver = <span class="string">"none"</span></span><br></pre></td></tr></table></figure></p>
<p>重启libvirt服务<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/init.d/libvirt-bin restart</span><br></pre></td></tr></table></figure></p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h2 id="前情介绍">前情介绍</h2><h3 id="SPDK">SPDK</h3><p>SPDK的全称为Storage Performance Development Kit ，是Intel发起的一个开源驱动项目，这个是一个开发套件，可以让应用程序在用户态去访问存储资源，具体做能做什么可以去官网看一下 <a href="https://software.intel.com/en-us/articles/introduction-to-the-storage-performance-development-kit-spdk">SPDK官网</a></p>
<h3 id="NVME">NVME</h3><p>NVMe其实与AHCI一样都是逻辑设备接口标准，NVMe全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，NVMe的设计之初就有充分利用到PCI-E SSD的低延时以及并行性，还有当代处理器、平台与应用的并行性。SSD的并行性可以充分被主机的硬件与软件充分利用，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升。</p>
<h3 id="Bluestore">Bluestore</h3><p>BlueStore 是用来存储ceph的数据的地方，提供了一种在块设备上直接写入方式的存储。这个是因为之前ceph社区尝试做了一个kvstore，但是性能达不到想要的效果，然后基于rocksdb的原型，重新开发了一套存储系统，BlueStore直接消耗原始分区。还有一个分区是存储元数据的，实际上就是一个RocksDB键/值数据库存储，这个比之前的filestore最大的优势就是去掉了journal，从而提供了更平滑的IO</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mon到底能坏几个]]></title>
    <link href="http://www.zphj1987.com/2016/05/26/mon%E5%88%B0%E5%BA%95%E8%83%BD%E5%9D%8F%E5%87%A0%E4%B8%AA/"/>
    <id>http://www.zphj1987.com/2016/05/26/mon到底能坏几个/</id>
    <published>2016-05-25T16:28:21.000Z</published>
    <updated>2016-06-03T17:29:11.446Z</updated>
    <content type="html"><![CDATA[<p>如果是在做ceph的配置，我们会经常遇到这几个问题</p>
<ol>
<li>问：ceph需要配置几个mon<br>答：配置一个可以，但是坏了一个就不行了，需要配置只是三个mon，并且需要是奇数个</li>
<li>问：ceph的mon能跟osd放在一起么，需要配置很好么？<br>答：能跟放在一起，但是建议在环境允许的情况下一定独立机器，并且mon的配置能好尽量好，能上ssd就上ssd</li>
</ol>
<p>这两个问题的答案不能说是错的，但是为什么这么说，这么说有没有问题，这篇文章将根据实际的数据来告诉你，到底mon的极限在哪里，为什么都说要奇数，偶数难道就不行么</p>
<a id="more"></a>
<h3 id="前言">前言</h3><p>本篇将从真实的实践中，让你更能够理解mon的故障极限，本次测试的场景数据样本足够大，最大的一个测试使用了10个mon，我想目前就算PB基本的ceph集群里也没有人会超过10个mon，所以足够覆盖大部分的场景，先来一个数据图看下10个mon的集群长什么样<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e10: <span class="number">10</span> mons at &#123;<span class="number">10</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">0</span>,<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br><span class="line">            election epoch <span class="number">58</span>, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span></span><br><span class="line">     osdmap e7: <span class="number">1</span> osds: <span class="number">1</span> up, <span class="number">1</span> <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise</span><br><span class="line">      pgmap v13: <span class="number">64</span> pgs, <span class="number">1</span> pools, <span class="number">0</span> bytes data, <span class="number">0</span> objects</span><br><span class="line">            <span class="number">34268</span> kB used, <span class="number">274</span> GB / <span class="number">274</span> GB avail</span><br><span class="line">                  <span class="number">64</span> active+clean</span><br></pre></td></tr></table></figure></p>
<p>mon的地方可以看到10个mon了</p>
<h3 id="测试结论">测试结论</h3><p><img src="http://static.zybuluo.com/zphj1987/jz15y76o7yytmxksk2ps4eyi/1.png" alt="mondown"></p>
<p>ceph的mon能够正常情况需要保证，当前剩余的mon的个数需要大于总mon个数的一半，例如10个mon，mon个数一半就是5个，那么大于5个就是6个，也就是最少需要6个，上面的测试结论也符合这个规则，为什么不去偶数个，是因为当mon的个数为偶数个的时候，允许down的mon的个数与少一个mon的情况下的mon的个数允许的个数是一样的，所以要么多两个，多一个增加不了可靠性，并不是不允许</p>
<h3 id="测试过程的数据">测试过程的数据</h3><h4 id="10个mon集群">10个mon集群</h4><p>10个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">   health HEALTH_WARN</span><br><span class="line">          <span class="number">4</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span></span><br><span class="line">   monmap e10: <span class="number">10</span> mons at &#123;<span class="number">10</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6800</span>/<span class="number">0</span>,<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.10</span></span><br></pre></td></tr></table></figure></p>
<p>10个mon关闭4个没问题，关闭5个就卡死</p>
<h4 id="9个mon集群">9个mon集群</h4><p>9个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">4</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">  monmap e11: <span class="number">9</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,<span class="number">9</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6799</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>9个mon关闭4个没问题，关闭5个就卡死</p>
<h4 id="8个mon集群">8个mon集群</h4><p>8个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">3</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span></span><br><span class="line">  monmap e12: <span class="number">8</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,<span class="number">8</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6798</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>8个mon关闭3个没问题，关闭4个就卡死</p>
<h4 id="7个mon集群">7个mon集群</h4><p>7个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">   health HEALTH_WARN</span><br><span class="line">          <span class="number">3</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">   monmap e13: <span class="number">7</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,<span class="number">7</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6797</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>7个mon关闭3个没问题，关闭4个就卡死</p>
<h4 id="6个mon集群">6个mon集群</h4><p>6个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">2</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span> lab8107,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">  monmap e14: <span class="number">6</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,<span class="number">6</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6796</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>6个mon关闭2个没问题，关闭3个就卡死</p>
<h4 id="5个mon集群">5个mon集群</h4><p>5个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">2</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8107,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line">  monmap e15: <span class="number">5</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,<span class="number">5</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6795</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>5个mon关闭2个没问题，关闭3个就卡死</p>
<h4 id="4个mon集群">4个mon集群</h4><p>4个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span> lab8107,<span class="number">2</span>,<span class="number">3</span></span><br><span class="line">  monmap e16: <span class="number">4</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,<span class="number">4</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6794</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>4个mon关闭1个没问题，关闭2个就卡死</p>
<h4 id="3个mon集群">3个mon集群</h4><p>3个mon的极限<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cluster ace3c18f-b4a5-<span class="number">4342</span><span class="operator">-a</span>598-<span class="number">8104</span>a770d4a8</span><br><span class="line">  health HEALTH_WARN</span><br><span class="line">         <span class="number">1</span> mons down, quorum <span class="number">0</span>,<span class="number">1</span> lab8107,<span class="number">2</span></span><br><span class="line">  monmap e17: <span class="number">3</span> mons at &#123;<span class="number">2</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6792</span>/<span class="number">0</span>,<span class="number">3</span>=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6793</span>/<span class="number">0</span>,lab8107=<span class="number">192.168</span>.<span class="number">8.107</span>:<span class="number">6789</span>/<span class="number">0</span>&#125;</span><br></pre></td></tr></table></figure></p>
<p>3个mon关闭1个没问题，关闭2个就卡死</p>
<h3 id="测试结束">测试结束</h3><p>下面为自己玩的一个动态图，10个mon正常，down 4个还是好的，down 5个就无法使用了</p>
<p><img src="http://7xi6lo.com1.z0.glb.clouddn.com/10mon.gif" alt=""></p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>如果是在做ceph的配置，我们会经常遇到这几个问题</p>
<ol>
<li>问：ceph需要配置几个mon<br>答：配置一个可以，但是坏了一个就不行了，需要配置只是三个mon，并且需要是奇数个</li>
<li>问：ceph的mon能跟osd放在一起么，需要配置很好么？<br>答：能跟放在一起，但是建议在环境允许的情况下一定独立机器，并且mon的配置能好尽量好，能上ssd就上ssd</li>
</ol>
<p>这两个问题的答案不能说是错的，但是为什么这么说，这么说有没有问题，这篇文章将根据实际的数据来告诉你，到底mon的极限在哪里，为什么都说要奇数，偶数难道就不行么</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[磨磨自动问答系统上线]]></title>
    <link href="http://www.zphj1987.com/2016/05/21/%E7%A3%A8%E7%A3%A8%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%BA%BF/"/>
    <id>http://www.zphj1987.com/2016/05/21/磨磨自动问答系统上线/</id>
    <published>2016-05-20T16:22:38.000Z</published>
    <updated>2016-05-20T16:49:07.273Z</updated>
    <content type="html"><![CDATA[<p>新上线磨磨自动问答系统，系统也是在自我学习当中，欢迎大家踊跃提问，生命不息折腾不止</p>
<p>本问答系统支持自动回答，有时会在线回答</p>
<p>测试阶段，欢迎吐槽</p>
<a id="more"></a>
<p>使用方法：</p>
<blockquote>
<p>戳图片下面的链接</p>
</blockquote>
<p><img src="http://static.zybuluo.com/zphj1987/ki000574lo1su5fnvqtzsshq/dribbbb.gif" alt=""></p>
<p><a href="http://v3.faqrobot.org/robot/faqrobot.html?sysNum=146375952481212447" target="_blank" rel="external">磨磨自动问答系统</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>新上线磨磨自动问答系统，系统也是在自我学习当中，欢迎大家踊跃提问，生命不息折腾不止</p>
<p>本问答系统支持自动回答，有时会在线回答</p>
<p>测试阶段，欢迎吐槽</p>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph的jewel新支持的rbd-nbd]]></title>
    <link href="http://www.zphj1987.com/2016/05/19/ceph%E7%9A%84jewel%E6%96%B0%E6%94%AF%E6%8C%81%E7%9A%84rbd-nbd/"/>
    <id>http://www.zphj1987.com/2016/05/19/ceph的jewel新支持的rbd-nbd/</id>
    <published>2016-05-19T04:41:11.000Z</published>
    <updated>2016-05-20T12:56:25.438Z</updated>
    <content type="html"><![CDATA[<p>jewel版本新增加了一个驱动NBD，允许librbd实现一个内核级别的rbd</p>
<p>NBD相比较于kernel rbd：</p>
<ul>
<li>rbd-ko是根据内核主线走的，升级kernel</li>
<li>rbd需要升级到相应的内核，改动太大</li>
<li>rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd</li>
</ul>
<p>rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障<br><a id="more"></a></p>
<h3 id="怎么理解用户态和内核态？">怎么理解用户态和内核态？</h3><ul>
<li>librbd就是用户态，一般的kvm对接的就是librbd的</li>
<li>kernel rbd就是内核态，这个是一个内核模块，是内核直接与osd交互的，一般来说内核态的性能会优于用户态</li>
</ul>
<h2 id="下面来做下基本的操作：">下面来做下基本的操作：</h2><h3 id="1、创建一个image">1、创建一个image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd create testnbdrbd -s 10G</span></span><br></pre></td></tr></table></figure>
<h3 id="2、映射这个image">2、映射这个image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd map rbd/testnbdrbd</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure>
<h3 id="3、查询已经映射的nbd">3、查询已经映射的nbd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd list-mapped</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure>
<p>上面说了这么多，那么来点直观的认识,nbd带来的好处<br>查询下image的信息<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd info testnbdrbd</span></span><br><span class="line">rbd image <span class="string">'testnbdrbd'</span>:</span><br><span class="line">	size <span class="number">10240</span> MB <span class="keyword">in</span> <span class="number">2560</span> objects</span><br><span class="line">	order <span class="number">22</span> (<span class="number">4096</span> kB objects)</span><br><span class="line">	block_name_prefix: rbd_data.<span class="number">10</span>ad2ae8944a</span><br><span class="line">	format: <span class="number">2</span></span><br><span class="line">	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">	flags:</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>jewel版本默认开启了features: layering, exclusive-lock, object-map, fast-diff, deep-flatten这么多的属性，而这些属性是kernel-rbd还不支持的</p>
</blockquote>
<p>所以做rbd map的时候就会出现下面的问题:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment"># rbd map  testnbdrbd</span></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature <span class="built_in">set</span> mismatch. You can <span class="built_in">disable</span> features unsupported by the kernel with <span class="string">"rbd feature disable"</span>.</span><br><span class="line">In some cases useful info is found <span class="keyword">in</span> syslog - try <span class="string">"dmesg | tail"</span> or so.</span><br><span class="line">rbd: map failed: (<span class="number">6</span>) No such device or address</span><br></pre></td></tr></table></figure></p>
<p>如果非要用，就默认禁用掉这些属性，在配置文件增加</p>
<blockquote>
<p>rbd_default_features = 3</p>
</blockquote>
<p>那么现在开启属性还行想用块设备方式怎么用，就可以用nbd了<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@lab8106 ~]<span class="comment">#  rbd-nbd map rbd/testnbdrbd</span></span><br><span class="line">/dev/nbd0</span><br></pre></td></tr></table></figure></p>
<p>这样就可以用了。不用担心接口的问题了,因为只要librbd支持的属性，nbd就默认支持了</p>
<p>rbd几种常用的模式和新模式图：<br><img src="http://static.zybuluo.com/zphj1987/33s6u70sw4qhgqvfhvwe5d5b/nbd.png" alt="nbd.png-40.5kB"></p>
<h3 id="本篇ceph版本">本篇ceph版本</h3><blockquote>
<p>ceph version 10.2.1 (3a66dd4f30852819c1bdaa8ec23c795d4ad77269)</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>jewel版本新增加了一个驱动NBD，允许librbd实现一个内核级别的rbd</p>
<p>NBD相比较于kernel rbd：</p>
<ul>
<li>rbd-ko是根据内核主线走的，升级kernel</li>
<li>rbd需要升级到相应的内核，改动太大</li>
<li>rbd-ko的开发要慢于librbd，需要很多的时间才能追赶上librbd</li>
</ul>
<p>rbd-nbd是通过librbd这个用户空间通过nbd的内核模块实现了内核级别的驱动，稳定性和性能都有保障<br>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[支持jewel版本的calamari]]></title>
    <link href="http://www.zphj1987.com/2016/05/16/%E6%94%AF%E6%8C%81jewel%E7%89%88%E6%9C%AC%E7%9A%84calamari/"/>
    <id>http://www.zphj1987.com/2016/05/16/支持jewel版本的calamari/</id>
    <published>2016-05-15T16:48:46.000Z</published>
    <updated>2016-07-12T06:26:32.981Z</updated>
    <content type="html"><![CDATA[<p>之前测试了下，发现calamari不支持jewel版本的，是因为接口了有了一些变化，在提出这个问题后，作者给出了回答，说肯定会支持的，并且做了一点小的改动，就可以支持了，这个作者merge了到了github的一些分支当中，但是还没有merge到最新的1.4的分支合master分支当中，这个可能是因为1.4还在做一些功能的开发</p>
<p>我使用作者的修改好的分支打好了包，直接可以使用，测试了ubuntu14.04 和centos 7的版本都可以使用，下面是百度云的链接，欢迎使用和测试</p>
<a id="more"></a>
<h3 id="ubuntu版本下载地址：">ubuntu版本下载地址：</h3><p><a href="http://pan.baidu.com/s/1i44Am25" target="_blank" rel="external">ubuntu-jewel-calamari</a><br>密码：w2dt</p>
<h3 id="centos7版本下载地址">centos7版本下载地址</h3><p><a href="http://pan.baidu.com/s/1nuMdbU5" target="_blank" rel="external">centos-jewel-calamari</a><br>密码：u9nl</p>
<h3 id="六、变更记录">六、变更记录</h3><table>
<thead>
<tr>
<th style="text-align:center">Why</th>
<th style="text-align:center">Who</th>
<th style="text-align:center">When</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">创建</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-05-16</td>
</tr>
<tr>
<td style="text-align:center">修改资源，解决无法获取iops和容量的bug</td>
<td style="text-align:center">武汉-运维-磨渣</td>
<td style="text-align:center">2016-07-12</td>
</tr>
</tbody>
</table>
<blockquote>
<p>如果您觉得本篇资源对您有用，欢迎通过下面的打赏通道进行打赏</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>之前测试了下，发现calamari不支持jewel版本的，是因为接口了有了一些变化，在提出这个问题后，作者给出了回答，说肯定会支持的，并且做了一点小的改动，就可以支持了，这个作者merge了到了github的一些分支当中，但是还没有merge到最新的1.4的分支合master分支当中，这个可能是因为1.4还在做一些功能的开发</p>
<p>我使用作者的修改好的分支打好了包，直接可以使用，测试了ubuntu14.04 和centos 7的版本都可以使用，下面是百度云的链接，欢迎使用和测试</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ceph卡在active+remapped状态]]></title>
    <link href="http://www.zphj1987.com/2016/05/14/ceph%E5%8D%A1%E5%9C%A8active-remapped%E7%8A%B6%E6%80%81/"/>
    <id>http://www.zphj1987.com/2016/05/14/ceph卡在active-remapped状态/</id>
    <published>2016-05-13T16:12:26.000Z</published>
    <updated>2016-05-13T16:15:13.519Z</updated>
    <content type="html"><![CDATA[<p>最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常</p>
<p>这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd</p>
<ul>
<li>一台机器上面的磁盘的容量不一样，有的3T，有的1T</li>
<li>两台主机上面的OSD个数不一样，有的5个，有的2个<a id="more"></a>
</li>
</ul>
<p>这样会造成主机的crush 的weight的差别很大的问题，以及分布算法上的不平衡问题，建议对于一个存储池来说，它所映射的osd至少需要是磁盘大小一致和个数一致的</p>
<p>这个问题我在我的环境下做了复现，确实有卡在remapped的问题</p>
<h3 id="出现这个情况一般是什么操作引起的？">出现这个情况一般是什么操作引起的？</h3><p>做osd的reweight的操作引起的，这个因为一般在做reweight的操作的时候，根据算法，这个上面的pg是会尽量分布在这个主机上的，而crush reweight不变的情况下，去修改osd 的reweight的时候，可能算法上会出现无法映射的问题</p>
<h3 id="怎么解决这个问题？">怎么解决这个问题？</h3><p>直接做osd crush reweigh的调整即可避免这个问题，这个straw算法里面还是有点小问题的，在调整某个因子的时候会引起整个因子的变动</p>
<blockquote>
<p>之前看到过sage在回复这种remapped问题的时候，都是不把这个归到bug里面去的，这个我也认为是配置问题引起的极端的问题，正常情况下都能避免的</p>
</blockquote>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>最近看到了有人的环境出现了出现了卡在active+remapped状态，并且卡住不动的状态，从pg的状态去看，这个pg值分配了主的pg，没有分配到副本的osd，集群的其他设置一切正常</p>
<p>这个从网上搜寻到的资料来看，大多数都是由于不均衡的主机osd引起的，所谓不平衡的osd</p>
<ul>
<li>一台机器上面的磁盘的容量不一样，有的3T，有的1T</li>
<li>两台主机上面的OSD个数不一样，有的5个，有的2个]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[如何开启内网的外网访问]]></title>
    <link href="http://www.zphj1987.com/2016/05/09/%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E5%86%85%E7%BD%91%E7%9A%84%E5%A4%96%E7%BD%91%E8%AE%BF%E9%97%AE/"/>
    <id>http://www.zphj1987.com/2016/05/09/如何开启内网的外网访问/</id>
    <published>2016-05-09T14:27:28.000Z</published>
    <updated>2016-05-09T14:52:45.469Z</updated>
    <content type="html"><![CDATA[<h3 id="为什么不要用QQ远程">为什么不要用QQ远程</h3><p>很多人在涉及到需要ssh远程的时候，第一印象应该就是QQ远程，这个是一般的普通用户都知道的方式，这个方式我为什么不推荐呢</p>
<ul>
<li>QQ是图形界面的远程，远程的操作在网络不好的时候很容易卡顿</li>
<li>窗口大小无法自适应</li>
<li>无法传递一些按键，比如需要Esc退出vim，变成了退出远程窗口的最大化</li>
<li>QQ远程适合帮助家里人处理一些界面上或者操作上的小问题</li>
</ul>
<a id="more"></a>
<h3 id="为什么使用Teamviver">为什么使用Teamviver</h3><p>那么还有什么替代么，相信有几年上班经验的人或者有过远程运维经验的人，都知道Teamviver，这个软件的出现，减少了多少的路途奔波，为什么推荐这个</p>
<ul>
<li>远程画面自适应</li>
<li>方便传递文件</li>
<li>方便开启守护模式，随时可以连接，无需对方有人</li>
<li>画面延时非常小</li>
</ul>
<p><a href="http://www.teamviewer.com/zhCN/" target="_blank" rel="external">Teamviver</a>下载链接<br>界面如下：<br><img src="http://static.zybuluo.com/zphj1987/f2k1n6th65unbijxpfj5dyvx/teamviver.png" alt="Teamviver"><br>适合远程的windows机器上正好有需要调试的应用的情况</p>
<h3 id="还有谁？">还有谁？</h3><p>其实对于这种远程的软件，需要的是连接数，需要的流量并不大，国内在这一刻唯一有所作为的也就是花生壳了，在提供免费的同时也提供更高级的企业级别服务，当然一般情况下免费的也能满足我们的需求</p>
<h4 id="长啥样？">长啥样？</h4><p><img src="http://static.zybuluo.com/zphj1987/tj157zs3zyiek4i697btrnvr/haushengke.png" alt="花生壳"><br>这个是最新的3.0版本的，安装包非常小，操作非常简单，比起以前刚开始用的时候优化了太多<br>下载地址：<a href="http://www.oray.com/activity/160302/?ici=act_160302&amp;icn=hsk_home-banner" target="_blank" rel="external">花生壳</a><br>点击内网映射就可以开启内网的端口映射了，根据注册的账号提供的免费域名和一个随机端口，就可以ssh到内网的指定的机器了，非常方便远程linux服务器的调试</p>
<p>上面介绍了3种远程的软件，一般情况下，需要操作界面的就推荐使用teamviver,需要调试服务器的就推荐使用花生壳做端口映射，或者有固定的IP的时候，也可以直接做端口映射，这个看具体的需求了</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>



]]></content>
    <summary type="html">
    <![CDATA[<h3 id="为什么不要用QQ远程">为什么不要用QQ远程</h3><p>很多人在涉及到需要ssh远程的时候，第一印象应该就是QQ远程，这个是一般的普通用户都知道的方式，这个方式我为什么不推荐呢</p>
<ul>
<li>QQ是图形界面的远程，远程的操作在网络不好的时候很容易卡顿</li>
<li>窗口大小无法自适应</li>
<li>无法传递一些按键，比如需要Esc退出vim，变成了退出远程窗口的最大化</li>
<li>QQ远程适合帮助家里人处理一些界面上或者操作上的小问题</li>
</ul>]]>
    
    </summary>
    
      <category term="linux" scheme="http://www.zphj1987.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[calamari不支持ceph的jewel版本]]></title>
    <link href="http://www.zphj1987.com/2016/05/07/calamari%E4%B8%8D%E6%94%AF%E6%8C%81ceph%E7%9A%84jewel%E7%89%88%E6%9C%AC/"/>
    <id>http://www.zphj1987.com/2016/05/07/calamari不支持ceph的jewel版本/</id>
    <published>2016-05-07T09:51:07.000Z</published>
    <updated>2016-05-07T10:03:52.278Z</updated>
    <content type="html"><![CDATA[<p>之前已经在0.94.x系列下进行了calamari的相关的打包和测试，基本没有太多问题，最近看到ceph发布了jewel的版本，想尝试下是否兼容，测试的结果是不兼容的</p>
<p>从调试的信息来看，ceph在jewel版本下对输出做了很大的改动，其中第一条，配置完成后，面板都无法显示，这个里面是因为ceph把原来的mdsmap这个字段在输出的时候改成了fsmap，造成里面的接口识别不到，这个改动以后，又发现restapi接口里面的osd地方又报错了</p>
<a id="more"></a>
<p>这个是管理接口的一些错误，系统信息的收集的地方diamond的接口也有改变</p>
<p>所以在目前环境下，要么自己对calamari做大量的改动去兼容新版本，要么就是等待发布以后的新版本，从版本的发布频率来看，红帽并不重视calamari这一块了，似乎是为后面出的统一平台做准备，之前calamari是作为红帽官方版本发布出去的</p>
<p>calmari在hammer版本下的使用还是不错了</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前已经在0.94.x系列下进行了calamari的相关的打包和测试，基本没有太多问题，最近看到ceph发布了jewel的版本，想尝试下是否兼容，测试的结果是不兼容的</p>
<p>从调试的信息来看，ceph在jewel版本下对输出做了很大的改动，其中第一条，配置完成后，面板都无法显示，这个里面是因为ceph把原来的mdsmap这个字段在输出的时候改成了fsmap，造成里面的接口识别不到，这个改动以后，又发现restapi接口里面的osd地方又报错了</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[性能数据的可视化]]></title>
    <link href="http://www.zphj1987.com/2016/04/27/%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    <id>http://www.zphj1987.com/2016/04/27/性能数据的可视化/</id>
    <published>2016-04-27T15:25:03.000Z</published>
    <updated>2016-04-27T16:04:32.585Z</updated>
    <content type="html"><![CDATA[<p>在工作当中，很多时候我们在去分析一个性能的时候，会产生大量的数据，面对数据的时候我们一般应该会有以下几个处理过程</p>
<ul>
<li>直接肉眼看<br>这个属于第一个级别，比如监控系统负载的时候去用top观察，这个方法是我最开始经常使用的一种方法，这个适合异常的时候使用，但是实际上获取的数据是有偏差的</li>
<li>有监控系统<br>使用数据监控系统对需要监控的数据进行监控，这个前提是有一个监控系统，并且方便的去增加数据，可以根据需求去设定数据，这个监控系统有很多，能可视化的也很多，这篇文章就不做介绍</li>
<li>使用监控脚本采集数据，采用excel进行可视化<br>使用脚本收集大量的数据，然后将数据导入到excel当中，然后显示出来，这个是我们公司测试人员采用的方法，也是比较容易实现的一个方式</li>
</ul>
<a id="more"></a>
<p>也可能还有其他的方法，总之一图胜千言，通过图形来展示数据，会获取到更多的信息，我也一直在寻找一些方案来方便的展示数据，从目前的监控系统来看，一般的实现方法都是</p>
<ul>
<li>数据采集到数据库</li>
<li>使用图形展示数据库中间的问题</li>
</ul>
<p>我现在需要的一个功能就是，使用一个采集工具将数据收集起来，然后直接将数据输出为图片，这个图片的渲染是可以根据我的需要进行定制的，最近在研究web自动化测试的发现可以有办法对html进行渲染成图片，想到这个地方可以跟这个地方进行结合</p>
<p>这里的思路是使用html+highchart方式进行数据的渲染，然后将页面导出成图片，最终做成一个简单的数据展示工具，并且为其他地方的提供数据图片</p>
<p>下面是这个工具渲染的图片效果：</p>
<p><img src="http://static.zybuluo.com/zphj1987/ixyicn1b5ps5m3x380nsbpz1/chart.png" alt=""></p>
<p>可以根据自己的需要去显示数据，后续会记录方法，这里暂时只记录一个思路</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在工作当中，很多时候我们在去分析一个性能的时候，会产生大量的数据，面对数据的时候我们一般应该会有以下几个处理过程</p>
<ul>
<li>直接肉眼看<br>这个属于第一个级别，比如监控系统负载的时候去用top观察，这个方法是我最开始经常使用的一种方法，这个适合异常的时候使用，但是实际上获取的数据是有偏差的</li>
<li>有监控系统<br>使用数据监控系统对需要监控的数据进行监控，这个前提是有一个监控系统，并且方便的去增加数据，可以根据需求去设定数据，这个监控系统有很多，能可视化的也很多，这篇文章就不做介绍</li>
<li>使用监控脚本采集数据，采用excel进行可视化<br>使用脚本收集大量的数据，然后将数据导入到excel当中，然后显示出来，这个是我们公司测试人员采用的方法，也是比较容易实现的一个方式</li>
</ul>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[mon的稳定性问题]]></title>
    <link href="http://www.zphj1987.com/2016/04/25/mon%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/"/>
    <id>http://www.zphj1987.com/2016/04/25/mon的稳定性问题/</id>
    <published>2016-04-25T15:36:28.000Z</published>
    <updated>2016-04-25T15:51:59.949Z</updated>
    <content type="html"><![CDATA[<h3 id="MON的稳定性问题：">MON的稳定性问题：</h3><ul>
<li>mon的选举风暴影响客户端IO</li>
<li>LevelDB的暴涨</li>
<li>频繁的客户端请求的DDOS</li>
</ul>
<h4 id="mon选举风暴：">mon选举风暴：</h4><p>monmap会因为mon之间或者mon与客户端之间网络的影响或者消息传递的异常发生变化,从而触发选举<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="LevelDB的暴涨：">LevelDB的暴涨：</h4><p>LevelDB的大小会涨到几十GB然后影响了osd的请求<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="频繁的客户端请求的DDOS：">频繁的客户端请求的DDOS：</h4><p>mon的响应因为levelDB变慢或者选举风暴，都会造成客户端发出大量的消息流<br>让客户端操作失效，包括卷创建，rbd的连接</p>
<a id="more"></a>
<h3 id="解决办法：">解决办法：</h3><h4 id="LevelDB的暴涨的问题解决办法">LevelDB的暴涨的问题解决办法</h4><p>升级ceph的版本，这个在0.94.6版本解决了这个问题<br><img src="http://static.zybuluo.com/zphj1987/yjdvldb3xyxl84bwlc3cjaxw/leveldb.png" alt="leveldb.png-21.9kB"></p>
<h4 id="选举风暴问题解决办法">选举风暴问题解决办法</h4><blockquote>
<p>[mon]<br>mon_lease = 20 (default = 5)<br>mon_lease_renew_interval = 12 (default 3)<br>mon_lease_ack_timeout = 40 (default 10)<br>mon_accept_timeout = 40 (default 10)<br>[client]<br>mon_client_hunt_interval = 40 (defaiult 3)</p>
</blockquote>
<p>将mon的数据放置在ssd上，因为LevelDB存储了集群的 metadata,包括 osdmap, pgmap, monmap,clientID, authID etc 等等，很大的leveldb会有更长的查询时间，更长的IO等待，然后就是更慢的客户端请求</p>
<p>这个地方是增长了mon之间判断需要切换的时间，降低客户端的请求的频率，使用ssd加快查询的速度</p>
<hr>
<p>这个问题是一个不太容易发觉的问题，有时候就是ceph -s反应的很慢，但是很多时候可能体现的就是客户端出现请求缓慢，然后还找不到原因，所以说硬件的隔离是非常有必要的，不要为了省成本然后影响了整个环境的稳定性和性能，对于很大的环境mon需要用三台独立的机器，这个机器需要一定的内存和cpu，磁盘使用ssd，1U服务器就可以了，上面可以运行一些其他类似管理平台，或者一些监控服务什么的，混合在osd的机器上的时候，一旦OSD出现大量的数据迁移的时候，或者大量的请求的时候，会阻塞了消息，这个就是做方案的时候需要考虑的问题，当然在性能要求不那么高的时候将mon混合在osd上使用也是可以的，这个时候尽量有更多的内存和更好的磁盘性能也能减少一些负担</p>
<h3 id="打赏通道">打赏通道</h3><center><br><img src="http://static.zybuluo.com/zphj1987/fi2557g55ucsdpyblopyfudl/newpay.png" alt="打赏" title="打赏"><br></center>

]]></content>
    <summary type="html">
    <![CDATA[<h3 id="MON的稳定性问题：">MON的稳定性问题：</h3><ul>
<li>mon的选举风暴影响客户端IO</li>
<li>LevelDB的暴涨</li>
<li>频繁的客户端请求的DDOS</li>
</ul>
<h4 id="mon选举风暴：">mon选举风暴：</h4><p>monmap会因为mon之间或者mon与客户端之间网络的影响或者消息传递的异常发生变化,从而触发选举<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="LevelDB的暴涨：">LevelDB的暴涨：</h4><p>LevelDB的大小会涨到几十GB然后影响了osd的请求<br>会造成客户端的请求变慢或者锁住</p>
<h4 id="频繁的客户端请求的DDOS：">频繁的客户端请求的DDOS：</h4><p>mon的响应因为levelDB变慢或者选举风暴，都会造成客户端发出大量的消息流<br>让客户端操作失效，包括卷创建，rbd的连接</p>]]>
    
    </summary>
    
      <category term="ceph" scheme="http://www.zphj1987.com/tags/ceph/"/>
    
  </entry>
  
</feed>
